<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />

<title>IS4242 Group Project</title>

<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>



<style type="text/css">
    /*!
*
* Twitter Bootstrap
*
*/
/*!
 * Bootstrap v3.3.7 (http://getbootstrap.com)
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 */
/*! normalize.css v3.0.3 | MIT License | github.com/necolas/normalize.css */
html {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
}
body {
  margin: 0;
}
article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
menu,
nav,
section,
summary {
  display: block;
}
audio,
canvas,
progress,
video {
  display: inline-block;
  vertical-align: baseline;
}
audio:not([controls]) {
  display: none;
  height: 0;
}
[hidden],
template {
  display: none;
}
a {
  background-color: transparent;
}
a:active,
a:hover {
  outline: 0;
}
abbr[title] {
  border-bottom: 1px dotted;
}
b,
strong {
  font-weight: bold;
}
dfn {
  font-style: italic;
}
h1 {
  font-size: 2em;
  margin: 0.67em 0;
}
mark {
  background: #ff0;
  color: #000;
}
small {
  font-size: 80%;
}
sub,
sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
sup {
  top: -0.5em;
}
sub {
  bottom: -0.25em;
}
img {
  border: 0;
}
svg:not(:root) {
  overflow: hidden;
}
figure {
  margin: 1em 40px;
}
hr {
  box-sizing: content-box;
  height: 0;
}
pre {
  overflow: auto;
}
code,
kbd,
pre,
samp {
  font-family: monospace, monospace;
  font-size: 1em;
}
button,
input,
optgroup,
select,
textarea {
  color: inherit;
  font: inherit;
  margin: 0;
}
button {
  overflow: visible;
}
button,
select {
  text-transform: none;
}
button,
html input[type="button"],
input[type="reset"],
input[type="submit"] {
  -webkit-appearance: button;
  cursor: pointer;
}
button[disabled],
html input[disabled] {
  cursor: default;
}
button::-moz-focus-inner,
input::-moz-focus-inner {
  border: 0;
  padding: 0;
}
input {
  line-height: normal;
}
input[type="checkbox"],
input[type="radio"] {
  box-sizing: border-box;
  padding: 0;
}
input[type="number"]::-webkit-inner-spin-button,
input[type="number"]::-webkit-outer-spin-button {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: textfield;
  box-sizing: content-box;
}
input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}
fieldset {
  border: 1px solid #c0c0c0;
  margin: 0 2px;
  padding: 0.35em 0.625em 0.75em;
}
legend {
  border: 0;
  padding: 0;
}
textarea {
  overflow: auto;
}
optgroup {
  font-weight: bold;
}
table {
  border-collapse: collapse;
  border-spacing: 0;
}
td,
th {
  padding: 0;
}
/*! Source: https://github.com/h5bp/html5-boilerplate/blob/master/src/css/main.css */
@media print {
  *,
  *:before,
  *:after {
    background: transparent !important;
    box-shadow: none !important;
    text-shadow: none !important;
  }
  a,
  a:visited {
    text-decoration: underline;
  }
  a[href]:after {
    content: " (" attr(href) ")";
  }
  abbr[title]:after {
    content: " (" attr(title) ")";
  }
  a[href^="#"]:after,
  a[href^="javascript:"]:after {
    content: "";
  }
  pre,
  blockquote {
    border: 1px solid #999;
    page-break-inside: avoid;
  }
  thead {
    display: table-header-group;
  }
  tr,
  img {
    page-break-inside: avoid;
  }
  img {
    max-width: 100% !important;
  }
  p,
  h2,
  h3 {
    orphans: 3;
    widows: 3;
  }
  h2,
  h3 {
    page-break-after: avoid;
  }
  .navbar {
    display: none;
  }
  .btn > .caret,
  .dropup > .btn > .caret {
    border-top-color: #000 !important;
  }
  .label {
    border: 1px solid #000;
  }
  .table {
    border-collapse: collapse !important;
  }
  .table td,
  .table th {
    background-color: #fff !important;
  }
  .table-bordered th,
  .table-bordered td {
    border: 1px solid #ddd !important;
  }
}
@font-face {
  font-family: 'Glyphicons Halflings';
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot');
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot?#iefix') format('embedded-opentype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff2') format('woff2'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff') format('woff'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.ttf') format('truetype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.svg#glyphicons_halflingsregular') format('svg');
}
.glyphicon {
  position: relative;
  top: 1px;
  display: inline-block;
  font-family: 'Glyphicons Halflings';
  font-style: normal;
  font-weight: normal;
  line-height: 1;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
.glyphicon-asterisk:before {
  content: "\002a";
}
.glyphicon-plus:before {
  content: "\002b";
}
.glyphicon-euro:before,
.glyphicon-eur:before {
  content: "\20ac";
}
.glyphicon-minus:before {
  content: "\2212";
}
.glyphicon-cloud:before {
  content: "\2601";
}
.glyphicon-envelope:before {
  content: "\2709";
}
.glyphicon-pencil:before {
  content: "\270f";
}
.glyphicon-glass:before {
  content: "\e001";
}
.glyphicon-music:before {
  content: "\e002";
}
.glyphicon-search:before {
  content: "\e003";
}
.glyphicon-heart:before {
  content: "\e005";
}
.glyphicon-star:before {
  content: "\e006";
}
.glyphicon-star-empty:before {
  content: "\e007";
}
.glyphicon-user:before {
  content: "\e008";
}
.glyphicon-film:before {
  content: "\e009";
}
.glyphicon-th-large:before {
  content: "\e010";
}
.glyphicon-th:before {
  content: "\e011";
}
.glyphicon-th-list:before {
  content: "\e012";
}
.glyphicon-ok:before {
  content: "\e013";
}
.glyphicon-remove:before {
  content: "\e014";
}
.glyphicon-zoom-in:before {
  content: "\e015";
}
.glyphicon-zoom-out:before {
  content: "\e016";
}
.glyphicon-off:before {
  content: "\e017";
}
.glyphicon-signal:before {
  content: "\e018";
}
.glyphicon-cog:before {
  content: "\e019";
}
.glyphicon-trash:before {
  content: "\e020";
}
.glyphicon-home:before {
  content: "\e021";
}
.glyphicon-file:before {
  content: "\e022";
}
.glyphicon-time:before {
  content: "\e023";
}
.glyphicon-road:before {
  content: "\e024";
}
.glyphicon-download-alt:before {
  content: "\e025";
}
.glyphicon-download:before {
  content: "\e026";
}
.glyphicon-upload:before {
  content: "\e027";
}
.glyphicon-inbox:before {
  content: "\e028";
}
.glyphicon-play-circle:before {
  content: "\e029";
}
.glyphicon-repeat:before {
  content: "\e030";
}
.glyphicon-refresh:before {
  content: "\e031";
}
.glyphicon-list-alt:before {
  content: "\e032";
}
.glyphicon-lock:before {
  content: "\e033";
}
.glyphicon-flag:before {
  content: "\e034";
}
.glyphicon-headphones:before {
  content: "\e035";
}
.glyphicon-volume-off:before {
  content: "\e036";
}
.glyphicon-volume-down:before {
  content: "\e037";
}
.glyphicon-volume-up:before {
  content: "\e038";
}
.glyphicon-qrcode:before {
  content: "\e039";
}
.glyphicon-barcode:before {
  content: "\e040";
}
.glyphicon-tag:before {
  content: "\e041";
}
.glyphicon-tags:before {
  content: "\e042";
}
.glyphicon-book:before {
  content: "\e043";
}
.glyphicon-bookmark:before {
  content: "\e044";
}
.glyphicon-print:before {
  content: "\e045";
}
.glyphicon-camera:before {
  content: "\e046";
}
.glyphicon-font:before {
  content: "\e047";
}
.glyphicon-bold:before {
  content: "\e048";
}
.glyphicon-italic:before {
  content: "\e049";
}
.glyphicon-text-height:before {
  content: "\e050";
}
.glyphicon-text-width:before {
  content: "\e051";
}
.glyphicon-align-left:before {
  content: "\e052";
}
.glyphicon-align-center:before {
  content: "\e053";
}
.glyphicon-align-right:before {
  content: "\e054";
}
.glyphicon-align-justify:before {
  content: "\e055";
}
.glyphicon-list:before {
  content: "\e056";
}
.glyphicon-indent-left:before {
  content: "\e057";
}
.glyphicon-indent-right:before {
  content: "\e058";
}
.glyphicon-facetime-video:before {
  content: "\e059";
}
.glyphicon-picture:before {
  content: "\e060";
}
.glyphicon-map-marker:before {
  content: "\e062";
}
.glyphicon-adjust:before {
  content: "\e063";
}
.glyphicon-tint:before {
  content: "\e064";
}
.glyphicon-edit:before {
  content: "\e065";
}
.glyphicon-share:before {
  content: "\e066";
}
.glyphicon-check:before {
  content: "\e067";
}
.glyphicon-move:before {
  content: "\e068";
}
.glyphicon-step-backward:before {
  content: "\e069";
}
.glyphicon-fast-backward:before {
  content: "\e070";
}
.glyphicon-backward:before {
  content: "\e071";
}
.glyphicon-play:before {
  content: "\e072";
}
.glyphicon-pause:before {
  content: "\e073";
}
.glyphicon-stop:before {
  content: "\e074";
}
.glyphicon-forward:before {
  content: "\e075";
}
.glyphicon-fast-forward:before {
  content: "\e076";
}
.glyphicon-step-forward:before {
  content: "\e077";
}
.glyphicon-eject:before {
  content: "\e078";
}
.glyphicon-chevron-left:before {
  content: "\e079";
}
.glyphicon-chevron-right:before {
  content: "\e080";
}
.glyphicon-plus-sign:before {
  content: "\e081";
}
.glyphicon-minus-sign:before {
  content: "\e082";
}
.glyphicon-remove-sign:before {
  content: "\e083";
}
.glyphicon-ok-sign:before {
  content: "\e084";
}
.glyphicon-question-sign:before {
  content: "\e085";
}
.glyphicon-info-sign:before {
  content: "\e086";
}
.glyphicon-screenshot:before {
  content: "\e087";
}
.glyphicon-remove-circle:before {
  content: "\e088";
}
.glyphicon-ok-circle:before {
  content: "\e089";
}
.glyphicon-ban-circle:before {
  content: "\e090";
}
.glyphicon-arrow-left:before {
  content: "\e091";
}
.glyphicon-arrow-right:before {
  content: "\e092";
}
.glyphicon-arrow-up:before {
  content: "\e093";
}
.glyphicon-arrow-down:before {
  content: "\e094";
}
.glyphicon-share-alt:before {
  content: "\e095";
}
.glyphicon-resize-full:before {
  content: "\e096";
}
.glyphicon-resize-small:before {
  content: "\e097";
}
.glyphicon-exclamation-sign:before {
  content: "\e101";
}
.glyphicon-gift:before {
  content: "\e102";
}
.glyphicon-leaf:before {
  content: "\e103";
}
.glyphicon-fire:before {
  content: "\e104";
}
.glyphicon-eye-open:before {
  content: "\e105";
}
.glyphicon-eye-close:before {
  content: "\e106";
}
.glyphicon-warning-sign:before {
  content: "\e107";
}
.glyphicon-plane:before {
  content: "\e108";
}
.glyphicon-calendar:before {
  content: "\e109";
}
.glyphicon-random:before {
  content: "\e110";
}
.glyphicon-comment:before {
  content: "\e111";
}
.glyphicon-magnet:before {
  content: "\e112";
}
.glyphicon-chevron-up:before {
  content: "\e113";
}
.glyphicon-chevron-down:before {
  content: "\e114";
}
.glyphicon-retweet:before {
  content: "\e115";
}
.glyphicon-shopping-cart:before {
  content: "\e116";
}
.glyphicon-folder-close:before {
  content: "\e117";
}
.glyphicon-folder-open:before {
  content: "\e118";
}
.glyphicon-resize-vertical:before {
  content: "\e119";
}
.glyphicon-resize-horizontal:before {
  content: "\e120";
}
.glyphicon-hdd:before {
  content: "\e121";
}
.glyphicon-bullhorn:before {
  content: "\e122";
}
.glyphicon-bell:before {
  content: "\e123";
}
.glyphicon-certificate:before {
  content: "\e124";
}
.glyphicon-thumbs-up:before {
  content: "\e125";
}
.glyphicon-thumbs-down:before {
  content: "\e126";
}
.glyphicon-hand-right:before {
  content: "\e127";
}
.glyphicon-hand-left:before {
  content: "\e128";
}
.glyphicon-hand-up:before {
  content: "\e129";
}
.glyphicon-hand-down:before {
  content: "\e130";
}
.glyphicon-circle-arrow-right:before {
  content: "\e131";
}
.glyphicon-circle-arrow-left:before {
  content: "\e132";
}
.glyphicon-circle-arrow-up:before {
  content: "\e133";
}
.glyphicon-circle-arrow-down:before {
  content: "\e134";
}
.glyphicon-globe:before {
  content: "\e135";
}
.glyphicon-wrench:before {
  content: "\e136";
}
.glyphicon-tasks:before {
  content: "\e137";
}
.glyphicon-filter:before {
  content: "\e138";
}
.glyphicon-briefcase:before {
  content: "\e139";
}
.glyphicon-fullscreen:before {
  content: "\e140";
}
.glyphicon-dashboard:before {
  content: "\e141";
}
.glyphicon-paperclip:before {
  content: "\e142";
}
.glyphicon-heart-empty:before {
  content: "\e143";
}
.glyphicon-link:before {
  content: "\e144";
}
.glyphicon-phone:before {
  content: "\e145";
}
.glyphicon-pushpin:before {
  content: "\e146";
}
.glyphicon-usd:before {
  content: "\e148";
}
.glyphicon-gbp:before {
  content: "\e149";
}
.glyphicon-sort:before {
  content: "\e150";
}
.glyphicon-sort-by-alphabet:before {
  content: "\e151";
}
.glyphicon-sort-by-alphabet-alt:before {
  content: "\e152";
}
.glyphicon-sort-by-order:before {
  content: "\e153";
}
.glyphicon-sort-by-order-alt:before {
  content: "\e154";
}
.glyphicon-sort-by-attributes:before {
  content: "\e155";
}
.glyphicon-sort-by-attributes-alt:before {
  content: "\e156";
}
.glyphicon-unchecked:before {
  content: "\e157";
}
.glyphicon-expand:before {
  content: "\e158";
}
.glyphicon-collapse-down:before {
  content: "\e159";
}
.glyphicon-collapse-up:before {
  content: "\e160";
}
.glyphicon-log-in:before {
  content: "\e161";
}
.glyphicon-flash:before {
  content: "\e162";
}
.glyphicon-log-out:before {
  content: "\e163";
}
.glyphicon-new-window:before {
  content: "\e164";
}
.glyphicon-record:before {
  content: "\e165";
}
.glyphicon-save:before {
  content: "\e166";
}
.glyphicon-open:before {
  content: "\e167";
}
.glyphicon-saved:before {
  content: "\e168";
}
.glyphicon-import:before {
  content: "\e169";
}
.glyphicon-export:before {
  content: "\e170";
}
.glyphicon-send:before {
  content: "\e171";
}
.glyphicon-floppy-disk:before {
  content: "\e172";
}
.glyphicon-floppy-saved:before {
  content: "\e173";
}
.glyphicon-floppy-remove:before {
  content: "\e174";
}
.glyphicon-floppy-save:before {
  content: "\e175";
}
.glyphicon-floppy-open:before {
  content: "\e176";
}
.glyphicon-credit-card:before {
  content: "\e177";
}
.glyphicon-transfer:before {
  content: "\e178";
}
.glyphicon-cutlery:before {
  content: "\e179";
}
.glyphicon-header:before {
  content: "\e180";
}
.glyphicon-compressed:before {
  content: "\e181";
}
.glyphicon-earphone:before {
  content: "\e182";
}
.glyphicon-phone-alt:before {
  content: "\e183";
}
.glyphicon-tower:before {
  content: "\e184";
}
.glyphicon-stats:before {
  content: "\e185";
}
.glyphicon-sd-video:before {
  content: "\e186";
}
.glyphicon-hd-video:before {
  content: "\e187";
}
.glyphicon-subtitles:before {
  content: "\e188";
}
.glyphicon-sound-stereo:before {
  content: "\e189";
}
.glyphicon-sound-dolby:before {
  content: "\e190";
}
.glyphicon-sound-5-1:before {
  content: "\e191";
}
.glyphicon-sound-6-1:before {
  content: "\e192";
}
.glyphicon-sound-7-1:before {
  content: "\e193";
}
.glyphicon-copyright-mark:before {
  content: "\e194";
}
.glyphicon-registration-mark:before {
  content: "\e195";
}
.glyphicon-cloud-download:before {
  content: "\e197";
}
.glyphicon-cloud-upload:before {
  content: "\e198";
}
.glyphicon-tree-conifer:before {
  content: "\e199";
}
.glyphicon-tree-deciduous:before {
  content: "\e200";
}
.glyphicon-cd:before {
  content: "\e201";
}
.glyphicon-save-file:before {
  content: "\e202";
}
.glyphicon-open-file:before {
  content: "\e203";
}
.glyphicon-level-up:before {
  content: "\e204";
}
.glyphicon-copy:before {
  content: "\e205";
}
.glyphicon-paste:before {
  content: "\e206";
}
.glyphicon-alert:before {
  content: "\e209";
}
.glyphicon-equalizer:before {
  content: "\e210";
}
.glyphicon-king:before {
  content: "\e211";
}
.glyphicon-queen:before {
  content: "\e212";
}
.glyphicon-pawn:before {
  content: "\e213";
}
.glyphicon-bishop:before {
  content: "\e214";
}
.glyphicon-knight:before {
  content: "\e215";
}
.glyphicon-baby-formula:before {
  content: "\e216";
}
.glyphicon-tent:before {
  content: "\26fa";
}
.glyphicon-blackboard:before {
  content: "\e218";
}
.glyphicon-bed:before {
  content: "\e219";
}
.glyphicon-apple:before {
  content: "\f8ff";
}
.glyphicon-erase:before {
  content: "\e221";
}
.glyphicon-hourglass:before {
  content: "\231b";
}
.glyphicon-lamp:before {
  content: "\e223";
}
.glyphicon-duplicate:before {
  content: "\e224";
}
.glyphicon-piggy-bank:before {
  content: "\e225";
}
.glyphicon-scissors:before {
  content: "\e226";
}
.glyphicon-bitcoin:before {
  content: "\e227";
}
.glyphicon-btc:before {
  content: "\e227";
}
.glyphicon-xbt:before {
  content: "\e227";
}
.glyphicon-yen:before {
  content: "\00a5";
}
.glyphicon-jpy:before {
  content: "\00a5";
}
.glyphicon-ruble:before {
  content: "\20bd";
}
.glyphicon-rub:before {
  content: "\20bd";
}
.glyphicon-scale:before {
  content: "\e230";
}
.glyphicon-ice-lolly:before {
  content: "\e231";
}
.glyphicon-ice-lolly-tasted:before {
  content: "\e232";
}
.glyphicon-education:before {
  content: "\e233";
}
.glyphicon-option-horizontal:before {
  content: "\e234";
}
.glyphicon-option-vertical:before {
  content: "\e235";
}
.glyphicon-menu-hamburger:before {
  content: "\e236";
}
.glyphicon-modal-window:before {
  content: "\e237";
}
.glyphicon-oil:before {
  content: "\e238";
}
.glyphicon-grain:before {
  content: "\e239";
}
.glyphicon-sunglasses:before {
  content: "\e240";
}
.glyphicon-text-size:before {
  content: "\e241";
}
.glyphicon-text-color:before {
  content: "\e242";
}
.glyphicon-text-background:before {
  content: "\e243";
}
.glyphicon-object-align-top:before {
  content: "\e244";
}
.glyphicon-object-align-bottom:before {
  content: "\e245";
}
.glyphicon-object-align-horizontal:before {
  content: "\e246";
}
.glyphicon-object-align-left:before {
  content: "\e247";
}
.glyphicon-object-align-vertical:before {
  content: "\e248";
}
.glyphicon-object-align-right:before {
  content: "\e249";
}
.glyphicon-triangle-right:before {
  content: "\e250";
}
.glyphicon-triangle-left:before {
  content: "\e251";
}
.glyphicon-triangle-bottom:before {
  content: "\e252";
}
.glyphicon-triangle-top:before {
  content: "\e253";
}
.glyphicon-console:before {
  content: "\e254";
}
.glyphicon-superscript:before {
  content: "\e255";
}
.glyphicon-subscript:before {
  content: "\e256";
}
.glyphicon-menu-left:before {
  content: "\e257";
}
.glyphicon-menu-right:before {
  content: "\e258";
}
.glyphicon-menu-down:before {
  content: "\e259";
}
.glyphicon-menu-up:before {
  content: "\e260";
}
* {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
*:before,
*:after {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
html {
  font-size: 10px;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
}
body {
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-size: 13px;
  line-height: 1.42857143;
  color: #000;
  background-color: #fff;
}
input,
button,
select,
textarea {
  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
}
a {
  color: #337ab7;
  text-decoration: none;
}
a:hover,
a:focus {
  color: #23527c;
  text-decoration: underline;
}
a:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
figure {
  margin: 0;
}
img {
  vertical-align: middle;
}
.img-responsive,
.thumbnail > img,
.thumbnail a > img,
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  display: block;
  max-width: 100%;
  height: auto;
}
.img-rounded {
  border-radius: 3px;
}
.img-thumbnail {
  padding: 4px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: all 0.2s ease-in-out;
  -o-transition: all 0.2s ease-in-out;
  transition: all 0.2s ease-in-out;
  display: inline-block;
  max-width: 100%;
  height: auto;
}
.img-circle {
  border-radius: 50%;
}
hr {
  margin-top: 18px;
  margin-bottom: 18px;
  border: 0;
  border-top: 1px solid #eeeeee;
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  margin: -1px;
  padding: 0;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
[role="button"] {
  cursor: pointer;
}
h1,
h2,
h3,
h4,
h5,
h6,
.h1,
.h2,
.h3,
.h4,
.h5,
.h6 {
  font-family: inherit;
  font-weight: 500;
  line-height: 1.1;
  color: inherit;
}
h1 small,
h2 small,
h3 small,
h4 small,
h5 small,
h6 small,
.h1 small,
.h2 small,
.h3 small,
.h4 small,
.h5 small,
.h6 small,
h1 .small,
h2 .small,
h3 .small,
h4 .small,
h5 .small,
h6 .small,
.h1 .small,
.h2 .small,
.h3 .small,
.h4 .small,
.h5 .small,
.h6 .small {
  font-weight: normal;
  line-height: 1;
  color: #777777;
}
h1,
.h1,
h2,
.h2,
h3,
.h3 {
  margin-top: 18px;
  margin-bottom: 9px;
}
h1 small,
.h1 small,
h2 small,
.h2 small,
h3 small,
.h3 small,
h1 .small,
.h1 .small,
h2 .small,
.h2 .small,
h3 .small,
.h3 .small {
  font-size: 65%;
}
h4,
.h4,
h5,
.h5,
h6,
.h6 {
  margin-top: 9px;
  margin-bottom: 9px;
}
h4 small,
.h4 small,
h5 small,
.h5 small,
h6 small,
.h6 small,
h4 .small,
.h4 .small,
h5 .small,
.h5 .small,
h6 .small,
.h6 .small {
  font-size: 75%;
}
h1,
.h1 {
  font-size: 33px;
}
h2,
.h2 {
  font-size: 27px;
}
h3,
.h3 {
  font-size: 23px;
}
h4,
.h4 {
  font-size: 17px;
}
h5,
.h5 {
  font-size: 13px;
}
h6,
.h6 {
  font-size: 12px;
}
p {
  margin: 0 0 9px;
}
.lead {
  margin-bottom: 18px;
  font-size: 14px;
  font-weight: 300;
  line-height: 1.4;
}
@media (min-width: 768px) {
  .lead {
    font-size: 19.5px;
  }
}
small,
.small {
  font-size: 92%;
}
mark,
.mark {
  background-color: #fcf8e3;
  padding: .2em;
}
.text-left {
  text-align: left;
}
.text-right {
  text-align: right;
}
.text-center {
  text-align: center;
}
.text-justify {
  text-align: justify;
}
.text-nowrap {
  white-space: nowrap;
}
.text-lowercase {
  text-transform: lowercase;
}
.text-uppercase {
  text-transform: uppercase;
}
.text-capitalize {
  text-transform: capitalize;
}
.text-muted {
  color: #777777;
}
.text-primary {
  color: #337ab7;
}
a.text-primary:hover,
a.text-primary:focus {
  color: #286090;
}
.text-success {
  color: #3c763d;
}
a.text-success:hover,
a.text-success:focus {
  color: #2b542c;
}
.text-info {
  color: #31708f;
}
a.text-info:hover,
a.text-info:focus {
  color: #245269;
}
.text-warning {
  color: #8a6d3b;
}
a.text-warning:hover,
a.text-warning:focus {
  color: #66512c;
}
.text-danger {
  color: #a94442;
}
a.text-danger:hover,
a.text-danger:focus {
  color: #843534;
}
.bg-primary {
  color: #fff;
  background-color: #337ab7;
}
a.bg-primary:hover,
a.bg-primary:focus {
  background-color: #286090;
}
.bg-success {
  background-color: #dff0d8;
}
a.bg-success:hover,
a.bg-success:focus {
  background-color: #c1e2b3;
}
.bg-info {
  background-color: #d9edf7;
}
a.bg-info:hover,
a.bg-info:focus {
  background-color: #afd9ee;
}
.bg-warning {
  background-color: #fcf8e3;
}
a.bg-warning:hover,
a.bg-warning:focus {
  background-color: #f7ecb5;
}
.bg-danger {
  background-color: #f2dede;
}
a.bg-danger:hover,
a.bg-danger:focus {
  background-color: #e4b9b9;
}
.page-header {
  padding-bottom: 8px;
  margin: 36px 0 18px;
  border-bottom: 1px solid #eeeeee;
}
ul,
ol {
  margin-top: 0;
  margin-bottom: 9px;
}
ul ul,
ol ul,
ul ol,
ol ol {
  margin-bottom: 0;
}
.list-unstyled {
  padding-left: 0;
  list-style: none;
}
.list-inline {
  padding-left: 0;
  list-style: none;
  margin-left: -5px;
}
.list-inline > li {
  display: inline-block;
  padding-left: 5px;
  padding-right: 5px;
}
dl {
  margin-top: 0;
  margin-bottom: 18px;
}
dt,
dd {
  line-height: 1.42857143;
}
dt {
  font-weight: bold;
}
dd {
  margin-left: 0;
}
@media (min-width: 541px) {
  .dl-horizontal dt {
    float: left;
    width: 160px;
    clear: left;
    text-align: right;
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
  }
  .dl-horizontal dd {
    margin-left: 180px;
  }
}
abbr[title],
abbr[data-original-title] {
  cursor: help;
  border-bottom: 1px dotted #777777;
}
.initialism {
  font-size: 90%;
  text-transform: uppercase;
}
blockquote {
  padding: 9px 18px;
  margin: 0 0 18px;
  font-size: inherit;
  border-left: 5px solid #eeeeee;
}
blockquote p:last-child,
blockquote ul:last-child,
blockquote ol:last-child {
  margin-bottom: 0;
}
blockquote footer,
blockquote small,
blockquote .small {
  display: block;
  font-size: 80%;
  line-height: 1.42857143;
  color: #777777;
}
blockquote footer:before,
blockquote small:before,
blockquote .small:before {
  content: '\2014 \00A0';
}
.blockquote-reverse,
blockquote.pull-right {
  padding-right: 15px;
  padding-left: 0;
  border-right: 5px solid #eeeeee;
  border-left: 0;
  text-align: right;
}
.blockquote-reverse footer:before,
blockquote.pull-right footer:before,
.blockquote-reverse small:before,
blockquote.pull-right small:before,
.blockquote-reverse .small:before,
blockquote.pull-right .small:before {
  content: '';
}
.blockquote-reverse footer:after,
blockquote.pull-right footer:after,
.blockquote-reverse small:after,
blockquote.pull-right small:after,
.blockquote-reverse .small:after,
blockquote.pull-right .small:after {
  content: '\00A0 \2014';
}
address {
  margin-bottom: 18px;
  font-style: normal;
  line-height: 1.42857143;
}
code,
kbd,
pre,
samp {
  font-family: monospace;
}
code {
  padding: 2px 4px;
  font-size: 90%;
  color: #c7254e;
  background-color: #f9f2f4;
  border-radius: 2px;
}
kbd {
  padding: 2px 4px;
  font-size: 90%;
  color: #888;
  background-color: transparent;
  border-radius: 1px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
}
kbd kbd {
  padding: 0;
  font-size: 100%;
  font-weight: bold;
  box-shadow: none;
}
pre {
  display: block;
  padding: 8.5px;
  margin: 0 0 9px;
  font-size: 12px;
  line-height: 1.42857143;
  word-break: break-all;
  word-wrap: break-word;
  color: #333333;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 2px;
}
pre code {
  padding: 0;
  font-size: inherit;
  color: inherit;
  white-space: pre-wrap;
  background-color: transparent;
  border-radius: 0;
}
.pre-scrollable {
  max-height: 340px;
  overflow-y: scroll;
}
.container {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
@media (min-width: 768px) {
  .container {
    width: 768px;
  }
}
@media (min-width: 992px) {
  .container {
    width: 940px;
  }
}
@media (min-width: 1200px) {
  .container {
    width: 1140px;
  }
}
.container-fluid {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
.row {
  margin-left: 0px;
  margin-right: 0px;
}
.col-xs-1, .col-sm-1, .col-md-1, .col-lg-1, .col-xs-2, .col-sm-2, .col-md-2, .col-lg-2, .col-xs-3, .col-sm-3, .col-md-3, .col-lg-3, .col-xs-4, .col-sm-4, .col-md-4, .col-lg-4, .col-xs-5, .col-sm-5, .col-md-5, .col-lg-5, .col-xs-6, .col-sm-6, .col-md-6, .col-lg-6, .col-xs-7, .col-sm-7, .col-md-7, .col-lg-7, .col-xs-8, .col-sm-8, .col-md-8, .col-lg-8, .col-xs-9, .col-sm-9, .col-md-9, .col-lg-9, .col-xs-10, .col-sm-10, .col-md-10, .col-lg-10, .col-xs-11, .col-sm-11, .col-md-11, .col-lg-11, .col-xs-12, .col-sm-12, .col-md-12, .col-lg-12 {
  position: relative;
  min-height: 1px;
  padding-left: 0px;
  padding-right: 0px;
}
.col-xs-1, .col-xs-2, .col-xs-3, .col-xs-4, .col-xs-5, .col-xs-6, .col-xs-7, .col-xs-8, .col-xs-9, .col-xs-10, .col-xs-11, .col-xs-12 {
  float: left;
}
.col-xs-12 {
  width: 100%;
}
.col-xs-11 {
  width: 91.66666667%;
}
.col-xs-10 {
  width: 83.33333333%;
}
.col-xs-9 {
  width: 75%;
}
.col-xs-8 {
  width: 66.66666667%;
}
.col-xs-7 {
  width: 58.33333333%;
}
.col-xs-6 {
  width: 50%;
}
.col-xs-5 {
  width: 41.66666667%;
}
.col-xs-4 {
  width: 33.33333333%;
}
.col-xs-3 {
  width: 25%;
}
.col-xs-2 {
  width: 16.66666667%;
}
.col-xs-1 {
  width: 8.33333333%;
}
.col-xs-pull-12 {
  right: 100%;
}
.col-xs-pull-11 {
  right: 91.66666667%;
}
.col-xs-pull-10 {
  right: 83.33333333%;
}
.col-xs-pull-9 {
  right: 75%;
}
.col-xs-pull-8 {
  right: 66.66666667%;
}
.col-xs-pull-7 {
  right: 58.33333333%;
}
.col-xs-pull-6 {
  right: 50%;
}
.col-xs-pull-5 {
  right: 41.66666667%;
}
.col-xs-pull-4 {
  right: 33.33333333%;
}
.col-xs-pull-3 {
  right: 25%;
}
.col-xs-pull-2 {
  right: 16.66666667%;
}
.col-xs-pull-1 {
  right: 8.33333333%;
}
.col-xs-pull-0 {
  right: auto;
}
.col-xs-push-12 {
  left: 100%;
}
.col-xs-push-11 {
  left: 91.66666667%;
}
.col-xs-push-10 {
  left: 83.33333333%;
}
.col-xs-push-9 {
  left: 75%;
}
.col-xs-push-8 {
  left: 66.66666667%;
}
.col-xs-push-7 {
  left: 58.33333333%;
}
.col-xs-push-6 {
  left: 50%;
}
.col-xs-push-5 {
  left: 41.66666667%;
}
.col-xs-push-4 {
  left: 33.33333333%;
}
.col-xs-push-3 {
  left: 25%;
}
.col-xs-push-2 {
  left: 16.66666667%;
}
.col-xs-push-1 {
  left: 8.33333333%;
}
.col-xs-push-0 {
  left: auto;
}
.col-xs-offset-12 {
  margin-left: 100%;
}
.col-xs-offset-11 {
  margin-left: 91.66666667%;
}
.col-xs-offset-10 {
  margin-left: 83.33333333%;
}
.col-xs-offset-9 {
  margin-left: 75%;
}
.col-xs-offset-8 {
  margin-left: 66.66666667%;
}
.col-xs-offset-7 {
  margin-left: 58.33333333%;
}
.col-xs-offset-6 {
  margin-left: 50%;
}
.col-xs-offset-5 {
  margin-left: 41.66666667%;
}
.col-xs-offset-4 {
  margin-left: 33.33333333%;
}
.col-xs-offset-3 {
  margin-left: 25%;
}
.col-xs-offset-2 {
  margin-left: 16.66666667%;
}
.col-xs-offset-1 {
  margin-left: 8.33333333%;
}
.col-xs-offset-0 {
  margin-left: 0%;
}
@media (min-width: 768px) {
  .col-sm-1, .col-sm-2, .col-sm-3, .col-sm-4, .col-sm-5, .col-sm-6, .col-sm-7, .col-sm-8, .col-sm-9, .col-sm-10, .col-sm-11, .col-sm-12 {
    float: left;
  }
  .col-sm-12 {
    width: 100%;
  }
  .col-sm-11 {
    width: 91.66666667%;
  }
  .col-sm-10 {
    width: 83.33333333%;
  }
  .col-sm-9 {
    width: 75%;
  }
  .col-sm-8 {
    width: 66.66666667%;
  }
  .col-sm-7 {
    width: 58.33333333%;
  }
  .col-sm-6 {
    width: 50%;
  }
  .col-sm-5 {
    width: 41.66666667%;
  }
  .col-sm-4 {
    width: 33.33333333%;
  }
  .col-sm-3 {
    width: 25%;
  }
  .col-sm-2 {
    width: 16.66666667%;
  }
  .col-sm-1 {
    width: 8.33333333%;
  }
  .col-sm-pull-12 {
    right: 100%;
  }
  .col-sm-pull-11 {
    right: 91.66666667%;
  }
  .col-sm-pull-10 {
    right: 83.33333333%;
  }
  .col-sm-pull-9 {
    right: 75%;
  }
  .col-sm-pull-8 {
    right: 66.66666667%;
  }
  .col-sm-pull-7 {
    right: 58.33333333%;
  }
  .col-sm-pull-6 {
    right: 50%;
  }
  .col-sm-pull-5 {
    right: 41.66666667%;
  }
  .col-sm-pull-4 {
    right: 33.33333333%;
  }
  .col-sm-pull-3 {
    right: 25%;
  }
  .col-sm-pull-2 {
    right: 16.66666667%;
  }
  .col-sm-pull-1 {
    right: 8.33333333%;
  }
  .col-sm-pull-0 {
    right: auto;
  }
  .col-sm-push-12 {
    left: 100%;
  }
  .col-sm-push-11 {
    left: 91.66666667%;
  }
  .col-sm-push-10 {
    left: 83.33333333%;
  }
  .col-sm-push-9 {
    left: 75%;
  }
  .col-sm-push-8 {
    left: 66.66666667%;
  }
  .col-sm-push-7 {
    left: 58.33333333%;
  }
  .col-sm-push-6 {
    left: 50%;
  }
  .col-sm-push-5 {
    left: 41.66666667%;
  }
  .col-sm-push-4 {
    left: 33.33333333%;
  }
  .col-sm-push-3 {
    left: 25%;
  }
  .col-sm-push-2 {
    left: 16.66666667%;
  }
  .col-sm-push-1 {
    left: 8.33333333%;
  }
  .col-sm-push-0 {
    left: auto;
  }
  .col-sm-offset-12 {
    margin-left: 100%;
  }
  .col-sm-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-sm-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-sm-offset-9 {
    margin-left: 75%;
  }
  .col-sm-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-sm-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-sm-offset-6 {
    margin-left: 50%;
  }
  .col-sm-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-sm-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-sm-offset-3 {
    margin-left: 25%;
  }
  .col-sm-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-sm-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-sm-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 992px) {
  .col-md-1, .col-md-2, .col-md-3, .col-md-4, .col-md-5, .col-md-6, .col-md-7, .col-md-8, .col-md-9, .col-md-10, .col-md-11, .col-md-12 {
    float: left;
  }
  .col-md-12 {
    width: 100%;
  }
  .col-md-11 {
    width: 91.66666667%;
  }
  .col-md-10 {
    width: 83.33333333%;
  }
  .col-md-9 {
    width: 75%;
  }
  .col-md-8 {
    width: 66.66666667%;
  }
  .col-md-7 {
    width: 58.33333333%;
  }
  .col-md-6 {
    width: 50%;
  }
  .col-md-5 {
    width: 41.66666667%;
  }
  .col-md-4 {
    width: 33.33333333%;
  }
  .col-md-3 {
    width: 25%;
  }
  .col-md-2 {
    width: 16.66666667%;
  }
  .col-md-1 {
    width: 8.33333333%;
  }
  .col-md-pull-12 {
    right: 100%;
  }
  .col-md-pull-11 {
    right: 91.66666667%;
  }
  .col-md-pull-10 {
    right: 83.33333333%;
  }
  .col-md-pull-9 {
    right: 75%;
  }
  .col-md-pull-8 {
    right: 66.66666667%;
  }
  .col-md-pull-7 {
    right: 58.33333333%;
  }
  .col-md-pull-6 {
    right: 50%;
  }
  .col-md-pull-5 {
    right: 41.66666667%;
  }
  .col-md-pull-4 {
    right: 33.33333333%;
  }
  .col-md-pull-3 {
    right: 25%;
  }
  .col-md-pull-2 {
    right: 16.66666667%;
  }
  .col-md-pull-1 {
    right: 8.33333333%;
  }
  .col-md-pull-0 {
    right: auto;
  }
  .col-md-push-12 {
    left: 100%;
  }
  .col-md-push-11 {
    left: 91.66666667%;
  }
  .col-md-push-10 {
    left: 83.33333333%;
  }
  .col-md-push-9 {
    left: 75%;
  }
  .col-md-push-8 {
    left: 66.66666667%;
  }
  .col-md-push-7 {
    left: 58.33333333%;
  }
  .col-md-push-6 {
    left: 50%;
  }
  .col-md-push-5 {
    left: 41.66666667%;
  }
  .col-md-push-4 {
    left: 33.33333333%;
  }
  .col-md-push-3 {
    left: 25%;
  }
  .col-md-push-2 {
    left: 16.66666667%;
  }
  .col-md-push-1 {
    left: 8.33333333%;
  }
  .col-md-push-0 {
    left: auto;
  }
  .col-md-offset-12 {
    margin-left: 100%;
  }
  .col-md-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-md-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-md-offset-9 {
    margin-left: 75%;
  }
  .col-md-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-md-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-md-offset-6 {
    margin-left: 50%;
  }
  .col-md-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-md-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-md-offset-3 {
    margin-left: 25%;
  }
  .col-md-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-md-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-md-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 1200px) {
  .col-lg-1, .col-lg-2, .col-lg-3, .col-lg-4, .col-lg-5, .col-lg-6, .col-lg-7, .col-lg-8, .col-lg-9, .col-lg-10, .col-lg-11, .col-lg-12 {
    float: left;
  }
  .col-lg-12 {
    width: 100%;
  }
  .col-lg-11 {
    width: 91.66666667%;
  }
  .col-lg-10 {
    width: 83.33333333%;
  }
  .col-lg-9 {
    width: 75%;
  }
  .col-lg-8 {
    width: 66.66666667%;
  }
  .col-lg-7 {
    width: 58.33333333%;
  }
  .col-lg-6 {
    width: 50%;
  }
  .col-lg-5 {
    width: 41.66666667%;
  }
  .col-lg-4 {
    width: 33.33333333%;
  }
  .col-lg-3 {
    width: 25%;
  }
  .col-lg-2 {
    width: 16.66666667%;
  }
  .col-lg-1 {
    width: 8.33333333%;
  }
  .col-lg-pull-12 {
    right: 100%;
  }
  .col-lg-pull-11 {
    right: 91.66666667%;
  }
  .col-lg-pull-10 {
    right: 83.33333333%;
  }
  .col-lg-pull-9 {
    right: 75%;
  }
  .col-lg-pull-8 {
    right: 66.66666667%;
  }
  .col-lg-pull-7 {
    right: 58.33333333%;
  }
  .col-lg-pull-6 {
    right: 50%;
  }
  .col-lg-pull-5 {
    right: 41.66666667%;
  }
  .col-lg-pull-4 {
    right: 33.33333333%;
  }
  .col-lg-pull-3 {
    right: 25%;
  }
  .col-lg-pull-2 {
    right: 16.66666667%;
  }
  .col-lg-pull-1 {
    right: 8.33333333%;
  }
  .col-lg-pull-0 {
    right: auto;
  }
  .col-lg-push-12 {
    left: 100%;
  }
  .col-lg-push-11 {
    left: 91.66666667%;
  }
  .col-lg-push-10 {
    left: 83.33333333%;
  }
  .col-lg-push-9 {
    left: 75%;
  }
  .col-lg-push-8 {
    left: 66.66666667%;
  }
  .col-lg-push-7 {
    left: 58.33333333%;
  }
  .col-lg-push-6 {
    left: 50%;
  }
  .col-lg-push-5 {
    left: 41.66666667%;
  }
  .col-lg-push-4 {
    left: 33.33333333%;
  }
  .col-lg-push-3 {
    left: 25%;
  }
  .col-lg-push-2 {
    left: 16.66666667%;
  }
  .col-lg-push-1 {
    left: 8.33333333%;
  }
  .col-lg-push-0 {
    left: auto;
  }
  .col-lg-offset-12 {
    margin-left: 100%;
  }
  .col-lg-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-lg-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-lg-offset-9 {
    margin-left: 75%;
  }
  .col-lg-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-lg-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-lg-offset-6 {
    margin-left: 50%;
  }
  .col-lg-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-lg-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-lg-offset-3 {
    margin-left: 25%;
  }
  .col-lg-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-lg-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-lg-offset-0 {
    margin-left: 0%;
  }
}
table {
  background-color: transparent;
}
caption {
  padding-top: 8px;
  padding-bottom: 8px;
  color: #777777;
  text-align: left;
}
th {
  text-align: left;
}
.table {
  width: 100%;
  max-width: 100%;
  margin-bottom: 18px;
}
.table > thead > tr > th,
.table > tbody > tr > th,
.table > tfoot > tr > th,
.table > thead > tr > td,
.table > tbody > tr > td,
.table > tfoot > tr > td {
  padding: 8px;
  line-height: 1.42857143;
  vertical-align: top;
  border-top: 1px solid #ddd;
}
.table > thead > tr > th {
  vertical-align: bottom;
  border-bottom: 2px solid #ddd;
}
.table > caption + thead > tr:first-child > th,
.table > colgroup + thead > tr:first-child > th,
.table > thead:first-child > tr:first-child > th,
.table > caption + thead > tr:first-child > td,
.table > colgroup + thead > tr:first-child > td,
.table > thead:first-child > tr:first-child > td {
  border-top: 0;
}
.table > tbody + tbody {
  border-top: 2px solid #ddd;
}
.table .table {
  background-color: #fff;
}
.table-condensed > thead > tr > th,
.table-condensed > tbody > tr > th,
.table-condensed > tfoot > tr > th,
.table-condensed > thead > tr > td,
.table-condensed > tbody > tr > td,
.table-condensed > tfoot > tr > td {
  padding: 5px;
}
.table-bordered {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > tbody > tr > th,
.table-bordered > tfoot > tr > th,
.table-bordered > thead > tr > td,
.table-bordered > tbody > tr > td,
.table-bordered > tfoot > tr > td {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > thead > tr > td {
  border-bottom-width: 2px;
}
.table-striped > tbody > tr:nth-of-type(odd) {
  background-color: #f9f9f9;
}
.table-hover > tbody > tr:hover {
  background-color: #f5f5f5;
}
table col[class*="col-"] {
  position: static;
  float: none;
  display: table-column;
}
table td[class*="col-"],
table th[class*="col-"] {
  position: static;
  float: none;
  display: table-cell;
}
.table > thead > tr > td.active,
.table > tbody > tr > td.active,
.table > tfoot > tr > td.active,
.table > thead > tr > th.active,
.table > tbody > tr > th.active,
.table > tfoot > tr > th.active,
.table > thead > tr.active > td,
.table > tbody > tr.active > td,
.table > tfoot > tr.active > td,
.table > thead > tr.active > th,
.table > tbody > tr.active > th,
.table > tfoot > tr.active > th {
  background-color: #f5f5f5;
}
.table-hover > tbody > tr > td.active:hover,
.table-hover > tbody > tr > th.active:hover,
.table-hover > tbody > tr.active:hover > td,
.table-hover > tbody > tr:hover > .active,
.table-hover > tbody > tr.active:hover > th {
  background-color: #e8e8e8;
}
.table > thead > tr > td.success,
.table > tbody > tr > td.success,
.table > tfoot > tr > td.success,
.table > thead > tr > th.success,
.table > tbody > tr > th.success,
.table > tfoot > tr > th.success,
.table > thead > tr.success > td,
.table > tbody > tr.success > td,
.table > tfoot > tr.success > td,
.table > thead > tr.success > th,
.table > tbody > tr.success > th,
.table > tfoot > tr.success > th {
  background-color: #dff0d8;
}
.table-hover > tbody > tr > td.success:hover,
.table-hover > tbody > tr > th.success:hover,
.table-hover > tbody > tr.success:hover > td,
.table-hover > tbody > tr:hover > .success,
.table-hover > tbody > tr.success:hover > th {
  background-color: #d0e9c6;
}
.table > thead > tr > td.info,
.table > tbody > tr > td.info,
.table > tfoot > tr > td.info,
.table > thead > tr > th.info,
.table > tbody > tr > th.info,
.table > tfoot > tr > th.info,
.table > thead > tr.info > td,
.table > tbody > tr.info > td,
.table > tfoot > tr.info > td,
.table > thead > tr.info > th,
.table > tbody > tr.info > th,
.table > tfoot > tr.info > th {
  background-color: #d9edf7;
}
.table-hover > tbody > tr > td.info:hover,
.table-hover > tbody > tr > th.info:hover,
.table-hover > tbody > tr.info:hover > td,
.table-hover > tbody > tr:hover > .info,
.table-hover > tbody > tr.info:hover > th {
  background-color: #c4e3f3;
}
.table > thead > tr > td.warning,
.table > tbody > tr > td.warning,
.table > tfoot > tr > td.warning,
.table > thead > tr > th.warning,
.table > tbody > tr > th.warning,
.table > tfoot > tr > th.warning,
.table > thead > tr.warning > td,
.table > tbody > tr.warning > td,
.table > tfoot > tr.warning > td,
.table > thead > tr.warning > th,
.table > tbody > tr.warning > th,
.table > tfoot > tr.warning > th {
  background-color: #fcf8e3;
}
.table-hover > tbody > tr > td.warning:hover,
.table-hover > tbody > tr > th.warning:hover,
.table-hover > tbody > tr.warning:hover > td,
.table-hover > tbody > tr:hover > .warning,
.table-hover > tbody > tr.warning:hover > th {
  background-color: #faf2cc;
}
.table > thead > tr > td.danger,
.table > tbody > tr > td.danger,
.table > tfoot > tr > td.danger,
.table > thead > tr > th.danger,
.table > tbody > tr > th.danger,
.table > tfoot > tr > th.danger,
.table > thead > tr.danger > td,
.table > tbody > tr.danger > td,
.table > tfoot > tr.danger > td,
.table > thead > tr.danger > th,
.table > tbody > tr.danger > th,
.table > tfoot > tr.danger > th {
  background-color: #f2dede;
}
.table-hover > tbody > tr > td.danger:hover,
.table-hover > tbody > tr > th.danger:hover,
.table-hover > tbody > tr.danger:hover > td,
.table-hover > tbody > tr:hover > .danger,
.table-hover > tbody > tr.danger:hover > th {
  background-color: #ebcccc;
}
.table-responsive {
  overflow-x: auto;
  min-height: 0.01%;
}
@media screen and (max-width: 767px) {
  .table-responsive {
    width: 100%;
    margin-bottom: 13.5px;
    overflow-y: hidden;
    -ms-overflow-style: -ms-autohiding-scrollbar;
    border: 1px solid #ddd;
  }
  .table-responsive > .table {
    margin-bottom: 0;
  }
  .table-responsive > .table > thead > tr > th,
  .table-responsive > .table > tbody > tr > th,
  .table-responsive > .table > tfoot > tr > th,
  .table-responsive > .table > thead > tr > td,
  .table-responsive > .table > tbody > tr > td,
  .table-responsive > .table > tfoot > tr > td {
    white-space: nowrap;
  }
  .table-responsive > .table-bordered {
    border: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:first-child,
  .table-responsive > .table-bordered > tbody > tr > th:first-child,
  .table-responsive > .table-bordered > tfoot > tr > th:first-child,
  .table-responsive > .table-bordered > thead > tr > td:first-child,
  .table-responsive > .table-bordered > tbody > tr > td:first-child,
  .table-responsive > .table-bordered > tfoot > tr > td:first-child {
    border-left: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:last-child,
  .table-responsive > .table-bordered > tbody > tr > th:last-child,
  .table-responsive > .table-bordered > tfoot > tr > th:last-child,
  .table-responsive > .table-bordered > thead > tr > td:last-child,
  .table-responsive > .table-bordered > tbody > tr > td:last-child,
  .table-responsive > .table-bordered > tfoot > tr > td:last-child {
    border-right: 0;
  }
  .table-responsive > .table-bordered > tbody > tr:last-child > th,
  .table-responsive > .table-bordered > tfoot > tr:last-child > th,
  .table-responsive > .table-bordered > tbody > tr:last-child > td,
  .table-responsive > .table-bordered > tfoot > tr:last-child > td {
    border-bottom: 0;
  }
}
fieldset {
  padding: 0;
  margin: 0;
  border: 0;
  min-width: 0;
}
legend {
  display: block;
  width: 100%;
  padding: 0;
  margin-bottom: 18px;
  font-size: 19.5px;
  line-height: inherit;
  color: #333333;
  border: 0;
  border-bottom: 1px solid #e5e5e5;
}
label {
  display: inline-block;
  max-width: 100%;
  margin-bottom: 5px;
  font-weight: bold;
}
input[type="search"] {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
input[type="radio"],
input[type="checkbox"] {
  margin: 4px 0 0;
  margin-top: 1px \9;
  line-height: normal;
}
input[type="file"] {
  display: block;
}
input[type="range"] {
  display: block;
  width: 100%;
}
select[multiple],
select[size] {
  height: auto;
}
input[type="file"]:focus,
input[type="radio"]:focus,
input[type="checkbox"]:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
output {
  display: block;
  padding-top: 7px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
}
.form-control {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
}
.form-control:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.form-control::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.form-control:-ms-input-placeholder {
  color: #999;
}
.form-control::-webkit-input-placeholder {
  color: #999;
}
.form-control::-ms-expand {
  border: 0;
  background-color: transparent;
}
.form-control[disabled],
.form-control[readonly],
fieldset[disabled] .form-control {
  background-color: #eeeeee;
  opacity: 1;
}
.form-control[disabled],
fieldset[disabled] .form-control {
  cursor: not-allowed;
}
textarea.form-control {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: none;
}
@media screen and (-webkit-min-device-pixel-ratio: 0) {
  input[type="date"].form-control,
  input[type="time"].form-control,
  input[type="datetime-local"].form-control,
  input[type="month"].form-control {
    line-height: 32px;
  }
  input[type="date"].input-sm,
  input[type="time"].input-sm,
  input[type="datetime-local"].input-sm,
  input[type="month"].input-sm,
  .input-group-sm input[type="date"],
  .input-group-sm input[type="time"],
  .input-group-sm input[type="datetime-local"],
  .input-group-sm input[type="month"] {
    line-height: 30px;
  }
  input[type="date"].input-lg,
  input[type="time"].input-lg,
  input[type="datetime-local"].input-lg,
  input[type="month"].input-lg,
  .input-group-lg input[type="date"],
  .input-group-lg input[type="time"],
  .input-group-lg input[type="datetime-local"],
  .input-group-lg input[type="month"] {
    line-height: 45px;
  }
}
.form-group {
  margin-bottom: 15px;
}
.radio,
.checkbox {
  position: relative;
  display: block;
  margin-top: 10px;
  margin-bottom: 10px;
}
.radio label,
.checkbox label {
  min-height: 18px;
  padding-left: 20px;
  margin-bottom: 0;
  font-weight: normal;
  cursor: pointer;
}
.radio input[type="radio"],
.radio-inline input[type="radio"],
.checkbox input[type="checkbox"],
.checkbox-inline input[type="checkbox"] {
  position: absolute;
  margin-left: -20px;
  margin-top: 4px \9;
}
.radio + .radio,
.checkbox + .checkbox {
  margin-top: -5px;
}
.radio-inline,
.checkbox-inline {
  position: relative;
  display: inline-block;
  padding-left: 20px;
  margin-bottom: 0;
  vertical-align: middle;
  font-weight: normal;
  cursor: pointer;
}
.radio-inline + .radio-inline,
.checkbox-inline + .checkbox-inline {
  margin-top: 0;
  margin-left: 10px;
}
input[type="radio"][disabled],
input[type="checkbox"][disabled],
input[type="radio"].disabled,
input[type="checkbox"].disabled,
fieldset[disabled] input[type="radio"],
fieldset[disabled] input[type="checkbox"] {
  cursor: not-allowed;
}
.radio-inline.disabled,
.checkbox-inline.disabled,
fieldset[disabled] .radio-inline,
fieldset[disabled] .checkbox-inline {
  cursor: not-allowed;
}
.radio.disabled label,
.checkbox.disabled label,
fieldset[disabled] .radio label,
fieldset[disabled] .checkbox label {
  cursor: not-allowed;
}
.form-control-static {
  padding-top: 7px;
  padding-bottom: 7px;
  margin-bottom: 0;
  min-height: 31px;
}
.form-control-static.input-lg,
.form-control-static.input-sm {
  padding-left: 0;
  padding-right: 0;
}
.input-sm {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-sm {
  height: 30px;
  line-height: 30px;
}
textarea.input-sm,
select[multiple].input-sm {
  height: auto;
}
.form-group-sm .form-control {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.form-group-sm select.form-control {
  height: 30px;
  line-height: 30px;
}
.form-group-sm textarea.form-control,
.form-group-sm select[multiple].form-control {
  height: auto;
}
.form-group-sm .form-control-static {
  height: 30px;
  min-height: 30px;
  padding: 6px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.input-lg {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-lg {
  height: 45px;
  line-height: 45px;
}
textarea.input-lg,
select[multiple].input-lg {
  height: auto;
}
.form-group-lg .form-control {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.form-group-lg select.form-control {
  height: 45px;
  line-height: 45px;
}
.form-group-lg textarea.form-control,
.form-group-lg select[multiple].form-control {
  height: auto;
}
.form-group-lg .form-control-static {
  height: 45px;
  min-height: 35px;
  padding: 11px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.has-feedback {
  position: relative;
}
.has-feedback .form-control {
  padding-right: 40px;
}
.form-control-feedback {
  position: absolute;
  top: 0;
  right: 0;
  z-index: 2;
  display: block;
  width: 32px;
  height: 32px;
  line-height: 32px;
  text-align: center;
  pointer-events: none;
}
.input-lg + .form-control-feedback,
.input-group-lg + .form-control-feedback,
.form-group-lg .form-control + .form-control-feedback {
  width: 45px;
  height: 45px;
  line-height: 45px;
}
.input-sm + .form-control-feedback,
.input-group-sm + .form-control-feedback,
.form-group-sm .form-control + .form-control-feedback {
  width: 30px;
  height: 30px;
  line-height: 30px;
}
.has-success .help-block,
.has-success .control-label,
.has-success .radio,
.has-success .checkbox,
.has-success .radio-inline,
.has-success .checkbox-inline,
.has-success.radio label,
.has-success.checkbox label,
.has-success.radio-inline label,
.has-success.checkbox-inline label {
  color: #3c763d;
}
.has-success .form-control {
  border-color: #3c763d;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-success .form-control:focus {
  border-color: #2b542c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
}
.has-success .input-group-addon {
  color: #3c763d;
  border-color: #3c763d;
  background-color: #dff0d8;
}
.has-success .form-control-feedback {
  color: #3c763d;
}
.has-warning .help-block,
.has-warning .control-label,
.has-warning .radio,
.has-warning .checkbox,
.has-warning .radio-inline,
.has-warning .checkbox-inline,
.has-warning.radio label,
.has-warning.checkbox label,
.has-warning.radio-inline label,
.has-warning.checkbox-inline label {
  color: #8a6d3b;
}
.has-warning .form-control {
  border-color: #8a6d3b;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-warning .form-control:focus {
  border-color: #66512c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
}
.has-warning .input-group-addon {
  color: #8a6d3b;
  border-color: #8a6d3b;
  background-color: #fcf8e3;
}
.has-warning .form-control-feedback {
  color: #8a6d3b;
}
.has-error .help-block,
.has-error .control-label,
.has-error .radio,
.has-error .checkbox,
.has-error .radio-inline,
.has-error .checkbox-inline,
.has-error.radio label,
.has-error.checkbox label,
.has-error.radio-inline label,
.has-error.checkbox-inline label {
  color: #a94442;
}
.has-error .form-control {
  border-color: #a94442;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-error .form-control:focus {
  border-color: #843534;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
}
.has-error .input-group-addon {
  color: #a94442;
  border-color: #a94442;
  background-color: #f2dede;
}
.has-error .form-control-feedback {
  color: #a94442;
}
.has-feedback label ~ .form-control-feedback {
  top: 23px;
}
.has-feedback label.sr-only ~ .form-control-feedback {
  top: 0;
}
.help-block {
  display: block;
  margin-top: 5px;
  margin-bottom: 10px;
  color: #404040;
}
@media (min-width: 768px) {
  .form-inline .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .form-inline .form-control-static {
    display: inline-block;
  }
  .form-inline .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .form-inline .input-group .input-group-addon,
  .form-inline .input-group .input-group-btn,
  .form-inline .input-group .form-control {
    width: auto;
  }
  .form-inline .input-group > .form-control {
    width: 100%;
  }
  .form-inline .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio,
  .form-inline .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio label,
  .form-inline .checkbox label {
    padding-left: 0;
  }
  .form-inline .radio input[type="radio"],
  .form-inline .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .form-inline .has-feedback .form-control-feedback {
    top: 0;
  }
}
.form-horizontal .radio,
.form-horizontal .checkbox,
.form-horizontal .radio-inline,
.form-horizontal .checkbox-inline {
  margin-top: 0;
  margin-bottom: 0;
  padding-top: 7px;
}
.form-horizontal .radio,
.form-horizontal .checkbox {
  min-height: 25px;
}
.form-horizontal .form-group {
  margin-left: 0px;
  margin-right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .control-label {
    text-align: right;
    margin-bottom: 0;
    padding-top: 7px;
  }
}
.form-horizontal .has-feedback .form-control-feedback {
  right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .form-group-lg .control-label {
    padding-top: 11px;
    font-size: 17px;
  }
}
@media (min-width: 768px) {
  .form-horizontal .form-group-sm .control-label {
    padding-top: 6px;
    font-size: 12px;
  }
}
.btn {
  display: inline-block;
  margin-bottom: 0;
  font-weight: normal;
  text-align: center;
  vertical-align: middle;
  touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  white-space: nowrap;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  border-radius: 2px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}
.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #333;
  text-decoration: none;
}
.btn:active,
.btn.active {
  outline: 0;
  background-image: none;
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  opacity: 0.65;
  filter: alpha(opacity=65);
  -webkit-box-shadow: none;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.btn-default:focus,
.btn-default.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.btn-default:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active:hover,
.btn-default.active:hover,
.open > .dropdown-toggle.btn-default:hover,
.btn-default:active:focus,
.btn-default.active:focus,
.open > .dropdown-toggle.btn-default:focus,
.btn-default:active.focus,
.btn-default.active.focus,
.open > .dropdown-toggle.btn-default.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  background-image: none;
}
.btn-default.disabled:hover,
.btn-default[disabled]:hover,
fieldset[disabled] .btn-default:hover,
.btn-default.disabled:focus,
.btn-default[disabled]:focus,
fieldset[disabled] .btn-default:focus,
.btn-default.disabled.focus,
.btn-default[disabled].focus,
fieldset[disabled] .btn-default.focus {
  background-color: #fff;
  border-color: #ccc;
}
.btn-default .badge {
  color: #fff;
  background-color: #333;
}
.btn-primary {
  color: #fff;
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary:focus,
.btn-primary.focus {
  color: #fff;
  background-color: #286090;
  border-color: #122b40;
}
.btn-primary:hover {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active:hover,
.btn-primary.active:hover,
.open > .dropdown-toggle.btn-primary:hover,
.btn-primary:active:focus,
.btn-primary.active:focus,
.open > .dropdown-toggle.btn-primary:focus,
.btn-primary:active.focus,
.btn-primary.active.focus,
.open > .dropdown-toggle.btn-primary.focus {
  color: #fff;
  background-color: #204d74;
  border-color: #122b40;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  background-image: none;
}
.btn-primary.disabled:hover,
.btn-primary[disabled]:hover,
fieldset[disabled] .btn-primary:hover,
.btn-primary.disabled:focus,
.btn-primary[disabled]:focus,
fieldset[disabled] .btn-primary:focus,
.btn-primary.disabled.focus,
.btn-primary[disabled].focus,
fieldset[disabled] .btn-primary.focus {
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary .badge {
  color: #337ab7;
  background-color: #fff;
}
.btn-success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success:focus,
.btn-success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.btn-success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active:hover,
.btn-success.active:hover,
.open > .dropdown-toggle.btn-success:hover,
.btn-success:active:focus,
.btn-success.active:focus,
.open > .dropdown-toggle.btn-success:focus,
.btn-success:active.focus,
.btn-success.active.focus,
.open > .dropdown-toggle.btn-success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  background-image: none;
}
.btn-success.disabled:hover,
.btn-success[disabled]:hover,
fieldset[disabled] .btn-success:hover,
.btn-success.disabled:focus,
.btn-success[disabled]:focus,
fieldset[disabled] .btn-success:focus,
.btn-success.disabled.focus,
.btn-success[disabled].focus,
fieldset[disabled] .btn-success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.btn-info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info:focus,
.btn-info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.btn-info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active:hover,
.btn-info.active:hover,
.open > .dropdown-toggle.btn-info:hover,
.btn-info:active:focus,
.btn-info.active:focus,
.open > .dropdown-toggle.btn-info:focus,
.btn-info:active.focus,
.btn-info.active.focus,
.open > .dropdown-toggle.btn-info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  background-image: none;
}
.btn-info.disabled:hover,
.btn-info[disabled]:hover,
fieldset[disabled] .btn-info:hover,
.btn-info.disabled:focus,
.btn-info[disabled]:focus,
fieldset[disabled] .btn-info:focus,
.btn-info.disabled.focus,
.btn-info[disabled].focus,
fieldset[disabled] .btn-info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.btn-warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning:focus,
.btn-warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.btn-warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active:hover,
.btn-warning.active:hover,
.open > .dropdown-toggle.btn-warning:hover,
.btn-warning:active:focus,
.btn-warning.active:focus,
.open > .dropdown-toggle.btn-warning:focus,
.btn-warning:active.focus,
.btn-warning.active.focus,
.open > .dropdown-toggle.btn-warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  background-image: none;
}
.btn-warning.disabled:hover,
.btn-warning[disabled]:hover,
fieldset[disabled] .btn-warning:hover,
.btn-warning.disabled:focus,
.btn-warning[disabled]:focus,
fieldset[disabled] .btn-warning:focus,
.btn-warning.disabled.focus,
.btn-warning[disabled].focus,
fieldset[disabled] .btn-warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.btn-danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger:focus,
.btn-danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.btn-danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active:hover,
.btn-danger.active:hover,
.open > .dropdown-toggle.btn-danger:hover,
.btn-danger:active:focus,
.btn-danger.active:focus,
.open > .dropdown-toggle.btn-danger:focus,
.btn-danger:active.focus,
.btn-danger.active.focus,
.open > .dropdown-toggle.btn-danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  background-image: none;
}
.btn-danger.disabled:hover,
.btn-danger[disabled]:hover,
fieldset[disabled] .btn-danger:hover,
.btn-danger.disabled:focus,
.btn-danger[disabled]:focus,
fieldset[disabled] .btn-danger:focus,
.btn-danger.disabled.focus,
.btn-danger[disabled].focus,
fieldset[disabled] .btn-danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger .badge {
  color: #d9534f;
  background-color: #fff;
}
.btn-link {
  color: #337ab7;
  font-weight: normal;
  border-radius: 0;
}
.btn-link,
.btn-link:active,
.btn-link.active,
.btn-link[disabled],
fieldset[disabled] .btn-link {
  background-color: transparent;
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn-link,
.btn-link:hover,
.btn-link:focus,
.btn-link:active {
  border-color: transparent;
}
.btn-link:hover,
.btn-link:focus {
  color: #23527c;
  text-decoration: underline;
  background-color: transparent;
}
.btn-link[disabled]:hover,
fieldset[disabled] .btn-link:hover,
.btn-link[disabled]:focus,
fieldset[disabled] .btn-link:focus {
  color: #777777;
  text-decoration: none;
}
.btn-lg,
.btn-group-lg > .btn {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.btn-sm,
.btn-group-sm > .btn {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-xs,
.btn-group-xs > .btn {
  padding: 1px 5px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-block {
  display: block;
  width: 100%;
}
.btn-block + .btn-block {
  margin-top: 5px;
}
input[type="submit"].btn-block,
input[type="reset"].btn-block,
input[type="button"].btn-block {
  width: 100%;
}
.fade {
  opacity: 0;
  -webkit-transition: opacity 0.15s linear;
  -o-transition: opacity 0.15s linear;
  transition: opacity 0.15s linear;
}
.fade.in {
  opacity: 1;
}
.collapse {
  display: none;
}
.collapse.in {
  display: block;
}
tr.collapse.in {
  display: table-row;
}
tbody.collapse.in {
  display: table-row-group;
}
.collapsing {
  position: relative;
  height: 0;
  overflow: hidden;
  -webkit-transition-property: height, visibility;
  transition-property: height, visibility;
  -webkit-transition-duration: 0.35s;
  transition-duration: 0.35s;
  -webkit-transition-timing-function: ease;
  transition-timing-function: ease;
}
.caret {
  display: inline-block;
  width: 0;
  height: 0;
  margin-left: 2px;
  vertical-align: middle;
  border-top: 4px dashed;
  border-top: 4px solid \9;
  border-right: 4px solid transparent;
  border-left: 4px solid transparent;
}
.dropup,
.dropdown {
  position: relative;
}
.dropdown-toggle:focus {
  outline: 0;
}
.dropdown-menu {
  position: absolute;
  top: 100%;
  left: 0;
  z-index: 1000;
  display: none;
  float: left;
  min-width: 160px;
  padding: 5px 0;
  margin: 2px 0 0;
  list-style: none;
  font-size: 13px;
  text-align: left;
  background-color: #fff;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.15);
  border-radius: 2px;
  -webkit-box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  background-clip: padding-box;
}
.dropdown-menu.pull-right {
  right: 0;
  left: auto;
}
.dropdown-menu .divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: normal;
  line-height: 1.42857143;
  color: #333333;
  white-space: nowrap;
}
.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  text-decoration: none;
  color: #262626;
  background-color: #f5f5f5;
}
.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #fff;
  text-decoration: none;
  outline: 0;
  background-color: #337ab7;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #777777;
}
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
  cursor: not-allowed;
}
.open > .dropdown-menu {
  display: block;
}
.open > a {
  outline: 0;
}
.dropdown-menu-right {
  left: auto;
  right: 0;
}
.dropdown-menu-left {
  left: 0;
  right: auto;
}
.dropdown-header {
  display: block;
  padding: 3px 20px;
  font-size: 12px;
  line-height: 1.42857143;
  color: #777777;
  white-space: nowrap;
}
.dropdown-backdrop {
  position: fixed;
  left: 0;
  right: 0;
  bottom: 0;
  top: 0;
  z-index: 990;
}
.pull-right > .dropdown-menu {
  right: 0;
  left: auto;
}
.dropup .caret,
.navbar-fixed-bottom .dropdown .caret {
  border-top: 0;
  border-bottom: 4px dashed;
  border-bottom: 4px solid \9;
  content: "";
}
.dropup .dropdown-menu,
.navbar-fixed-bottom .dropdown .dropdown-menu {
  top: auto;
  bottom: 100%;
  margin-bottom: 2px;
}
@media (min-width: 541px) {
  .navbar-right .dropdown-menu {
    left: auto;
    right: 0;
  }
  .navbar-right .dropdown-menu-left {
    left: 0;
    right: auto;
  }
}
.btn-group,
.btn-group-vertical {
  position: relative;
  display: inline-block;
  vertical-align: middle;
}
.btn-group > .btn,
.btn-group-vertical > .btn {
  position: relative;
  float: left;
}
.btn-group > .btn:hover,
.btn-group-vertical > .btn:hover,
.btn-group > .btn:focus,
.btn-group-vertical > .btn:focus,
.btn-group > .btn:active,
.btn-group-vertical > .btn:active,
.btn-group > .btn.active,
.btn-group-vertical > .btn.active {
  z-index: 2;
}
.btn-group .btn + .btn,
.btn-group .btn + .btn-group,
.btn-group .btn-group + .btn,
.btn-group .btn-group + .btn-group {
  margin-left: -1px;
}
.btn-toolbar {
  margin-left: -5px;
}
.btn-toolbar .btn,
.btn-toolbar .btn-group,
.btn-toolbar .input-group {
  float: left;
}
.btn-toolbar > .btn,
.btn-toolbar > .btn-group,
.btn-toolbar > .input-group {
  margin-left: 5px;
}
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-left: 8px;
  padding-right: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-left: 12px;
  padding-right: 12px;
}
.btn-group.open .dropdown-toggle {
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn .caret {
  margin-left: 0;
}
.btn-lg .caret {
  border-width: 5px 5px 0;
  border-bottom-width: 0;
}
.dropup .btn-lg .caret {
  border-width: 0 5px 5px;
}
.btn-group-vertical > .btn,
.btn-group-vertical > .btn-group,
.btn-group-vertical > .btn-group > .btn {
  display: block;
  float: none;
  width: 100%;
  max-width: 100%;
}
.btn-group-vertical > .btn-group > .btn {
  float: none;
}
.btn-group-vertical > .btn + .btn,
.btn-group-vertical > .btn + .btn-group,
.btn-group-vertical > .btn-group + .btn,
.btn-group-vertical > .btn-group + .btn-group {
  margin-top: -1px;
  margin-left: 0;
}
.btn-group-vertical > .btn:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.btn-group-vertical > .btn:first-child:not(:last-child) {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn:last-child:not(:first-child) {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
.btn-group-vertical > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.btn-group-justified {
  display: table;
  width: 100%;
  table-layout: fixed;
  border-collapse: separate;
}
.btn-group-justified > .btn,
.btn-group-justified > .btn-group {
  float: none;
  display: table-cell;
  width: 1%;
}
.btn-group-justified > .btn-group .btn {
  width: 100%;
}
.btn-group-justified > .btn-group .dropdown-menu {
  left: auto;
}
[data-toggle="buttons"] > .btn input[type="radio"],
[data-toggle="buttons"] > .btn-group > .btn input[type="radio"],
[data-toggle="buttons"] > .btn input[type="checkbox"],
[data-toggle="buttons"] > .btn-group > .btn input[type="checkbox"] {
  position: absolute;
  clip: rect(0, 0, 0, 0);
  pointer-events: none;
}
.input-group {
  position: relative;
  display: table;
  border-collapse: separate;
}
.input-group[class*="col-"] {
  float: none;
  padding-left: 0;
  padding-right: 0;
}
.input-group .form-control {
  position: relative;
  z-index: 2;
  float: left;
  width: 100%;
  margin-bottom: 0;
}
.input-group .form-control:focus {
  z-index: 3;
}
.input-group-lg > .form-control,
.input-group-lg > .input-group-addon,
.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-group-lg > .form-control,
select.input-group-lg > .input-group-addon,
select.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  line-height: 45px;
}
textarea.input-group-lg > .form-control,
textarea.input-group-lg > .input-group-addon,
textarea.input-group-lg > .input-group-btn > .btn,
select[multiple].input-group-lg > .form-control,
select[multiple].input-group-lg > .input-group-addon,
select[multiple].input-group-lg > .input-group-btn > .btn {
  height: auto;
}
.input-group-sm > .form-control,
.input-group-sm > .input-group-addon,
.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-group-sm > .form-control,
select.input-group-sm > .input-group-addon,
select.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  line-height: 30px;
}
textarea.input-group-sm > .form-control,
textarea.input-group-sm > .input-group-addon,
textarea.input-group-sm > .input-group-btn > .btn,
select[multiple].input-group-sm > .form-control,
select[multiple].input-group-sm > .input-group-addon,
select[multiple].input-group-sm > .input-group-btn > .btn {
  height: auto;
}
.input-group-addon,
.input-group-btn,
.input-group .form-control {
  display: table-cell;
}
.input-group-addon:not(:first-child):not(:last-child),
.input-group-btn:not(:first-child):not(:last-child),
.input-group .form-control:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.input-group-addon,
.input-group-btn {
  width: 1%;
  white-space: nowrap;
  vertical-align: middle;
}
.input-group-addon {
  padding: 6px 12px;
  font-size: 13px;
  font-weight: normal;
  line-height: 1;
  color: #555555;
  text-align: center;
  background-color: #eeeeee;
  border: 1px solid #ccc;
  border-radius: 2px;
}
.input-group-addon.input-sm {
  padding: 5px 10px;
  font-size: 12px;
  border-radius: 1px;
}
.input-group-addon.input-lg {
  padding: 10px 16px;
  font-size: 17px;
  border-radius: 3px;
}
.input-group-addon input[type="radio"],
.input-group-addon input[type="checkbox"] {
  margin-top: 0;
}
.input-group .form-control:first-child,
.input-group-addon:first-child,
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group > .btn,
.input-group-btn:first-child > .dropdown-toggle,
.input-group-btn:last-child > .btn:not(:last-child):not(.dropdown-toggle),
.input-group-btn:last-child > .btn-group:not(:last-child) > .btn {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.input-group-addon:first-child {
  border-right: 0;
}
.input-group .form-control:last-child,
.input-group-addon:last-child,
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group > .btn,
.input-group-btn:last-child > .dropdown-toggle,
.input-group-btn:first-child > .btn:not(:first-child),
.input-group-btn:first-child > .btn-group:not(:first-child) > .btn {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.input-group-addon:last-child {
  border-left: 0;
}
.input-group-btn {
  position: relative;
  font-size: 0;
  white-space: nowrap;
}
.input-group-btn > .btn {
  position: relative;
}
.input-group-btn > .btn + .btn {
  margin-left: -1px;
}
.input-group-btn > .btn:hover,
.input-group-btn > .btn:focus,
.input-group-btn > .btn:active {
  z-index: 2;
}
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group {
  margin-right: -1px;
}
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group {
  z-index: 2;
  margin-left: -1px;
}
.nav {
  margin-bottom: 0;
  padding-left: 0;
  list-style: none;
}
.nav > li {
  position: relative;
  display: block;
}
.nav > li > a {
  position: relative;
  display: block;
  padding: 10px 15px;
}
.nav > li > a:hover,
.nav > li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.nav > li.disabled > a {
  color: #777777;
}
.nav > li.disabled > a:hover,
.nav > li.disabled > a:focus {
  color: #777777;
  text-decoration: none;
  background-color: transparent;
  cursor: not-allowed;
}
.nav .open > a,
.nav .open > a:hover,
.nav .open > a:focus {
  background-color: #eeeeee;
  border-color: #337ab7;
}
.nav .nav-divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.nav > li > a > img {
  max-width: none;
}
.nav-tabs {
  border-bottom: 1px solid #ddd;
}
.nav-tabs > li {
  float: left;
  margin-bottom: -1px;
}
.nav-tabs > li > a {
  margin-right: 2px;
  line-height: 1.42857143;
  border: 1px solid transparent;
  border-radius: 2px 2px 0 0;
}
.nav-tabs > li > a:hover {
  border-color: #eeeeee #eeeeee #ddd;
}
.nav-tabs > li.active > a,
.nav-tabs > li.active > a:hover,
.nav-tabs > li.active > a:focus {
  color: #555555;
  background-color: #fff;
  border: 1px solid #ddd;
  border-bottom-color: transparent;
  cursor: default;
}
.nav-tabs.nav-justified {
  width: 100%;
  border-bottom: 0;
}
.nav-tabs.nav-justified > li {
  float: none;
}
.nav-tabs.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-tabs.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-tabs.nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs.nav-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs.nav-justified > .active > a,
.nav-tabs.nav-justified > .active > a:hover,
.nav-tabs.nav-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs.nav-justified > .active > a,
  .nav-tabs.nav-justified > .active > a:hover,
  .nav-tabs.nav-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.nav-pills > li {
  float: left;
}
.nav-pills > li > a {
  border-radius: 2px;
}
.nav-pills > li + li {
  margin-left: 2px;
}
.nav-pills > li.active > a,
.nav-pills > li.active > a:hover,
.nav-pills > li.active > a:focus {
  color: #fff;
  background-color: #337ab7;
}
.nav-stacked > li {
  float: none;
}
.nav-stacked > li + li {
  margin-top: 2px;
  margin-left: 0;
}
.nav-justified {
  width: 100%;
}
.nav-justified > li {
  float: none;
}
.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs-justified {
  border-bottom: 0;
}
.nav-tabs-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs-justified > .active > a,
.nav-tabs-justified > .active > a:hover,
.nav-tabs-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs-justified > .active > a,
  .nav-tabs-justified > .active > a:hover,
  .nav-tabs-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.tab-content > .tab-pane {
  display: none;
}
.tab-content > .active {
  display: block;
}
.nav-tabs .dropdown-menu {
  margin-top: -1px;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar {
  position: relative;
  min-height: 30px;
  margin-bottom: 18px;
  border: 1px solid transparent;
}
@media (min-width: 541px) {
  .navbar {
    border-radius: 2px;
  }
}
@media (min-width: 541px) {
  .navbar-header {
    float: left;
  }
}
.navbar-collapse {
  overflow-x: visible;
  padding-right: 0px;
  padding-left: 0px;
  border-top: 1px solid transparent;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1);
  -webkit-overflow-scrolling: touch;
}
.navbar-collapse.in {
  overflow-y: auto;
}
@media (min-width: 541px) {
  .navbar-collapse {
    width: auto;
    border-top: 0;
    box-shadow: none;
  }
  .navbar-collapse.collapse {
    display: block !important;
    height: auto !important;
    padding-bottom: 0;
    overflow: visible !important;
  }
  .navbar-collapse.in {
    overflow-y: visible;
  }
  .navbar-fixed-top .navbar-collapse,
  .navbar-static-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    padding-left: 0;
    padding-right: 0;
  }
}
.navbar-fixed-top .navbar-collapse,
.navbar-fixed-bottom .navbar-collapse {
  max-height: 340px;
}
@media (max-device-width: 540px) and (orientation: landscape) {
  .navbar-fixed-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    max-height: 200px;
  }
}
.container > .navbar-header,
.container-fluid > .navbar-header,
.container > .navbar-collapse,
.container-fluid > .navbar-collapse {
  margin-right: 0px;
  margin-left: 0px;
}
@media (min-width: 541px) {
  .container > .navbar-header,
  .container-fluid > .navbar-header,
  .container > .navbar-collapse,
  .container-fluid > .navbar-collapse {
    margin-right: 0;
    margin-left: 0;
  }
}
.navbar-static-top {
  z-index: 1000;
  border-width: 0 0 1px;
}
@media (min-width: 541px) {
  .navbar-static-top {
    border-radius: 0;
  }
}
.navbar-fixed-top,
.navbar-fixed-bottom {
  position: fixed;
  right: 0;
  left: 0;
  z-index: 1030;
}
@media (min-width: 541px) {
  .navbar-fixed-top,
  .navbar-fixed-bottom {
    border-radius: 0;
  }
}
.navbar-fixed-top {
  top: 0;
  border-width: 0 0 1px;
}
.navbar-fixed-bottom {
  bottom: 0;
  margin-bottom: 0;
  border-width: 1px 0 0;
}
.navbar-brand {
  float: left;
  padding: 6px 0px;
  font-size: 17px;
  line-height: 18px;
  height: 30px;
}
.navbar-brand:hover,
.navbar-brand:focus {
  text-decoration: none;
}
.navbar-brand > img {
  display: block;
}
@media (min-width: 541px) {
  .navbar > .container .navbar-brand,
  .navbar > .container-fluid .navbar-brand {
    margin-left: 0px;
  }
}
.navbar-toggle {
  position: relative;
  float: right;
  margin-right: 0px;
  padding: 9px 10px;
  margin-top: -2px;
  margin-bottom: -2px;
  background-color: transparent;
  background-image: none;
  border: 1px solid transparent;
  border-radius: 2px;
}
.navbar-toggle:focus {
  outline: 0;
}
.navbar-toggle .icon-bar {
  display: block;
  width: 22px;
  height: 2px;
  border-radius: 1px;
}
.navbar-toggle .icon-bar + .icon-bar {
  margin-top: 4px;
}
@media (min-width: 541px) {
  .navbar-toggle {
    display: none;
  }
}
.navbar-nav {
  margin: 3px 0px;
}
.navbar-nav > li > a {
  padding-top: 10px;
  padding-bottom: 10px;
  line-height: 18px;
}
@media (max-width: 540px) {
  .navbar-nav .open .dropdown-menu {
    position: static;
    float: none;
    width: auto;
    margin-top: 0;
    background-color: transparent;
    border: 0;
    box-shadow: none;
  }
  .navbar-nav .open .dropdown-menu > li > a,
  .navbar-nav .open .dropdown-menu .dropdown-header {
    padding: 5px 15px 5px 25px;
  }
  .navbar-nav .open .dropdown-menu > li > a {
    line-height: 18px;
  }
  .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-nav .open .dropdown-menu > li > a:focus {
    background-image: none;
  }
}
@media (min-width: 541px) {
  .navbar-nav {
    float: left;
    margin: 0;
  }
  .navbar-nav > li {
    float: left;
  }
  .navbar-nav > li > a {
    padding-top: 6px;
    padding-bottom: 6px;
  }
}
.navbar-form {
  margin-left: 0px;
  margin-right: 0px;
  padding: 10px 0px;
  border-top: 1px solid transparent;
  border-bottom: 1px solid transparent;
  -webkit-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  margin-top: -1px;
  margin-bottom: -1px;
}
@media (min-width: 768px) {
  .navbar-form .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .navbar-form .form-control-static {
    display: inline-block;
  }
  .navbar-form .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .navbar-form .input-group .input-group-addon,
  .navbar-form .input-group .input-group-btn,
  .navbar-form .input-group .form-control {
    width: auto;
  }
  .navbar-form .input-group > .form-control {
    width: 100%;
  }
  .navbar-form .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio,
  .navbar-form .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio label,
  .navbar-form .checkbox label {
    padding-left: 0;
  }
  .navbar-form .radio input[type="radio"],
  .navbar-form .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .navbar-form .has-feedback .form-control-feedback {
    top: 0;
  }
}
@media (max-width: 540px) {
  .navbar-form .form-group {
    margin-bottom: 5px;
  }
  .navbar-form .form-group:last-child {
    margin-bottom: 0;
  }
}
@media (min-width: 541px) {
  .navbar-form {
    width: auto;
    border: 0;
    margin-left: 0;
    margin-right: 0;
    padding-top: 0;
    padding-bottom: 0;
    -webkit-box-shadow: none;
    box-shadow: none;
  }
}
.navbar-nav > li > .dropdown-menu {
  margin-top: 0;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar-fixed-bottom .navbar-nav > li > .dropdown-menu {
  margin-bottom: 0;
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.navbar-btn {
  margin-top: -1px;
  margin-bottom: -1px;
}
.navbar-btn.btn-sm {
  margin-top: 0px;
  margin-bottom: 0px;
}
.navbar-btn.btn-xs {
  margin-top: 4px;
  margin-bottom: 4px;
}
.navbar-text {
  margin-top: 6px;
  margin-bottom: 6px;
}
@media (min-width: 541px) {
  .navbar-text {
    float: left;
    margin-left: 0px;
    margin-right: 0px;
  }
}
@media (min-width: 541px) {
  .navbar-left {
    float: left !important;
    float: left;
  }
  .navbar-right {
    float: right !important;
    float: right;
    margin-right: 0px;
  }
  .navbar-right ~ .navbar-right {
    margin-right: 0;
  }
}
.navbar-default {
  background-color: #f8f8f8;
  border-color: #e7e7e7;
}
.navbar-default .navbar-brand {
  color: #777;
}
.navbar-default .navbar-brand:hover,
.navbar-default .navbar-brand:focus {
  color: #5e5e5e;
  background-color: transparent;
}
.navbar-default .navbar-text {
  color: #777;
}
.navbar-default .navbar-nav > li > a {
  color: #777;
}
.navbar-default .navbar-nav > li > a:hover,
.navbar-default .navbar-nav > li > a:focus {
  color: #333;
  background-color: transparent;
}
.navbar-default .navbar-nav > .active > a,
.navbar-default .navbar-nav > .active > a:hover,
.navbar-default .navbar-nav > .active > a:focus {
  color: #555;
  background-color: #e7e7e7;
}
.navbar-default .navbar-nav > .disabled > a,
.navbar-default .navbar-nav > .disabled > a:hover,
.navbar-default .navbar-nav > .disabled > a:focus {
  color: #ccc;
  background-color: transparent;
}
.navbar-default .navbar-toggle {
  border-color: #ddd;
}
.navbar-default .navbar-toggle:hover,
.navbar-default .navbar-toggle:focus {
  background-color: #ddd;
}
.navbar-default .navbar-toggle .icon-bar {
  background-color: #888;
}
.navbar-default .navbar-collapse,
.navbar-default .navbar-form {
  border-color: #e7e7e7;
}
.navbar-default .navbar-nav > .open > a,
.navbar-default .navbar-nav > .open > a:hover,
.navbar-default .navbar-nav > .open > a:focus {
  background-color: #e7e7e7;
  color: #555;
}
@media (max-width: 540px) {
  .navbar-default .navbar-nav .open .dropdown-menu > li > a {
    color: #777;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #333;
    background-color: transparent;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #555;
    background-color: #e7e7e7;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #ccc;
    background-color: transparent;
  }
}
.navbar-default .navbar-link {
  color: #777;
}
.navbar-default .navbar-link:hover {
  color: #333;
}
.navbar-default .btn-link {
  color: #777;
}
.navbar-default .btn-link:hover,
.navbar-default .btn-link:focus {
  color: #333;
}
.navbar-default .btn-link[disabled]:hover,
fieldset[disabled] .navbar-default .btn-link:hover,
.navbar-default .btn-link[disabled]:focus,
fieldset[disabled] .navbar-default .btn-link:focus {
  color: #ccc;
}
.navbar-inverse {
  background-color: #222;
  border-color: #080808;
}
.navbar-inverse .navbar-brand {
  color: #9d9d9d;
}
.navbar-inverse .navbar-brand:hover,
.navbar-inverse .navbar-brand:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-text {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a:hover,
.navbar-inverse .navbar-nav > li > a:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-nav > .active > a,
.navbar-inverse .navbar-nav > .active > a:hover,
.navbar-inverse .navbar-nav > .active > a:focus {
  color: #fff;
  background-color: #080808;
}
.navbar-inverse .navbar-nav > .disabled > a,
.navbar-inverse .navbar-nav > .disabled > a:hover,
.navbar-inverse .navbar-nav > .disabled > a:focus {
  color: #444;
  background-color: transparent;
}
.navbar-inverse .navbar-toggle {
  border-color: #333;
}
.navbar-inverse .navbar-toggle:hover,
.navbar-inverse .navbar-toggle:focus {
  background-color: #333;
}
.navbar-inverse .navbar-toggle .icon-bar {
  background-color: #fff;
}
.navbar-inverse .navbar-collapse,
.navbar-inverse .navbar-form {
  border-color: #101010;
}
.navbar-inverse .navbar-nav > .open > a,
.navbar-inverse .navbar-nav > .open > a:hover,
.navbar-inverse .navbar-nav > .open > a:focus {
  background-color: #080808;
  color: #fff;
}
@media (max-width: 540px) {
  .navbar-inverse .navbar-nav .open .dropdown-menu > .dropdown-header {
    border-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu .divider {
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a {
    color: #9d9d9d;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #fff;
    background-color: transparent;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #fff;
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #444;
    background-color: transparent;
  }
}
.navbar-inverse .navbar-link {
  color: #9d9d9d;
}
.navbar-inverse .navbar-link:hover {
  color: #fff;
}
.navbar-inverse .btn-link {
  color: #9d9d9d;
}
.navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link:focus {
  color: #fff;
}
.navbar-inverse .btn-link[disabled]:hover,
fieldset[disabled] .navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link[disabled]:focus,
fieldset[disabled] .navbar-inverse .btn-link:focus {
  color: #444;
}
.breadcrumb {
  padding: 8px 15px;
  margin-bottom: 18px;
  list-style: none;
  background-color: #f5f5f5;
  border-radius: 2px;
}
.breadcrumb > li {
  display: inline-block;
}
.breadcrumb > li + li:before {
  content: "/\00a0";
  padding: 0 5px;
  color: #5e5e5e;
}
.breadcrumb > .active {
  color: #777777;
}
.pagination {
  display: inline-block;
  padding-left: 0;
  margin: 18px 0;
  border-radius: 2px;
}
.pagination > li {
  display: inline;
}
.pagination > li > a,
.pagination > li > span {
  position: relative;
  float: left;
  padding: 6px 12px;
  line-height: 1.42857143;
  text-decoration: none;
  color: #337ab7;
  background-color: #fff;
  border: 1px solid #ddd;
  margin-left: -1px;
}
.pagination > li:first-child > a,
.pagination > li:first-child > span {
  margin-left: 0;
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.pagination > li:last-child > a,
.pagination > li:last-child > span {
  border-bottom-right-radius: 2px;
  border-top-right-radius: 2px;
}
.pagination > li > a:hover,
.pagination > li > span:hover,
.pagination > li > a:focus,
.pagination > li > span:focus {
  z-index: 2;
  color: #23527c;
  background-color: #eeeeee;
  border-color: #ddd;
}
.pagination > .active > a,
.pagination > .active > span,
.pagination > .active > a:hover,
.pagination > .active > span:hover,
.pagination > .active > a:focus,
.pagination > .active > span:focus {
  z-index: 3;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
  cursor: default;
}
.pagination > .disabled > span,
.pagination > .disabled > span:hover,
.pagination > .disabled > span:focus,
.pagination > .disabled > a,
.pagination > .disabled > a:hover,
.pagination > .disabled > a:focus {
  color: #777777;
  background-color: #fff;
  border-color: #ddd;
  cursor: not-allowed;
}
.pagination-lg > li > a,
.pagination-lg > li > span {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.pagination-lg > li:first-child > a,
.pagination-lg > li:first-child > span {
  border-bottom-left-radius: 3px;
  border-top-left-radius: 3px;
}
.pagination-lg > li:last-child > a,
.pagination-lg > li:last-child > span {
  border-bottom-right-radius: 3px;
  border-top-right-radius: 3px;
}
.pagination-sm > li > a,
.pagination-sm > li > span {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.pagination-sm > li:first-child > a,
.pagination-sm > li:first-child > span {
  border-bottom-left-radius: 1px;
  border-top-left-radius: 1px;
}
.pagination-sm > li:last-child > a,
.pagination-sm > li:last-child > span {
  border-bottom-right-radius: 1px;
  border-top-right-radius: 1px;
}
.pager {
  padding-left: 0;
  margin: 18px 0;
  list-style: none;
  text-align: center;
}
.pager li {
  display: inline;
}
.pager li > a,
.pager li > span {
  display: inline-block;
  padding: 5px 14px;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 15px;
}
.pager li > a:hover,
.pager li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.pager .next > a,
.pager .next > span {
  float: right;
}
.pager .previous > a,
.pager .previous > span {
  float: left;
}
.pager .disabled > a,
.pager .disabled > a:hover,
.pager .disabled > a:focus,
.pager .disabled > span {
  color: #777777;
  background-color: #fff;
  cursor: not-allowed;
}
.label {
  display: inline;
  padding: .2em .6em .3em;
  font-size: 75%;
  font-weight: bold;
  line-height: 1;
  color: #fff;
  text-align: center;
  white-space: nowrap;
  vertical-align: baseline;
  border-radius: .25em;
}
a.label:hover,
a.label:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.label:empty {
  display: none;
}
.btn .label {
  position: relative;
  top: -1px;
}
.label-default {
  background-color: #777777;
}
.label-default[href]:hover,
.label-default[href]:focus {
  background-color: #5e5e5e;
}
.label-primary {
  background-color: #337ab7;
}
.label-primary[href]:hover,
.label-primary[href]:focus {
  background-color: #286090;
}
.label-success {
  background-color: #5cb85c;
}
.label-success[href]:hover,
.label-success[href]:focus {
  background-color: #449d44;
}
.label-info {
  background-color: #5bc0de;
}
.label-info[href]:hover,
.label-info[href]:focus {
  background-color: #31b0d5;
}
.label-warning {
  background-color: #f0ad4e;
}
.label-warning[href]:hover,
.label-warning[href]:focus {
  background-color: #ec971f;
}
.label-danger {
  background-color: #d9534f;
}
.label-danger[href]:hover,
.label-danger[href]:focus {
  background-color: #c9302c;
}
.badge {
  display: inline-block;
  min-width: 10px;
  padding: 3px 7px;
  font-size: 12px;
  font-weight: bold;
  color: #fff;
  line-height: 1;
  vertical-align: middle;
  white-space: nowrap;
  text-align: center;
  background-color: #777777;
  border-radius: 10px;
}
.badge:empty {
  display: none;
}
.btn .badge {
  position: relative;
  top: -1px;
}
.btn-xs .badge,
.btn-group-xs > .btn .badge {
  top: 0;
  padding: 1px 5px;
}
a.badge:hover,
a.badge:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.list-group-item.active > .badge,
.nav-pills > .active > a > .badge {
  color: #337ab7;
  background-color: #fff;
}
.list-group-item > .badge {
  float: right;
}
.list-group-item > .badge + .badge {
  margin-right: 5px;
}
.nav-pills > li > a > .badge {
  margin-left: 3px;
}
.jumbotron {
  padding-top: 30px;
  padding-bottom: 30px;
  margin-bottom: 30px;
  color: inherit;
  background-color: #eeeeee;
}
.jumbotron h1,
.jumbotron .h1 {
  color: inherit;
}
.jumbotron p {
  margin-bottom: 15px;
  font-size: 20px;
  font-weight: 200;
}
.jumbotron > hr {
  border-top-color: #d5d5d5;
}
.container .jumbotron,
.container-fluid .jumbotron {
  border-radius: 3px;
  padding-left: 0px;
  padding-right: 0px;
}
.jumbotron .container {
  max-width: 100%;
}
@media screen and (min-width: 768px) {
  .jumbotron {
    padding-top: 48px;
    padding-bottom: 48px;
  }
  .container .jumbotron,
  .container-fluid .jumbotron {
    padding-left: 60px;
    padding-right: 60px;
  }
  .jumbotron h1,
  .jumbotron .h1 {
    font-size: 59px;
  }
}
.thumbnail {
  display: block;
  padding: 4px;
  margin-bottom: 18px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: border 0.2s ease-in-out;
  -o-transition: border 0.2s ease-in-out;
  transition: border 0.2s ease-in-out;
}
.thumbnail > img,
.thumbnail a > img {
  margin-left: auto;
  margin-right: auto;
}
a.thumbnail:hover,
a.thumbnail:focus,
a.thumbnail.active {
  border-color: #337ab7;
}
.thumbnail .caption {
  padding: 9px;
  color: #000;
}
.alert {
  padding: 15px;
  margin-bottom: 18px;
  border: 1px solid transparent;
  border-radius: 2px;
}
.alert h4 {
  margin-top: 0;
  color: inherit;
}
.alert .alert-link {
  font-weight: bold;
}
.alert > p,
.alert > ul {
  margin-bottom: 0;
}
.alert > p + p {
  margin-top: 5px;
}
.alert-dismissable,
.alert-dismissible {
  padding-right: 35px;
}
.alert-dismissable .close,
.alert-dismissible .close {
  position: relative;
  top: -2px;
  right: -21px;
  color: inherit;
}
.alert-success {
  background-color: #dff0d8;
  border-color: #d6e9c6;
  color: #3c763d;
}
.alert-success hr {
  border-top-color: #c9e2b3;
}
.alert-success .alert-link {
  color: #2b542c;
}
.alert-info {
  background-color: #d9edf7;
  border-color: #bce8f1;
  color: #31708f;
}
.alert-info hr {
  border-top-color: #a6e1ec;
}
.alert-info .alert-link {
  color: #245269;
}
.alert-warning {
  background-color: #fcf8e3;
  border-color: #faebcc;
  color: #8a6d3b;
}
.alert-warning hr {
  border-top-color: #f7e1b5;
}
.alert-warning .alert-link {
  color: #66512c;
}
.alert-danger {
  background-color: #f2dede;
  border-color: #ebccd1;
  color: #a94442;
}
.alert-danger hr {
  border-top-color: #e4b9c0;
}
.alert-danger .alert-link {
  color: #843534;
}
@-webkit-keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
@keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
.progress {
  overflow: hidden;
  height: 18px;
  margin-bottom: 18px;
  background-color: #f5f5f5;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
  box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
}
.progress-bar {
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 18px;
  color: #fff;
  text-align: center;
  background-color: #337ab7;
  -webkit-box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  -webkit-transition: width 0.6s ease;
  -o-transition: width 0.6s ease;
  transition: width 0.6s ease;
}
.progress-striped .progress-bar,
.progress-bar-striped {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-size: 40px 40px;
}
.progress.active .progress-bar,
.progress-bar.active {
  -webkit-animation: progress-bar-stripes 2s linear infinite;
  -o-animation: progress-bar-stripes 2s linear infinite;
  animation: progress-bar-stripes 2s linear infinite;
}
.progress-bar-success {
  background-color: #5cb85c;
}
.progress-striped .progress-bar-success {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-info {
  background-color: #5bc0de;
}
.progress-striped .progress-bar-info {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-warning {
  background-color: #f0ad4e;
}
.progress-striped .progress-bar-warning {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-danger {
  background-color: #d9534f;
}
.progress-striped .progress-bar-danger {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.media {
  margin-top: 15px;
}
.media:first-child {
  margin-top: 0;
}
.media,
.media-body {
  zoom: 1;
  overflow: hidden;
}
.media-body {
  width: 10000px;
}
.media-object {
  display: block;
}
.media-object.img-thumbnail {
  max-width: none;
}
.media-right,
.media > .pull-right {
  padding-left: 10px;
}
.media-left,
.media > .pull-left {
  padding-right: 10px;
}
.media-left,
.media-right,
.media-body {
  display: table-cell;
  vertical-align: top;
}
.media-middle {
  vertical-align: middle;
}
.media-bottom {
  vertical-align: bottom;
}
.media-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.media-list {
  padding-left: 0;
  list-style: none;
}
.list-group {
  margin-bottom: 20px;
  padding-left: 0;
}
.list-group-item {
  position: relative;
  display: block;
  padding: 10px 15px;
  margin-bottom: -1px;
  background-color: #fff;
  border: 1px solid #ddd;
}
.list-group-item:first-child {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
}
.list-group-item:last-child {
  margin-bottom: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
a.list-group-item,
button.list-group-item {
  color: #555;
}
a.list-group-item .list-group-item-heading,
button.list-group-item .list-group-item-heading {
  color: #333;
}
a.list-group-item:hover,
button.list-group-item:hover,
a.list-group-item:focus,
button.list-group-item:focus {
  text-decoration: none;
  color: #555;
  background-color: #f5f5f5;
}
button.list-group-item {
  width: 100%;
  text-align: left;
}
.list-group-item.disabled,
.list-group-item.disabled:hover,
.list-group-item.disabled:focus {
  background-color: #eeeeee;
  color: #777777;
  cursor: not-allowed;
}
.list-group-item.disabled .list-group-item-heading,
.list-group-item.disabled:hover .list-group-item-heading,
.list-group-item.disabled:focus .list-group-item-heading {
  color: inherit;
}
.list-group-item.disabled .list-group-item-text,
.list-group-item.disabled:hover .list-group-item-text,
.list-group-item.disabled:focus .list-group-item-text {
  color: #777777;
}
.list-group-item.active,
.list-group-item.active:hover,
.list-group-item.active:focus {
  z-index: 2;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.list-group-item.active .list-group-item-heading,
.list-group-item.active:hover .list-group-item-heading,
.list-group-item.active:focus .list-group-item-heading,
.list-group-item.active .list-group-item-heading > small,
.list-group-item.active:hover .list-group-item-heading > small,
.list-group-item.active:focus .list-group-item-heading > small,
.list-group-item.active .list-group-item-heading > .small,
.list-group-item.active:hover .list-group-item-heading > .small,
.list-group-item.active:focus .list-group-item-heading > .small {
  color: inherit;
}
.list-group-item.active .list-group-item-text,
.list-group-item.active:hover .list-group-item-text,
.list-group-item.active:focus .list-group-item-text {
  color: #c7ddef;
}
.list-group-item-success {
  color: #3c763d;
  background-color: #dff0d8;
}
a.list-group-item-success,
button.list-group-item-success {
  color: #3c763d;
}
a.list-group-item-success .list-group-item-heading,
button.list-group-item-success .list-group-item-heading {
  color: inherit;
}
a.list-group-item-success:hover,
button.list-group-item-success:hover,
a.list-group-item-success:focus,
button.list-group-item-success:focus {
  color: #3c763d;
  background-color: #d0e9c6;
}
a.list-group-item-success.active,
button.list-group-item-success.active,
a.list-group-item-success.active:hover,
button.list-group-item-success.active:hover,
a.list-group-item-success.active:focus,
button.list-group-item-success.active:focus {
  color: #fff;
  background-color: #3c763d;
  border-color: #3c763d;
}
.list-group-item-info {
  color: #31708f;
  background-color: #d9edf7;
}
a.list-group-item-info,
button.list-group-item-info {
  color: #31708f;
}
a.list-group-item-info .list-group-item-heading,
button.list-group-item-info .list-group-item-heading {
  color: inherit;
}
a.list-group-item-info:hover,
button.list-group-item-info:hover,
a.list-group-item-info:focus,
button.list-group-item-info:focus {
  color: #31708f;
  background-color: #c4e3f3;
}
a.list-group-item-info.active,
button.list-group-item-info.active,
a.list-group-item-info.active:hover,
button.list-group-item-info.active:hover,
a.list-group-item-info.active:focus,
button.list-group-item-info.active:focus {
  color: #fff;
  background-color: #31708f;
  border-color: #31708f;
}
.list-group-item-warning {
  color: #8a6d3b;
  background-color: #fcf8e3;
}
a.list-group-item-warning,
button.list-group-item-warning {
  color: #8a6d3b;
}
a.list-group-item-warning .list-group-item-heading,
button.list-group-item-warning .list-group-item-heading {
  color: inherit;
}
a.list-group-item-warning:hover,
button.list-group-item-warning:hover,
a.list-group-item-warning:focus,
button.list-group-item-warning:focus {
  color: #8a6d3b;
  background-color: #faf2cc;
}
a.list-group-item-warning.active,
button.list-group-item-warning.active,
a.list-group-item-warning.active:hover,
button.list-group-item-warning.active:hover,
a.list-group-item-warning.active:focus,
button.list-group-item-warning.active:focus {
  color: #fff;
  background-color: #8a6d3b;
  border-color: #8a6d3b;
}
.list-group-item-danger {
  color: #a94442;
  background-color: #f2dede;
}
a.list-group-item-danger,
button.list-group-item-danger {
  color: #a94442;
}
a.list-group-item-danger .list-group-item-heading,
button.list-group-item-danger .list-group-item-heading {
  color: inherit;
}
a.list-group-item-danger:hover,
button.list-group-item-danger:hover,
a.list-group-item-danger:focus,
button.list-group-item-danger:focus {
  color: #a94442;
  background-color: #ebcccc;
}
a.list-group-item-danger.active,
button.list-group-item-danger.active,
a.list-group-item-danger.active:hover,
button.list-group-item-danger.active:hover,
a.list-group-item-danger.active:focus,
button.list-group-item-danger.active:focus {
  color: #fff;
  background-color: #a94442;
  border-color: #a94442;
}
.list-group-item-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.list-group-item-text {
  margin-bottom: 0;
  line-height: 1.3;
}
.panel {
  margin-bottom: 18px;
  background-color: #fff;
  border: 1px solid transparent;
  border-radius: 2px;
  -webkit-box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
}
.panel-body {
  padding: 15px;
}
.panel-heading {
  padding: 10px 15px;
  border-bottom: 1px solid transparent;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel-heading > .dropdown .dropdown-toggle {
  color: inherit;
}
.panel-title {
  margin-top: 0;
  margin-bottom: 0;
  font-size: 15px;
  color: inherit;
}
.panel-title > a,
.panel-title > small,
.panel-title > .small,
.panel-title > small > a,
.panel-title > .small > a {
  color: inherit;
}
.panel-footer {
  padding: 10px 15px;
  background-color: #f5f5f5;
  border-top: 1px solid #ddd;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .list-group,
.panel > .panel-collapse > .list-group {
  margin-bottom: 0;
}
.panel > .list-group .list-group-item,
.panel > .panel-collapse > .list-group .list-group-item {
  border-width: 1px 0;
  border-radius: 0;
}
.panel > .list-group:first-child .list-group-item:first-child,
.panel > .panel-collapse > .list-group:first-child .list-group-item:first-child {
  border-top: 0;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .list-group:last-child .list-group-item:last-child,
.panel > .panel-collapse > .list-group:last-child .list-group-item:last-child {
  border-bottom: 0;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .panel-heading + .panel-collapse > .list-group .list-group-item:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.panel-heading + .list-group .list-group-item:first-child {
  border-top-width: 0;
}
.list-group + .panel-footer {
  border-top-width: 0;
}
.panel > .table,
.panel > .table-responsive > .table,
.panel > .panel-collapse > .table {
  margin-bottom: 0;
}
.panel > .table caption,
.panel > .table-responsive > .table caption,
.panel > .panel-collapse > .table caption {
  padding-left: 15px;
  padding-right: 15px;
}
.panel > .table:first-child,
.panel > .table-responsive:first-child > .table:first-child {
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child {
  border-top-left-radius: 1px;
  border-top-right-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:first-child {
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:last-child {
  border-top-right-radius: 1px;
}
.panel > .table:last-child,
.panel > .table-responsive:last-child > .table:last-child {
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child {
  border-bottom-left-radius: 1px;
  border-bottom-right-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:first-child {
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:last-child {
  border-bottom-right-radius: 1px;
}
.panel > .panel-body + .table,
.panel > .panel-body + .table-responsive,
.panel > .table + .panel-body,
.panel > .table-responsive + .panel-body {
  border-top: 1px solid #ddd;
}
.panel > .table > tbody:first-child > tr:first-child th,
.panel > .table > tbody:first-child > tr:first-child td {
  border-top: 0;
}
.panel > .table-bordered,
.panel > .table-responsive > .table-bordered {
  border: 0;
}
.panel > .table-bordered > thead > tr > th:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:first-child,
.panel > .table-bordered > tbody > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:first-child,
.panel > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-bordered > thead > tr > td:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:first-child,
.panel > .table-bordered > tbody > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:first-child,
.panel > .table-bordered > tfoot > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:first-child {
  border-left: 0;
}
.panel > .table-bordered > thead > tr > th:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:last-child,
.panel > .table-bordered > tbody > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:last-child,
.panel > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-bordered > thead > tr > td:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:last-child,
.panel > .table-bordered > tbody > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:last-child,
.panel > .table-bordered > tfoot > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:last-child {
  border-right: 0;
}
.panel > .table-bordered > thead > tr:first-child > td,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > td,
.panel > .table-bordered > tbody > tr:first-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > td,
.panel > .table-bordered > thead > tr:first-child > th,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > th,
.panel > .table-bordered > tbody > tr:first-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > th {
  border-bottom: 0;
}
.panel > .table-bordered > tbody > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > td,
.panel > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-bordered > tbody > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > th,
.panel > .table-bordered > tfoot > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > th {
  border-bottom: 0;
}
.panel > .table-responsive {
  border: 0;
  margin-bottom: 0;
}
.panel-group {
  margin-bottom: 18px;
}
.panel-group .panel {
  margin-bottom: 0;
  border-radius: 2px;
}
.panel-group .panel + .panel {
  margin-top: 5px;
}
.panel-group .panel-heading {
  border-bottom: 0;
}
.panel-group .panel-heading + .panel-collapse > .panel-body,
.panel-group .panel-heading + .panel-collapse > .list-group {
  border-top: 1px solid #ddd;
}
.panel-group .panel-footer {
  border-top: 0;
}
.panel-group .panel-footer + .panel-collapse .panel-body {
  border-bottom: 1px solid #ddd;
}
.panel-default {
  border-color: #ddd;
}
.panel-default > .panel-heading {
  color: #333333;
  background-color: #f5f5f5;
  border-color: #ddd;
}
.panel-default > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ddd;
}
.panel-default > .panel-heading .badge {
  color: #f5f5f5;
  background-color: #333333;
}
.panel-default > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ddd;
}
.panel-primary {
  border-color: #337ab7;
}
.panel-primary > .panel-heading {
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.panel-primary > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #337ab7;
}
.panel-primary > .panel-heading .badge {
  color: #337ab7;
  background-color: #fff;
}
.panel-primary > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #337ab7;
}
.panel-success {
  border-color: #d6e9c6;
}
.panel-success > .panel-heading {
  color: #3c763d;
  background-color: #dff0d8;
  border-color: #d6e9c6;
}
.panel-success > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #d6e9c6;
}
.panel-success > .panel-heading .badge {
  color: #dff0d8;
  background-color: #3c763d;
}
.panel-success > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #d6e9c6;
}
.panel-info {
  border-color: #bce8f1;
}
.panel-info > .panel-heading {
  color: #31708f;
  background-color: #d9edf7;
  border-color: #bce8f1;
}
.panel-info > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #bce8f1;
}
.panel-info > .panel-heading .badge {
  color: #d9edf7;
  background-color: #31708f;
}
.panel-info > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #bce8f1;
}
.panel-warning {
  border-color: #faebcc;
}
.panel-warning > .panel-heading {
  color: #8a6d3b;
  background-color: #fcf8e3;
  border-color: #faebcc;
}
.panel-warning > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #faebcc;
}
.panel-warning > .panel-heading .badge {
  color: #fcf8e3;
  background-color: #8a6d3b;
}
.panel-warning > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #faebcc;
}
.panel-danger {
  border-color: #ebccd1;
}
.panel-danger > .panel-heading {
  color: #a94442;
  background-color: #f2dede;
  border-color: #ebccd1;
}
.panel-danger > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ebccd1;
}
.panel-danger > .panel-heading .badge {
  color: #f2dede;
  background-color: #a94442;
}
.panel-danger > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ebccd1;
}
.embed-responsive {
  position: relative;
  display: block;
  height: 0;
  padding: 0;
  overflow: hidden;
}
.embed-responsive .embed-responsive-item,
.embed-responsive iframe,
.embed-responsive embed,
.embed-responsive object,
.embed-responsive video {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  height: 100%;
  width: 100%;
  border: 0;
}
.embed-responsive-16by9 {
  padding-bottom: 56.25%;
}
.embed-responsive-4by3 {
  padding-bottom: 75%;
}
.well {
  min-height: 20px;
  padding: 19px;
  margin-bottom: 20px;
  background-color: #f5f5f5;
  border: 1px solid #e3e3e3;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
}
.well blockquote {
  border-color: #ddd;
  border-color: rgba(0, 0, 0, 0.15);
}
.well-lg {
  padding: 24px;
  border-radius: 3px;
}
.well-sm {
  padding: 9px;
  border-radius: 1px;
}
.close {
  float: right;
  font-size: 19.5px;
  font-weight: bold;
  line-height: 1;
  color: #000;
  text-shadow: 0 1px 0 #fff;
  opacity: 0.2;
  filter: alpha(opacity=20);
}
.close:hover,
.close:focus {
  color: #000;
  text-decoration: none;
  cursor: pointer;
  opacity: 0.5;
  filter: alpha(opacity=50);
}
button.close {
  padding: 0;
  cursor: pointer;
  background: transparent;
  border: 0;
  -webkit-appearance: none;
}
.modal-open {
  overflow: hidden;
}
.modal {
  display: none;
  overflow: hidden;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1050;
  -webkit-overflow-scrolling: touch;
  outline: 0;
}
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, -25%);
  -ms-transform: translate(0, -25%);
  -o-transform: translate(0, -25%);
  transform: translate(0, -25%);
  -webkit-transition: -webkit-transform 0.3s ease-out;
  -moz-transition: -moz-transform 0.3s ease-out;
  -o-transition: -o-transform 0.3s ease-out;
  transition: transform 0.3s ease-out;
}
.modal.in .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
.modal-open .modal {
  overflow-x: hidden;
  overflow-y: auto;
}
.modal-dialog {
  position: relative;
  width: auto;
  margin: 10px;
}
.modal-content {
  position: relative;
  background-color: #fff;
  border: 1px solid #999;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  background-clip: padding-box;
  outline: 0;
}
.modal-backdrop {
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1040;
  background-color: #000;
}
.modal-backdrop.fade {
  opacity: 0;
  filter: alpha(opacity=0);
}
.modal-backdrop.in {
  opacity: 0.5;
  filter: alpha(opacity=50);
}
.modal-header {
  padding: 15px;
  border-bottom: 1px solid #e5e5e5;
}
.modal-header .close {
  margin-top: -2px;
}
.modal-title {
  margin: 0;
  line-height: 1.42857143;
}
.modal-body {
  position: relative;
  padding: 15px;
}
.modal-footer {
  padding: 15px;
  text-align: right;
  border-top: 1px solid #e5e5e5;
}
.modal-footer .btn + .btn {
  margin-left: 5px;
  margin-bottom: 0;
}
.modal-footer .btn-group .btn + .btn {
  margin-left: -1px;
}
.modal-footer .btn-block + .btn-block {
  margin-left: 0;
}
.modal-scrollbar-measure {
  position: absolute;
  top: -9999px;
  width: 50px;
  height: 50px;
  overflow: scroll;
}
@media (min-width: 768px) {
  .modal-dialog {
    width: 600px;
    margin: 30px auto;
  }
  .modal-content {
    -webkit-box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
  }
  .modal-sm {
    width: 300px;
  }
}
@media (min-width: 992px) {
  .modal-lg {
    width: 900px;
  }
}
.tooltip {
  position: absolute;
  z-index: 1070;
  display: block;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 12px;
  opacity: 0;
  filter: alpha(opacity=0);
}
.tooltip.in {
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.tooltip.top {
  margin-top: -3px;
  padding: 5px 0;
}
.tooltip.right {
  margin-left: 3px;
  padding: 0 5px;
}
.tooltip.bottom {
  margin-top: 3px;
  padding: 5px 0;
}
.tooltip.left {
  margin-left: -3px;
  padding: 0 5px;
}
.tooltip-inner {
  max-width: 200px;
  padding: 3px 8px;
  color: #fff;
  text-align: center;
  background-color: #000;
  border-radius: 2px;
}
.tooltip-arrow {
  position: absolute;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.tooltip.top .tooltip-arrow {
  bottom: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-left .tooltip-arrow {
  bottom: 0;
  right: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-right .tooltip-arrow {
  bottom: 0;
  left: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.right .tooltip-arrow {
  top: 50%;
  left: 0;
  margin-top: -5px;
  border-width: 5px 5px 5px 0;
  border-right-color: #000;
}
.tooltip.left .tooltip-arrow {
  top: 50%;
  right: 0;
  margin-top: -5px;
  border-width: 5px 0 5px 5px;
  border-left-color: #000;
}
.tooltip.bottom .tooltip-arrow {
  top: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-left .tooltip-arrow {
  top: 0;
  right: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-right .tooltip-arrow {
  top: 0;
  left: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.popover {
  position: absolute;
  top: 0;
  left: 0;
  z-index: 1060;
  display: none;
  max-width: 276px;
  padding: 1px;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 13px;
  background-color: #fff;
  background-clip: padding-box;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
  box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
}
.popover.top {
  margin-top: -10px;
}
.popover.right {
  margin-left: 10px;
}
.popover.bottom {
  margin-top: 10px;
}
.popover.left {
  margin-left: -10px;
}
.popover-title {
  margin: 0;
  padding: 8px 14px;
  font-size: 13px;
  background-color: #f7f7f7;
  border-bottom: 1px solid #ebebeb;
  border-radius: 2px 2px 0 0;
}
.popover-content {
  padding: 9px 14px;
}
.popover > .arrow,
.popover > .arrow:after {
  position: absolute;
  display: block;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.popover > .arrow {
  border-width: 11px;
}
.popover > .arrow:after {
  border-width: 10px;
  content: "";
}
.popover.top > .arrow {
  left: 50%;
  margin-left: -11px;
  border-bottom-width: 0;
  border-top-color: #999999;
  border-top-color: rgba(0, 0, 0, 0.25);
  bottom: -11px;
}
.popover.top > .arrow:after {
  content: " ";
  bottom: 1px;
  margin-left: -10px;
  border-bottom-width: 0;
  border-top-color: #fff;
}
.popover.right > .arrow {
  top: 50%;
  left: -11px;
  margin-top: -11px;
  border-left-width: 0;
  border-right-color: #999999;
  border-right-color: rgba(0, 0, 0, 0.25);
}
.popover.right > .arrow:after {
  content: " ";
  left: 1px;
  bottom: -10px;
  border-left-width: 0;
  border-right-color: #fff;
}
.popover.bottom > .arrow {
  left: 50%;
  margin-left: -11px;
  border-top-width: 0;
  border-bottom-color: #999999;
  border-bottom-color: rgba(0, 0, 0, 0.25);
  top: -11px;
}
.popover.bottom > .arrow:after {
  content: " ";
  top: 1px;
  margin-left: -10px;
  border-top-width: 0;
  border-bottom-color: #fff;
}
.popover.left > .arrow {
  top: 50%;
  right: -11px;
  margin-top: -11px;
  border-right-width: 0;
  border-left-color: #999999;
  border-left-color: rgba(0, 0, 0, 0.25);
}
.popover.left > .arrow:after {
  content: " ";
  right: 1px;
  border-right-width: 0;
  border-left-color: #fff;
  bottom: -10px;
}
.carousel {
  position: relative;
}
.carousel-inner {
  position: relative;
  overflow: hidden;
  width: 100%;
}
.carousel-inner > .item {
  display: none;
  position: relative;
  -webkit-transition: 0.6s ease-in-out left;
  -o-transition: 0.6s ease-in-out left;
  transition: 0.6s ease-in-out left;
}
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  line-height: 1;
}
@media all and (transform-3d), (-webkit-transform-3d) {
  .carousel-inner > .item {
    -webkit-transition: -webkit-transform 0.6s ease-in-out;
    -moz-transition: -moz-transform 0.6s ease-in-out;
    -o-transition: -o-transform 0.6s ease-in-out;
    transition: transform 0.6s ease-in-out;
    -webkit-backface-visibility: hidden;
    -moz-backface-visibility: hidden;
    backface-visibility: hidden;
    -webkit-perspective: 1000px;
    -moz-perspective: 1000px;
    perspective: 1000px;
  }
  .carousel-inner > .item.next,
  .carousel-inner > .item.active.right {
    -webkit-transform: translate3d(100%, 0, 0);
    transform: translate3d(100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.prev,
  .carousel-inner > .item.active.left {
    -webkit-transform: translate3d(-100%, 0, 0);
    transform: translate3d(-100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.next.left,
  .carousel-inner > .item.prev.right,
  .carousel-inner > .item.active {
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
    left: 0;
  }
}
.carousel-inner > .active,
.carousel-inner > .next,
.carousel-inner > .prev {
  display: block;
}
.carousel-inner > .active {
  left: 0;
}
.carousel-inner > .next,
.carousel-inner > .prev {
  position: absolute;
  top: 0;
  width: 100%;
}
.carousel-inner > .next {
  left: 100%;
}
.carousel-inner > .prev {
  left: -100%;
}
.carousel-inner > .next.left,
.carousel-inner > .prev.right {
  left: 0;
}
.carousel-inner > .active.left {
  left: -100%;
}
.carousel-inner > .active.right {
  left: 100%;
}
.carousel-control {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  width: 15%;
  opacity: 0.5;
  filter: alpha(opacity=50);
  font-size: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
  background-color: rgba(0, 0, 0, 0);
}
.carousel-control.left {
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#80000000', endColorstr='#00000000', GradientType=1);
}
.carousel-control.right {
  left: auto;
  right: 0;
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#00000000', endColorstr='#80000000', GradientType=1);
}
.carousel-control:hover,
.carousel-control:focus {
  outline: 0;
  color: #fff;
  text-decoration: none;
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.carousel-control .icon-prev,
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-left,
.carousel-control .glyphicon-chevron-right {
  position: absolute;
  top: 50%;
  margin-top: -10px;
  z-index: 5;
  display: inline-block;
}
.carousel-control .icon-prev,
.carousel-control .glyphicon-chevron-left {
  left: 50%;
  margin-left: -10px;
}
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-right {
  right: 50%;
  margin-right: -10px;
}
.carousel-control .icon-prev,
.carousel-control .icon-next {
  width: 20px;
  height: 20px;
  line-height: 1;
  font-family: serif;
}
.carousel-control .icon-prev:before {
  content: '\2039';
}
.carousel-control .icon-next:before {
  content: '\203a';
}
.carousel-indicators {
  position: absolute;
  bottom: 10px;
  left: 50%;
  z-index: 15;
  width: 60%;
  margin-left: -30%;
  padding-left: 0;
  list-style: none;
  text-align: center;
}
.carousel-indicators li {
  display: inline-block;
  width: 10px;
  height: 10px;
  margin: 1px;
  text-indent: -999px;
  border: 1px solid #fff;
  border-radius: 10px;
  cursor: pointer;
  background-color: #000 \9;
  background-color: rgba(0, 0, 0, 0);
}
.carousel-indicators .active {
  margin: 0;
  width: 12px;
  height: 12px;
  background-color: #fff;
}
.carousel-caption {
  position: absolute;
  left: 15%;
  right: 15%;
  bottom: 20px;
  z-index: 10;
  padding-top: 20px;
  padding-bottom: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
}
.carousel-caption .btn {
  text-shadow: none;
}
@media screen and (min-width: 768px) {
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-prev,
  .carousel-control .icon-next {
    width: 30px;
    height: 30px;
    margin-top: -10px;
    font-size: 30px;
  }
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .icon-prev {
    margin-left: -10px;
  }
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-next {
    margin-right: -10px;
  }
  .carousel-caption {
    left: 20%;
    right: 20%;
    padding-bottom: 30px;
  }
  .carousel-indicators {
    bottom: 20px;
  }
}
.clearfix:before,
.clearfix:after,
.dl-horizontal dd:before,
.dl-horizontal dd:after,
.container:before,
.container:after,
.container-fluid:before,
.container-fluid:after,
.row:before,
.row:after,
.form-horizontal .form-group:before,
.form-horizontal .form-group:after,
.btn-toolbar:before,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:before,
.btn-group-vertical > .btn-group:after,
.nav:before,
.nav:after,
.navbar:before,
.navbar:after,
.navbar-header:before,
.navbar-header:after,
.navbar-collapse:before,
.navbar-collapse:after,
.pager:before,
.pager:after,
.panel-body:before,
.panel-body:after,
.modal-header:before,
.modal-header:after,
.modal-footer:before,
.modal-footer:after,
.item_buttons:before,
.item_buttons:after {
  content: " ";
  display: table;
}
.clearfix:after,
.dl-horizontal dd:after,
.container:after,
.container-fluid:after,
.row:after,
.form-horizontal .form-group:after,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:after,
.nav:after,
.navbar:after,
.navbar-header:after,
.navbar-collapse:after,
.pager:after,
.panel-body:after,
.modal-header:after,
.modal-footer:after,
.item_buttons:after {
  clear: both;
}
.center-block {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.pull-right {
  float: right !important;
}
.pull-left {
  float: left !important;
}
.hide {
  display: none !important;
}
.show {
  display: block !important;
}
.invisible {
  visibility: hidden;
}
.text-hide {
  font: 0/0 a;
  color: transparent;
  text-shadow: none;
  background-color: transparent;
  border: 0;
}
.hidden {
  display: none !important;
}
.affix {
  position: fixed;
}
@-ms-viewport {
  width: device-width;
}
.visible-xs,
.visible-sm,
.visible-md,
.visible-lg {
  display: none !important;
}
.visible-xs-block,
.visible-xs-inline,
.visible-xs-inline-block,
.visible-sm-block,
.visible-sm-inline,
.visible-sm-inline-block,
.visible-md-block,
.visible-md-inline,
.visible-md-inline-block,
.visible-lg-block,
.visible-lg-inline,
.visible-lg-inline-block {
  display: none !important;
}
@media (max-width: 767px) {
  .visible-xs {
    display: block !important;
  }
  table.visible-xs {
    display: table !important;
  }
  tr.visible-xs {
    display: table-row !important;
  }
  th.visible-xs,
  td.visible-xs {
    display: table-cell !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-block {
    display: block !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline {
    display: inline !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm {
    display: block !important;
  }
  table.visible-sm {
    display: table !important;
  }
  tr.visible-sm {
    display: table-row !important;
  }
  th.visible-sm,
  td.visible-sm {
    display: table-cell !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-block {
    display: block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline {
    display: inline !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md {
    display: block !important;
  }
  table.visible-md {
    display: table !important;
  }
  tr.visible-md {
    display: table-row !important;
  }
  th.visible-md,
  td.visible-md {
    display: table-cell !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-block {
    display: block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline {
    display: inline !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg {
    display: block !important;
  }
  table.visible-lg {
    display: table !important;
  }
  tr.visible-lg {
    display: table-row !important;
  }
  th.visible-lg,
  td.visible-lg {
    display: table-cell !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-block {
    display: block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline {
    display: inline !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline-block {
    display: inline-block !important;
  }
}
@media (max-width: 767px) {
  .hidden-xs {
    display: none !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .hidden-sm {
    display: none !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .hidden-md {
    display: none !important;
  }
}
@media (min-width: 1200px) {
  .hidden-lg {
    display: none !important;
  }
}
.visible-print {
  display: none !important;
}
@media print {
  .visible-print {
    display: block !important;
  }
  table.visible-print {
    display: table !important;
  }
  tr.visible-print {
    display: table-row !important;
  }
  th.visible-print,
  td.visible-print {
    display: table-cell !important;
  }
}
.visible-print-block {
  display: none !important;
}
@media print {
  .visible-print-block {
    display: block !important;
  }
}
.visible-print-inline {
  display: none !important;
}
@media print {
  .visible-print-inline {
    display: inline !important;
  }
}
.visible-print-inline-block {
  display: none !important;
}
@media print {
  .visible-print-inline-block {
    display: inline-block !important;
  }
}
@media print {
  .hidden-print {
    display: none !important;
  }
}
/*!
*
* Font Awesome
*
*/
/*!
 *  Font Awesome 4.7.0 by @davegandy - http://fontawesome.io - @fontawesome
 *  License - http://fontawesome.io/license (Font: SIL OFL 1.1, CSS: MIT License)
 */
/* FONT PATH
 * -------------------------- */
@font-face {
  font-family: 'FontAwesome';
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?v=4.7.0');
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?#iefix&v=4.7.0') format('embedded-opentype'), url('../components/font-awesome/fonts/fontawesome-webfont.woff2?v=4.7.0') format('woff2'), url('../components/font-awesome/fonts/fontawesome-webfont.woff?v=4.7.0') format('woff'), url('../components/font-awesome/fonts/fontawesome-webfont.ttf?v=4.7.0') format('truetype'), url('../components/font-awesome/fonts/fontawesome-webfont.svg?v=4.7.0#fontawesomeregular') format('svg');
  font-weight: normal;
  font-style: normal;
}
.fa {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
/* makes the font 33% larger relative to the icon container */
.fa-lg {
  font-size: 1.33333333em;
  line-height: 0.75em;
  vertical-align: -15%;
}
.fa-2x {
  font-size: 2em;
}
.fa-3x {
  font-size: 3em;
}
.fa-4x {
  font-size: 4em;
}
.fa-5x {
  font-size: 5em;
}
.fa-fw {
  width: 1.28571429em;
  text-align: center;
}
.fa-ul {
  padding-left: 0;
  margin-left: 2.14285714em;
  list-style-type: none;
}
.fa-ul > li {
  position: relative;
}
.fa-li {
  position: absolute;
  left: -2.14285714em;
  width: 2.14285714em;
  top: 0.14285714em;
  text-align: center;
}
.fa-li.fa-lg {
  left: -1.85714286em;
}
.fa-border {
  padding: .2em .25em .15em;
  border: solid 0.08em #eee;
  border-radius: .1em;
}
.fa-pull-left {
  float: left;
}
.fa-pull-right {
  float: right;
}
.fa.fa-pull-left {
  margin-right: .3em;
}
.fa.fa-pull-right {
  margin-left: .3em;
}
/* Deprecated as of 4.4.0 */
.pull-right {
  float: right;
}
.pull-left {
  float: left;
}
.fa.pull-left {
  margin-right: .3em;
}
.fa.pull-right {
  margin-left: .3em;
}
.fa-spin {
  -webkit-animation: fa-spin 2s infinite linear;
  animation: fa-spin 2s infinite linear;
}
.fa-pulse {
  -webkit-animation: fa-spin 1s infinite steps(8);
  animation: fa-spin 1s infinite steps(8);
}
@-webkit-keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
@keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
.fa-rotate-90 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=1)";
  -webkit-transform: rotate(90deg);
  -ms-transform: rotate(90deg);
  transform: rotate(90deg);
}
.fa-rotate-180 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=2)";
  -webkit-transform: rotate(180deg);
  -ms-transform: rotate(180deg);
  transform: rotate(180deg);
}
.fa-rotate-270 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=3)";
  -webkit-transform: rotate(270deg);
  -ms-transform: rotate(270deg);
  transform: rotate(270deg);
}
.fa-flip-horizontal {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=0, mirror=1)";
  -webkit-transform: scale(-1, 1);
  -ms-transform: scale(-1, 1);
  transform: scale(-1, 1);
}
.fa-flip-vertical {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=2, mirror=1)";
  -webkit-transform: scale(1, -1);
  -ms-transform: scale(1, -1);
  transform: scale(1, -1);
}
:root .fa-rotate-90,
:root .fa-rotate-180,
:root .fa-rotate-270,
:root .fa-flip-horizontal,
:root .fa-flip-vertical {
  filter: none;
}
.fa-stack {
  position: relative;
  display: inline-block;
  width: 2em;
  height: 2em;
  line-height: 2em;
  vertical-align: middle;
}
.fa-stack-1x,
.fa-stack-2x {
  position: absolute;
  left: 0;
  width: 100%;
  text-align: center;
}
.fa-stack-1x {
  line-height: inherit;
}
.fa-stack-2x {
  font-size: 2em;
}
.fa-inverse {
  color: #fff;
}
/* Font Awesome uses the Unicode Private Use Area (PUA) to ensure screen
   readers do not read off random characters that represent icons */
.fa-glass:before {
  content: "\f000";
}
.fa-music:before {
  content: "\f001";
}
.fa-search:before {
  content: "\f002";
}
.fa-envelope-o:before {
  content: "\f003";
}
.fa-heart:before {
  content: "\f004";
}
.fa-star:before {
  content: "\f005";
}
.fa-star-o:before {
  content: "\f006";
}
.fa-user:before {
  content: "\f007";
}
.fa-film:before {
  content: "\f008";
}
.fa-th-large:before {
  content: "\f009";
}
.fa-th:before {
  content: "\f00a";
}
.fa-th-list:before {
  content: "\f00b";
}
.fa-check:before {
  content: "\f00c";
}
.fa-remove:before,
.fa-close:before,
.fa-times:before {
  content: "\f00d";
}
.fa-search-plus:before {
  content: "\f00e";
}
.fa-search-minus:before {
  content: "\f010";
}
.fa-power-off:before {
  content: "\f011";
}
.fa-signal:before {
  content: "\f012";
}
.fa-gear:before,
.fa-cog:before {
  content: "\f013";
}
.fa-trash-o:before {
  content: "\f014";
}
.fa-home:before {
  content: "\f015";
}
.fa-file-o:before {
  content: "\f016";
}
.fa-clock-o:before {
  content: "\f017";
}
.fa-road:before {
  content: "\f018";
}
.fa-download:before {
  content: "\f019";
}
.fa-arrow-circle-o-down:before {
  content: "\f01a";
}
.fa-arrow-circle-o-up:before {
  content: "\f01b";
}
.fa-inbox:before {
  content: "\f01c";
}
.fa-play-circle-o:before {
  content: "\f01d";
}
.fa-rotate-right:before,
.fa-repeat:before {
  content: "\f01e";
}
.fa-refresh:before {
  content: "\f021";
}
.fa-list-alt:before {
  content: "\f022";
}
.fa-lock:before {
  content: "\f023";
}
.fa-flag:before {
  content: "\f024";
}
.fa-headphones:before {
  content: "\f025";
}
.fa-volume-off:before {
  content: "\f026";
}
.fa-volume-down:before {
  content: "\f027";
}
.fa-volume-up:before {
  content: "\f028";
}
.fa-qrcode:before {
  content: "\f029";
}
.fa-barcode:before {
  content: "\f02a";
}
.fa-tag:before {
  content: "\f02b";
}
.fa-tags:before {
  content: "\f02c";
}
.fa-book:before {
  content: "\f02d";
}
.fa-bookmark:before {
  content: "\f02e";
}
.fa-print:before {
  content: "\f02f";
}
.fa-camera:before {
  content: "\f030";
}
.fa-font:before {
  content: "\f031";
}
.fa-bold:before {
  content: "\f032";
}
.fa-italic:before {
  content: "\f033";
}
.fa-text-height:before {
  content: "\f034";
}
.fa-text-width:before {
  content: "\f035";
}
.fa-align-left:before {
  content: "\f036";
}
.fa-align-center:before {
  content: "\f037";
}
.fa-align-right:before {
  content: "\f038";
}
.fa-align-justify:before {
  content: "\f039";
}
.fa-list:before {
  content: "\f03a";
}
.fa-dedent:before,
.fa-outdent:before {
  content: "\f03b";
}
.fa-indent:before {
  content: "\f03c";
}
.fa-video-camera:before {
  content: "\f03d";
}
.fa-photo:before,
.fa-image:before,
.fa-picture-o:before {
  content: "\f03e";
}
.fa-pencil:before {
  content: "\f040";
}
.fa-map-marker:before {
  content: "\f041";
}
.fa-adjust:before {
  content: "\f042";
}
.fa-tint:before {
  content: "\f043";
}
.fa-edit:before,
.fa-pencil-square-o:before {
  content: "\f044";
}
.fa-share-square-o:before {
  content: "\f045";
}
.fa-check-square-o:before {
  content: "\f046";
}
.fa-arrows:before {
  content: "\f047";
}
.fa-step-backward:before {
  content: "\f048";
}
.fa-fast-backward:before {
  content: "\f049";
}
.fa-backward:before {
  content: "\f04a";
}
.fa-play:before {
  content: "\f04b";
}
.fa-pause:before {
  content: "\f04c";
}
.fa-stop:before {
  content: "\f04d";
}
.fa-forward:before {
  content: "\f04e";
}
.fa-fast-forward:before {
  content: "\f050";
}
.fa-step-forward:before {
  content: "\f051";
}
.fa-eject:before {
  content: "\f052";
}
.fa-chevron-left:before {
  content: "\f053";
}
.fa-chevron-right:before {
  content: "\f054";
}
.fa-plus-circle:before {
  content: "\f055";
}
.fa-minus-circle:before {
  content: "\f056";
}
.fa-times-circle:before {
  content: "\f057";
}
.fa-check-circle:before {
  content: "\f058";
}
.fa-question-circle:before {
  content: "\f059";
}
.fa-info-circle:before {
  content: "\f05a";
}
.fa-crosshairs:before {
  content: "\f05b";
}
.fa-times-circle-o:before {
  content: "\f05c";
}
.fa-check-circle-o:before {
  content: "\f05d";
}
.fa-ban:before {
  content: "\f05e";
}
.fa-arrow-left:before {
  content: "\f060";
}
.fa-arrow-right:before {
  content: "\f061";
}
.fa-arrow-up:before {
  content: "\f062";
}
.fa-arrow-down:before {
  content: "\f063";
}
.fa-mail-forward:before,
.fa-share:before {
  content: "\f064";
}
.fa-expand:before {
  content: "\f065";
}
.fa-compress:before {
  content: "\f066";
}
.fa-plus:before {
  content: "\f067";
}
.fa-minus:before {
  content: "\f068";
}
.fa-asterisk:before {
  content: "\f069";
}
.fa-exclamation-circle:before {
  content: "\f06a";
}
.fa-gift:before {
  content: "\f06b";
}
.fa-leaf:before {
  content: "\f06c";
}
.fa-fire:before {
  content: "\f06d";
}
.fa-eye:before {
  content: "\f06e";
}
.fa-eye-slash:before {
  content: "\f070";
}
.fa-warning:before,
.fa-exclamation-triangle:before {
  content: "\f071";
}
.fa-plane:before {
  content: "\f072";
}
.fa-calendar:before {
  content: "\f073";
}
.fa-random:before {
  content: "\f074";
}
.fa-comment:before {
  content: "\f075";
}
.fa-magnet:before {
  content: "\f076";
}
.fa-chevron-up:before {
  content: "\f077";
}
.fa-chevron-down:before {
  content: "\f078";
}
.fa-retweet:before {
  content: "\f079";
}
.fa-shopping-cart:before {
  content: "\f07a";
}
.fa-folder:before {
  content: "\f07b";
}
.fa-folder-open:before {
  content: "\f07c";
}
.fa-arrows-v:before {
  content: "\f07d";
}
.fa-arrows-h:before {
  content: "\f07e";
}
.fa-bar-chart-o:before,
.fa-bar-chart:before {
  content: "\f080";
}
.fa-twitter-square:before {
  content: "\f081";
}
.fa-facebook-square:before {
  content: "\f082";
}
.fa-camera-retro:before {
  content: "\f083";
}
.fa-key:before {
  content: "\f084";
}
.fa-gears:before,
.fa-cogs:before {
  content: "\f085";
}
.fa-comments:before {
  content: "\f086";
}
.fa-thumbs-o-up:before {
  content: "\f087";
}
.fa-thumbs-o-down:before {
  content: "\f088";
}
.fa-star-half:before {
  content: "\f089";
}
.fa-heart-o:before {
  content: "\f08a";
}
.fa-sign-out:before {
  content: "\f08b";
}
.fa-linkedin-square:before {
  content: "\f08c";
}
.fa-thumb-tack:before {
  content: "\f08d";
}
.fa-external-link:before {
  content: "\f08e";
}
.fa-sign-in:before {
  content: "\f090";
}
.fa-trophy:before {
  content: "\f091";
}
.fa-github-square:before {
  content: "\f092";
}
.fa-upload:before {
  content: "\f093";
}
.fa-lemon-o:before {
  content: "\f094";
}
.fa-phone:before {
  content: "\f095";
}
.fa-square-o:before {
  content: "\f096";
}
.fa-bookmark-o:before {
  content: "\f097";
}
.fa-phone-square:before {
  content: "\f098";
}
.fa-twitter:before {
  content: "\f099";
}
.fa-facebook-f:before,
.fa-facebook:before {
  content: "\f09a";
}
.fa-github:before {
  content: "\f09b";
}
.fa-unlock:before {
  content: "\f09c";
}
.fa-credit-card:before {
  content: "\f09d";
}
.fa-feed:before,
.fa-rss:before {
  content: "\f09e";
}
.fa-hdd-o:before {
  content: "\f0a0";
}
.fa-bullhorn:before {
  content: "\f0a1";
}
.fa-bell:before {
  content: "\f0f3";
}
.fa-certificate:before {
  content: "\f0a3";
}
.fa-hand-o-right:before {
  content: "\f0a4";
}
.fa-hand-o-left:before {
  content: "\f0a5";
}
.fa-hand-o-up:before {
  content: "\f0a6";
}
.fa-hand-o-down:before {
  content: "\f0a7";
}
.fa-arrow-circle-left:before {
  content: "\f0a8";
}
.fa-arrow-circle-right:before {
  content: "\f0a9";
}
.fa-arrow-circle-up:before {
  content: "\f0aa";
}
.fa-arrow-circle-down:before {
  content: "\f0ab";
}
.fa-globe:before {
  content: "\f0ac";
}
.fa-wrench:before {
  content: "\f0ad";
}
.fa-tasks:before {
  content: "\f0ae";
}
.fa-filter:before {
  content: "\f0b0";
}
.fa-briefcase:before {
  content: "\f0b1";
}
.fa-arrows-alt:before {
  content: "\f0b2";
}
.fa-group:before,
.fa-users:before {
  content: "\f0c0";
}
.fa-chain:before,
.fa-link:before {
  content: "\f0c1";
}
.fa-cloud:before {
  content: "\f0c2";
}
.fa-flask:before {
  content: "\f0c3";
}
.fa-cut:before,
.fa-scissors:before {
  content: "\f0c4";
}
.fa-copy:before,
.fa-files-o:before {
  content: "\f0c5";
}
.fa-paperclip:before {
  content: "\f0c6";
}
.fa-save:before,
.fa-floppy-o:before {
  content: "\f0c7";
}
.fa-square:before {
  content: "\f0c8";
}
.fa-navicon:before,
.fa-reorder:before,
.fa-bars:before {
  content: "\f0c9";
}
.fa-list-ul:before {
  content: "\f0ca";
}
.fa-list-ol:before {
  content: "\f0cb";
}
.fa-strikethrough:before {
  content: "\f0cc";
}
.fa-underline:before {
  content: "\f0cd";
}
.fa-table:before {
  content: "\f0ce";
}
.fa-magic:before {
  content: "\f0d0";
}
.fa-truck:before {
  content: "\f0d1";
}
.fa-pinterest:before {
  content: "\f0d2";
}
.fa-pinterest-square:before {
  content: "\f0d3";
}
.fa-google-plus-square:before {
  content: "\f0d4";
}
.fa-google-plus:before {
  content: "\f0d5";
}
.fa-money:before {
  content: "\f0d6";
}
.fa-caret-down:before {
  content: "\f0d7";
}
.fa-caret-up:before {
  content: "\f0d8";
}
.fa-caret-left:before {
  content: "\f0d9";
}
.fa-caret-right:before {
  content: "\f0da";
}
.fa-columns:before {
  content: "\f0db";
}
.fa-unsorted:before,
.fa-sort:before {
  content: "\f0dc";
}
.fa-sort-down:before,
.fa-sort-desc:before {
  content: "\f0dd";
}
.fa-sort-up:before,
.fa-sort-asc:before {
  content: "\f0de";
}
.fa-envelope:before {
  content: "\f0e0";
}
.fa-linkedin:before {
  content: "\f0e1";
}
.fa-rotate-left:before,
.fa-undo:before {
  content: "\f0e2";
}
.fa-legal:before,
.fa-gavel:before {
  content: "\f0e3";
}
.fa-dashboard:before,
.fa-tachometer:before {
  content: "\f0e4";
}
.fa-comment-o:before {
  content: "\f0e5";
}
.fa-comments-o:before {
  content: "\f0e6";
}
.fa-flash:before,
.fa-bolt:before {
  content: "\f0e7";
}
.fa-sitemap:before {
  content: "\f0e8";
}
.fa-umbrella:before {
  content: "\f0e9";
}
.fa-paste:before,
.fa-clipboard:before {
  content: "\f0ea";
}
.fa-lightbulb-o:before {
  content: "\f0eb";
}
.fa-exchange:before {
  content: "\f0ec";
}
.fa-cloud-download:before {
  content: "\f0ed";
}
.fa-cloud-upload:before {
  content: "\f0ee";
}
.fa-user-md:before {
  content: "\f0f0";
}
.fa-stethoscope:before {
  content: "\f0f1";
}
.fa-suitcase:before {
  content: "\f0f2";
}
.fa-bell-o:before {
  content: "\f0a2";
}
.fa-coffee:before {
  content: "\f0f4";
}
.fa-cutlery:before {
  content: "\f0f5";
}
.fa-file-text-o:before {
  content: "\f0f6";
}
.fa-building-o:before {
  content: "\f0f7";
}
.fa-hospital-o:before {
  content: "\f0f8";
}
.fa-ambulance:before {
  content: "\f0f9";
}
.fa-medkit:before {
  content: "\f0fa";
}
.fa-fighter-jet:before {
  content: "\f0fb";
}
.fa-beer:before {
  content: "\f0fc";
}
.fa-h-square:before {
  content: "\f0fd";
}
.fa-plus-square:before {
  content: "\f0fe";
}
.fa-angle-double-left:before {
  content: "\f100";
}
.fa-angle-double-right:before {
  content: "\f101";
}
.fa-angle-double-up:before {
  content: "\f102";
}
.fa-angle-double-down:before {
  content: "\f103";
}
.fa-angle-left:before {
  content: "\f104";
}
.fa-angle-right:before {
  content: "\f105";
}
.fa-angle-up:before {
  content: "\f106";
}
.fa-angle-down:before {
  content: "\f107";
}
.fa-desktop:before {
  content: "\f108";
}
.fa-laptop:before {
  content: "\f109";
}
.fa-tablet:before {
  content: "\f10a";
}
.fa-mobile-phone:before,
.fa-mobile:before {
  content: "\f10b";
}
.fa-circle-o:before {
  content: "\f10c";
}
.fa-quote-left:before {
  content: "\f10d";
}
.fa-quote-right:before {
  content: "\f10e";
}
.fa-spinner:before {
  content: "\f110";
}
.fa-circle:before {
  content: "\f111";
}
.fa-mail-reply:before,
.fa-reply:before {
  content: "\f112";
}
.fa-github-alt:before {
  content: "\f113";
}
.fa-folder-o:before {
  content: "\f114";
}
.fa-folder-open-o:before {
  content: "\f115";
}
.fa-smile-o:before {
  content: "\f118";
}
.fa-frown-o:before {
  content: "\f119";
}
.fa-meh-o:before {
  content: "\f11a";
}
.fa-gamepad:before {
  content: "\f11b";
}
.fa-keyboard-o:before {
  content: "\f11c";
}
.fa-flag-o:before {
  content: "\f11d";
}
.fa-flag-checkered:before {
  content: "\f11e";
}
.fa-terminal:before {
  content: "\f120";
}
.fa-code:before {
  content: "\f121";
}
.fa-mail-reply-all:before,
.fa-reply-all:before {
  content: "\f122";
}
.fa-star-half-empty:before,
.fa-star-half-full:before,
.fa-star-half-o:before {
  content: "\f123";
}
.fa-location-arrow:before {
  content: "\f124";
}
.fa-crop:before {
  content: "\f125";
}
.fa-code-fork:before {
  content: "\f126";
}
.fa-unlink:before,
.fa-chain-broken:before {
  content: "\f127";
}
.fa-question:before {
  content: "\f128";
}
.fa-info:before {
  content: "\f129";
}
.fa-exclamation:before {
  content: "\f12a";
}
.fa-superscript:before {
  content: "\f12b";
}
.fa-subscript:before {
  content: "\f12c";
}
.fa-eraser:before {
  content: "\f12d";
}
.fa-puzzle-piece:before {
  content: "\f12e";
}
.fa-microphone:before {
  content: "\f130";
}
.fa-microphone-slash:before {
  content: "\f131";
}
.fa-shield:before {
  content: "\f132";
}
.fa-calendar-o:before {
  content: "\f133";
}
.fa-fire-extinguisher:before {
  content: "\f134";
}
.fa-rocket:before {
  content: "\f135";
}
.fa-maxcdn:before {
  content: "\f136";
}
.fa-chevron-circle-left:before {
  content: "\f137";
}
.fa-chevron-circle-right:before {
  content: "\f138";
}
.fa-chevron-circle-up:before {
  content: "\f139";
}
.fa-chevron-circle-down:before {
  content: "\f13a";
}
.fa-html5:before {
  content: "\f13b";
}
.fa-css3:before {
  content: "\f13c";
}
.fa-anchor:before {
  content: "\f13d";
}
.fa-unlock-alt:before {
  content: "\f13e";
}
.fa-bullseye:before {
  content: "\f140";
}
.fa-ellipsis-h:before {
  content: "\f141";
}
.fa-ellipsis-v:before {
  content: "\f142";
}
.fa-rss-square:before {
  content: "\f143";
}
.fa-play-circle:before {
  content: "\f144";
}
.fa-ticket:before {
  content: "\f145";
}
.fa-minus-square:before {
  content: "\f146";
}
.fa-minus-square-o:before {
  content: "\f147";
}
.fa-level-up:before {
  content: "\f148";
}
.fa-level-down:before {
  content: "\f149";
}
.fa-check-square:before {
  content: "\f14a";
}
.fa-pencil-square:before {
  content: "\f14b";
}
.fa-external-link-square:before {
  content: "\f14c";
}
.fa-share-square:before {
  content: "\f14d";
}
.fa-compass:before {
  content: "\f14e";
}
.fa-toggle-down:before,
.fa-caret-square-o-down:before {
  content: "\f150";
}
.fa-toggle-up:before,
.fa-caret-square-o-up:before {
  content: "\f151";
}
.fa-toggle-right:before,
.fa-caret-square-o-right:before {
  content: "\f152";
}
.fa-euro:before,
.fa-eur:before {
  content: "\f153";
}
.fa-gbp:before {
  content: "\f154";
}
.fa-dollar:before,
.fa-usd:before {
  content: "\f155";
}
.fa-rupee:before,
.fa-inr:before {
  content: "\f156";
}
.fa-cny:before,
.fa-rmb:before,
.fa-yen:before,
.fa-jpy:before {
  content: "\f157";
}
.fa-ruble:before,
.fa-rouble:before,
.fa-rub:before {
  content: "\f158";
}
.fa-won:before,
.fa-krw:before {
  content: "\f159";
}
.fa-bitcoin:before,
.fa-btc:before {
  content: "\f15a";
}
.fa-file:before {
  content: "\f15b";
}
.fa-file-text:before {
  content: "\f15c";
}
.fa-sort-alpha-asc:before {
  content: "\f15d";
}
.fa-sort-alpha-desc:before {
  content: "\f15e";
}
.fa-sort-amount-asc:before {
  content: "\f160";
}
.fa-sort-amount-desc:before {
  content: "\f161";
}
.fa-sort-numeric-asc:before {
  content: "\f162";
}
.fa-sort-numeric-desc:before {
  content: "\f163";
}
.fa-thumbs-up:before {
  content: "\f164";
}
.fa-thumbs-down:before {
  content: "\f165";
}
.fa-youtube-square:before {
  content: "\f166";
}
.fa-youtube:before {
  content: "\f167";
}
.fa-xing:before {
  content: "\f168";
}
.fa-xing-square:before {
  content: "\f169";
}
.fa-youtube-play:before {
  content: "\f16a";
}
.fa-dropbox:before {
  content: "\f16b";
}
.fa-stack-overflow:before {
  content: "\f16c";
}
.fa-instagram:before {
  content: "\f16d";
}
.fa-flickr:before {
  content: "\f16e";
}
.fa-adn:before {
  content: "\f170";
}
.fa-bitbucket:before {
  content: "\f171";
}
.fa-bitbucket-square:before {
  content: "\f172";
}
.fa-tumblr:before {
  content: "\f173";
}
.fa-tumblr-square:before {
  content: "\f174";
}
.fa-long-arrow-down:before {
  content: "\f175";
}
.fa-long-arrow-up:before {
  content: "\f176";
}
.fa-long-arrow-left:before {
  content: "\f177";
}
.fa-long-arrow-right:before {
  content: "\f178";
}
.fa-apple:before {
  content: "\f179";
}
.fa-windows:before {
  content: "\f17a";
}
.fa-android:before {
  content: "\f17b";
}
.fa-linux:before {
  content: "\f17c";
}
.fa-dribbble:before {
  content: "\f17d";
}
.fa-skype:before {
  content: "\f17e";
}
.fa-foursquare:before {
  content: "\f180";
}
.fa-trello:before {
  content: "\f181";
}
.fa-female:before {
  content: "\f182";
}
.fa-male:before {
  content: "\f183";
}
.fa-gittip:before,
.fa-gratipay:before {
  content: "\f184";
}
.fa-sun-o:before {
  content: "\f185";
}
.fa-moon-o:before {
  content: "\f186";
}
.fa-archive:before {
  content: "\f187";
}
.fa-bug:before {
  content: "\f188";
}
.fa-vk:before {
  content: "\f189";
}
.fa-weibo:before {
  content: "\f18a";
}
.fa-renren:before {
  content: "\f18b";
}
.fa-pagelines:before {
  content: "\f18c";
}
.fa-stack-exchange:before {
  content: "\f18d";
}
.fa-arrow-circle-o-right:before {
  content: "\f18e";
}
.fa-arrow-circle-o-left:before {
  content: "\f190";
}
.fa-toggle-left:before,
.fa-caret-square-o-left:before {
  content: "\f191";
}
.fa-dot-circle-o:before {
  content: "\f192";
}
.fa-wheelchair:before {
  content: "\f193";
}
.fa-vimeo-square:before {
  content: "\f194";
}
.fa-turkish-lira:before,
.fa-try:before {
  content: "\f195";
}
.fa-plus-square-o:before {
  content: "\f196";
}
.fa-space-shuttle:before {
  content: "\f197";
}
.fa-slack:before {
  content: "\f198";
}
.fa-envelope-square:before {
  content: "\f199";
}
.fa-wordpress:before {
  content: "\f19a";
}
.fa-openid:before {
  content: "\f19b";
}
.fa-institution:before,
.fa-bank:before,
.fa-university:before {
  content: "\f19c";
}
.fa-mortar-board:before,
.fa-graduation-cap:before {
  content: "\f19d";
}
.fa-yahoo:before {
  content: "\f19e";
}
.fa-google:before {
  content: "\f1a0";
}
.fa-reddit:before {
  content: "\f1a1";
}
.fa-reddit-square:before {
  content: "\f1a2";
}
.fa-stumbleupon-circle:before {
  content: "\f1a3";
}
.fa-stumbleupon:before {
  content: "\f1a4";
}
.fa-delicious:before {
  content: "\f1a5";
}
.fa-digg:before {
  content: "\f1a6";
}
.fa-pied-piper-pp:before {
  content: "\f1a7";
}
.fa-pied-piper-alt:before {
  content: "\f1a8";
}
.fa-drupal:before {
  content: "\f1a9";
}
.fa-joomla:before {
  content: "\f1aa";
}
.fa-language:before {
  content: "\f1ab";
}
.fa-fax:before {
  content: "\f1ac";
}
.fa-building:before {
  content: "\f1ad";
}
.fa-child:before {
  content: "\f1ae";
}
.fa-paw:before {
  content: "\f1b0";
}
.fa-spoon:before {
  content: "\f1b1";
}
.fa-cube:before {
  content: "\f1b2";
}
.fa-cubes:before {
  content: "\f1b3";
}
.fa-behance:before {
  content: "\f1b4";
}
.fa-behance-square:before {
  content: "\f1b5";
}
.fa-steam:before {
  content: "\f1b6";
}
.fa-steam-square:before {
  content: "\f1b7";
}
.fa-recycle:before {
  content: "\f1b8";
}
.fa-automobile:before,
.fa-car:before {
  content: "\f1b9";
}
.fa-cab:before,
.fa-taxi:before {
  content: "\f1ba";
}
.fa-tree:before {
  content: "\f1bb";
}
.fa-spotify:before {
  content: "\f1bc";
}
.fa-deviantart:before {
  content: "\f1bd";
}
.fa-soundcloud:before {
  content: "\f1be";
}
.fa-database:before {
  content: "\f1c0";
}
.fa-file-pdf-o:before {
  content: "\f1c1";
}
.fa-file-word-o:before {
  content: "\f1c2";
}
.fa-file-excel-o:before {
  content: "\f1c3";
}
.fa-file-powerpoint-o:before {
  content: "\f1c4";
}
.fa-file-photo-o:before,
.fa-file-picture-o:before,
.fa-file-image-o:before {
  content: "\f1c5";
}
.fa-file-zip-o:before,
.fa-file-archive-o:before {
  content: "\f1c6";
}
.fa-file-sound-o:before,
.fa-file-audio-o:before {
  content: "\f1c7";
}
.fa-file-movie-o:before,
.fa-file-video-o:before {
  content: "\f1c8";
}
.fa-file-code-o:before {
  content: "\f1c9";
}
.fa-vine:before {
  content: "\f1ca";
}
.fa-codepen:before {
  content: "\f1cb";
}
.fa-jsfiddle:before {
  content: "\f1cc";
}
.fa-life-bouy:before,
.fa-life-buoy:before,
.fa-life-saver:before,
.fa-support:before,
.fa-life-ring:before {
  content: "\f1cd";
}
.fa-circle-o-notch:before {
  content: "\f1ce";
}
.fa-ra:before,
.fa-resistance:before,
.fa-rebel:before {
  content: "\f1d0";
}
.fa-ge:before,
.fa-empire:before {
  content: "\f1d1";
}
.fa-git-square:before {
  content: "\f1d2";
}
.fa-git:before {
  content: "\f1d3";
}
.fa-y-combinator-square:before,
.fa-yc-square:before,
.fa-hacker-news:before {
  content: "\f1d4";
}
.fa-tencent-weibo:before {
  content: "\f1d5";
}
.fa-qq:before {
  content: "\f1d6";
}
.fa-wechat:before,
.fa-weixin:before {
  content: "\f1d7";
}
.fa-send:before,
.fa-paper-plane:before {
  content: "\f1d8";
}
.fa-send-o:before,
.fa-paper-plane-o:before {
  content: "\f1d9";
}
.fa-history:before {
  content: "\f1da";
}
.fa-circle-thin:before {
  content: "\f1db";
}
.fa-header:before {
  content: "\f1dc";
}
.fa-paragraph:before {
  content: "\f1dd";
}
.fa-sliders:before {
  content: "\f1de";
}
.fa-share-alt:before {
  content: "\f1e0";
}
.fa-share-alt-square:before {
  content: "\f1e1";
}
.fa-bomb:before {
  content: "\f1e2";
}
.fa-soccer-ball-o:before,
.fa-futbol-o:before {
  content: "\f1e3";
}
.fa-tty:before {
  content: "\f1e4";
}
.fa-binoculars:before {
  content: "\f1e5";
}
.fa-plug:before {
  content: "\f1e6";
}
.fa-slideshare:before {
  content: "\f1e7";
}
.fa-twitch:before {
  content: "\f1e8";
}
.fa-yelp:before {
  content: "\f1e9";
}
.fa-newspaper-o:before {
  content: "\f1ea";
}
.fa-wifi:before {
  content: "\f1eb";
}
.fa-calculator:before {
  content: "\f1ec";
}
.fa-paypal:before {
  content: "\f1ed";
}
.fa-google-wallet:before {
  content: "\f1ee";
}
.fa-cc-visa:before {
  content: "\f1f0";
}
.fa-cc-mastercard:before {
  content: "\f1f1";
}
.fa-cc-discover:before {
  content: "\f1f2";
}
.fa-cc-amex:before {
  content: "\f1f3";
}
.fa-cc-paypal:before {
  content: "\f1f4";
}
.fa-cc-stripe:before {
  content: "\f1f5";
}
.fa-bell-slash:before {
  content: "\f1f6";
}
.fa-bell-slash-o:before {
  content: "\f1f7";
}
.fa-trash:before {
  content: "\f1f8";
}
.fa-copyright:before {
  content: "\f1f9";
}
.fa-at:before {
  content: "\f1fa";
}
.fa-eyedropper:before {
  content: "\f1fb";
}
.fa-paint-brush:before {
  content: "\f1fc";
}
.fa-birthday-cake:before {
  content: "\f1fd";
}
.fa-area-chart:before {
  content: "\f1fe";
}
.fa-pie-chart:before {
  content: "\f200";
}
.fa-line-chart:before {
  content: "\f201";
}
.fa-lastfm:before {
  content: "\f202";
}
.fa-lastfm-square:before {
  content: "\f203";
}
.fa-toggle-off:before {
  content: "\f204";
}
.fa-toggle-on:before {
  content: "\f205";
}
.fa-bicycle:before {
  content: "\f206";
}
.fa-bus:before {
  content: "\f207";
}
.fa-ioxhost:before {
  content: "\f208";
}
.fa-angellist:before {
  content: "\f209";
}
.fa-cc:before {
  content: "\f20a";
}
.fa-shekel:before,
.fa-sheqel:before,
.fa-ils:before {
  content: "\f20b";
}
.fa-meanpath:before {
  content: "\f20c";
}
.fa-buysellads:before {
  content: "\f20d";
}
.fa-connectdevelop:before {
  content: "\f20e";
}
.fa-dashcube:before {
  content: "\f210";
}
.fa-forumbee:before {
  content: "\f211";
}
.fa-leanpub:before {
  content: "\f212";
}
.fa-sellsy:before {
  content: "\f213";
}
.fa-shirtsinbulk:before {
  content: "\f214";
}
.fa-simplybuilt:before {
  content: "\f215";
}
.fa-skyatlas:before {
  content: "\f216";
}
.fa-cart-plus:before {
  content: "\f217";
}
.fa-cart-arrow-down:before {
  content: "\f218";
}
.fa-diamond:before {
  content: "\f219";
}
.fa-ship:before {
  content: "\f21a";
}
.fa-user-secret:before {
  content: "\f21b";
}
.fa-motorcycle:before {
  content: "\f21c";
}
.fa-street-view:before {
  content: "\f21d";
}
.fa-heartbeat:before {
  content: "\f21e";
}
.fa-venus:before {
  content: "\f221";
}
.fa-mars:before {
  content: "\f222";
}
.fa-mercury:before {
  content: "\f223";
}
.fa-intersex:before,
.fa-transgender:before {
  content: "\f224";
}
.fa-transgender-alt:before {
  content: "\f225";
}
.fa-venus-double:before {
  content: "\f226";
}
.fa-mars-double:before {
  content: "\f227";
}
.fa-venus-mars:before {
  content: "\f228";
}
.fa-mars-stroke:before {
  content: "\f229";
}
.fa-mars-stroke-v:before {
  content: "\f22a";
}
.fa-mars-stroke-h:before {
  content: "\f22b";
}
.fa-neuter:before {
  content: "\f22c";
}
.fa-genderless:before {
  content: "\f22d";
}
.fa-facebook-official:before {
  content: "\f230";
}
.fa-pinterest-p:before {
  content: "\f231";
}
.fa-whatsapp:before {
  content: "\f232";
}
.fa-server:before {
  content: "\f233";
}
.fa-user-plus:before {
  content: "\f234";
}
.fa-user-times:before {
  content: "\f235";
}
.fa-hotel:before,
.fa-bed:before {
  content: "\f236";
}
.fa-viacoin:before {
  content: "\f237";
}
.fa-train:before {
  content: "\f238";
}
.fa-subway:before {
  content: "\f239";
}
.fa-medium:before {
  content: "\f23a";
}
.fa-yc:before,
.fa-y-combinator:before {
  content: "\f23b";
}
.fa-optin-monster:before {
  content: "\f23c";
}
.fa-opencart:before {
  content: "\f23d";
}
.fa-expeditedssl:before {
  content: "\f23e";
}
.fa-battery-4:before,
.fa-battery:before,
.fa-battery-full:before {
  content: "\f240";
}
.fa-battery-3:before,
.fa-battery-three-quarters:before {
  content: "\f241";
}
.fa-battery-2:before,
.fa-battery-half:before {
  content: "\f242";
}
.fa-battery-1:before,
.fa-battery-quarter:before {
  content: "\f243";
}
.fa-battery-0:before,
.fa-battery-empty:before {
  content: "\f244";
}
.fa-mouse-pointer:before {
  content: "\f245";
}
.fa-i-cursor:before {
  content: "\f246";
}
.fa-object-group:before {
  content: "\f247";
}
.fa-object-ungroup:before {
  content: "\f248";
}
.fa-sticky-note:before {
  content: "\f249";
}
.fa-sticky-note-o:before {
  content: "\f24a";
}
.fa-cc-jcb:before {
  content: "\f24b";
}
.fa-cc-diners-club:before {
  content: "\f24c";
}
.fa-clone:before {
  content: "\f24d";
}
.fa-balance-scale:before {
  content: "\f24e";
}
.fa-hourglass-o:before {
  content: "\f250";
}
.fa-hourglass-1:before,
.fa-hourglass-start:before {
  content: "\f251";
}
.fa-hourglass-2:before,
.fa-hourglass-half:before {
  content: "\f252";
}
.fa-hourglass-3:before,
.fa-hourglass-end:before {
  content: "\f253";
}
.fa-hourglass:before {
  content: "\f254";
}
.fa-hand-grab-o:before,
.fa-hand-rock-o:before {
  content: "\f255";
}
.fa-hand-stop-o:before,
.fa-hand-paper-o:before {
  content: "\f256";
}
.fa-hand-scissors-o:before {
  content: "\f257";
}
.fa-hand-lizard-o:before {
  content: "\f258";
}
.fa-hand-spock-o:before {
  content: "\f259";
}
.fa-hand-pointer-o:before {
  content: "\f25a";
}
.fa-hand-peace-o:before {
  content: "\f25b";
}
.fa-trademark:before {
  content: "\f25c";
}
.fa-registered:before {
  content: "\f25d";
}
.fa-creative-commons:before {
  content: "\f25e";
}
.fa-gg:before {
  content: "\f260";
}
.fa-gg-circle:before {
  content: "\f261";
}
.fa-tripadvisor:before {
  content: "\f262";
}
.fa-odnoklassniki:before {
  content: "\f263";
}
.fa-odnoklassniki-square:before {
  content: "\f264";
}
.fa-get-pocket:before {
  content: "\f265";
}
.fa-wikipedia-w:before {
  content: "\f266";
}
.fa-safari:before {
  content: "\f267";
}
.fa-chrome:before {
  content: "\f268";
}
.fa-firefox:before {
  content: "\f269";
}
.fa-opera:before {
  content: "\f26a";
}
.fa-internet-explorer:before {
  content: "\f26b";
}
.fa-tv:before,
.fa-television:before {
  content: "\f26c";
}
.fa-contao:before {
  content: "\f26d";
}
.fa-500px:before {
  content: "\f26e";
}
.fa-amazon:before {
  content: "\f270";
}
.fa-calendar-plus-o:before {
  content: "\f271";
}
.fa-calendar-minus-o:before {
  content: "\f272";
}
.fa-calendar-times-o:before {
  content: "\f273";
}
.fa-calendar-check-o:before {
  content: "\f274";
}
.fa-industry:before {
  content: "\f275";
}
.fa-map-pin:before {
  content: "\f276";
}
.fa-map-signs:before {
  content: "\f277";
}
.fa-map-o:before {
  content: "\f278";
}
.fa-map:before {
  content: "\f279";
}
.fa-commenting:before {
  content: "\f27a";
}
.fa-commenting-o:before {
  content: "\f27b";
}
.fa-houzz:before {
  content: "\f27c";
}
.fa-vimeo:before {
  content: "\f27d";
}
.fa-black-tie:before {
  content: "\f27e";
}
.fa-fonticons:before {
  content: "\f280";
}
.fa-reddit-alien:before {
  content: "\f281";
}
.fa-edge:before {
  content: "\f282";
}
.fa-credit-card-alt:before {
  content: "\f283";
}
.fa-codiepie:before {
  content: "\f284";
}
.fa-modx:before {
  content: "\f285";
}
.fa-fort-awesome:before {
  content: "\f286";
}
.fa-usb:before {
  content: "\f287";
}
.fa-product-hunt:before {
  content: "\f288";
}
.fa-mixcloud:before {
  content: "\f289";
}
.fa-scribd:before {
  content: "\f28a";
}
.fa-pause-circle:before {
  content: "\f28b";
}
.fa-pause-circle-o:before {
  content: "\f28c";
}
.fa-stop-circle:before {
  content: "\f28d";
}
.fa-stop-circle-o:before {
  content: "\f28e";
}
.fa-shopping-bag:before {
  content: "\f290";
}
.fa-shopping-basket:before {
  content: "\f291";
}
.fa-hashtag:before {
  content: "\f292";
}
.fa-bluetooth:before {
  content: "\f293";
}
.fa-bluetooth-b:before {
  content: "\f294";
}
.fa-percent:before {
  content: "\f295";
}
.fa-gitlab:before {
  content: "\f296";
}
.fa-wpbeginner:before {
  content: "\f297";
}
.fa-wpforms:before {
  content: "\f298";
}
.fa-envira:before {
  content: "\f299";
}
.fa-universal-access:before {
  content: "\f29a";
}
.fa-wheelchair-alt:before {
  content: "\f29b";
}
.fa-question-circle-o:before {
  content: "\f29c";
}
.fa-blind:before {
  content: "\f29d";
}
.fa-audio-description:before {
  content: "\f29e";
}
.fa-volume-control-phone:before {
  content: "\f2a0";
}
.fa-braille:before {
  content: "\f2a1";
}
.fa-assistive-listening-systems:before {
  content: "\f2a2";
}
.fa-asl-interpreting:before,
.fa-american-sign-language-interpreting:before {
  content: "\f2a3";
}
.fa-deafness:before,
.fa-hard-of-hearing:before,
.fa-deaf:before {
  content: "\f2a4";
}
.fa-glide:before {
  content: "\f2a5";
}
.fa-glide-g:before {
  content: "\f2a6";
}
.fa-signing:before,
.fa-sign-language:before {
  content: "\f2a7";
}
.fa-low-vision:before {
  content: "\f2a8";
}
.fa-viadeo:before {
  content: "\f2a9";
}
.fa-viadeo-square:before {
  content: "\f2aa";
}
.fa-snapchat:before {
  content: "\f2ab";
}
.fa-snapchat-ghost:before {
  content: "\f2ac";
}
.fa-snapchat-square:before {
  content: "\f2ad";
}
.fa-pied-piper:before {
  content: "\f2ae";
}
.fa-first-order:before {
  content: "\f2b0";
}
.fa-yoast:before {
  content: "\f2b1";
}
.fa-themeisle:before {
  content: "\f2b2";
}
.fa-google-plus-circle:before,
.fa-google-plus-official:before {
  content: "\f2b3";
}
.fa-fa:before,
.fa-font-awesome:before {
  content: "\f2b4";
}
.fa-handshake-o:before {
  content: "\f2b5";
}
.fa-envelope-open:before {
  content: "\f2b6";
}
.fa-envelope-open-o:before {
  content: "\f2b7";
}
.fa-linode:before {
  content: "\f2b8";
}
.fa-address-book:before {
  content: "\f2b9";
}
.fa-address-book-o:before {
  content: "\f2ba";
}
.fa-vcard:before,
.fa-address-card:before {
  content: "\f2bb";
}
.fa-vcard-o:before,
.fa-address-card-o:before {
  content: "\f2bc";
}
.fa-user-circle:before {
  content: "\f2bd";
}
.fa-user-circle-o:before {
  content: "\f2be";
}
.fa-user-o:before {
  content: "\f2c0";
}
.fa-id-badge:before {
  content: "\f2c1";
}
.fa-drivers-license:before,
.fa-id-card:before {
  content: "\f2c2";
}
.fa-drivers-license-o:before,
.fa-id-card-o:before {
  content: "\f2c3";
}
.fa-quora:before {
  content: "\f2c4";
}
.fa-free-code-camp:before {
  content: "\f2c5";
}
.fa-telegram:before {
  content: "\f2c6";
}
.fa-thermometer-4:before,
.fa-thermometer:before,
.fa-thermometer-full:before {
  content: "\f2c7";
}
.fa-thermometer-3:before,
.fa-thermometer-three-quarters:before {
  content: "\f2c8";
}
.fa-thermometer-2:before,
.fa-thermometer-half:before {
  content: "\f2c9";
}
.fa-thermometer-1:before,
.fa-thermometer-quarter:before {
  content: "\f2ca";
}
.fa-thermometer-0:before,
.fa-thermometer-empty:before {
  content: "\f2cb";
}
.fa-shower:before {
  content: "\f2cc";
}
.fa-bathtub:before,
.fa-s15:before,
.fa-bath:before {
  content: "\f2cd";
}
.fa-podcast:before {
  content: "\f2ce";
}
.fa-window-maximize:before {
  content: "\f2d0";
}
.fa-window-minimize:before {
  content: "\f2d1";
}
.fa-window-restore:before {
  content: "\f2d2";
}
.fa-times-rectangle:before,
.fa-window-close:before {
  content: "\f2d3";
}
.fa-times-rectangle-o:before,
.fa-window-close-o:before {
  content: "\f2d4";
}
.fa-bandcamp:before {
  content: "\f2d5";
}
.fa-grav:before {
  content: "\f2d6";
}
.fa-etsy:before {
  content: "\f2d7";
}
.fa-imdb:before {
  content: "\f2d8";
}
.fa-ravelry:before {
  content: "\f2d9";
}
.fa-eercast:before {
  content: "\f2da";
}
.fa-microchip:before {
  content: "\f2db";
}
.fa-snowflake-o:before {
  content: "\f2dc";
}
.fa-superpowers:before {
  content: "\f2dd";
}
.fa-wpexplorer:before {
  content: "\f2de";
}
.fa-meetup:before {
  content: "\f2e0";
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  padding: 0;
  margin: -1px;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
/*!
*
* IPython base
*
*/
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
code {
  color: #000;
}
pre {
  font-size: inherit;
  line-height: inherit;
}
label {
  font-weight: normal;
}
/* Make the page background atleast 100% the height of the view port */
/* Make the page itself atleast 70% the height of the view port */
.border-box-sizing {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.corner-all {
  border-radius: 2px;
}
.no-padding {
  padding: 0px;
}
/* Flexible box model classes */
/* Taken from Alex Russell http://infrequently.org/2009/08/css-3-progress/ */
/* This file is a compatability layer.  It allows the usage of flexible box 
model layouts accross multiple browsers, including older browsers.  The newest,
universal implementation of the flexible box model is used when available (see
`Modern browsers` comments below).  Browsers that are known to implement this 
new spec completely include:

    Firefox 28.0+
    Chrome 29.0+
    Internet Explorer 11+ 
    Opera 17.0+

Browsers not listed, including Safari, are supported via the styling under the
`Old browsers` comments below.
*/
.hbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
.hbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.vbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
.vbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.hbox.reverse,
.vbox.reverse,
.reverse {
  /* Old browsers */
  -webkit-box-direction: reverse;
  -moz-box-direction: reverse;
  box-direction: reverse;
  /* Modern browsers */
  flex-direction: row-reverse;
}
.hbox.box-flex0,
.vbox.box-flex0,
.box-flex0 {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
  width: auto;
}
.hbox.box-flex1,
.vbox.box-flex1,
.box-flex1 {
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex,
.vbox.box-flex,
.box-flex {
  /* Old browsers */
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex2,
.vbox.box-flex2,
.box-flex2 {
  /* Old browsers */
  -webkit-box-flex: 2;
  -moz-box-flex: 2;
  box-flex: 2;
  /* Modern browsers */
  flex: 2;
}
.box-group1 {
  /*  Deprecated */
  -webkit-box-flex-group: 1;
  -moz-box-flex-group: 1;
  box-flex-group: 1;
}
.box-group2 {
  /* Deprecated */
  -webkit-box-flex-group: 2;
  -moz-box-flex-group: 2;
  box-flex-group: 2;
}
.hbox.start,
.vbox.start,
.start {
  /* Old browsers */
  -webkit-box-pack: start;
  -moz-box-pack: start;
  box-pack: start;
  /* Modern browsers */
  justify-content: flex-start;
}
.hbox.end,
.vbox.end,
.end {
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
}
.hbox.center,
.vbox.center,
.center {
  /* Old browsers */
  -webkit-box-pack: center;
  -moz-box-pack: center;
  box-pack: center;
  /* Modern browsers */
  justify-content: center;
}
.hbox.baseline,
.vbox.baseline,
.baseline {
  /* Old browsers */
  -webkit-box-pack: baseline;
  -moz-box-pack: baseline;
  box-pack: baseline;
  /* Modern browsers */
  justify-content: baseline;
}
.hbox.stretch,
.vbox.stretch,
.stretch {
  /* Old browsers */
  -webkit-box-pack: stretch;
  -moz-box-pack: stretch;
  box-pack: stretch;
  /* Modern browsers */
  justify-content: stretch;
}
.hbox.align-start,
.vbox.align-start,
.align-start {
  /* Old browsers */
  -webkit-box-align: start;
  -moz-box-align: start;
  box-align: start;
  /* Modern browsers */
  align-items: flex-start;
}
.hbox.align-end,
.vbox.align-end,
.align-end {
  /* Old browsers */
  -webkit-box-align: end;
  -moz-box-align: end;
  box-align: end;
  /* Modern browsers */
  align-items: flex-end;
}
.hbox.align-center,
.vbox.align-center,
.align-center {
  /* Old browsers */
  -webkit-box-align: center;
  -moz-box-align: center;
  box-align: center;
  /* Modern browsers */
  align-items: center;
}
.hbox.align-baseline,
.vbox.align-baseline,
.align-baseline {
  /* Old browsers */
  -webkit-box-align: baseline;
  -moz-box-align: baseline;
  box-align: baseline;
  /* Modern browsers */
  align-items: baseline;
}
.hbox.align-stretch,
.vbox.align-stretch,
.align-stretch {
  /* Old browsers */
  -webkit-box-align: stretch;
  -moz-box-align: stretch;
  box-align: stretch;
  /* Modern browsers */
  align-items: stretch;
}
div.error {
  margin: 2em;
  text-align: center;
}
div.error > h1 {
  font-size: 500%;
  line-height: normal;
}
div.error > p {
  font-size: 200%;
  line-height: normal;
}
div.traceback-wrapper {
  text-align: left;
  max-width: 800px;
  margin: auto;
}
div.traceback-wrapper pre.traceback {
  max-height: 600px;
  overflow: auto;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
body {
  background-color: #fff;
  /* This makes sure that the body covers the entire window and needs to
       be in a different element than the display: box in wrapper below */
  position: absolute;
  left: 0px;
  right: 0px;
  top: 0px;
  bottom: 0px;
  overflow: visible;
}
body > #header {
  /* Initially hidden to prevent FLOUC */
  display: none;
  background-color: #fff;
  /* Display over codemirror */
  position: relative;
  z-index: 100;
}
body > #header #header-container {
  display: flex;
  flex-direction: row;
  justify-content: space-between;
  padding: 5px;
  padding-bottom: 5px;
  padding-top: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
body > #header .header-bar {
  width: 100%;
  height: 1px;
  background: #e7e7e7;
  margin-bottom: -1px;
}
@media print {
  body > #header {
    display: none !important;
  }
}
#header-spacer {
  width: 100%;
  visibility: hidden;
}
@media print {
  #header-spacer {
    display: none;
  }
}
#ipython_notebook {
  padding-left: 0px;
  padding-top: 1px;
  padding-bottom: 1px;
}
[dir="rtl"] #ipython_notebook {
  margin-right: 10px;
  margin-left: 0;
}
[dir="rtl"] #ipython_notebook.pull-left {
  float: right !important;
  float: right;
}
.flex-spacer {
  flex: 1;
}
#noscript {
  width: auto;
  padding-top: 16px;
  padding-bottom: 16px;
  text-align: center;
  font-size: 22px;
  color: red;
  font-weight: bold;
}
#ipython_notebook img {
  height: 28px;
}
#site {
  width: 100%;
  display: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  overflow: auto;
}
@media print {
  #site {
    height: auto !important;
  }
}
/* Smaller buttons */
.ui-button .ui-button-text {
  padding: 0.2em 0.8em;
  font-size: 77%;
}
input.ui-button {
  padding: 0.3em 0.9em;
}
span#kernel_logo_widget {
  margin: 0 10px;
}
span#login_widget {
  float: right;
}
[dir="rtl"] span#login_widget {
  float: left;
}
span#login_widget > .button,
#logout {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button:focus,
#logout:focus,
span#login_widget > .button.focus,
#logout.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
span#login_widget > .button:hover,
#logout:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active:hover,
#logout:active:hover,
span#login_widget > .button.active:hover,
#logout.active:hover,
.open > .dropdown-togglespan#login_widget > .button:hover,
.open > .dropdown-toggle#logout:hover,
span#login_widget > .button:active:focus,
#logout:active:focus,
span#login_widget > .button.active:focus,
#logout.active:focus,
.open > .dropdown-togglespan#login_widget > .button:focus,
.open > .dropdown-toggle#logout:focus,
span#login_widget > .button:active.focus,
#logout:active.focus,
span#login_widget > .button.active.focus,
#logout.active.focus,
.open > .dropdown-togglespan#login_widget > .button.focus,
.open > .dropdown-toggle#logout.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  background-image: none;
}
span#login_widget > .button.disabled:hover,
#logout.disabled:hover,
span#login_widget > .button[disabled]:hover,
#logout[disabled]:hover,
fieldset[disabled] span#login_widget > .button:hover,
fieldset[disabled] #logout:hover,
span#login_widget > .button.disabled:focus,
#logout.disabled:focus,
span#login_widget > .button[disabled]:focus,
#logout[disabled]:focus,
fieldset[disabled] span#login_widget > .button:focus,
fieldset[disabled] #logout:focus,
span#login_widget > .button.disabled.focus,
#logout.disabled.focus,
span#login_widget > .button[disabled].focus,
#logout[disabled].focus,
fieldset[disabled] span#login_widget > .button.focus,
fieldset[disabled] #logout.focus {
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button .badge,
#logout .badge {
  color: #fff;
  background-color: #333;
}
.nav-header {
  text-transform: none;
}
#header > span {
  margin-top: 10px;
}
.modal_stretch .modal-dialog {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  min-height: 80vh;
}
.modal_stretch .modal-dialog .modal-body {
  max-height: calc(100vh - 200px);
  overflow: auto;
  flex: 1;
}
.modal-header {
  cursor: move;
}
@media (min-width: 768px) {
  .modal .modal-dialog {
    width: 700px;
  }
}
@media (min-width: 768px) {
  select.form-control {
    margin-left: 12px;
    margin-right: 12px;
  }
}
/*!
*
* IPython auth
*
*/
.center-nav {
  display: inline-block;
  margin-bottom: -4px;
}
[dir="rtl"] .center-nav form.pull-left {
  float: right !important;
  float: right;
}
[dir="rtl"] .center-nav .navbar-text {
  float: right;
}
[dir="rtl"] .navbar-inner {
  text-align: right;
}
[dir="rtl"] div.text-left {
  text-align: right;
}
/*!
*
* IPython tree view
*
*/
/* We need an invisible input field on top of the sentense*/
/* "Drag file onto the list ..." */
.alternate_upload {
  background-color: none;
  display: inline;
}
.alternate_upload.form {
  padding: 0;
  margin: 0;
}
.alternate_upload input.fileinput {
  position: absolute;
  display: block;
  width: 100%;
  height: 100%;
  overflow: hidden;
  cursor: pointer;
  opacity: 0;
  z-index: 2;
}
.alternate_upload .btn-xs > input.fileinput {
  margin: -1px -5px;
}
.alternate_upload .btn-upload {
  position: relative;
  height: 22px;
}
::-webkit-file-upload-button {
  cursor: pointer;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
ul#tabs {
  margin-bottom: 4px;
}
ul#tabs a {
  padding-top: 6px;
  padding-bottom: 4px;
}
[dir="rtl"] ul#tabs.nav-tabs > li {
  float: right;
}
[dir="rtl"] ul#tabs.nav.nav-tabs {
  padding-right: 0;
}
ul.breadcrumb a:focus,
ul.breadcrumb a:hover {
  text-decoration: none;
}
ul.breadcrumb i.icon-home {
  font-size: 16px;
  margin-right: 4px;
}
ul.breadcrumb span {
  color: #5e5e5e;
}
.list_toolbar {
  padding: 4px 0 4px 0;
  vertical-align: middle;
}
.list_toolbar .tree-buttons {
  padding-top: 1px;
}
[dir="rtl"] .list_toolbar .tree-buttons .pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .list_toolbar .col-sm-4,
[dir="rtl"] .list_toolbar .col-sm-8 {
  float: right;
}
.dynamic-buttons {
  padding-top: 3px;
  display: inline-block;
}
.list_toolbar [class*="span"] {
  min-height: 24px;
}
.list_header {
  font-weight: bold;
  background-color: #EEE;
}
.list_placeholder {
  font-weight: bold;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
}
.list_container {
  margin-top: 4px;
  margin-bottom: 20px;
  border: 1px solid #ddd;
  border-radius: 2px;
}
.list_container > div {
  border-bottom: 1px solid #ddd;
}
.list_container > div:hover .list-item {
  background-color: red;
}
.list_container > div:last-child {
  border: none;
}
.list_item:hover .list_item {
  background-color: #ddd;
}
.list_item a {
  text-decoration: none;
}
.list_item:hover {
  background-color: #fafafa;
}
.list_header > div,
.list_item > div {
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
.list_header > div input,
.list_item > div input {
  margin-right: 7px;
  margin-left: 14px;
  vertical-align: text-bottom;
  line-height: 22px;
  position: relative;
  top: -1px;
}
.list_header > div .item_link,
.list_item > div .item_link {
  margin-left: -1px;
  vertical-align: baseline;
  line-height: 22px;
}
[dir="rtl"] .list_item > div input {
  margin-right: 0;
}
.new-file input[type=checkbox] {
  visibility: hidden;
}
.item_name {
  line-height: 22px;
  height: 24px;
}
.item_icon {
  font-size: 14px;
  color: #5e5e5e;
  margin-right: 7px;
  margin-left: 7px;
  line-height: 22px;
  vertical-align: baseline;
}
.item_modified {
  margin-right: 7px;
  margin-left: 7px;
}
[dir="rtl"] .item_modified.pull-right {
  float: left !important;
  float: left;
}
.item_buttons {
  line-height: 1em;
  margin-left: -5px;
}
.item_buttons .btn,
.item_buttons .btn-group,
.item_buttons .input-group {
  float: left;
}
.item_buttons > .btn,
.item_buttons > .btn-group,
.item_buttons > .input-group {
  margin-left: 5px;
}
.item_buttons .btn {
  min-width: 13ex;
}
.item_buttons .running-indicator {
  padding-top: 4px;
  color: #5cb85c;
}
.item_buttons .kernel-name {
  padding-top: 4px;
  color: #5bc0de;
  margin-right: 7px;
  float: left;
}
[dir="rtl"] .item_buttons.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .item_buttons .kernel-name {
  margin-left: 7px;
  float: right;
}
.toolbar_info {
  height: 24px;
  line-height: 24px;
}
.list_item input:not([type=checkbox]) {
  padding-top: 3px;
  padding-bottom: 3px;
  height: 22px;
  line-height: 14px;
  margin: 0px;
}
.highlight_text {
  color: blue;
}
#project_name {
  display: inline-block;
  padding-left: 7px;
  margin-left: -2px;
}
#project_name > .breadcrumb {
  padding: 0px;
  margin-bottom: 0px;
  background-color: transparent;
  font-weight: bold;
}
.sort_button {
  display: inline-block;
  padding-left: 7px;
}
[dir="rtl"] .sort_button.pull-right {
  float: left !important;
  float: left;
}
#tree-selector {
  padding-right: 0px;
}
#button-select-all {
  min-width: 50px;
}
[dir="rtl"] #button-select-all.btn {
  float: right ;
}
#select-all {
  margin-left: 7px;
  margin-right: 2px;
  margin-top: 2px;
  height: 16px;
}
[dir="rtl"] #select-all.pull-left {
  float: right !important;
  float: right;
}
.menu_icon {
  margin-right: 2px;
}
.tab-content .row {
  margin-left: 0px;
  margin-right: 0px;
}
.folder_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f114";
}
.folder_icon:before.fa-pull-left {
  margin-right: .3em;
}
.folder_icon:before.fa-pull-right {
  margin-left: .3em;
}
.folder_icon:before.pull-left {
  margin-right: .3em;
}
.folder_icon:before.pull-right {
  margin-left: .3em;
}
.notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
}
.notebook_icon:before.fa-pull-left {
  margin-right: .3em;
}
.notebook_icon:before.fa-pull-right {
  margin-left: .3em;
}
.notebook_icon:before.pull-left {
  margin-right: .3em;
}
.notebook_icon:before.pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
  color: #5cb85c;
}
.running_notebook_icon:before.fa-pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.fa-pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before.pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.pull-right {
  margin-left: .3em;
}
.file_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f016";
  position: relative;
  top: -2px;
}
.file_icon:before.fa-pull-left {
  margin-right: .3em;
}
.file_icon:before.fa-pull-right {
  margin-left: .3em;
}
.file_icon:before.pull-left {
  margin-right: .3em;
}
.file_icon:before.pull-right {
  margin-left: .3em;
}
#notebook_toolbar .pull-right {
  padding-top: 0px;
  margin-right: -1px;
}
ul#new-menu {
  left: auto;
  right: 0;
}
#new-menu .dropdown-header {
  font-size: 10px;
  border-bottom: 1px solid #e5e5e5;
  padding: 0 0 3px;
  margin: -3px 20px 0;
}
.kernel-menu-icon {
  padding-right: 12px;
  width: 24px;
  content: "\f096";
}
.kernel-menu-icon:before {
  content: "\f096";
}
.kernel-menu-icon-current:before {
  content: "\f00c";
}
#tab_content {
  padding-top: 20px;
}
#running .panel-group .panel {
  margin-top: 3px;
  margin-bottom: 1em;
}
#running .panel-group .panel .panel-heading {
  background-color: #EEE;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
#running .panel-group .panel .panel-heading a:focus,
#running .panel-group .panel .panel-heading a:hover {
  text-decoration: none;
}
#running .panel-group .panel .panel-body {
  padding: 0px;
}
#running .panel-group .panel .panel-body .list_container {
  margin-top: 0px;
  margin-bottom: 0px;
  border: 0px;
  border-radius: 0px;
}
#running .panel-group .panel .panel-body .list_container .list_item {
  border-bottom: 1px solid #ddd;
}
#running .panel-group .panel .panel-body .list_container .list_item:last-child {
  border-bottom: 0px;
}
.delete-button {
  display: none;
}
.duplicate-button {
  display: none;
}
.rename-button {
  display: none;
}
.move-button {
  display: none;
}
.download-button {
  display: none;
}
.shutdown-button {
  display: none;
}
.dynamic-instructions {
  display: inline-block;
  padding-top: 4px;
}
/*!
*
* IPython text editor webapp
*
*/
.selected-keymap i.fa {
  padding: 0px 5px;
}
.selected-keymap i.fa:before {
  content: "\f00c";
}
#mode-menu {
  overflow: auto;
  max-height: 20em;
}
.edit_app #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.edit_app #menubar .navbar {
  /* Use a negative 1 bottom margin, so the border overlaps the border of the
    header */
  margin-bottom: -1px;
}
.dirty-indicator {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator.pull-left {
  margin-right: .3em;
}
.dirty-indicator.pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-dirty.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty.pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-clean.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f00c";
}
.dirty-indicator-clean:before.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.pull-right {
  margin-left: .3em;
}
#filename {
  font-size: 16pt;
  display: table;
  padding: 0px 5px;
}
#current-mode {
  padding-left: 5px;
  padding-right: 5px;
}
#texteditor-backdrop {
  padding-top: 20px;
  padding-bottom: 20px;
}
@media not print {
  #texteditor-backdrop {
    background-color: #EEE;
  }
}
@media print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container {
    padding: 0px;
    background-color: #fff;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
.CodeMirror-dialog {
  background-color: #fff;
}
/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI escape sequences */
/* The color values are a mix of
   http://www.xcolors.net/dl/baskerville-ivorylight and
   http://www.xcolors.net/dl/euphrasia */
.ansi-black-fg {
  color: #3E424D;
}
.ansi-black-bg {
  background-color: #3E424D;
}
.ansi-black-intense-fg {
  color: #282C36;
}
.ansi-black-intense-bg {
  background-color: #282C36;
}
.ansi-red-fg {
  color: #E75C58;
}
.ansi-red-bg {
  background-color: #E75C58;
}
.ansi-red-intense-fg {
  color: #B22B31;
}
.ansi-red-intense-bg {
  background-color: #B22B31;
}
.ansi-green-fg {
  color: #00A250;
}
.ansi-green-bg {
  background-color: #00A250;
}
.ansi-green-intense-fg {
  color: #007427;
}
.ansi-green-intense-bg {
  background-color: #007427;
}
.ansi-yellow-fg {
  color: #DDB62B;
}
.ansi-yellow-bg {
  background-color: #DDB62B;
}
.ansi-yellow-intense-fg {
  color: #B27D12;
}
.ansi-yellow-intense-bg {
  background-color: #B27D12;
}
.ansi-blue-fg {
  color: #208FFB;
}
.ansi-blue-bg {
  background-color: #208FFB;
}
.ansi-blue-intense-fg {
  color: #0065CA;
}
.ansi-blue-intense-bg {
  background-color: #0065CA;
}
.ansi-magenta-fg {
  color: #D160C4;
}
.ansi-magenta-bg {
  background-color: #D160C4;
}
.ansi-magenta-intense-fg {
  color: #A03196;
}
.ansi-magenta-intense-bg {
  background-color: #A03196;
}
.ansi-cyan-fg {
  color: #60C6C8;
}
.ansi-cyan-bg {
  background-color: #60C6C8;
}
.ansi-cyan-intense-fg {
  color: #258F8F;
}
.ansi-cyan-intense-bg {
  background-color: #258F8F;
}
.ansi-white-fg {
  color: #C5C1B4;
}
.ansi-white-bg {
  background-color: #C5C1B4;
}
.ansi-white-intense-fg {
  color: #A1A6B2;
}
.ansi-white-intense-bg {
  background-color: #A1A6B2;
}
.ansi-default-inverse-fg {
  color: #FFFFFF;
}
.ansi-default-inverse-bg {
  background-color: #000000;
}
.ansi-bold {
  font-weight: bold;
}
.ansi-underline {
  text-decoration: underline;
}
/* The following styles are deprecated an will be removed in a future version */
.ansibold {
  font-weight: bold;
}
.ansi-inverse {
  outline: 0.5px dotted;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  position: relative;
  overflow: visible;
}
div.cell:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: transparent;
}
div.cell.jupyter-soft-selected {
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected,
div.cell.selected.jupyter-soft-selected {
  border-color: #ababab;
}
div.cell.selected:before,
div.cell.selected.jupyter-soft-selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #42A5F5;
}
@media print {
  div.cell.selected,
  div.cell.selected.jupyter-soft-selected {
    border-color: transparent;
  }
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
}
.edit_mode div.cell.selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #66BB6A;
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  /* Note that this should set vertical padding only, since CodeMirror assumes
       that horizontal padding will be set on CodeMirror pre */
  padding: 0.4em 0;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only,
    use .CodeMirror-lines for vertical */
  padding: 0 0.4em;
  border: 0;
  border-radius: 0;
}
.CodeMirror-cursor {
  border-left: 1.4px solid black;
}
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .CodeMirror-cursor {
    border-left: 2px solid black;
  }
}
@media screen and (min-width: 4320px) {
  .CodeMirror-cursor {
    border-left: 4px solid black;
  }
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area .rendered_html table {
  margin-left: 0;
  margin-right: 0;
}
div.output_area .rendered_html img {
  margin-left: 0;
  margin-right: 0;
}
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
div.output_area .mglyph > img {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 1px 0 1px 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}
.rendered_html em {
  font-style: italic;
}
.rendered_html strong {
  font-weight: bold;
}
.rendered_html u {
  text-decoration: underline;
}
.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}
.rendered_html h1 {
  font-size: 185.7%;
  margin: 1.08em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h2 {
  font-size: 157.1%;
  margin: 1.27em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h3 {
  font-size: 128.6%;
  margin: 1.55em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h4 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h5 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h6 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul:not(.list-inline),
.rendered_html ol:not(.list-inline) {
  padding-left: 2em;
}
.rendered_html ul {
  list-style: disc;
}
.rendered_html ul ul {
  list-style: square;
  margin-top: 0;
}
.rendered_html ul ul ul {
  list-style: circle;
}
.rendered_html ol {
  list-style: decimal;
}
.rendered_html ol ol {
  list-style: upper-alpha;
  margin-top: 0;
}
.rendered_html ol ol ol {
  list-style: lower-alpha;
}
.rendered_html ol ol ol ol {
  list-style: lower-roman;
}
.rendered_html ol ol ol ol ol {
  list-style: decimal;
}
.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}
.rendered_html hr {
  color: black;
  background-color: black;
}
.rendered_html pre {
  margin: 1em 2em;
  padding: 0px;
  background-color: #fff;
}
.rendered_html code {
  background-color: #eff0f1;
}
.rendered_html p code {
  padding: 1px 5px;
}
.rendered_html pre code {
  background-color: #fff;
}
.rendered_html pre,
.rendered_html code {
  border: 0;
  color: #000;
  font-size: 100%;
}
.rendered_html blockquote {
  margin: 1em 2em;
}
.rendered_html table {
  margin-left: auto;
  margin-right: auto;
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.rendered_html tr,
.rendered_html th,
.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.rendered_html th {
  font-weight: bold;
}
.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
.rendered_html * + table {
  margin-top: 1em;
}
.rendered_html p {
  text-align: left;
}
.rendered_html * + p {
  margin-top: 1em;
}
.rendered_html img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,
.rendered_html svg {
  max-width: 100%;
  height: auto;
}
.rendered_html img.unconfined,
.rendered_html svg.unconfined {
  max-width: none;
}
.rendered_html .alert {
  margin-bottom: initial;
}
.rendered_html * + .alert {
  margin-top: 1em;
}
[dir="rtl"] .rendered_html p {
  text-align: right;
}
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered .rendered_html {
  overflow-x: auto;
  overflow-y: hidden;
}
.text_cell.rendered .rendered_html tr,
.text_cell.rendered .rendered_html th,
.text_cell.rendered .rendered_html td {
  max-width: none;
}
.text_cell.unrendered .text_cell_render {
  display: none;
}
.text_cell .dropzone .input_area {
  border: 2px dashed #bababa;
  margin: -1px;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
/*!
*
* IPython notebook webapp
*
*/
@media (max-width: 767px) {
  .notebook_app {
    padding-left: 0px;
    padding-right: 0px;
  }
}
#ipython-main-app {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook_panel {
  margin: 0px;
  padding: 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook {
  font-size: 14px;
  line-height: 20px;
  overflow-y: hidden;
  overflow-x: auto;
  width: 100%;
  /* This spaces the page away from the edge of the notebook area */
  padding-top: 20px;
  margin: 0px;
  outline: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  min-height: 100%;
}
@media not print {
  #notebook-container {
    padding: 15px;
    background-color: #fff;
    min-height: 0;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
@media print {
  #notebook-container {
    width: 100%;
  }
}
div.ui-widget-content {
  border: 1px solid #ababab;
  outline: none;
}
pre.dialog {
  background-color: #f7f7f7;
  border: 1px solid #ddd;
  border-radius: 2px;
  padding: 0.4em;
  padding-left: 2em;
}
p.dialog {
  padding: 0.2em;
}
/* Word-wrap output correctly.  This is the CSS3 spelling, though Firefox seems
   to not honor it correctly.  Webkit browsers (Chrome, rekonq, Safari) do.
 */
pre,
code,
kbd,
samp {
  white-space: pre-wrap;
}
#fonttest {
  font-family: monospace;
}
p {
  margin-bottom: 0;
}
.end_space {
  min-height: 100px;
  transition: height .2s ease;
}
.notebook_app > #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
@media not print {
  .notebook_app {
    background-color: #EEE;
  }
}
kbd {
  border-style: solid;
  border-width: 1px;
  box-shadow: none;
  margin: 2px;
  padding-left: 2px;
  padding-right: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
.jupyter-keybindings {
  padding: 1px;
  line-height: 24px;
  border-bottom: 1px solid gray;
}
.jupyter-keybindings input {
  margin: 0;
  padding: 0;
  border: none;
}
.jupyter-keybindings i {
  padding: 6px;
}
.well code {
  background-color: #ffffff;
  border-color: #ababab;
  border-width: 1px;
  border-style: solid;
  padding: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
/* CSS for the cell toolbar */
.celltoolbar {
  border: thin solid #CFCFCF;
  border-bottom: none;
  background: #EEE;
  border-radius: 2px 2px 0px 0px;
  width: 100%;
  height: 29px;
  padding-right: 4px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
  display: -webkit-flex;
}
@media print {
  .celltoolbar {
    display: none;
  }
}
.ctb_hideshow {
  display: none;
  vertical-align: bottom;
}
/* ctb_show is added to the ctb_hideshow div to show the cell toolbar.
   Cell toolbars are only shown when the ctb_global_show class is also set.
*/
.ctb_global_show .ctb_show.ctb_hideshow {
  display: block;
}
.ctb_global_show .ctb_show + .input_area,
.ctb_global_show .ctb_show + div.text_cell_input,
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border-top-right-radius: 0px;
  border-top-left-radius: 0px;
}
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border: 1px solid #cfcfcf;
}
.celltoolbar {
  font-size: 87%;
  padding-top: 3px;
}
.celltoolbar select {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  width: inherit;
  font-size: inherit;
  height: 22px;
  padding: 0px;
  display: inline-block;
}
.celltoolbar select:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.celltoolbar select::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.celltoolbar select:-ms-input-placeholder {
  color: #999;
}
.celltoolbar select::-webkit-input-placeholder {
  color: #999;
}
.celltoolbar select::-ms-expand {
  border: 0;
  background-color: transparent;
}
.celltoolbar select[disabled],
.celltoolbar select[readonly],
fieldset[disabled] .celltoolbar select {
  background-color: #eeeeee;
  opacity: 1;
}
.celltoolbar select[disabled],
fieldset[disabled] .celltoolbar select {
  cursor: not-allowed;
}
textarea.celltoolbar select {
  height: auto;
}
select.celltoolbar select {
  height: 30px;
  line-height: 30px;
}
textarea.celltoolbar select,
select[multiple].celltoolbar select {
  height: auto;
}
.celltoolbar label {
  margin-left: 5px;
  margin-right: 5px;
}
.tags_button_container {
  width: 100%;
  display: flex;
}
.tag-container {
  display: flex;
  flex-direction: row;
  flex-grow: 1;
  overflow: hidden;
  position: relative;
}
.tag-container > * {
  margin: 0 4px;
}
.remove-tag-btn {
  margin-left: 4px;
}
.tags-input {
  display: flex;
}
.cell-tag:last-child:after {
  content: "";
  position: absolute;
  right: 0;
  width: 40px;
  height: 100%;
  /* Fade to background color of cell toolbar */
  background: linear-gradient(to right, rgba(0, 0, 0, 0), #EEE);
}
.tags-input > * {
  margin-left: 4px;
}
.cell-tag,
.tags-input input,
.tags-input button {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  box-shadow: none;
  width: inherit;
  font-size: inherit;
  height: 22px;
  line-height: 22px;
  padding: 0px 4px;
  display: inline-block;
}
.cell-tag:focus,
.tags-input input:focus,
.tags-input button:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.cell-tag::-moz-placeholder,
.tags-input input::-moz-placeholder,
.tags-input button::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.cell-tag:-ms-input-placeholder,
.tags-input input:-ms-input-placeholder,
.tags-input button:-ms-input-placeholder {
  color: #999;
}
.cell-tag::-webkit-input-placeholder,
.tags-input input::-webkit-input-placeholder,
.tags-input button::-webkit-input-placeholder {
  color: #999;
}
.cell-tag::-ms-expand,
.tags-input input::-ms-expand,
.tags-input button::-ms-expand {
  border: 0;
  background-color: transparent;
}
.cell-tag[disabled],
.tags-input input[disabled],
.tags-input button[disabled],
.cell-tag[readonly],
.tags-input input[readonly],
.tags-input button[readonly],
fieldset[disabled] .cell-tag,
fieldset[disabled] .tags-input input,
fieldset[disabled] .tags-input button {
  background-color: #eeeeee;
  opacity: 1;
}
.cell-tag[disabled],
.tags-input input[disabled],
.tags-input button[disabled],
fieldset[disabled] .cell-tag,
fieldset[disabled] .tags-input input,
fieldset[disabled] .tags-input button {
  cursor: not-allowed;
}
textarea.cell-tag,
textarea.tags-input input,
textarea.tags-input button {
  height: auto;
}
select.cell-tag,
select.tags-input input,
select.tags-input button {
  height: 30px;
  line-height: 30px;
}
textarea.cell-tag,
textarea.tags-input input,
textarea.tags-input button,
select[multiple].cell-tag,
select[multiple].tags-input input,
select[multiple].tags-input button {
  height: auto;
}
.cell-tag,
.tags-input button {
  padding: 0px 4px;
}
.cell-tag {
  background-color: #fff;
  white-space: nowrap;
}
.tags-input input[type=text]:focus {
  outline: none;
  box-shadow: none;
  border-color: #ccc;
}
.completions {
  position: absolute;
  z-index: 110;
  overflow: hidden;
  border: 1px solid #ababab;
  border-radius: 2px;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  line-height: 1;
}
.completions select {
  background: white;
  outline: none;
  border: none;
  padding: 0px;
  margin: 0px;
  overflow: auto;
  font-family: monospace;
  font-size: 110%;
  color: #000;
  width: auto;
}
.completions select option.context {
  color: #286090;
}
#kernel_logo_widget .current_kernel_logo {
  display: none;
  margin-top: -1px;
  margin-bottom: -1px;
  width: 32px;
  height: 32px;
}
[dir="rtl"] #kernel_logo_widget {
  float: left !important;
  float: left;
}
.modal .modal-body .move-path {
  display: flex;
  flex-direction: row;
  justify-content: space;
  align-items: center;
}
.modal .modal-body .move-path .server-root {
  padding-right: 20px;
}
.modal .modal-body .move-path .path-input {
  flex: 1;
}
#menubar {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  margin-top: 1px;
}
#menubar .navbar {
  border-top: 1px;
  border-radius: 0px 0px 2px 2px;
  margin-bottom: 0px;
}
#menubar .navbar-toggle {
  float: left;
  padding-top: 7px;
  padding-bottom: 7px;
  border: none;
}
#menubar .navbar-collapse {
  clear: left;
}
[dir="rtl"] #menubar .navbar-toggle {
  float: right;
}
[dir="rtl"] #menubar .navbar-collapse {
  clear: right;
}
[dir="rtl"] #menubar .navbar-nav {
  float: right;
}
[dir="rtl"] #menubar .nav {
  padding-right: 0px;
}
[dir="rtl"] #menubar .navbar-nav > li {
  float: right;
}
[dir="rtl"] #menubar .navbar-right {
  float: left !important;
}
[dir="rtl"] ul.dropdown-menu {
  text-align: right;
  left: auto;
}
[dir="rtl"] ul#new-menu.dropdown-menu {
  right: auto;
  left: 0;
}
.nav-wrapper {
  border-bottom: 1px solid #e7e7e7;
}
i.menu-icon {
  padding-top: 4px;
}
[dir="rtl"] i.menu-icon.pull-right {
  float: left !important;
  float: left;
}
ul#help_menu li a {
  overflow: hidden;
  padding-right: 2.2em;
}
ul#help_menu li a i {
  margin-right: -1.2em;
}
[dir="rtl"] ul#help_menu li a {
  padding-left: 2.2em;
}
[dir="rtl"] ul#help_menu li a i {
  margin-right: 0;
  margin-left: -1.2em;
}
[dir="rtl"] ul#help_menu li a i.pull-right {
  float: left !important;
  float: left;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu > .dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
}
[dir="rtl"] .dropdown-submenu > .dropdown-menu {
  right: 100%;
  margin-right: -1px;
}
.dropdown-submenu:hover > .dropdown-menu {
  display: block;
}
.dropdown-submenu > a:after {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  display: block;
  content: "\f0da";
  float: right;
  color: #333333;
  margin-top: 2px;
  margin-right: -10px;
}
.dropdown-submenu > a:after.fa-pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.fa-pull-right {
  margin-left: .3em;
}
.dropdown-submenu > a:after.pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.pull-right {
  margin-left: .3em;
}
[dir="rtl"] .dropdown-submenu > a:after {
  float: left;
  content: "\f0d9";
  margin-right: 0;
  margin-left: -10px;
}
.dropdown-submenu:hover > a:after {
  color: #262626;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left > .dropdown-menu {
  left: -100%;
  margin-left: 10px;
}
#notification_area {
  float: right !important;
  float: right;
  z-index: 10;
}
[dir="rtl"] #notification_area {
  float: left !important;
  float: left;
}
.indicator_area {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
[dir="rtl"] .indicator_area {
  float: left !important;
  float: left;
}
#kernel_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  border-left: 1px solid;
}
#kernel_indicator .kernel_indicator_name {
  padding-left: 5px;
  padding-right: 5px;
}
[dir="rtl"] #kernel_indicator {
  float: left !important;
  float: left;
  border-left: 0;
  border-right: 1px solid;
}
#modal_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
[dir="rtl"] #modal_indicator {
  float: left !important;
  float: left;
}
#readonly-indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  margin-top: 2px;
  margin-bottom: 0px;
  margin-left: 0px;
  margin-right: 0px;
  display: none;
}
.modal_indicator:before {
  width: 1.28571429em;
  text-align: center;
}
.edit_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f040";
}
.edit_mode .modal_indicator:before.fa-pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.fa-pull-right {
  margin-left: .3em;
}
.edit_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: ' ';
}
.command_mode .modal_indicator:before.fa-pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.fa-pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f10c";
}
.kernel_idle_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f111";
}
.kernel_busy_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f1e2";
}
.kernel_dead_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f127";
}
.kernel_disconnected_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.pull-right {
  margin-left: .3em;
}
.notification_widget {
  color: #777;
  z-index: 10;
  background: rgba(240, 240, 240, 0.5);
  margin-right: 4px;
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget:focus,
.notification_widget.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.notification_widget:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active:hover,
.notification_widget.active:hover,
.open > .dropdown-toggle.notification_widget:hover,
.notification_widget:active:focus,
.notification_widget.active:focus,
.open > .dropdown-toggle.notification_widget:focus,
.notification_widget:active.focus,
.notification_widget.active.focus,
.open > .dropdown-toggle.notification_widget.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  background-image: none;
}
.notification_widget.disabled:hover,
.notification_widget[disabled]:hover,
fieldset[disabled] .notification_widget:hover,
.notification_widget.disabled:focus,
.notification_widget[disabled]:focus,
fieldset[disabled] .notification_widget:focus,
.notification_widget.disabled.focus,
.notification_widget[disabled].focus,
fieldset[disabled] .notification_widget.focus {
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget .badge {
  color: #fff;
  background-color: #333;
}
.notification_widget.warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning:focus,
.notification_widget.warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.notification_widget.warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active:hover,
.notification_widget.warning.active:hover,
.open > .dropdown-toggle.notification_widget.warning:hover,
.notification_widget.warning:active:focus,
.notification_widget.warning.active:focus,
.open > .dropdown-toggle.notification_widget.warning:focus,
.notification_widget.warning:active.focus,
.notification_widget.warning.active.focus,
.open > .dropdown-toggle.notification_widget.warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  background-image: none;
}
.notification_widget.warning.disabled:hover,
.notification_widget.warning[disabled]:hover,
fieldset[disabled] .notification_widget.warning:hover,
.notification_widget.warning.disabled:focus,
.notification_widget.warning[disabled]:focus,
fieldset[disabled] .notification_widget.warning:focus,
.notification_widget.warning.disabled.focus,
.notification_widget.warning[disabled].focus,
fieldset[disabled] .notification_widget.warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.notification_widget.success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success:focus,
.notification_widget.success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.notification_widget.success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active:hover,
.notification_widget.success.active:hover,
.open > .dropdown-toggle.notification_widget.success:hover,
.notification_widget.success:active:focus,
.notification_widget.success.active:focus,
.open > .dropdown-toggle.notification_widget.success:focus,
.notification_widget.success:active.focus,
.notification_widget.success.active.focus,
.open > .dropdown-toggle.notification_widget.success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  background-image: none;
}
.notification_widget.success.disabled:hover,
.notification_widget.success[disabled]:hover,
fieldset[disabled] .notification_widget.success:hover,
.notification_widget.success.disabled:focus,
.notification_widget.success[disabled]:focus,
fieldset[disabled] .notification_widget.success:focus,
.notification_widget.success.disabled.focus,
.notification_widget.success[disabled].focus,
fieldset[disabled] .notification_widget.success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.notification_widget.info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info:focus,
.notification_widget.info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.notification_widget.info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active:hover,
.notification_widget.info.active:hover,
.open > .dropdown-toggle.notification_widget.info:hover,
.notification_widget.info:active:focus,
.notification_widget.info.active:focus,
.open > .dropdown-toggle.notification_widget.info:focus,
.notification_widget.info:active.focus,
.notification_widget.info.active.focus,
.open > .dropdown-toggle.notification_widget.info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  background-image: none;
}
.notification_widget.info.disabled:hover,
.notification_widget.info[disabled]:hover,
fieldset[disabled] .notification_widget.info:hover,
.notification_widget.info.disabled:focus,
.notification_widget.info[disabled]:focus,
fieldset[disabled] .notification_widget.info:focus,
.notification_widget.info.disabled.focus,
.notification_widget.info[disabled].focus,
fieldset[disabled] .notification_widget.info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.notification_widget.danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger:focus,
.notification_widget.danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.notification_widget.danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active:hover,
.notification_widget.danger.active:hover,
.open > .dropdown-toggle.notification_widget.danger:hover,
.notification_widget.danger:active:focus,
.notification_widget.danger.active:focus,
.open > .dropdown-toggle.notification_widget.danger:focus,
.notification_widget.danger:active.focus,
.notification_widget.danger.active.focus,
.open > .dropdown-toggle.notification_widget.danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  background-image: none;
}
.notification_widget.danger.disabled:hover,
.notification_widget.danger[disabled]:hover,
fieldset[disabled] .notification_widget.danger:hover,
.notification_widget.danger.disabled:focus,
.notification_widget.danger[disabled]:focus,
fieldset[disabled] .notification_widget.danger:focus,
.notification_widget.danger.disabled.focus,
.notification_widget.danger[disabled].focus,
fieldset[disabled] .notification_widget.danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger .badge {
  color: #d9534f;
  background-color: #fff;
}
div#pager {
  background-color: #fff;
  font-size: 14px;
  line-height: 20px;
  overflow: hidden;
  display: none;
  position: fixed;
  bottom: 0px;
  width: 100%;
  max-height: 50%;
  padding-top: 8px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  /* Display over codemirror */
  z-index: 100;
  /* Hack which prevents jquery ui resizable from changing top. */
  top: auto !important;
}
div#pager pre {
  line-height: 1.21429em;
  color: #000;
  background-color: #f7f7f7;
  padding: 0.4em;
}
div#pager #pager-button-area {
  position: absolute;
  top: 8px;
  right: 20px;
}
div#pager #pager-contents {
  position: relative;
  overflow: auto;
  width: 100%;
  height: 100%;
}
div#pager #pager-contents #pager-container {
  position: relative;
  padding: 15px 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
div#pager .ui-resizable-handle {
  top: 0px;
  height: 8px;
  background: #f7f7f7;
  border-top: 1px solid #cfcfcf;
  border-bottom: 1px solid #cfcfcf;
  /* This injects handle bars (a short, wide = symbol) for 
        the resize handle. */
}
div#pager .ui-resizable-handle::after {
  content: '';
  top: 2px;
  left: 50%;
  height: 3px;
  width: 30px;
  margin-left: -15px;
  position: absolute;
  border-top: 1px solid #cfcfcf;
}
.quickhelp {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  line-height: 1.8em;
}
.shortcut_key {
  display: inline-block;
  width: 21ex;
  text-align: right;
  font-family: monospace;
}
.shortcut_descr {
  display: inline-block;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
span.save_widget {
  height: 30px;
  margin-top: 4px;
  display: flex;
  justify-content: flex-start;
  align-items: baseline;
  width: 50%;
  flex: 1;
}
span.save_widget span.filename {
  height: 100%;
  line-height: 1em;
  margin-left: 16px;
  border: none;
  font-size: 146.5%;
  text-overflow: ellipsis;
  overflow: hidden;
  white-space: nowrap;
  border-radius: 2px;
}
span.save_widget span.filename:hover {
  background-color: #e6e6e6;
}
[dir="rtl"] span.save_widget.pull-left {
  float: right !important;
  float: right;
}
[dir="rtl"] span.save_widget span.filename {
  margin-left: 0;
  margin-right: 16px;
}
span.checkpoint_status,
span.autosave_status {
  font-size: small;
  white-space: nowrap;
  padding: 0 5px;
}
@media (max-width: 767px) {
  span.save_widget {
    font-size: small;
    padding: 0 0 0 5px;
  }
  span.checkpoint_status,
  span.autosave_status {
    display: none;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  span.checkpoint_status {
    display: none;
  }
  span.autosave_status {
    font-size: x-small;
  }
}
.toolbar {
  padding: 0px;
  margin-left: -5px;
  margin-top: 2px;
  margin-bottom: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.toolbar select,
.toolbar label {
  width: auto;
  vertical-align: middle;
  margin-right: 2px;
  margin-bottom: 0px;
  display: inline;
  font-size: 92%;
  margin-left: 0.3em;
  margin-right: 0.3em;
  padding: 0px;
  padding-top: 3px;
}
.toolbar .btn {
  padding: 2px 8px;
}
.toolbar .btn-group {
  margin-top: 0px;
  margin-left: 5px;
}
.toolbar-btn-label {
  margin-left: 6px;
}
#maintoolbar {
  margin-bottom: -3px;
  margin-top: -8px;
  border: 0px;
  min-height: 27px;
  margin-left: 0px;
  padding-top: 11px;
  padding-bottom: 3px;
}
#maintoolbar .navbar-text {
  float: none;
  vertical-align: middle;
  text-align: right;
  margin-left: 5px;
  margin-right: 0px;
  margin-top: 0px;
}
.select-xs {
  height: 24px;
}
[dir="rtl"] .btn-group > .btn,
.btn-group-vertical > .btn {
  float: right;
}
.pulse,
.dropdown-menu > li > a.pulse,
li.pulse > a.dropdown-toggle,
li.pulse.open > a.dropdown-toggle {
  background-color: #F37626;
  color: white;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
/** WARNING IF YOU ARE EDITTING THIS FILE, if this is a .css file, It has a lot
 * of chance of beeing generated from the ../less/[samename].less file, you can
 * try to get back the less file by reverting somme commit in history
 **/
/*
 * We'll try to get something pretty, so we
 * have some strange css to have the scroll bar on
 * the left with fix button on the top right of the tooltip
 */
@-moz-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-webkit-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-moz-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
@-webkit-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
/*properties of tooltip after "expand"*/
.bigtooltip {
  overflow: auto;
  height: 200px;
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
}
/*properties of tooltip before "expand"*/
.smalltooltip {
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
  text-overflow: ellipsis;
  overflow: hidden;
  height: 80px;
}
.tooltipbuttons {
  position: absolute;
  padding-right: 15px;
  top: 0px;
  right: 0px;
}
.tooltiptext {
  /*avoid the button to overlap on some docstring*/
  padding-right: 30px;
}
.ipython_tooltip {
  max-width: 700px;
  /*fade-in animation when inserted*/
  -webkit-animation: fadeOut 400ms;
  -moz-animation: fadeOut 400ms;
  animation: fadeOut 400ms;
  -webkit-animation: fadeIn 400ms;
  -moz-animation: fadeIn 400ms;
  animation: fadeIn 400ms;
  vertical-align: middle;
  background-color: #f7f7f7;
  overflow: visible;
  border: #ababab 1px solid;
  outline: none;
  padding: 3px;
  margin: 0px;
  padding-left: 7px;
  font-family: monospace;
  min-height: 50px;
  -moz-box-shadow: 0px 6px 10px -1px #adadad;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  border-radius: 2px;
  position: absolute;
  z-index: 1000;
}
.ipython_tooltip a {
  float: right;
}
.ipython_tooltip .tooltiptext pre {
  border: 0;
  border-radius: 0;
  font-size: 100%;
  background-color: #f7f7f7;
}
.pretooltiparrow {
  left: 0px;
  margin: 0px;
  top: -16px;
  width: 40px;
  height: 16px;
  overflow: hidden;
  position: absolute;
}
.pretooltiparrow:before {
  background-color: #f7f7f7;
  border: 1px #ababab solid;
  z-index: 11;
  content: "";
  position: absolute;
  left: 15px;
  top: 10px;
  width: 25px;
  height: 25px;
  -webkit-transform: rotate(45deg);
  -moz-transform: rotate(45deg);
  -ms-transform: rotate(45deg);
  -o-transform: rotate(45deg);
}
ul.typeahead-list i {
  margin-left: -10px;
  width: 18px;
}
[dir="rtl"] ul.typeahead-list i {
  margin-left: 0;
  margin-right: -10px;
}
ul.typeahead-list {
  max-height: 80vh;
  overflow: auto;
}
ul.typeahead-list > li > a {
  /** Firefox bug **/
  /* see https://github.com/jupyter/notebook/issues/559 */
  white-space: normal;
}
ul.typeahead-list  > li > a.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .typeahead-list {
  text-align: right;
}
.cmd-palette .modal-body {
  padding: 7px;
}
.cmd-palette form {
  background: white;
}
.cmd-palette input {
  outline: none;
}
.no-shortcut {
  min-width: 20px;
  color: transparent;
}
[dir="rtl"] .no-shortcut.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .command-shortcut.pull-right {
  float: left !important;
  float: left;
}
.command-shortcut:before {
  content: "(command mode)";
  padding-right: 3px;
  color: #777777;
}
.edit-shortcut:before {
  content: "(edit)";
  padding-right: 3px;
  color: #777777;
}
[dir="rtl"] .edit-shortcut.pull-right {
  float: left !important;
  float: left;
}
#find-and-replace #replace-preview .match,
#find-and-replace #replace-preview .insert {
  background-color: #BBDEFB;
  border-color: #90CAF9;
  border-style: solid;
  border-width: 1px;
  border-radius: 0px;
}
[dir="ltr"] #find-and-replace .input-group-btn + .form-control {
  border-left: none;
}
[dir="rtl"] #find-and-replace .input-group-btn + .form-control {
  border-right: none;
}
#find-and-replace #replace-preview .replace .match {
  background-color: #FFCDD2;
  border-color: #EF9A9A;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .insert {
  background-color: #C8E6C9;
  border-color: #A5D6A7;
  border-radius: 0px;
}
#find-and-replace #replace-preview {
  max-height: 60vh;
  overflow: auto;
}
#find-and-replace #replace-preview pre {
  padding: 5px 10px;
}
.terminal-app {
  background: #EEE;
}
.terminal-app #header {
  background: #fff;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.terminal-app .terminal {
  width: 100%;
  float: left;
  font-family: monospace;
  color: white;
  background: black;
  padding: 0.4em;
  border-radius: 2px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
}
.terminal-app .terminal,
.terminal-app .terminal dummy-screen {
  line-height: 1em;
  font-size: 14px;
}
.terminal-app .terminal .xterm-rows {
  padding: 10px;
}
.terminal-app .terminal-cursor {
  color: black;
  background: white;
}
.terminal-app #terminado-container {
  margin-top: 20px;
}
/*# sourceMappingURL=style.min.css.map */
    </style>
<style type="text/css">
    .highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */
    </style>
<style type="text/css">
    
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }

    </style>


<style type="text/css">
/* Overrides of notebook CSS for static HTML export */
body {
  overflow: visible;
  padding: 8px;
}

div#notebook {
  overflow: visible;
  border-top: none;
}@media print {
  div.cell {
    display: block;
    page-break-inside: avoid;
  } 
  div.output_wrapper { 
    display: block;
    page-break-inside: avoid; 
  }
  div.output { 
    display: block;
    page-break-inside: avoid; 
  }
}
</style>

<!-- Custom stylesheet, it must be in the same directory as the html file -->
<link rel="stylesheet" href="custom.css">

<!-- Loading mathjax macro -->
<!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration --></head>
<body>
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Done by:</p>
<p>Kristoporus Nathan Wilianto A0170760J</p>
<p>Samuel Ng A0164633E</p>
<p>Toh Jun Hao A0168686H</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1>IS4242 Group Project</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><b>Import necessary libraries</b></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="k">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="k">import</span> <span class="n">SimpleImputer</span>
<span class="kn">from</span> <span class="nn">sklearn.compose</span> <span class="k">import</span> <span class="n">ColumnTransformer</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="k">import</span> <span class="n">SelectKBest</span><span class="p">,</span> <span class="n">f_classif</span><span class="p">,</span> <span class="n">VarianceThreshold</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LinearRegression</span><span class="p">,</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">roc_auc_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">mean_squared_error</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="k">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="k">import</span> <span class="n">KNeighborsRegressor</span><span class="p">,</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process</span> <span class="k">import</span> <span class="n">GaussianProcessClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="k">import</span> <span class="n">DecisionTreeClassifier</span><span class="p">,</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="k">import</span> <span class="n">MLPRegressor</span><span class="p">,</span> <span class="n">MLPClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">AdaBoostClassifier</span><span class="p">,</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="k">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="nn">imblearn.pipeline</span> <span class="k">import</span> <span class="n">Pipeline</span> <span class="k">as</span> <span class="n">imPipeline</span>
<span class="kn">from</span> <span class="nn">imblearn.over_sampling</span> <span class="k">import</span> <span class="n">SMOTE</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">imblearn.pipeline</span> <span class="k">import</span> <span class="n">Pipeline</span> <span class="k">as</span> <span class="n">imPipeline</span>
<span class="kn">from</span> <span class="nn">imblearn.over_sampling</span> <span class="k">import</span> <span class="n">SMOTE</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><h5>Running the code</h5>
Ensure that you are in the root folder of all the fold folders and target files
read_text(fold_name):
    fold_name: this is the name of the fold you want to read ALL patient files of. It will be read into a 2 dimensional
    list. If you would like to retrieve just the first patient instead, you will need to change the line 
    "txt_all.extend(txt[1:])" to "txt_all.append(txt[1:])" and you will be to use "read_text(fold1.txt)[0]" to retrieve
    the relevant patient's data
read_ans(file_name):
    file_name: this is the name of the file you want to read ALL targets of. It will be read into a 2 dimensional
    list. To retrieve the first patient's target: read_ans(ans.csv)[0]
put_single_into_dataframe(txt): This functions takes in 2 dimensional list ie the output of read_text(fold1.txt) 
put_multiple_into_dataframe(txt): Multiple is for using it with the output of read_text after you wanted to change it to append</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">read_text</span><span class="p">(</span><span class="n">fold_name</span><span class="p">):</span>
    <span class="n">txt_all</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">fold_name</span><span class="p">):</span> <span class="c1"># for each file in the directory</span>
        <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.txt&quot;</span><span class="p">):</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">fold_name</span><span class="p">,</span> <span class="n">f</span><span class="p">),</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span> <span class="c1"># open each file</span>
                <span class="n">txt</span> <span class="o">=</span> <span class="n">fp</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span> <span class="c1"># read inside the file</span>
                <span class="n">recordid</span> <span class="o">=</span> <span class="n">txt</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">rstrip</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># get recordid</span>
                <span class="n">txt</span> <span class="o">=</span> <span class="p">[[</span><span class="nb">int</span><span class="p">(</span><span class="n">recordid</span><span class="p">)]</span> <span class="o">+</span> <span class="n">t</span><span class="o">.</span><span class="n">rstrip</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">txt</span><span class="p">]</span> <span class="c1"># preface each row with the recordid as all patients are 1 file</span>
                <span class="n">txt_all</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">txt</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="c1"># skip the parameter list</span>
    <span class="k">return</span> <span class="n">txt_all</span>

<span class="k">def</span> <span class="nf">read_one_text</span><span class="p">(</span><span class="n">fold_name</span><span class="p">):</span>
    <span class="n">txt_all</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">fold_name</span><span class="p">):</span> <span class="c1"># for each file in the directory</span>
        <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.txt&quot;</span><span class="p">):</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">fold_name</span><span class="p">,</span> <span class="n">f</span><span class="p">),</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span> <span class="c1"># open each file</span>
                <span class="n">txt</span> <span class="o">=</span> <span class="n">fp</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span> <span class="c1"># read inside the file</span>
            <span class="n">recordid</span> <span class="o">=</span> <span class="n">txt</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">rstrip</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># get recordid</span>
            <span class="n">txt</span> <span class="o">=</span> <span class="p">[[</span><span class="nb">int</span><span class="p">(</span><span class="n">recordid</span><span class="p">)]</span> <span class="o">+</span> <span class="n">t</span><span class="o">.</span><span class="n">rstrip</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">txt</span><span class="p">]</span> <span class="c1"># preface each row with the recordid as all patients are 1 file</span>
            <span class="n">txt_all</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">txt</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="c1"># skip the parameter list</span>
    <span class="k">return</span> <span class="n">txt_all</span>

<span class="k">def</span> <span class="nf">read_ans</span><span class="p">(</span><span class="n">file_name</span><span class="p">):</span>
    <span class="n">txt_all</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_name</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span> <span class="c1"># opens the csv file</span>
        <span class="n">txt</span> <span class="o">=</span> <span class="n">fp</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span> 
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">txt</span><span class="p">)):</span> <span class="c1"># similar to above read_text</span>
        <span class="n">record_id</span><span class="p">,</span> <span class="n">length_of_stay</span><span class="p">,</span> <span class="n">hospital_death</span> <span class="o">=</span> <span class="n">txt</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">rstrip</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>
        <span class="n">txt_all</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">record_id</span><span class="p">,</span> <span class="n">length_of_stay</span><span class="p">,</span> <span class="n">hospital_death</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">txt_all</span>

<span class="k">def</span> <span class="nf">put_multiple_into_dataframe</span><span class="p">(</span><span class="n">txt_all</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">txt_all</span><span class="p">:</span>
        <span class="n">df2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;recordid&#39;</span><span class="p">,</span> <span class="s1">&#39;time&#39;</span><span class="p">,</span> <span class="s1">&#39;parameter&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">])</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df</span>

<span class="k">def</span> <span class="nf">put_single_into_dataframe</span><span class="p">(</span><span class="n">txt_all</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">txt_all</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;recordid&#39;</span><span class="p">,</span> <span class="s1">&#39;time&#39;</span><span class="p">,</span> <span class="s1">&#39;parameter&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">df</span>

<span class="k">def</span> <span class="nf">get_X_add_ready</span><span class="p">(</span><span class="n">X_add</span><span class="p">,</span> <span class="n">stat</span><span class="p">):</span>
    <span class="n">X_add</span> <span class="o">=</span> <span class="n">X_add</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
    <span class="n">X_add</span> <span class="o">=</span> <span class="n">X_add</span><span class="o">.</span><span class="n">pivot</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="s1">&#39;recordid&#39;</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="s1">&#39;parameter&#39;</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="s1">&#39;value&#39;</span><span class="p">)</span>
    <span class="n">X_add</span> <span class="o">=</span> <span class="n">X_add</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">stat_feat</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> 
<span class="c1">#     X_add = X_add.drop([&#39;RecordID&#39;], axis = 1) </span>
    <span class="n">X_add</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">+</span><span class="n">stat</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X_add</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>
    <span class="n">X_add</span> <span class="o">=</span> <span class="n">X_add</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">X_add</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1>1. Data Exploration</h1><p>Firstly, read_one_text function is used to read in the folder (eg. Fold1) containing the patient's individual record file. The index 0 in this case in the first line: p1 = read_one_text('../Project_Data/Fold1')[0] is to retrieve the first patient's record in the folder entered inside the function read_one_text. For example, to retrieve the last patient record in Fold3 folder, one has to type: p1 = read_one_text('../Project_Data/Fold3')[999]</p>
<p>Format to get individual record:<br>
Line 1: p1 = read_one_text('../Project_Data/<b>x</b>')[<b>y</b>]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>x</b> = folder of the patient to retrieve<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>y</b> = index row of the patient inside the folder - 1</p>
<p>Next, this patient record will then undergo preprocessing steps as follows to remove unnecessary information and transform the data into a more meaningful one. This patient record is transformed into dataframe with the columns 'recordid', 'time', 'parameter', 'value'. The recordid of this patient is stored in a variable called 'recordId' before dropping both the column 'recordid' and the first row in the dataframe which contains the recordid of this patient as they are now redundant.</p>
<p>Subsequently, data in the column for 'value' is converted to numeric so as to facilitate the plotting of graph later. A new column called 'time_value' is created to store a tuple of 'time' and 'value'. The dataframe is then grouped by the 'parameter' column and then the 'time_value' tuple is then stored in a list based on the parameter.</p>
<p>Finally, the groupby object is then plotted into a graph with each unique parameter as a different line in the graph with the x-axis in hours and y-axis as the values which the parameter is measured.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">p1</span> <span class="o">=</span> <span class="n">read_one_text</span><span class="p">(</span><span class="s2">&quot;../Project_Data/Fold1&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">p1df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;recordid&#39;</span><span class="p">,</span> <span class="s1">&#39;time&#39;</span><span class="p">,</span> <span class="s1">&#39;parameter&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">])</span>
<span class="n">recordId</span> <span class="o">=</span> <span class="n">p1df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
<span class="n">df_1</span> <span class="o">=</span> <span class="n">p1df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;recordid&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df_1</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">df_1</span><span class="o">.</span><span class="n">index</span><span class="p">[:</span><span class="mi">1</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># df_1</span>

<span class="n">df_1</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">df_1</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">])</span>
<span class="n">df_1</span><span class="o">.</span><span class="n">time</span> <span class="o">=</span> <span class="n">df_1</span><span class="o">.</span><span class="n">time</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;str&#39;</span><span class="p">)</span>
<span class="n">df_1</span><span class="p">[</span><span class="s1">&#39;time&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_1</span><span class="p">[</span><span class="s1">&#39;time&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
<span class="n">df_1</span><span class="p">[</span><span class="s1">&#39;time&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">df_1</span><span class="p">[</span><span class="s1">&#39;time&#39;</span><span class="p">])</span>
<span class="n">df_1</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">][</span><span class="n">df_1</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">df_1</span><span class="p">[</span><span class="s1">&#39;time_value&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">df_1</span><span class="o">.</span><span class="n">time</span><span class="p">,</span> <span class="n">df_1</span><span class="o">.</span><span class="n">value</span><span class="p">))</span>
<span class="n">df_2</span> <span class="o">=</span> <span class="n">df_1</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;parameter&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">time_value</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>

<span class="n">major_ticks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">51</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">minor_ticks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">49</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">30</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_2</span><span class="p">)):</span>
    <span class="n">testList2</span> <span class="o">=</span> <span class="p">[(</span><span class="n">elem1</span><span class="p">,</span> <span class="n">elem2</span><span class="p">)</span> <span class="k">for</span> <span class="n">elem1</span><span class="p">,</span> <span class="n">elem2</span> <span class="ow">in</span> <span class="n">df_2</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
    <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">testList2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">testList2</span><span class="p">),</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">df_2</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time (hours)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Value&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
<span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Temporal Data for the first 48 hours for patient of recordid: &#39;</span> <span class="o">+</span> <span class="n">recordId</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="c1"># plt.xlim(xmin=0.0)</span>
<span class="c1"># plt.ylim(ymin=0.0)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">minorticks_on</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">minor_ticks</span><span class="p">,</span> <span class="n">minor</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">major_ticks</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">minor_ticks</span><span class="p">,</span> <span class="n">minor</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">&#39;minor&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">&#39;major&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="c1"># ax.tick_params(which = &#39;both&#39;, direction = &#39;out&#39;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\samue\Anaconda3\lib\site-packages\ipykernel_launcher.py:12: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
  if sys.path[0] == &#39;&#39;:
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df_feat</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">numberOfFolds</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="s2">&quot;Fold1&quot;</span><span class="p">]):</span> <span class="c1"># what folds do you want to use?</span>
    <span class="n">string</span> <span class="o">=</span> <span class="s2">&quot;../Project_Data/&quot;</span>
    <span class="n">string</span> <span class="o">+=</span> <span class="n">name</span>
    <span class="n">df_feat</span> <span class="o">=</span> <span class="n">df_feat</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">put_single_into_dataframe</span><span class="p">(</span><span class="n">read_text</span><span class="p">(</span><span class="n">string</span><span class="p">)))</span>
    <span class="n">numberOfFolds</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="n">numberOfRows</span> <span class="o">=</span> <span class="n">numberOfFolds</span><span class="o">*</span><span class="mi">1000</span>
<span class="n">df_feat</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[3]:</div>



<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>recordid</th>
      <th>time</th>
      <th>parameter</th>
      <th>value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>132539</td>
      <td>00:00</td>
      <td>RecordID</td>
      <td>132539</td>
    </tr>
    <tr>
      <th>1</th>
      <td>132539</td>
      <td>00:00</td>
      <td>Age</td>
      <td>54</td>
    </tr>
    <tr>
      <th>2</th>
      <td>132539</td>
      <td>00:00</td>
      <td>Gender</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>132539</td>
      <td>00:00</td>
      <td>Height</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>132539</td>
      <td>00:00</td>
      <td>ICUType</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Reading Target</span>
<span class="n">df_target</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">read_ans</span><span class="p">(</span><span class="s1">&#39;../Project_Data/Fold1_Outcomes.csv&#39;</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;recordid&#39;</span><span class="p">,</span> <span class="s1">&#39;days_in_hospital&#39;</span><span class="p">,</span> <span class="s1">&#39;mortality&#39;</span><span class="p">])</span>
<span class="n">df_target</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[4]:</div>



<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>recordid</th>
      <th>days_in_hospital</th>
      <th>mortality</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>132539</td>
      <td>5</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>132540</td>
      <td>8</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>132541</td>
      <td>19</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>132543</td>
      <td>9</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>132545</td>
      <td>4</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bin_feat</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;MechVent&#39;</span><span class="p">]</span>
<span class="n">num_feat</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Albumin&#39;</span><span class="p">,</span> <span class="s1">&#39;ALP&#39;</span><span class="p">,</span> <span class="s1">&#39;ALT&#39;</span><span class="p">,</span> <span class="s1">&#39;AST&#39;</span><span class="p">,</span> <span class="s1">&#39;Bilirubin&#39;</span><span class="p">,</span> <span class="s1">&#39;BUN&#39;</span><span class="p">,</span> <span class="s1">&#39;Cholesterol&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Creatinine&#39;</span><span class="p">,</span> <span class="s1">&#39;DiasABP&#39;</span><span class="p">,</span> <span class="s1">&#39;FiO2&#39;</span><span class="p">,</span> <span class="s1">&#39;GCS&#39;</span><span class="p">,</span> <span class="s1">&#39;Glucose&#39;</span><span class="p">,</span> <span class="s1">&#39;HCO3&#39;</span><span class="p">,</span> <span class="s1">&#39;HCT&#39;</span><span class="p">,</span>
           <span class="s1">&#39;HR&#39;</span><span class="p">,</span> <span class="s1">&#39;K&#39;</span><span class="p">,</span> <span class="s1">&#39;Lactate&#39;</span><span class="p">,</span> <span class="s1">&#39;Mg&#39;</span><span class="p">,</span> <span class="s1">&#39;MAP&#39;</span><span class="p">,</span> <span class="s1">&#39;NA&#39;</span><span class="p">,</span> <span class="s1">&#39;NIDiasABP&#39;</span><span class="p">,</span> <span class="s1">&#39;NIMAP&#39;</span><span class="p">,</span>
           <span class="s1">&#39;NISysABP&#39;</span><span class="p">,</span> <span class="s1">&#39;PaCO2&#39;</span><span class="p">,</span> <span class="s1">&#39;PaO2&#39;</span><span class="p">,</span> <span class="s1">&#39;pH&#39;</span><span class="p">,</span> <span class="s1">&#39;Platelets&#39;</span><span class="p">,</span> <span class="s1">&#39;RespRate&#39;</span><span class="p">,</span> <span class="s1">&#39;SaO2&#39;</span><span class="p">,</span>
           <span class="s1">&#39;SysABP&#39;</span><span class="p">,</span> <span class="s1">&#39;Temp&#39;</span><span class="p">,</span> <span class="s1">&#39;Tropl&#39;</span><span class="p">,</span> <span class="s1">&#39;TropT&#39;</span><span class="p">,</span> <span class="s1">&#39;Urine&#39;</span><span class="p">,</span> <span class="s1">&#39;WBC&#39;</span><span class="p">,</span> <span class="s1">&#39;Weight&#39;</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of record ids:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">df_feat</span><span class="p">[</span><span class="s1">&#39;recordid&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()))</span>
<span class="n">unique_count</span> <span class="o">=</span> <span class="n">df_feat</span><span class="p">[</span><span class="s1">&#39;parameter&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">/</span><span class="n">numberOfRows</span>
<span class="nb">print</span><span class="p">(</span><span class="n">unique_count</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Number of record ids: 1000
HR             57.027
MAP            36.092
SysABP         35.979
DiasABP        35.955
Urine          34.208
Weight         33.679
NISysABP       24.457
NIDiasABP      24.424
NIMAP          24.088
Temp           21.204
GCS            15.214
RespRate       13.775
FiO2            7.815
MechVent        7.596
pH              5.770
PaO2            5.496
PaCO2           5.490
HCT             4.626
K               3.708
Creatinine      3.573
Platelets       3.566
BUN             3.547
HCO3            3.479
Mg              3.468
Na              3.462
Glucose         3.338
WBC             3.286
SaO2            1.985
Lactate         1.924
Height          1.000
RecordID        1.000
ICUType         1.000
Gender          1.000
Age             1.000
Bilirubin       0.858
AST             0.857
ALT             0.857
ALP             0.833
Albumin         0.617
TroponinT       0.566
TroponinI       0.130
Cholesterol     0.077
Name: parameter, dtype: float64
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><h2>Analysis of Features</h2></p>
<p>The data above shows the average number of times a variable observed per patient. Based on the data above and the feature description we classify the features into these categories:
<ul>
    <li>General Descriptors (static data) that are collected when the patient is admitted to the ICU. Weight is not included as weight are measured multiple times as a time series data. Each of the descriptors will be included as a feature into the model.</li>
    <li>Rare features: measured on average less than one time per patient (less than 1.0). We use the <u>existence</u> of these measurements for each patient as a feature.</li>
    <li>Features that measured often or more that one time per patient (more than 1.0). Calculate the hourly average of each measurements and put them into 48 columns. <i>Example, average HR on the first hour to HR_1, average HR on the second hour to HR_2, and so on.</i></li>
</ul>
</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">stat_feat</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">,</span> <span class="s1">&#39;Gender&#39;</span><span class="p">,</span> <span class="s1">&#39;Height&#39;</span><span class="p">,</span> <span class="s1">&#39;ICUType&#39;</span><span class="p">,</span> <span class="s1">&#39;RecordID&#39;</span><span class="p">]</span> <span class="c1">#General Descriptors</span>
<span class="n">rare_feat</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">nor_feat</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">unique_count</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">value</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
        <span class="n">rare_feat</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">index</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stat_feat</span><span class="p">:</span>
        <span class="n">nor_feat</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
<span class="n">rare_feat</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;MechVent&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Rare features&quot;</span><span class="p">,</span> <span class="n">rare_feat</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Normal features&quot;</span><span class="p">,</span> <span class="n">nor_feat</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Rare features [&#39;Bilirubin&#39;, &#39;AST&#39;, &#39;ALT&#39;, &#39;ALP&#39;, &#39;Albumin&#39;, &#39;TroponinT&#39;, &#39;TroponinI&#39;, &#39;Cholesterol&#39;, &#39;MechVent&#39;]
Normal features [&#39;HR&#39;, &#39;MAP&#39;, &#39;SysABP&#39;, &#39;DiasABP&#39;, &#39;Urine&#39;, &#39;Weight&#39;, &#39;NISysABP&#39;, &#39;NIDiasABP&#39;, &#39;NIMAP&#39;, &#39;Temp&#39;, &#39;GCS&#39;, &#39;RespRate&#39;, &#39;FiO2&#39;, &#39;MechVent&#39;, &#39;pH&#39;, &#39;PaO2&#39;, &#39;PaCO2&#39;, &#39;HCT&#39;, &#39;K&#39;, &#39;Creatinine&#39;, &#39;Platelets&#39;, &#39;BUN&#39;, &#39;HCO3&#39;, &#39;Mg&#39;, &#39;Na&#39;, &#39;Glucose&#39;, &#39;WBC&#39;, &#39;SaO2&#39;, &#39;Lactate&#39;]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># your code to produce test and train data</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df_feat</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><h2>Creation of Data Matrices</h2></p>
<p><p>We create 3 different matrices to convert temporal data into a matrix that is a single feature vector per patient.</p></p>
<p><h3>First Design Matrix</h3></p>
<ul> 
    <li>In the cell below, we create a matrix that generalises a patient's attributes across the whole 48 hours, such as his max BUN measurement over the 48 hours. This is so as to create a much denser feature matrix as well as provide a much clearer signal to the learner, allowing it to generalise to data outside of the train set.</li>
    <li>We will be utilising the min, max and mean of the measurements</li>
    <li>We have converted sparse features like ALS into a binary variable where 1 represents "It was recored" and 0 representing "It was not recorded at all".</li>
    <li>ICUType was one-hot encoded</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">preprocess_x_for_design_matrix_1</span><span class="p">(</span><span class="n">df_feat</span><span class="p">):</span>
    <span class="n">non_bin_feat</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
    <span class="c1"># your code to produce test and train data</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">df_feat</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">])</span>
    <span class="n">temp_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;time&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;00:00&#39;</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span> <span class="c1"># get all the variables at time 0</span>
    <span class="n">temp_df</span> <span class="o">=</span> <span class="n">temp_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">temp_df</span><span class="p">[</span><span class="s1">&#39;parameter&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">stat_feat</span><span class="p">)]</span> <span class="c1"># prune the dataframe to only those static variables</span>
    <span class="n">temp_df</span> <span class="o">=</span> <span class="n">temp_df</span><span class="o">.</span><span class="n">pivot</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="s1">&#39;recordid&#39;</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="s1">&#39;parameter&#39;</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="s1">&#39;value&#39;</span><span class="p">)</span> 
    <span class="n">temp_df</span> <span class="o">=</span> <span class="n">temp_df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">temp_df</span><span class="p">:</span> <span class="c1"># for loop to change all the -1 values for static variables into np.nan</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">temp_df</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="n">temp_df</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">idx</span><span class="p">:</span>
            <span class="n">temp_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
    <span class="n">final_df</span> <span class="o">=</span> <span class="n">temp_df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1">#     Dealing with rare_feat</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">df_feat</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">&#39;recordid&#39;</span><span class="p">,</span> <span class="s1">&#39;parameter&#39;</span><span class="p">])[[</span><span class="s1">&#39;value&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">specialFeature</span><span class="p">(</span><span class="n">special</span><span class="p">):</span>
        <span class="nb">id</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">d</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">index</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">special</span><span class="p">:</span>
                <span class="nb">id</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">return</span> <span class="nb">id</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">rare_feat</span><span class="p">:</span>
        <span class="nb">id</span> <span class="o">=</span> <span class="n">specialFeature</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">final_df</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">id</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">final_df</span><span class="o">.</span><span class="n">index</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">row</span> <span class="o">==</span> <span class="n">i</span><span class="p">:</span>
                    <span class="n">final_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">final_df</span> <span class="o">=</span> <span class="n">final_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s2">&quot;RecordID&quot;</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Getting the different attributes</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;display.max_columns&#39;</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
    <span class="n">temp_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;parameter&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">rare_feat</span><span class="p">)]</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
    <span class="n">temp_df</span> <span class="o">=</span> <span class="n">temp_df</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">&#39;recordid&#39;</span><span class="p">,</span> <span class="s1">&#39;parameter&#39;</span><span class="p">])[[</span><span class="s1">&#39;value&#39;</span><span class="p">]]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="s1">&#39;max&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">]:</span> <span class="c1"># the different parameters we will use</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span><span class="o">==</span><span class="s1">&#39;min&#39;</span><span class="p">):</span>
            <span class="n">X_add</span> <span class="o">=</span> <span class="n">temp_df</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="c1"># get the min of each parameter</span>
            <span class="n">X_add</span> <span class="o">=</span> <span class="n">get_X_add_ready</span><span class="p">(</span><span class="n">X_add</span><span class="p">,</span> <span class="s1">&#39;_min&#39;</span><span class="p">)</span>
            <span class="n">final_df</span> <span class="o">=</span> <span class="n">final_df</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">X_add</span><span class="p">,</span> <span class="n">left_on</span><span class="o">=</span><span class="s1">&#39;recordid&#39;</span><span class="p">,</span> <span class="n">right_on</span><span class="o">=</span><span class="s1">&#39;recordid&#39;</span><span class="p">)</span> <span class="c1"># merge the min of the parameters to the final dataframe</span>
        <span class="k">elif</span> <span class="p">(</span><span class="n">i</span><span class="o">==</span><span class="s1">&#39;max&#39;</span><span class="p">):</span>
            <span class="n">X_add</span> <span class="o">=</span> <span class="n">temp_df</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="c1"># get the max of each parameter</span>
            <span class="n">X_add</span> <span class="o">=</span> <span class="n">get_X_add_ready</span><span class="p">(</span><span class="n">X_add</span><span class="p">,</span> <span class="s1">&#39;_max&#39;</span><span class="p">)</span>
            <span class="n">final_df</span> <span class="o">=</span> <span class="n">final_df</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">X_add</span><span class="p">,</span> <span class="n">left_on</span><span class="o">=</span><span class="s1">&#39;recordid&#39;</span><span class="p">,</span> <span class="n">right_on</span><span class="o">=</span><span class="s1">&#39;recordid&#39;</span><span class="p">)</span> <span class="c1"># merge the min of the parameters to the final dataframe</span>
        <span class="k">elif</span> <span class="p">(</span><span class="n">i</span><span class="o">==</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
            <span class="n">X_add</span> <span class="o">=</span> <span class="n">temp_df</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="c1"># get the mean of each parameter</span>
            <span class="n">X_add</span> <span class="o">=</span> <span class="n">get_X_add_ready</span><span class="p">(</span><span class="n">X_add</span><span class="p">,</span> <span class="s1">&#39;_mean&#39;</span><span class="p">)</span>
            <span class="n">final_df</span> <span class="o">=</span> <span class="n">final_df</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">X_add</span><span class="p">,</span> <span class="n">left_on</span><span class="o">=</span><span class="s1">&#39;recordid&#39;</span><span class="p">,</span> <span class="n">right_on</span><span class="o">=</span><span class="s1">&#39;recordid&#39;</span><span class="p">)</span> <span class="c1"># merge the min of the parameters to the final dataframe</span>

    <span class="c1"># dealing with ICUType categorical</span>
    <span class="n">one_hot</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">final_df</span><span class="p">[</span><span class="s1">&#39;ICUType&#39;</span><span class="p">])</span>
    <span class="n">meaning_of_icu_types</span> <span class="o">=</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span><span class="s1">&#39;Coronary Care Unit&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="s1">&#39;Cardiac Surgery Recovery Unit&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="s1">&#39;Medical ICU&#39;</span><span class="p">,</span> <span class="mi">4</span><span class="p">:</span> <span class="s1">&#39;Surgical ICU&#39;</span><span class="p">}</span>
    <span class="n">one_hot</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">meaning_of_icu_types</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">one_hot</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>
    <span class="n">final_df</span> <span class="o">=</span> <span class="n">final_df</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">one_hot</span><span class="p">,</span> <span class="n">left_index</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">right_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">final_df</span> <span class="o">=</span> <span class="n">final_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;ICUType&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># Extreme height values is set to np.nan</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">final_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;Height&quot;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">40</span> <span class="ow">or</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;Height&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">210</span><span class="p">:</span>
            <span class="n">row</span><span class="p">[</span><span class="s2">&quot;Height&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>


    <span class="c1"># Drop recordID column</span>
    <span class="n">final_df</span> <span class="o">=</span> <span class="n">final_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;recordid&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            
    <span class="c1"># Creating non binary column list and filling na values with mean</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">final_df</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">rare_feat</span> <span class="ow">or</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">bin_feat</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">final_df</span> <span class="o">=</span> <span class="n">final_df</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">final_df</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
        <span class="n">non_bin_feat</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        
<span class="c1">#     display(final_df.head())</span>
    <span class="k">return</span> <span class="n">final_df</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><h3>Second Design Matrix</h3></p>
<ul> 
    <li>The second design matrix is created in such a way that every parameters is grouped into a 12-hour bin (eg. 0-12, 12-24, 24-36, 36-48). If a parameter is measured multiple times within a 12-hour bin, a mean of those measurement will be recorded to represent the measurement for that 12-hour bin instead. The assumption for using the mean for 12-hour is that once patients are sent to the ICU, they must be closely monitored and furthermore, the working shift of nurses in hospital is usually by 12-hour and so this ensures lesser human errors when the nurses change shift and patient is being measured.</li>
        <li>We have converted sparse features like ALS into a binary variable where 1 represents "It was recored" and 0 representing "It was not recorded at all".</li>
    <li>ICUType was one-hot encoded</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[29]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Second Matrix</span>
<span class="k">def</span> <span class="nf">preprocess_x_for_design_matrix_2</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="n">non_bin_feat</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
    
    <span class="c1"># retrieving static data of the patients at time 0 and transformed into dataframe with static feature as the column </span>
    <span class="c1"># headers and each row in the dataframe representing each patient record, which is indexed by their record id</span>
    <span class="n">df_static</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;time&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;00:00&#39;</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">static_vars</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;RecordID&#39;</span><span class="p">,</span> <span class="s1">&#39;Age&#39;</span><span class="p">,</span> <span class="s1">&#39;Gender&#39;</span><span class="p">,</span> <span class="s1">&#39;Height&#39;</span><span class="p">,</span> <span class="s1">&#39;ICUType&#39;</span><span class="p">,</span> <span class="s1">&#39;Weight&#39;</span><span class="p">]</span>
    <span class="n">df_static</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;time&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">df_static</span> <span class="o">=</span> <span class="n">df_static</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df_static</span><span class="p">[</span><span class="s1">&#39;parameter&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">static_vars</span><span class="p">)]</span>
    <span class="n">df_static</span> <span class="o">=</span> <span class="n">df_static</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">&#39;recordid&#39;</span><span class="p">,</span> <span class="s1">&#39;parameter&#39;</span><span class="p">])[[</span><span class="s1">&#39;value&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">last</span><span class="p">()</span>
    <span class="n">df_static</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">df_static</span> <span class="o">=</span> <span class="n">df_static</span><span class="o">.</span><span class="n">pivot</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="s1">&#39;recordid&#39;</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="s1">&#39;parameter&#39;</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="s1">&#39;value&#39;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">df_static</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
        <span class="n">df_static</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">df_static</span><span class="p">[</span><span class="n">c</span><span class="p">])</span>

    <span class="c1"># replacing height and weight which does not exist (represented with &#39;-1&#39;) with NaN</span>
    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">df_static</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">df_static</span><span class="p">[</span><span class="n">c</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">c</span> <span class="o">==</span> <span class="s1">&#39;Height&#39;</span><span class="p">:</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="mi">0</span>
            <span class="n">df_static</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
        <span class="k">elif</span> <span class="n">c</span> <span class="o">==</span> <span class="s1">&#39;Weight&#39;</span><span class="p">:</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="mi">0</span>
            <span class="n">df_static</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">&#39;recordid&#39;</span><span class="p">,</span> <span class="s1">&#39;parameter&#39;</span><span class="p">])[[</span><span class="s1">&#39;value&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">specialFeature</span><span class="p">(</span><span class="n">special</span><span class="p">):</span>
        <span class="nb">id</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">d</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">index</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">special</span><span class="p">:</span>
                <span class="nb">id</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">return</span> <span class="nb">id</span>

    <span class="c1"># RecordID column is dropped as every row is already indexed by the recordid</span>
    <span class="n">df2</span> <span class="o">=</span> <span class="n">df_static</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">df2</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;RecordID&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># rare features which are rarely measured such as Cholesterol are converted to binary features which indicates whether </span>
    <span class="c1"># any measurement for these variables were recorded within the first 48 hours. Binary feature was also created for </span>
    <span class="c1"># MechVent to indicate whether the patient was placed on mechanical ventilation</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">rare_feat</span><span class="p">:</span>
        <span class="nb">id</span> <span class="o">=</span> <span class="n">specialFeature</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">df2</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">id</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">df2</span><span class="o">.</span><span class="n">index</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">row</span> <span class="o">==</span> <span class="n">i</span><span class="p">:</span>
                    <span class="n">df2</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">df2</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
    <span class="n">normal_feat</span> <span class="o">=</span> <span class="n">nor_feat</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">normal_feat</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s1">&#39;MechVent&#39;</span><span class="p">)</span>
    
    <span class="c1"># grouped the rest of the parameters into 12-hour bins and the mean of the parameter in a given 12-hour interval</span>
    <span class="c1"># is taken if it was measured more than once during that 12-hour interval</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;parameter&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">normal_feat</span><span class="p">)</span>
    <span class="n">df3</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">df3</span><span class="p">[[</span><span class="s1">&#39;hour&#39;</span><span class="p">,</span><span class="s1">&#39;min&#39;</span><span class="p">]]</span> <span class="o">=</span> <span class="n">df3</span><span class="o">.</span><span class="n">time</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">expand</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">df3</span><span class="p">[</span><span class="s2">&quot;hour&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">df3</span><span class="p">[</span><span class="s2">&quot;hour&quot;</span><span class="p">])</span>
    <span class="n">df3</span><span class="p">[</span><span class="s2">&quot;value&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">df3</span><span class="p">[</span><span class="s2">&quot;value&quot;</span><span class="p">])</span>
    <span class="n">bins</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="mi">48</span><span class="p">]</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="s1">&#39;12&#39;</span><span class="p">,</span> <span class="s1">&#39;24&#39;</span><span class="p">,</span> <span class="s1">&#39;36&#39;</span><span class="p">]</span>
    
    <span class="c1"># grouped by recordid first, then by each 12-hour interval then by the parameters</span>
    <span class="n">df3</span> <span class="o">=</span> <span class="n">df3</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">&#39;recordid&#39;</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">df3</span><span class="o">.</span><span class="n">hour</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">),</span> <span class="s1">&#39;parameter&#39;</span><span class="p">])[[</span><span class="s1">&#39;value&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">df3</span>

    <span class="c1"># filled in the dataframe with NaN first</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">normal_feat</span><span class="p">:</span>    
        <span class="n">df2</span><span class="p">[</span><span class="n">n</span> <span class="o">+</span><span class="s1">&#39;0&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
        <span class="n">df2</span><span class="p">[</span><span class="n">n</span> <span class="o">+</span><span class="s1">&#39;12&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
        <span class="n">df2</span><span class="p">[</span><span class="n">n</span> <span class="o">+</span><span class="s1">&#39;24&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
        <span class="n">df2</span><span class="p">[</span><span class="n">n</span> <span class="o">+</span><span class="s1">&#39;36&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
    <span class="n">df2</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>

    <span class="c1"># go through every row in groupby table and insert these values into the dataframe with NaN, replacing them</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">df3</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
        <span class="n">recordId</span> <span class="o">=</span> <span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">hour</span> <span class="o">=</span> <span class="n">index</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">parameter</span> <span class="o">=</span> <span class="n">index</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">df2</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">recordId</span><span class="p">,</span> <span class="n">parameter</span><span class="o">+</span><span class="n">hour</span><span class="p">]</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;value&quot;</span><span class="p">]</span>

    <span class="n">one_hot</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;ICUType&#39;</span><span class="p">])</span>
    <span class="n">meaning_of_icu_types</span> <span class="o">=</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span><span class="s1">&#39;Coronary Care Unit&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="s1">&#39;Cardiac Surgery Recovery Unit&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="s1">&#39;Medical ICU&#39;</span><span class="p">,</span> <span class="mi">4</span><span class="p">:</span> <span class="s1">&#39;Surgical ICU&#39;</span><span class="p">}</span>
    <span class="n">one_hot</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">meaning_of_icu_types</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">one_hot</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>
    <span class="n">df2</span> <span class="o">=</span> <span class="n">df2</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">one_hot</span><span class="p">,</span> <span class="n">left_index</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">right_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">df2</span> <span class="o">=</span> <span class="n">df2</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;ICUType&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># Extreme height values is set to np.nan</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">df2</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;Height&quot;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">40</span> <span class="ow">or</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;Height&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">210</span><span class="p">:</span>
            <span class="n">row</span><span class="p">[</span><span class="s2">&quot;Height&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
            
    <span class="c1"># Creating non binary column list and filling na values with mean</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">df2</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">rare_feat</span> <span class="ow">or</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">bin_feat</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">df2</span> <span class="o">=</span> <span class="n">df2</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
        <span class="n">non_bin_feat</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

    <span class="n">display</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">df2</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><h3>Third Design Matrix</h3></p>
<p>This design matrix splits all numerical time series data into hourly bins. The average value of the measurements on each hour will be taken and put into the columns. The idea of this is to get the condition of the patient on each hour from coming in to the hospital until the 48 hour mark. All rare features will be taken as a binary feature of whether the measurement appear within 48 hours. All general descriptors taken as it is except weight.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">preprocess_x_for_design_matrix_3</span><span class="p">(</span><span class="n">df_feat</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    
    <span class="n">tot_values</span> <span class="o">=</span> <span class="p">{}</span> <span class="c1"># the sum of the values</span>
    <span class="n">count</span> <span class="o">=</span> <span class="p">{}</span> <span class="c1"># num of occurences of a measurement</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span><span class="n">row</span> <span class="ow">in</span> <span class="n">df_feat</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;parameter&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;RecordID&#39;</span> <span class="ow">and</span> <span class="n">index</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># count the average of the previous record</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span><span class="n">value</span> <span class="ow">in</span> <span class="n">tot_values</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">tot_values</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">tot_values</span><span class="p">[</span><span class="n">key</span><span class="p">])</span><span class="o">/</span><span class="n">count</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>

            <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tot_values</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">tot_values</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
            <span class="n">count</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">rare_feat</span><span class="p">:</span>
                <span class="n">tot_values</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
                <span class="n">count</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>

        <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;parameter&#39;</span><span class="p">]</span> <span class="ow">in</span> <span class="n">stat_feat</span><span class="p">:</span>
            <span class="n">tot_values</span><span class="p">[</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;parameter&#39;</span><span class="p">]]</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
            <span class="n">count</span><span class="p">[</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;parameter&#39;</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">elif</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;parameter&#39;</span><span class="p">]</span> <span class="ow">in</span> <span class="n">rare_feat</span><span class="p">:</span>
            <span class="n">tot_values</span><span class="p">[</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;parameter&#39;</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">count</span><span class="p">[</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;parameter&#39;</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">elif</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;parameter&#39;</span><span class="p">]</span> <span class="ow">in</span> <span class="n">nor_feat</span> <span class="ow">and</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;parameter&#39;</span><span class="p">]</span> <span class="o">!=</span> <span class="s1">&#39;MechVent&#39;</span><span class="p">:</span>
            <span class="n">hour</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;time&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">hour</span> <span class="o">==</span> <span class="mi">49</span><span class="p">:</span> <span class="n">hour</span><span class="o">-=</span><span class="mi">1</span>
            <span class="n">col</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;parameter&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39;_&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">hour</span><span class="p">)</span>
            <span class="n">tot_values</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">count</span><span class="p">:</span>
                <span class="n">count</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">count</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">count</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="c1"># count the average of the previous record</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span><span class="n">value</span> <span class="ow">in</span> <span class="n">tot_values</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">tot_values</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">tot_values</span><span class="p">[</span><span class="n">key</span><span class="p">])</span><span class="o">/</span><span class="n">count</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>

    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tot_values</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">astype</span><span class="p">({</span><span class="s1">&#39;RecordID&#39;</span><span class="p">:</span> <span class="s1">&#39;int32&#39;</span><span class="p">,</span> <span class="s1">&#39;ICUType&#39;</span><span class="p">:</span><span class="s1">&#39;int32&#39;</span><span class="p">})</span>
    
    <span class="k">return</span> <span class="n">df</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><h1>Model Building for M1</h1></p>
<p>We tried out various models and the ones listed below is the best we found empirically.</p>
<p>We develop models for our design matrix.</p><p>We have several common steps for all 3 model building which we will list. If there are any particularities, it will be further described at the respective model. Here are the steps.
    <ol>
        <li><b>Imputation</b>
            <ul>
                <li>Categorical: we impute empty data with the most frequent appearing value of that feature. Mean or median is not used for categorical because the value of a categorical feature doesn't depend on how big or small the value is but the value represents a category of that feature.</li>
                <li>Numerical: we impute the data with the mean value of the feature.</li>
            </ul>
        </li>
        <li><b>One-Hot Encoder</b> <i>(only for Categorical features)</i>
            <br>One-Hot encoding is used to replace the categories inside a categorical feature into multiple binary features. The purpose of this is to make sure that the model is not biased into thinking bigger values mean better categories. Example: A categorical feature has values 1,2,3 represents animals such as Dog, Cat, and Fish respectively. Say the model is counting the average of those numbers, then it can means the average of 1 and 3 is 2. Which equals to Dog and Fish on average is Cat which is obviously not true. Some models may have implemented this step in the creation of the design matrix already.</br>
            </li>
        <li><b>Scaler</b>
            <br>The data in this case is on different scale meaning some numbers are bigger than the others. The model might weigh the bigger values higher. To reduce this problem, we scale the data.
            </br>
        </li>
        <li><b>SelectKBest</b>
            <br>SelectKBest is used to extract features which is actually important for our prediction. There are a lot of features for some matrices (1400+) and some features might not be as influencing for our model. SelectKBest selects K best features that is useful for the model.</br>
        </li>
        <li><b>Variance Threshold</b>
            <br>Similar to how SelectKBest is used to extract features, where we work off the idea that when a feature doesnt vary much within itself, it generally has very little predictive power.</br>
        </li>
        <li><b>Dimensionality Reduction</b>
            <br>We understand that some features might be overlapping each other. We used PCA to extract the principle features to reduce redundant features.</br>
        </li>
        <li><b>Classifier Model</b>
            <br>We select our classifier model empirically based on which model has the best results on average across all folds. There are multiple learners we use that will be listed below.</br>
        </li>
    </ol>
    <p>We run <b>GridSearchCV</b> on our training data to optimize our hyperparameters. Our hyperparameter optimization is based on the ROC AUC or Mean Squared Error score. Hyperparameter optimization is important because each set of steps in our pipeline has different values such as the learning rate, K value for SelectKBest, or number of components at PCA, and each values will give different results and some are better than the others. We use this method mainly as way to find the best learner algorithm to use for M2.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[25]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># preprocess y dataframe for use in linear regression</span>
<span class="k">def</span> <span class="nf">preprocess_y_linear</span><span class="p">(</span><span class="n">temp_df</span><span class="p">):</span>
    <span class="n">temp_df</span> <span class="o">=</span> <span class="n">temp_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;recordid&#39;</span><span class="p">,</span><span class="s1">&#39;mortality&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">temp_df</span><span class="p">[</span><span class="s1">&#39;days_in_hospital&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">temp_df</span><span class="p">[</span><span class="s1">&#39;days_in_hospital&#39;</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">temp_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;days_in_hospital&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="n">row</span><span class="p">[</span><span class="s1">&#39;days_in_hospital&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>   
    <span class="k">return</span> <span class="n">temp_df</span>

<span class="c1"># preprocess y dataframe for use in classification</span>
<span class="k">def</span> <span class="nf">preprocess_y_class</span><span class="p">(</span><span class="n">temp_df</span><span class="p">):</span>
    <span class="n">temp_df</span> <span class="o">=</span> <span class="n">temp_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;recordid&#39;</span><span class="p">,</span><span class="s1">&#39;days_in_hospital&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">temp_df</span><span class="p">[</span><span class="s1">&#39;mortality&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">temp_df</span><span class="p">[</span><span class="s1">&#39;mortality&#39;</span><span class="p">])</span>  
    <span class="k">return</span> <span class="n">temp_df</span>

<span class="n">bin_feat</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Gender&quot;</span><span class="p">,</span> <span class="s2">&quot;Coronary Care Unit&quot;</span><span class="p">,</span> <span class="s2">&quot;Cardiac Surgery Recovery Unit&quot;</span><span class="p">,</span> <span class="s2">&quot;Medical ICU&quot;</span><span class="p">,</span> <span class="s2">&quot;Surgical ICU&quot;</span><span class="p">]</span>
<span class="n">non_bin_feat</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">(</span>
    <span class="n">remainder</span> <span class="o">=</span> <span class="s1">&#39;passthrough&#39;</span><span class="p">,</span>
    <span class="n">transformers</span><span class="o">=</span><span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;num&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">non_bin_feat</span><span class="p">)])</span>
<span class="n">all_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><h3>First Model</h3></p>
<ul> 
    <li>First, in the cell below, we make use of the first design matrix above to pass in as input to the models. Training is done on 3 folds and a test I.E. validation on the remaining fold. Included is 3 regresssion models (Linear, DecisionTree, MLP) and 3 classification models (AdaboostedDecisionTrees, MLP, GaussianNB). </li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Creating an array containing the preprocessed folds</span>
<span class="n">folds_x</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">folds_y_linear</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">folds_y_class</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">all_list</span><span class="p">:</span>
    <span class="n">x_string</span> <span class="o">=</span> <span class="s2">&quot;../Project_Data/Fold&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="n">y_string</span> <span class="o">=</span> <span class="s2">&quot;../Project_Data/Fold&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">+</span><span class="s2">&quot;_Outcomes.csv&quot;</span>
    <span class="n">temp_df_y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">read_ans</span><span class="p">(</span><span class="n">y_string</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;recordid&#39;</span><span class="p">,</span> <span class="s1">&#39;days_in_hospital&#39;</span><span class="p">,</span> <span class="s1">&#39;mortality&#39;</span><span class="p">])</span>
    <span class="n">temp_df_x</span> <span class="o">=</span> <span class="n">put_single_into_dataframe</span><span class="p">(</span><span class="n">read_text</span><span class="p">(</span><span class="n">x_string</span><span class="p">))</span>
    <span class="n">folds_x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">preprocess_x_for_design_matrix_1</span><span class="p">(</span><span class="n">temp_df_x</span><span class="p">)</span>
    <span class="n">folds_y_linear</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">preprocess_y_linear</span><span class="p">(</span><span class="n">temp_df_y</span><span class="p">)</span>
    <span class="n">folds_y_class</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">preprocess_y_class</span><span class="p">(</span><span class="n">temp_df_y</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4>Regressor Task</h4><p><p>Steps:</p></p>
<ol>
    <li><b>Standard Scaler</b></li>
    <li><b>Dimensionality Reducer with PCA</b></li>
    <li><b>Classifier</b>
        <br>We compare different regression models based on the mean squared error. The three classifiers are Linear Regression, DecisionTreeRegressor and MLPRegressor.</br>
    </li>
</ol>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[22]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Running CV</span>

<span class="c1">#Create a matrix to store the results where row represnets the model and column represents the result tested on fold i</span>
<span class="n">w</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span>
<span class="n">matrix</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">w</span><span class="p">)]</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">h</span><span class="p">)]</span> 
<span class="n">matrix_class</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">w</span><span class="p">)]</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">h</span><span class="p">)]</span> 

<span class="n">param</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>

<span class="n">all_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Testing on Fold&quot;</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">x_train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">y_train_linear_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">y_train_class_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">x_test_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">y_test_linear_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">y_test_class_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

    <span class="c1"># Getting train data set up</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;# Getting train data set up&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">all_list</span> <span class="k">if</span> <span class="n">x</span> <span class="o">!=</span> <span class="n">i</span><span class="p">]:</span>
        <span class="n">x_train_df</span> <span class="o">=</span> <span class="n">x_train_df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">folds_x</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
        <span class="n">y_train_linear_df</span> <span class="o">=</span> <span class="n">y_train_linear_df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">folds_y_linear</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
        <span class="n">y_train_class_df</span> <span class="o">=</span> <span class="n">y_train_class_df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">folds_y_class</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>

    <span class="c1"># Getting test data set up </span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;# Getting test data set up&quot;</span><span class="p">)</span>
    <span class="n">x_test_df</span> <span class="o">=</span> <span class="n">folds_x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">y_test_linear_df</span><span class="o">=</span> <span class="n">folds_y_linear</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">y_test_class_df</span> <span class="o">=</span> <span class="n">folds_y_class</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

    <span class="c1"># Linear</span>
    <span class="n">best</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">maxsize</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span>
                  <span class="s1">&#39;dim_reducer__n_components&#39;</span><span class="p">:[</span><span class="mi">60</span><span class="p">,</span><span class="mi">70</span><span class="p">]}</span>
    <span class="n">est</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[</span>
            <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">scaler</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;dim_reducer&#39;</span><span class="p">,</span> <span class="n">PCA</span><span class="p">()),</span>
            <span class="p">(</span><span class="s1">&#39;classifier&#39;</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">())])</span>
    <span class="n">estimator</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">est</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">)</span>
    <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train_df</span><span class="p">,</span> <span class="n">y_train_linear_df</span><span class="p">)</span><span class="c1"># gridsearch obejct has scanned thorugh all the best parameters to set it</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test_df</span><span class="p">)</span>
    <span class="n">matrix</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test_linear_df</span><span class="p">,</span> <span class="n">prediction</span><span class="p">))</span>
    
    
    <span class="c1"># DecisionTreeRegressor</span>
    <span class="n">best</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">maxsize</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span>
                  <span class="s1">&#39;dim_reducer__n_components&#39;</span><span class="p">:[</span><span class="mi">60</span><span class="p">,</span><span class="mi">70</span><span class="p">],</span>
                  <span class="s1">&#39;classifier__min_samples_leaf&#39;</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span>
                 <span class="p">}</span>
    <span class="n">est</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[</span>
            <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">scaler</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;dim_reducer&#39;</span><span class="p">,</span> <span class="n">PCA</span><span class="p">()),</span>
            <span class="p">(</span><span class="s1">&#39;classifier&#39;</span><span class="p">,</span> <span class="n">DecisionTreeRegressor</span><span class="p">())])</span>
    <span class="n">estimator</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">est</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">)</span>
    <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train_df</span><span class="p">,</span> <span class="n">y_train_linear_df</span><span class="p">)</span><span class="c1"># gridsearch obejct has scanned thorugh all the best parameters to set it</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test_df</span><span class="p">)</span>
    <span class="n">matrix</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test_linear_df</span><span class="p">,</span> <span class="n">prediction</span><span class="p">))</span>
    
    
    <span class="c1"># MLPRegressor</span>
    <span class="n">best</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">maxsize</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span> 
                  <span class="s1">&#39;dim_reducer__n_components&#39;</span><span class="p">:[</span><span class="mi">60</span><span class="p">,</span><span class="mi">70</span><span class="p">],</span>
                  <span class="s1">&#39;classifier__hidden_layer_sizes&#39;</span><span class="p">:[(</span><span class="mi">130</span><span class="p">,</span> <span class="mi">110</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="mi">130</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">50</span><span class="p">)],</span>
                  <span class="s1">&#39;classifier__learning_rate_init&#39;</span><span class="p">:[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.105</span><span class="p">]</span>
                 <span class="p">}</span>
    <span class="n">est</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[</span>
            <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">scaler</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;dim_reducer&#39;</span><span class="p">,</span> <span class="n">PCA</span><span class="p">()),</span>
            <span class="p">(</span><span class="s1">&#39;classifier&#39;</span><span class="p">,</span> <span class="n">MLPRegressor</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">700</span><span class="p">))])</span>
    <span class="n">estimator</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">est</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">)</span>
    <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train_df</span><span class="p">,</span> <span class="n">y_train_linear_df</span><span class="p">)</span><span class="c1"># gridsearch obejct has scanned thorugh all the best parameters to set it</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test_df</span><span class="p">)</span>
    <span class="n">matrix</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test_linear_df</span><span class="p">,</span> <span class="n">prediction</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Testing on Fold 1
# Getting train data set up
# Getting test data set up
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Testing on Fold 2
# Getting train data set up
# Getting test data set up
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (700) reached and the optimization hasn&#39;t converged yet.
  % self.max_iter, ConvergenceWarning)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Testing on Fold 3
# Getting train data set up
# Getting test data set up
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (700) reached and the optimization hasn&#39;t converged yet.
  % self.max_iter, ConvergenceWarning)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Testing on Fold 4
# Getting train data set up
# Getting test data set up
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (700) reached and the optimization hasn&#39;t converged yet.
  % self.max_iter, ConvergenceWarning)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4>Classifier Task</h4><p><p>Steps:</p></p>
<ol>
    <li><b>Standard Scaler</b></li>
    <li><b>SMOTE</b></li>
    <li><b>Dimensionality Reducer with Variance Threshold</b></li>
    <li><b>Dimensionality Reducer with PCA</b></li>
    <li><b>Classifier</b>
        <br>We compare four different classifier models based on the mean roc_auc_score error. The three classifiers are KNeighbours, Logistic Regression and RandomForestClassifier.</br>
    </li>
</ol>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[26]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">all_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">):</span>
    <span class="c1"># KNeighbours</span>
    <span class="n">best</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">maxsize</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;f_selecter__threshold&#39;</span><span class="p">:[</span><span class="mf">0.5</span><span class="p">],</span> 
                  <span class="s1">&#39;dim_reducer__n_components&#39;</span><span class="p">:[</span><span class="mi">60</span><span class="p">],</span>
                  <span class="s1">&#39;classifier__n_neighbors&#39;</span><span class="p">:[</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span>
                 <span class="p">}</span>
    <span class="n">est</span> <span class="o">=</span> <span class="n">imPipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[</span>
            <span class="p">(</span><span class="s1">&#39;smote&#39;</span><span class="p">,</span> <span class="n">SMOTE</span><span class="p">()),</span>
            <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">scaler</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;f_selecter&#39;</span><span class="p">,</span> <span class="n">VarianceThreshold</span><span class="p">()),</span>
            <span class="p">(</span><span class="s1">&#39;dim_reducer&#39;</span><span class="p">,</span> <span class="n">PCA</span><span class="p">()),</span>
            <span class="p">(</span><span class="s1">&#39;classifier&#39;</span><span class="p">,</span> <span class="n">KNeighborsClassifier</span><span class="p">())])</span>
    <span class="n">estimator</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">est</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">)</span>
    <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train_df</span><span class="p">,</span> <span class="n">y_train_class_df</span><span class="p">)</span><span class="c1"># gridsearch obejct has scanned thorugh all the best parameters to set it</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test_df</span><span class="p">)</span>
    <span class="n">matrix_class</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test_class_df</span><span class="p">,</span> <span class="n">prediction</span><span class="p">))</span>
    
    <span class="c1"># Logistic Regression</span>
    <span class="n">best</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">maxsize</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;f_selecter__threshold&#39;</span><span class="p">:[</span><span class="mf">0.5</span><span class="p">],</span> 
                  <span class="s1">&#39;dim_reducer__n_components&#39;</span><span class="p">:[</span><span class="mi">60</span><span class="p">]</span>
                 <span class="p">}</span>
    <span class="n">est</span> <span class="o">=</span> <span class="n">imPipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[</span>
            <span class="p">(</span><span class="s1">&#39;smote&#39;</span><span class="p">,</span> <span class="n">SMOTE</span><span class="p">()),</span>
            <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">scaler</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;f_selecter&#39;</span><span class="p">,</span> <span class="n">VarianceThreshold</span><span class="p">()),</span>
            <span class="p">(</span><span class="s1">&#39;dim_reducer&#39;</span><span class="p">,</span> <span class="n">PCA</span><span class="p">()),</span>
            <span class="p">(</span><span class="s1">&#39;classifier&#39;</span><span class="p">,</span> <span class="n">LogisticRegression</span><span class="p">())])</span>
    <span class="n">estimator</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">est</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train_df</span><span class="p">,</span> <span class="n">y_train_class_df</span><span class="p">)</span><span class="c1"># gridsearch obejct has scanned thorugh all the best parameters to set it</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test_df</span><span class="p">)</span>
    <span class="n">matrix_class</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test_class_df</span><span class="p">,</span> <span class="n">prediction</span><span class="p">))</span>
    
    <span class="c1"># RandomForestClassifier</span>
    <span class="n">best</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">maxsize</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;f_selecter__threshold&#39;</span><span class="p">:[</span><span class="mf">0.5</span><span class="p">],</span> 
                  <span class="s1">&#39;dim_reducer__n_components&#39;</span><span class="p">:[</span><span class="mi">60</span><span class="p">],</span>
                  <span class="s1">&#39;classifier__n_estimators&#39;</span><span class="p">:[</span><span class="mi">10</span><span class="p">,</span><span class="mi">15</span><span class="p">]</span>
                 <span class="p">}</span>
    <span class="n">est</span> <span class="o">=</span> <span class="n">imPipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[</span>
            <span class="p">(</span><span class="s1">&#39;smote&#39;</span><span class="p">,</span> <span class="n">SMOTE</span><span class="p">()),</span>
            <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">scaler</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;f_selecter&#39;</span><span class="p">,</span> <span class="n">VarianceThreshold</span><span class="p">()),</span>
            <span class="p">(</span><span class="s1">&#39;dim_reducer&#39;</span><span class="p">,</span> <span class="n">PCA</span><span class="p">()),</span>
            <span class="p">(</span><span class="s1">&#39;classifier&#39;</span><span class="p">,</span> <span class="n">RandomForestClassifier</span><span class="p">())])</span>
    <span class="n">estimator</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">est</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train_df</span><span class="p">,</span> <span class="n">y_train_class_df</span><span class="p">)</span><span class="c1"># gridsearch obejct has scanned thorugh all the best parameters to set it</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test_df</span><span class="p">)</span>
    <span class="n">matrix_class</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test_class_df</span><span class="p">,</span> <span class="n">prediction</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4>Results</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[27]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">number_of_learners</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">score_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Linear&quot;</span><span class="p">,</span> <span class="s2">&quot;DecisionTree&quot;</span><span class="p">,</span> <span class="s2">&quot;MLPRegressor&quot;</span><span class="p">,</span><span class="s2">&quot;&quot;</span><span class="p">])</span>
<span class="n">score_class_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">matrix_class</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;KNeighbour&quot;</span><span class="p">,</span> <span class="s2">&quot;Logistic&quot;</span><span class="p">,</span> <span class="s2">&quot;RandomForests&quot;</span><span class="p">,</span><span class="s2">&quot;&quot;</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">number_of_learners</span><span class="p">):</span>
    <span class="n">score_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">score_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">4</span><span class="p">,</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
    <span class="n">score_class_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">score_class_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">4</span><span class="p">,</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">display</span><span class="p">(</span><span class="n">score_df</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">score_class_df</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Linear</th>
      <th>DecisionTree</th>
      <th>MLPRegressor</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>181.931888</td>
      <td>319.447198</td>
      <td>200.483845</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>126.571049</td>
      <td>255.426125</td>
      <td>137.452468</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>128.414533</td>
      <td>218.447538</td>
      <td>139.760740</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>237.443435</td>
      <td>251.121457</td>
      <td>119.134381</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>12.984230</td>
      <td>16.158916</td>
      <td>12.215067</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>KNeighbour</th>
      <th>Logistic</th>
      <th>RandomForests</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.557483</td>
      <td>0.695993</td>
      <td>0.544187</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.553577</td>
      <td>0.687142</td>
      <td>0.540281</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.553003</td>
      <td>0.685314</td>
      <td>0.549814</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.541284</td>
      <td>0.691621</td>
      <td>0.566693</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.551337</td>
      <td>0.690018</td>
      <td>0.550244</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><h3>Second Model</h3></p>
<ul> 
    <li>We make use of the second Design matrix described above on the same parameters that we tried in the Model building 1.</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[30]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Creating an array containing the preprocessed folds</span>
<span class="n">folds_x</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">folds_y_linear</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">folds_y_class</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">all_list</span><span class="p">:</span>
    <span class="n">x_string</span> <span class="o">=</span> <span class="s2">&quot;../Project_Data/Fold&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="n">y_string</span> <span class="o">=</span> <span class="s2">&quot;../Project_Data/Fold&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">+</span><span class="s2">&quot;_Outcomes.csv&quot;</span>
    <span class="n">temp_df_y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">read_ans</span><span class="p">(</span><span class="n">y_string</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;recordid&#39;</span><span class="p">,</span> <span class="s1">&#39;days_in_hospital&#39;</span><span class="p">,</span> <span class="s1">&#39;mortality&#39;</span><span class="p">])</span>
    <span class="n">temp_df_x</span> <span class="o">=</span> <span class="n">put_single_into_dataframe</span><span class="p">(</span><span class="n">read_text</span><span class="p">(</span><span class="n">x_string</span><span class="p">))</span>
    <span class="n">folds_x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">preprocess_x_for_design_matrix_2</span><span class="p">(</span><span class="n">temp_df_x</span><span class="p">)</span>
    <span class="n">folds_y_linear</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">preprocess_y_linear</span><span class="p">(</span><span class="n">temp_df_y</span><span class="p">)</span>
    <span class="n">folds_y_class</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">preprocess_y_class</span><span class="p">(</span><span class="n">temp_df_y</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Gender</th>
      <th>Height</th>
      <th>Weight</th>
      <th>Bilirubin</th>
      <th>AST</th>
      <th>ALT</th>
      <th>ALP</th>
      <th>Albumin</th>
      <th>TroponinT</th>
      <th>TroponinI</th>
      <th>Cholesterol</th>
      <th>MechVent</th>
      <th>HR0</th>
      <th>HR12</th>
      <th>HR24</th>
      <th>HR36</th>
      <th>MAP0</th>
      <th>MAP12</th>
      <th>MAP24</th>
      <th>MAP36</th>
      <th>SysABP0</th>
      <th>SysABP12</th>
      <th>SysABP24</th>
      <th>SysABP36</th>
      <th>DiasABP0</th>
      <th>DiasABP12</th>
      <th>DiasABP24</th>
      <th>DiasABP36</th>
      <th>Urine0</th>
      <th>Urine12</th>
      <th>Urine24</th>
      <th>Urine36</th>
      <th>Weight0</th>
      <th>Weight12</th>
      <th>Weight24</th>
      <th>Weight36</th>
      <th>NISysABP0</th>
      <th>NISysABP12</th>
      <th>NISysABP24</th>
      <th>NISysABP36</th>
      <th>NIDiasABP0</th>
      <th>NIDiasABP12</th>
      <th>NIDiasABP24</th>
      <th>NIDiasABP36</th>
      <th>NIMAP0</th>
      <th>NIMAP12</th>
      <th>NIMAP24</th>
      <th>NIMAP36</th>
      <th>Temp0</th>
      <th>Temp12</th>
      <th>Temp24</th>
      <th>Temp36</th>
      <th>GCS0</th>
      <th>GCS12</th>
      <th>GCS24</th>
      <th>GCS36</th>
      <th>RespRate0</th>
      <th>RespRate12</th>
      <th>RespRate24</th>
      <th>RespRate36</th>
      <th>FiO20</th>
      <th>FiO212</th>
      <th>FiO224</th>
      <th>FiO236</th>
      <th>pH0</th>
      <th>pH12</th>
      <th>pH24</th>
      <th>pH36</th>
      <th>PaO20</th>
      <th>PaO212</th>
      <th>PaO224</th>
      <th>PaO236</th>
      <th>PaCO20</th>
      <th>PaCO212</th>
      <th>PaCO224</th>
      <th>PaCO236</th>
      <th>HCT0</th>
      <th>HCT12</th>
      <th>HCT24</th>
      <th>HCT36</th>
      <th>K0</th>
      <th>K12</th>
      <th>K24</th>
      <th>K36</th>
      <th>Creatinine0</th>
      <th>Creatinine12</th>
      <th>Creatinine24</th>
      <th>Creatinine36</th>
      <th>Platelets0</th>
      <th>Platelets12</th>
      <th>Platelets24</th>
      <th>Platelets36</th>
      <th>BUN0</th>
      <th>BUN12</th>
      <th>BUN24</th>
      <th>BUN36</th>
      <th>HCO30</th>
      <th>HCO312</th>
      <th>HCO324</th>
      <th>HCO336</th>
      <th>Mg0</th>
      <th>Mg12</th>
      <th>Mg24</th>
      <th>Mg36</th>
      <th>Na0</th>
      <th>Na12</th>
      <th>Na24</th>
      <th>Na36</th>
      <th>Glucose0</th>
      <th>Glucose12</th>
      <th>Glucose24</th>
      <th>Glucose36</th>
      <th>WBC0</th>
      <th>WBC12</th>
      <th>WBC24</th>
      <th>WBC36</th>
      <th>SaO20</th>
      <th>SaO212</th>
      <th>SaO224</th>
      <th>SaO236</th>
      <th>Lactate0</th>
      <th>Lactate12</th>
      <th>Lactate24</th>
      <th>Lactate36</th>
      <th>Coronary Care Unit</th>
      <th>Cardiac Surgery Recovery Unit</th>
      <th>Medical ICU</th>
      <th>Surgical ICU</th>
    </tr>
    <tr>
      <th>recordid</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>132539</th>
      <td>54</td>
      <td>0</td>
      <td>170.094476</td>
      <td>81.422068</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>65.454545</td>
      <td>66.090909</td>
      <td>78.833333</td>
      <td>78.571429</td>
      <td>80.711313</td>
      <td>79.358527</td>
      <td>80.922914</td>
      <td>81.500853</td>
      <td>115.156032</td>
      <td>118.234776</td>
      <td>120.376778</td>
      <td>121.372441</td>
      <td>58.554776</td>
      <td>58.822952</td>
      <td>59.476010</td>
      <td>59.876609</td>
      <td>92.000000</td>
      <td>86.250000</td>
      <td>173.333333</td>
      <td>308.333333</td>
      <td>82.648197</td>
      <td>83.383737</td>
      <td>83.651991</td>
      <td>83.759171</td>
      <td>111.700000</td>
      <td>104.125000</td>
      <td>111.333333</td>
      <td>120.875000</td>
      <td>50.600000</td>
      <td>45.125000</td>
      <td>44.333333</td>
      <td>56.125000</td>
      <td>70.967000</td>
      <td>64.792500</td>
      <td>66.666667</td>
      <td>77.708750</td>
      <td>37.833333</td>
      <td>37.100000</td>
      <td>38.066667</td>
      <td>37.766667</td>
      <td>15.000000</td>
      <td>15.0</td>
      <td>14.666667</td>
      <td>15.00</td>
      <td>16.545455</td>
      <td>15.545455</td>
      <td>17.857143</td>
      <td>19.636364</td>
      <td>0.575987</td>
      <td>0.509855</td>
      <td>0.506165</td>
      <td>0.500966</td>
      <td>7.422229</td>
      <td>7.390886</td>
      <td>7.565855</td>
      <td>7.394966</td>
      <td>167.119732</td>
      <td>128.397906</td>
      <td>121.516945</td>
      <td>119.794255</td>
      <td>40.839626</td>
      <td>39.716565</td>
      <td>39.386531</td>
      <td>39.795741</td>
      <td>33.600</td>
      <td>31.031831</td>
      <td>30.30</td>
      <td>30.158595</td>
      <td>4.400000</td>
      <td>4.168752</td>
      <td>4.000000</td>
      <td>4.114329</td>
      <td>0.8</td>
      <td>1.507457</td>
      <td>0.70000</td>
      <td>1.479688</td>
      <td>221.000000</td>
      <td>201.597978</td>
      <td>185.000000</td>
      <td>180.177778</td>
      <td>13.0</td>
      <td>26.8623</td>
      <td>8.000000</td>
      <td>27.386189</td>
      <td>26.0</td>
      <td>23.545873</td>
      <td>28.000000</td>
      <td>24.082561</td>
      <td>1.5</td>
      <td>2.038096</td>
      <td>1.900000</td>
      <td>2.070901</td>
      <td>137.000000</td>
      <td>139.053437</td>
      <td>136.000000</td>
      <td>138.672271</td>
      <td>205.000000</td>
      <td>131.459169</td>
      <td>115.000000</td>
      <td>133.509428</td>
      <td>11.2</td>
      <td>12.730003</td>
      <td>9.400000</td>
      <td>12.236603</td>
      <td>97.184859</td>
      <td>96.87319</td>
      <td>96.396396</td>
      <td>96.604748</td>
      <td>2.575443</td>
      <td>2.395522</td>
      <td>2.447271</td>
      <td>2.711155</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>132540</th>
      <td>76</td>
      <td>1</td>
      <td>175.300000</td>
      <td>76.000000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>87.076923</td>
      <td>80.055556</td>
      <td>75.307692</td>
      <td>73.636364</td>
      <td>75.740741</td>
      <td>76.111111</td>
      <td>78.384615</td>
      <td>80.111111</td>
      <td>106.714286</td>
      <td>114.222222</td>
      <td>119.923077</td>
      <td>123.222222</td>
      <td>60.892857</td>
      <td>56.888889</td>
      <td>57.846154</td>
      <td>58.222222</td>
      <td>158.933333</td>
      <td>103.181818</td>
      <td>170.714286</td>
      <td>187.500000</td>
      <td>82.648197</td>
      <td>80.600000</td>
      <td>80.600000</td>
      <td>81.236364</td>
      <td>117.173844</td>
      <td>111.750000</td>
      <td>115.142857</td>
      <td>107.333333</td>
      <td>57.986820</td>
      <td>57.500000</td>
      <td>61.000000</td>
      <td>45.666667</td>
      <td>76.655203</td>
      <td>75.580000</td>
      <td>79.047143</td>
      <td>66.223333</td>
      <td>36.688462</td>
      <td>37.500000</td>
      <td>36.850000</td>
      <td>36.800000</td>
      <td>10.800000</td>
      <td>15.0</td>
      <td>14.666667</td>
      <td>14.25</td>
      <td>19.613425</td>
      <td>19.551701</td>
      <td>19.646032</td>
      <td>19.636608</td>
      <td>0.560000</td>
      <td>0.509855</td>
      <td>0.506165</td>
      <td>0.500966</td>
      <td>7.385000</td>
      <td>7.400000</td>
      <td>7.565855</td>
      <td>7.385000</td>
      <td>226.250000</td>
      <td>128.397906</td>
      <td>121.516945</td>
      <td>111.000000</td>
      <td>37.000000</td>
      <td>39.716565</td>
      <td>39.386531</td>
      <td>45.000000</td>
      <td>27.625</td>
      <td>28.900000</td>
      <td>30.70</td>
      <td>29.450000</td>
      <td>4.154586</td>
      <td>4.300000</td>
      <td>4.057783</td>
      <td>3.500000</td>
      <td>0.8</td>
      <td>1.200000</td>
      <td>1.50415</td>
      <td>1.300000</td>
      <td>190.333333</td>
      <td>187.000000</td>
      <td>191.404915</td>
      <td>135.000000</td>
      <td>16.0</td>
      <td>18.0000</td>
      <td>28.141693</td>
      <td>21.000000</td>
      <td>21.0</td>
      <td>22.000000</td>
      <td>23.815756</td>
      <td>24.000000</td>
      <td>3.1</td>
      <td>1.900000</td>
      <td>2.061273</td>
      <td>2.100000</td>
      <td>139.341443</td>
      <td>139.000000</td>
      <td>139.309433</td>
      <td>135.000000</td>
      <td>150.083243</td>
      <td>105.000000</td>
      <td>132.354891</td>
      <td>146.000000</td>
      <td>7.4</td>
      <td>13.100000</td>
      <td>12.469809</td>
      <td>13.300000</td>
      <td>98.000000</td>
      <td>97.00000</td>
      <td>96.396396</td>
      <td>95.000000</td>
      <td>2.575443</td>
      <td>2.395522</td>
      <td>2.447271</td>
      <td>2.711155</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>132541</th>
      <td>44</td>
      <td>0</td>
      <td>170.094476</td>
      <td>56.700000</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>90.000000</td>
      <td>83.250000</td>
      <td>87.666667</td>
      <td>71.166667</td>
      <td>80.711313</td>
      <td>79.358527</td>
      <td>100.000000</td>
      <td>87.250000</td>
      <td>115.156032</td>
      <td>118.234776</td>
      <td>137.500000</td>
      <td>121.750000</td>
      <td>58.554776</td>
      <td>58.822952</td>
      <td>75.500000</td>
      <td>64.333333</td>
      <td>111.818182</td>
      <td>185.500000</td>
      <td>150.000000</td>
      <td>58.800000</td>
      <td>56.700000</td>
      <td>56.700000</td>
      <td>56.700000</td>
      <td>56.700000</td>
      <td>132.812500</td>
      <td>136.750000</td>
      <td>124.500000</td>
      <td>119.505340</td>
      <td>78.750000</td>
      <td>80.583333</td>
      <td>76.000000</td>
      <td>57.339935</td>
      <td>96.767500</td>
      <td>99.305000</td>
      <td>92.163750</td>
      <td>76.900645</td>
      <td>37.825000</td>
      <td>37.233333</td>
      <td>38.300000</td>
      <td>37.833333</td>
      <td>7.333333</td>
      <td>6.0</td>
      <td>5.000000</td>
      <td>5.00</td>
      <td>19.613425</td>
      <td>19.551701</td>
      <td>19.646032</td>
      <td>19.636608</td>
      <td>0.750000</td>
      <td>0.500000</td>
      <td>0.460000</td>
      <td>0.400000</td>
      <td>7.422229</td>
      <td>7.510000</td>
      <td>7.490000</td>
      <td>7.394966</td>
      <td>167.119732</td>
      <td>65.000000</td>
      <td>157.666667</td>
      <td>119.794255</td>
      <td>40.839626</td>
      <td>37.000000</td>
      <td>35.000000</td>
      <td>39.795741</td>
      <td>28.500</td>
      <td>26.700000</td>
      <td>28.85</td>
      <td>29.400000</td>
      <td>3.300000</td>
      <td>8.600000</td>
      <td>2.850000</td>
      <td>3.700000</td>
      <td>0.4</td>
      <td>0.300000</td>
      <td>1.50415</td>
      <td>0.300000</td>
      <td>72.000000</td>
      <td>84.000000</td>
      <td>191.404915</td>
      <td>113.000000</td>
      <td>8.0</td>
      <td>3.0000</td>
      <td>28.141693</td>
      <td>3.000000</td>
      <td>24.0</td>
      <td>26.000000</td>
      <td>23.815756</td>
      <td>25.000000</td>
      <td>1.9</td>
      <td>1.300000</td>
      <td>1.850000</td>
      <td>1.700000</td>
      <td>137.000000</td>
      <td>140.000000</td>
      <td>139.309433</td>
      <td>138.000000</td>
      <td>141.000000</td>
      <td>119.000000</td>
      <td>132.354891</td>
      <td>143.000000</td>
      <td>4.2</td>
      <td>3.700000</td>
      <td>12.469809</td>
      <td>6.200000</td>
      <td>97.184859</td>
      <td>95.00000</td>
      <td>96.396396</td>
      <td>96.604748</td>
      <td>1.300000</td>
      <td>1.900000</td>
      <td>0.900000</td>
      <td>2.711155</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>132543</th>
      <td>68</td>
      <td>1</td>
      <td>180.300000</td>
      <td>84.600000</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>72.238095</td>
      <td>72.500000</td>
      <td>63.769231</td>
      <td>74.000000</td>
      <td>80.711313</td>
      <td>79.358527</td>
      <td>80.922914</td>
      <td>81.500853</td>
      <td>115.156032</td>
      <td>118.234776</td>
      <td>120.376778</td>
      <td>121.372441</td>
      <td>58.554776</td>
      <td>58.822952</td>
      <td>59.476010</td>
      <td>59.876609</td>
      <td>600.000000</td>
      <td>400.000000</td>
      <td>675.000000</td>
      <td>600.000000</td>
      <td>84.600000</td>
      <td>84.600000</td>
      <td>84.600000</td>
      <td>84.600000</td>
      <td>122.571429</td>
      <td>122.090909</td>
      <td>117.750000</td>
      <td>121.545455</td>
      <td>68.285714</td>
      <td>61.636364</td>
      <td>61.500000</td>
      <td>66.181818</td>
      <td>86.381429</td>
      <td>81.788182</td>
      <td>80.250000</td>
      <td>84.637273</td>
      <td>35.966667</td>
      <td>36.433333</td>
      <td>36.133333</td>
      <td>36.333333</td>
      <td>14.500000</td>
      <td>15.0</td>
      <td>15.000000</td>
      <td>15.00</td>
      <td>16.100000</td>
      <td>14.916667</td>
      <td>13.384615</td>
      <td>16.750000</td>
      <td>0.575987</td>
      <td>0.509855</td>
      <td>0.506165</td>
      <td>0.500966</td>
      <td>7.422229</td>
      <td>7.390886</td>
      <td>7.565855</td>
      <td>7.394966</td>
      <td>167.119732</td>
      <td>128.397906</td>
      <td>121.516945</td>
      <td>119.794255</td>
      <td>40.839626</td>
      <td>39.716565</td>
      <td>39.386531</td>
      <td>39.795741</td>
      <td>37.300</td>
      <td>36.850000</td>
      <td>36.20</td>
      <td>36.300000</td>
      <td>4.200000</td>
      <td>4.168752</td>
      <td>3.800000</td>
      <td>4.114329</td>
      <td>0.7</td>
      <td>1.507457</td>
      <td>0.70000</td>
      <td>1.479688</td>
      <td>315.000000</td>
      <td>201.597978</td>
      <td>284.000000</td>
      <td>180.177778</td>
      <td>20.0</td>
      <td>26.8623</td>
      <td>10.000000</td>
      <td>27.386189</td>
      <td>27.0</td>
      <td>23.545873</td>
      <td>28.000000</td>
      <td>24.082561</td>
      <td>2.1</td>
      <td>2.038096</td>
      <td>1.900000</td>
      <td>2.070901</td>
      <td>141.000000</td>
      <td>139.053437</td>
      <td>137.000000</td>
      <td>138.672271</td>
      <td>106.000000</td>
      <td>131.459169</td>
      <td>117.000000</td>
      <td>133.509428</td>
      <td>8.8</td>
      <td>12.730003</td>
      <td>7.900000</td>
      <td>12.236603</td>
      <td>97.184859</td>
      <td>96.87319</td>
      <td>96.396396</td>
      <td>96.604748</td>
      <td>2.575443</td>
      <td>2.395522</td>
      <td>2.447271</td>
      <td>2.711155</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>132545</th>
      <td>88</td>
      <td>0</td>
      <td>170.094476</td>
      <td>81.422068</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>84.428571</td>
      <td>72.833333</td>
      <td>69.727273</td>
      <td>70.454545</td>
      <td>80.711313</td>
      <td>79.358527</td>
      <td>80.922914</td>
      <td>81.500853</td>
      <td>115.156032</td>
      <td>118.234776</td>
      <td>120.376778</td>
      <td>121.372441</td>
      <td>58.554776</td>
      <td>58.822952</td>
      <td>59.476010</td>
      <td>59.876609</td>
      <td>65.454545</td>
      <td>50.600000</td>
      <td>73.333333</td>
      <td>59.375000</td>
      <td>82.648197</td>
      <td>83.383737</td>
      <td>83.651991</td>
      <td>83.759171</td>
      <td>137.642857</td>
      <td>135.250000</td>
      <td>132.000000</td>
      <td>127.909091</td>
      <td>46.571429</td>
      <td>55.500000</td>
      <td>46.200000</td>
      <td>37.090909</td>
      <td>76.932143</td>
      <td>82.082500</td>
      <td>74.799000</td>
      <td>67.363636</td>
      <td>36.950000</td>
      <td>37.000000</td>
      <td>36.633333</td>
      <td>36.700000</td>
      <td>15.000000</td>
      <td>15.0</td>
      <td>15.000000</td>
      <td>15.00</td>
      <td>20.714286</td>
      <td>18.666667</td>
      <td>17.818182</td>
      <td>19.090909</td>
      <td>0.575987</td>
      <td>0.509855</td>
      <td>0.506165</td>
      <td>0.500966</td>
      <td>7.422229</td>
      <td>7.390886</td>
      <td>7.565855</td>
      <td>7.394966</td>
      <td>167.119732</td>
      <td>128.397906</td>
      <td>121.516945</td>
      <td>119.794255</td>
      <td>40.839626</td>
      <td>39.716565</td>
      <td>39.386531</td>
      <td>39.795741</td>
      <td>22.600</td>
      <td>30.466667</td>
      <td>32.40</td>
      <td>30.900000</td>
      <td>4.900000</td>
      <td>3.850000</td>
      <td>4.100000</td>
      <td>4.114329</td>
      <td>1.0</td>
      <td>1.507457</td>
      <td>1.00000</td>
      <td>1.479688</td>
      <td>109.000000</td>
      <td>201.597978</td>
      <td>97.000000</td>
      <td>180.177778</td>
      <td>45.0</td>
      <td>26.8623</td>
      <td>25.000000</td>
      <td>27.386189</td>
      <td>18.0</td>
      <td>23.545873</td>
      <td>20.000000</td>
      <td>24.082561</td>
      <td>1.5</td>
      <td>2.038096</td>
      <td>1.600000</td>
      <td>2.070901</td>
      <td>140.000000</td>
      <td>139.053437</td>
      <td>139.000000</td>
      <td>138.672271</td>
      <td>113.000000</td>
      <td>131.459169</td>
      <td>92.000000</td>
      <td>133.509428</td>
      <td>3.8</td>
      <td>12.730003</td>
      <td>4.800000</td>
      <td>12.236603</td>
      <td>97.184859</td>
      <td>96.87319</td>
      <td>96.396396</td>
      <td>96.604748</td>
      <td>2.575443</td>
      <td>2.395522</td>
      <td>2.447271</td>
      <td>2.711155</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Gender</th>
      <th>Height</th>
      <th>Weight</th>
      <th>Bilirubin</th>
      <th>AST</th>
      <th>ALT</th>
      <th>ALP</th>
      <th>Albumin</th>
      <th>TroponinT</th>
      <th>TroponinI</th>
      <th>Cholesterol</th>
      <th>MechVent</th>
      <th>HR0</th>
      <th>HR12</th>
      <th>HR24</th>
      <th>HR36</th>
      <th>MAP0</th>
      <th>MAP12</th>
      <th>MAP24</th>
      <th>MAP36</th>
      <th>SysABP0</th>
      <th>SysABP12</th>
      <th>SysABP24</th>
      <th>SysABP36</th>
      <th>DiasABP0</th>
      <th>DiasABP12</th>
      <th>DiasABP24</th>
      <th>DiasABP36</th>
      <th>Urine0</th>
      <th>Urine12</th>
      <th>Urine24</th>
      <th>Urine36</th>
      <th>Weight0</th>
      <th>Weight12</th>
      <th>Weight24</th>
      <th>Weight36</th>
      <th>NISysABP0</th>
      <th>NISysABP12</th>
      <th>NISysABP24</th>
      <th>NISysABP36</th>
      <th>NIDiasABP0</th>
      <th>NIDiasABP12</th>
      <th>NIDiasABP24</th>
      <th>NIDiasABP36</th>
      <th>NIMAP0</th>
      <th>NIMAP12</th>
      <th>NIMAP24</th>
      <th>NIMAP36</th>
      <th>Temp0</th>
      <th>Temp12</th>
      <th>Temp24</th>
      <th>Temp36</th>
      <th>GCS0</th>
      <th>GCS12</th>
      <th>GCS24</th>
      <th>GCS36</th>
      <th>RespRate0</th>
      <th>RespRate12</th>
      <th>RespRate24</th>
      <th>RespRate36</th>
      <th>FiO20</th>
      <th>FiO212</th>
      <th>FiO224</th>
      <th>FiO236</th>
      <th>pH0</th>
      <th>pH12</th>
      <th>pH24</th>
      <th>pH36</th>
      <th>PaO20</th>
      <th>PaO212</th>
      <th>PaO224</th>
      <th>PaO236</th>
      <th>PaCO20</th>
      <th>PaCO212</th>
      <th>PaCO224</th>
      <th>PaCO236</th>
      <th>HCT0</th>
      <th>HCT12</th>
      <th>HCT24</th>
      <th>HCT36</th>
      <th>K0</th>
      <th>K12</th>
      <th>K24</th>
      <th>K36</th>
      <th>Creatinine0</th>
      <th>Creatinine12</th>
      <th>Creatinine24</th>
      <th>Creatinine36</th>
      <th>Platelets0</th>
      <th>Platelets12</th>
      <th>Platelets24</th>
      <th>Platelets36</th>
      <th>BUN0</th>
      <th>BUN12</th>
      <th>BUN24</th>
      <th>BUN36</th>
      <th>HCO30</th>
      <th>HCO312</th>
      <th>HCO324</th>
      <th>HCO336</th>
      <th>Mg0</th>
      <th>Mg12</th>
      <th>Mg24</th>
      <th>Mg36</th>
      <th>Na0</th>
      <th>Na12</th>
      <th>Na24</th>
      <th>Na36</th>
      <th>Glucose0</th>
      <th>Glucose12</th>
      <th>Glucose24</th>
      <th>Glucose36</th>
      <th>WBC0</th>
      <th>WBC12</th>
      <th>WBC24</th>
      <th>WBC36</th>
      <th>SaO20</th>
      <th>SaO212</th>
      <th>SaO224</th>
      <th>SaO236</th>
      <th>Lactate0</th>
      <th>Lactate12</th>
      <th>Lactate24</th>
      <th>Lactate36</th>
      <th>Coronary Care Unit</th>
      <th>Cardiac Surgery Recovery Unit</th>
      <th>Medical ICU</th>
      <th>Surgical ICU</th>
    </tr>
    <tr>
      <th>recordid</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>135076</th>
      <td>56</td>
      <td>1</td>
      <td>177.80000</td>
      <td>110.0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>104.416667</td>
      <td>110.777778</td>
      <td>104.454545</td>
      <td>81.250000</td>
      <td>104.777778</td>
      <td>116.714286</td>
      <td>148.500000</td>
      <td>112.750000</td>
      <td>147.000000</td>
      <td>161.571429</td>
      <td>156.000000</td>
      <td>133.142857</td>
      <td>81.333333</td>
      <td>90.142857</td>
      <td>111.000000</td>
      <td>84.285714</td>
      <td>161.428571</td>
      <td>125.714286</td>
      <td>131.666667</td>
      <td>126.000000</td>
      <td>81.345255</td>
      <td>108.1</td>
      <td>108.1</td>
      <td>108.1</td>
      <td>120.500000</td>
      <td>158.875000</td>
      <td>148.272727</td>
      <td>146.000000</td>
      <td>66.500000</td>
      <td>70.375000</td>
      <td>77.818182</td>
      <td>76.090909</td>
      <td>84.510000</td>
      <td>99.862500</td>
      <td>101.308182</td>
      <td>99.390909</td>
      <td>38.066667</td>
      <td>37.600000</td>
      <td>37.70000</td>
      <td>37.850000</td>
      <td>7.000000</td>
      <td>11.75</td>
      <td>10.166667</td>
      <td>8.2</td>
      <td>19.424873</td>
      <td>19.622286</td>
      <td>19.55034</td>
      <td>19.782182</td>
      <td>0.600000</td>
      <td>0.507617</td>
      <td>0.515098</td>
      <td>0.50479</td>
      <td>7.315000</td>
      <td>7.430</td>
      <td>7.394394</td>
      <td>7.430000</td>
      <td>219.000000</td>
      <td>103.000000</td>
      <td>116.867607</td>
      <td>74.000000</td>
      <td>46.000000</td>
      <td>36.000000</td>
      <td>39.657571</td>
      <td>39.000000</td>
      <td>40.100000</td>
      <td>37.700000</td>
      <td>30.961398</td>
      <td>37.800000</td>
      <td>4.30000</td>
      <td>3.600000</td>
      <td>4.053152</td>
      <td>3.950000</td>
      <td>0.600000</td>
      <td>0.8</td>
      <td>1.449714</td>
      <td>0.700000</td>
      <td>228.000000</td>
      <td>188.00000</td>
      <td>194.81982</td>
      <td>231.00000</td>
      <td>12.00000</td>
      <td>13.0</td>
      <td>26.487741</td>
      <td>15.500000</td>
      <td>23.000000</td>
      <td>25.000000</td>
      <td>23.775188</td>
      <td>25.500000</td>
      <td>1.7</td>
      <td>1.700000</td>
      <td>2.051234</td>
      <td>2.000000</td>
      <td>140.000000</td>
      <td>139.000000</td>
      <td>138.878042</td>
      <td>143.000000</td>
      <td>114.000000</td>
      <td>141.000000</td>
      <td>130.070914</td>
      <td>119.000000</td>
      <td>14.40000</td>
      <td>15.900000</td>
      <td>12.230936</td>
      <td>16.000000</td>
      <td>97.027244</td>
      <td>96.526403</td>
      <td>96.56986</td>
      <td>96.444055</td>
      <td>2.695739</td>
      <td>2.383319</td>
      <td>2.131766</td>
      <td>2.090862</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>135077</th>
      <td>72</td>
      <td>1</td>
      <td>169.30233</td>
      <td>220.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>70.428571</td>
      <td>87.714286</td>
      <td>78.250000</td>
      <td>98.727273</td>
      <td>64.285714</td>
      <td>75.714286</td>
      <td>94.250000</td>
      <td>97.272727</td>
      <td>101.000000</td>
      <td>122.071429</td>
      <td>142.000000</td>
      <td>153.000000</td>
      <td>48.428571</td>
      <td>53.714286</td>
      <td>69.166667</td>
      <td>65.636364</td>
      <td>83.500000</td>
      <td>1.363636</td>
      <td>4.133333</td>
      <td>5.230769</td>
      <td>117.142857</td>
      <td>100.0</td>
      <td>100.0</td>
      <td>100.0</td>
      <td>117.325730</td>
      <td>115.694079</td>
      <td>160.000000</td>
      <td>119.579156</td>
      <td>58.746771</td>
      <td>57.478308</td>
      <td>70.000000</td>
      <td>57.771990</td>
      <td>76.931481</td>
      <td>75.786899</td>
      <td>100.000000</td>
      <td>77.191414</td>
      <td>37.766667</td>
      <td>38.280000</td>
      <td>38.24000</td>
      <td>37.700000</td>
      <td>9.000000</td>
      <td>13.80</td>
      <td>15.000000</td>
      <td>15.0</td>
      <td>19.424873</td>
      <td>19.622286</td>
      <td>19.55034</td>
      <td>19.782182</td>
      <td>0.420000</td>
      <td>0.400000</td>
      <td>0.550000</td>
      <td>0.66000</td>
      <td>7.260000</td>
      <td>7.316</td>
      <td>7.320000</td>
      <td>7.258000</td>
      <td>129.500000</td>
      <td>117.800000</td>
      <td>104.000000</td>
      <td>128.000000</td>
      <td>43.500000</td>
      <td>40.000000</td>
      <td>38.000000</td>
      <td>45.600000</td>
      <td>33.300000</td>
      <td>31.147414</td>
      <td>32.100000</td>
      <td>30.054173</td>
      <td>5.70000</td>
      <td>5.000000</td>
      <td>4.900000</td>
      <td>4.800000</td>
      <td>2.200000</td>
      <td>3.9</td>
      <td>4.600000</td>
      <td>5.000000</td>
      <td>167.000000</td>
      <td>199.19211</td>
      <td>150.00000</td>
      <td>176.60216</td>
      <td>24.00000</td>
      <td>31.0</td>
      <td>36.000000</td>
      <td>39.000000</td>
      <td>20.000000</td>
      <td>17.000000</td>
      <td>17.000000</td>
      <td>19.500000</td>
      <td>1.9</td>
      <td>2.052855</td>
      <td>2.100000</td>
      <td>2.200000</td>
      <td>138.000000</td>
      <td>139.000000</td>
      <td>140.000000</td>
      <td>140.500000</td>
      <td>100.000000</td>
      <td>102.000000</td>
      <td>119.000000</td>
      <td>98.000000</td>
      <td>10.10000</td>
      <td>12.437016</td>
      <td>13.200000</td>
      <td>12.394275</td>
      <td>97.027244</td>
      <td>96.526403</td>
      <td>96.56986</td>
      <td>96.444055</td>
      <td>2.695739</td>
      <td>2.383319</td>
      <td>2.131766</td>
      <td>2.090862</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>135079</th>
      <td>68</td>
      <td>1</td>
      <td>169.30233</td>
      <td>100.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>72.739130</td>
      <td>88.076923</td>
      <td>79.666667</td>
      <td>78.909091</td>
      <td>76.608696</td>
      <td>80.384615</td>
      <td>84.750000</td>
      <td>82.254622</td>
      <td>116.652174</td>
      <td>123.846154</td>
      <td>129.916667</td>
      <td>122.600009</td>
      <td>54.391304</td>
      <td>57.230769</td>
      <td>60.000000</td>
      <td>60.724172</td>
      <td>161.785714</td>
      <td>115.833333</td>
      <td>96.666667</td>
      <td>99.090909</td>
      <td>103.800000</td>
      <td>103.8</td>
      <td>104.2</td>
      <td>104.9</td>
      <td>108.666667</td>
      <td>115.694079</td>
      <td>118.492021</td>
      <td>128.000000</td>
      <td>57.666667</td>
      <td>57.478308</td>
      <td>58.259040</td>
      <td>63.636364</td>
      <td>70.000000</td>
      <td>75.786899</td>
      <td>77.178522</td>
      <td>79.545455</td>
      <td>36.485714</td>
      <td>37.275000</td>
      <td>36.45000</td>
      <td>36.500000</td>
      <td>7.250000</td>
      <td>14.00</td>
      <td>15.000000</td>
      <td>15.0</td>
      <td>19.424873</td>
      <td>19.622286</td>
      <td>19.55034</td>
      <td>19.782182</td>
      <td>0.433333</td>
      <td>0.507617</td>
      <td>0.515098</td>
      <td>0.50479</td>
      <td>7.325000</td>
      <td>7.340</td>
      <td>7.394394</td>
      <td>7.396834</td>
      <td>125.000000</td>
      <td>113.000000</td>
      <td>116.867607</td>
      <td>117.303447</td>
      <td>45.000000</td>
      <td>32.000000</td>
      <td>39.657571</td>
      <td>39.969184</td>
      <td>26.400000</td>
      <td>28.750000</td>
      <td>28.700000</td>
      <td>31.200000</td>
      <td>5.50000</td>
      <td>4.900000</td>
      <td>4.900000</td>
      <td>4.067108</td>
      <td>5.400000</td>
      <td>4.8</td>
      <td>3.700000</td>
      <td>1.503316</td>
      <td>133.000000</td>
      <td>126.00000</td>
      <td>108.00000</td>
      <td>176.60216</td>
      <td>54.00000</td>
      <td>53.0</td>
      <td>49.000000</td>
      <td>26.151218</td>
      <td>21.000000</td>
      <td>22.000000</td>
      <td>22.000000</td>
      <td>24.094853</td>
      <td>1.6</td>
      <td>1.600000</td>
      <td>1.800000</td>
      <td>2.039408</td>
      <td>137.000000</td>
      <td>139.000000</td>
      <td>138.000000</td>
      <td>138.872497</td>
      <td>263.000000</td>
      <td>180.000000</td>
      <td>146.000000</td>
      <td>128.065366</td>
      <td>9.40000</td>
      <td>9.500000</td>
      <td>6.950000</td>
      <td>12.394275</td>
      <td>97.027244</td>
      <td>96.526403</td>
      <td>96.56986</td>
      <td>96.444055</td>
      <td>1.700000</td>
      <td>2.383319</td>
      <td>2.131766</td>
      <td>2.090862</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>135080</th>
      <td>77</td>
      <td>1</td>
      <td>170.20000</td>
      <td>77.5</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>70.730769</td>
      <td>67.473684</td>
      <td>71.312500</td>
      <td>68.166667</td>
      <td>82.576923</td>
      <td>75.421053</td>
      <td>85.571429</td>
      <td>82.254622</td>
      <td>117.000000</td>
      <td>114.157895</td>
      <td>97.000000</td>
      <td>122.600009</td>
      <td>63.538462</td>
      <td>57.105263</td>
      <td>78.428571</td>
      <td>60.724172</td>
      <td>264.545455</td>
      <td>49.083333</td>
      <td>46.416667</td>
      <td>178.636364</td>
      <td>81.345255</td>
      <td>87.6</td>
      <td>87.6</td>
      <td>87.6</td>
      <td>117.325730</td>
      <td>115.694079</td>
      <td>116.533333</td>
      <td>122.833333</td>
      <td>58.746771</td>
      <td>57.478308</td>
      <td>53.466667</td>
      <td>55.083333</td>
      <td>76.931481</td>
      <td>75.786899</td>
      <td>74.489333</td>
      <td>77.665833</td>
      <td>36.584000</td>
      <td>37.684211</td>
      <td>37.50625</td>
      <td>37.080000</td>
      <td>9.500000</td>
      <td>11.50</td>
      <td>15.000000</td>
      <td>15.0</td>
      <td>19.424873</td>
      <td>19.622286</td>
      <td>19.55034</td>
      <td>19.782182</td>
      <td>0.653846</td>
      <td>0.500000</td>
      <td>0.515098</td>
      <td>0.50479</td>
      <td>7.385714</td>
      <td>7.375</td>
      <td>7.365000</td>
      <td>7.370000</td>
      <td>145.571429</td>
      <td>137.500000</td>
      <td>116.867607</td>
      <td>91.000000</td>
      <td>37.571429</td>
      <td>40.500000</td>
      <td>39.657571</td>
      <td>44.000000</td>
      <td>28.066667</td>
      <td>25.700000</td>
      <td>27.450000</td>
      <td>25.400000</td>
      <td>4.13294</td>
      <td>4.196807</td>
      <td>4.053152</td>
      <td>4.067108</td>
      <td>0.700000</td>
      <td>1.0</td>
      <td>1.449714</td>
      <td>1.100000</td>
      <td>156.666667</td>
      <td>276.00000</td>
      <td>194.81982</td>
      <td>165.00000</td>
      <td>10.00000</td>
      <td>11.0</td>
      <td>26.487741</td>
      <td>13.000000</td>
      <td>23.000849</td>
      <td>23.000000</td>
      <td>23.775188</td>
      <td>24.000000</td>
      <td>2.1</td>
      <td>2.100000</td>
      <td>1.900000</td>
      <td>1.900000</td>
      <td>138.744966</td>
      <td>138.918773</td>
      <td>138.878042</td>
      <td>138.872497</td>
      <td>152.224449</td>
      <td>131.516058</td>
      <td>130.070914</td>
      <td>128.065366</td>
      <td>6.15500</td>
      <td>9.600000</td>
      <td>12.230936</td>
      <td>8.600000</td>
      <td>97.027244</td>
      <td>98.000000</td>
      <td>96.56986</td>
      <td>96.444055</td>
      <td>2.695739</td>
      <td>2.800000</td>
      <td>2.131766</td>
      <td>2.090862</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>135081</th>
      <td>66</td>
      <td>1</td>
      <td>162.60000</td>
      <td>65.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>83.933333</td>
      <td>86.882353</td>
      <td>109.333333</td>
      <td>97.250000</td>
      <td>73.566667</td>
      <td>72.272727</td>
      <td>80.528667</td>
      <td>82.254622</td>
      <td>108.966667</td>
      <td>109.000000</td>
      <td>120.196298</td>
      <td>122.600009</td>
      <td>53.500000</td>
      <td>50.727273</td>
      <td>59.802690</td>
      <td>60.724172</td>
      <td>472.727273</td>
      <td>50.090909</td>
      <td>72.916667</td>
      <td>84.000000</td>
      <td>81.345255</td>
      <td>73.4</td>
      <td>73.4</td>
      <td>73.4</td>
      <td>117.325730</td>
      <td>114.428571</td>
      <td>100.142857</td>
      <td>99.500000</td>
      <td>58.746771</td>
      <td>57.000000</td>
      <td>53.047619</td>
      <td>49.500000</td>
      <td>76.931481</td>
      <td>76.142857</td>
      <td>68.746190</td>
      <td>66.166667</td>
      <td>36.640000</td>
      <td>37.618182</td>
      <td>36.35000</td>
      <td>36.033333</td>
      <td>4.833333</td>
      <td>15.00</td>
      <td>15.000000</td>
      <td>15.0</td>
      <td>19.424873</td>
      <td>19.622286</td>
      <td>19.55034</td>
      <td>19.782182</td>
      <td>0.584297</td>
      <td>0.507617</td>
      <td>0.515098</td>
      <td>0.50479</td>
      <td>7.380000</td>
      <td>7.395</td>
      <td>7.394394</td>
      <td>7.396834</td>
      <td>300.500000</td>
      <td>120.620393</td>
      <td>116.867607</td>
      <td>117.303447</td>
      <td>45.166667</td>
      <td>39.840366</td>
      <td>39.657571</td>
      <td>39.969184</td>
      <td>21.200000</td>
      <td>31.147414</td>
      <td>23.000000</td>
      <td>30.054173</td>
      <td>4.13294</td>
      <td>4.196807</td>
      <td>4.300000</td>
      <td>4.067108</td>
      <td>1.391063</td>
      <td>0.8</td>
      <td>1.449714</td>
      <td>1.503316</td>
      <td>116.000000</td>
      <td>199.19211</td>
      <td>194.81982</td>
      <td>176.60216</td>
      <td>25.69763</td>
      <td>12.0</td>
      <td>26.487741</td>
      <td>26.151218</td>
      <td>23.000849</td>
      <td>23.039752</td>
      <td>23.775188</td>
      <td>24.094853</td>
      <td>2.0</td>
      <td>2.052855</td>
      <td>1.600000</td>
      <td>1.900000</td>
      <td>138.744966</td>
      <td>138.918773</td>
      <td>138.878042</td>
      <td>138.872497</td>
      <td>152.224449</td>
      <td>131.516058</td>
      <td>221.000000</td>
      <td>128.065366</td>
      <td>13.00745</td>
      <td>12.437016</td>
      <td>12.230936</td>
      <td>12.394275</td>
      <td>98.750000</td>
      <td>96.526403</td>
      <td>96.56986</td>
      <td>96.444055</td>
      <td>2.695739</td>
      <td>2.383319</td>
      <td>2.131766</td>
      <td>2.090862</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Gender</th>
      <th>Height</th>
      <th>Weight</th>
      <th>Bilirubin</th>
      <th>AST</th>
      <th>ALT</th>
      <th>ALP</th>
      <th>Albumin</th>
      <th>TroponinT</th>
      <th>TroponinI</th>
      <th>Cholesterol</th>
      <th>MechVent</th>
      <th>HR0</th>
      <th>HR12</th>
      <th>HR24</th>
      <th>HR36</th>
      <th>MAP0</th>
      <th>MAP12</th>
      <th>MAP24</th>
      <th>MAP36</th>
      <th>SysABP0</th>
      <th>SysABP12</th>
      <th>SysABP24</th>
      <th>SysABP36</th>
      <th>DiasABP0</th>
      <th>DiasABP12</th>
      <th>DiasABP24</th>
      <th>DiasABP36</th>
      <th>Urine0</th>
      <th>Urine12</th>
      <th>Urine24</th>
      <th>Urine36</th>
      <th>Weight0</th>
      <th>Weight12</th>
      <th>Weight24</th>
      <th>Weight36</th>
      <th>NISysABP0</th>
      <th>NISysABP12</th>
      <th>NISysABP24</th>
      <th>NISysABP36</th>
      <th>NIDiasABP0</th>
      <th>NIDiasABP12</th>
      <th>NIDiasABP24</th>
      <th>NIDiasABP36</th>
      <th>NIMAP0</th>
      <th>NIMAP12</th>
      <th>NIMAP24</th>
      <th>NIMAP36</th>
      <th>Temp0</th>
      <th>Temp12</th>
      <th>Temp24</th>
      <th>Temp36</th>
      <th>GCS0</th>
      <th>GCS12</th>
      <th>GCS24</th>
      <th>GCS36</th>
      <th>RespRate0</th>
      <th>RespRate12</th>
      <th>RespRate24</th>
      <th>RespRate36</th>
      <th>FiO20</th>
      <th>FiO212</th>
      <th>FiO224</th>
      <th>FiO236</th>
      <th>pH0</th>
      <th>pH12</th>
      <th>pH24</th>
      <th>pH36</th>
      <th>PaO20</th>
      <th>PaO212</th>
      <th>PaO224</th>
      <th>PaO236</th>
      <th>PaCO20</th>
      <th>PaCO212</th>
      <th>PaCO224</th>
      <th>PaCO236</th>
      <th>HCT0</th>
      <th>HCT12</th>
      <th>HCT24</th>
      <th>HCT36</th>
      <th>K0</th>
      <th>K12</th>
      <th>K24</th>
      <th>K36</th>
      <th>Creatinine0</th>
      <th>Creatinine12</th>
      <th>Creatinine24</th>
      <th>Creatinine36</th>
      <th>Platelets0</th>
      <th>Platelets12</th>
      <th>Platelets24</th>
      <th>Platelets36</th>
      <th>BUN0</th>
      <th>BUN12</th>
      <th>BUN24</th>
      <th>BUN36</th>
      <th>HCO30</th>
      <th>HCO312</th>
      <th>HCO324</th>
      <th>HCO336</th>
      <th>Mg0</th>
      <th>Mg12</th>
      <th>Mg24</th>
      <th>Mg36</th>
      <th>Na0</th>
      <th>Na12</th>
      <th>Na24</th>
      <th>Na36</th>
      <th>Glucose0</th>
      <th>Glucose12</th>
      <th>Glucose24</th>
      <th>Glucose36</th>
      <th>WBC0</th>
      <th>WBC12</th>
      <th>WBC24</th>
      <th>WBC36</th>
      <th>SaO20</th>
      <th>SaO212</th>
      <th>SaO224</th>
      <th>SaO236</th>
      <th>Lactate0</th>
      <th>Lactate12</th>
      <th>Lactate24</th>
      <th>Lactate36</th>
      <th>Coronary Care Unit</th>
      <th>Cardiac Surgery Recovery Unit</th>
      <th>Medical ICU</th>
      <th>Surgical ICU</th>
    </tr>
    <tr>
      <th>recordid</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>137593</th>
      <td>57</td>
      <td>1</td>
      <td>157.500000</td>
      <td>88.60</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>98.931034</td>
      <td>90.428571</td>
      <td>90.000000</td>
      <td>87.181818</td>
      <td>71.310345</td>
      <td>72.285714</td>
      <td>67.235294</td>
      <td>73.230769</td>
      <td>106.482759</td>
      <td>112.357143</td>
      <td>105.000000</td>
      <td>115.538462</td>
      <td>56.068966</td>
      <td>53.571429</td>
      <td>50.176471</td>
      <td>55.769231</td>
      <td>257.916667</td>
      <td>55.833333</td>
      <td>130.454545</td>
      <td>62.500000</td>
      <td>82.704345</td>
      <td>89.0</td>
      <td>89.0</td>
      <td>88.200000</td>
      <td>116.815217</td>
      <td>116.953677</td>
      <td>118.307963</td>
      <td>120.750541</td>
      <td>58.166331</td>
      <td>57.120246</td>
      <td>57.762168</td>
      <td>57.936709</td>
      <td>76.704493</td>
      <td>76.099618</td>
      <td>76.978834</td>
      <td>77.800938</td>
      <td>38.373077</td>
      <td>38.214286</td>
      <td>37.821429</td>
      <td>37.266667</td>
      <td>10.800000</td>
      <td>15.000000</td>
      <td>15.000000</td>
      <td>14.500000</td>
      <td>19.736448</td>
      <td>19.336067</td>
      <td>19.339250</td>
      <td>19.523698</td>
      <td>0.600000</td>
      <td>0.400000</td>
      <td>0.850000</td>
      <td>0.700000</td>
      <td>7.391250</td>
      <td>7.370000</td>
      <td>7.380000</td>
      <td>7.420000</td>
      <td>168.250000</td>
      <td>125.811658</td>
      <td>85.250000</td>
      <td>88.500000</td>
      <td>45.125000</td>
      <td>39.343704</td>
      <td>46.000000</td>
      <td>50.00000</td>
      <td>31.200000</td>
      <td>34.300000</td>
      <td>30.446855</td>
      <td>30.300000</td>
      <td>4.500000</td>
      <td>4.600000</td>
      <td>4.048392</td>
      <td>3.700000</td>
      <td>0.600000</td>
      <td>0.600000</td>
      <td>1.407454</td>
      <td>0.700000</td>
      <td>214.000000</td>
      <td>229.000000</td>
      <td>194.35375</td>
      <td>171.500000</td>
      <td>11.000000</td>
      <td>9.000000</td>
      <td>27.093275</td>
      <td>16.000000</td>
      <td>27.000000</td>
      <td>27.00000</td>
      <td>23.755556</td>
      <td>30.00</td>
      <td>2.300000</td>
      <td>2.000000</td>
      <td>1.90000</td>
      <td>2.300000</td>
      <td>139.093356</td>
      <td>137.000000</td>
      <td>139.313275</td>
      <td>137.500000</td>
      <td>170.000000</td>
      <td>186.000000</td>
      <td>131.240753</td>
      <td>131.223025</td>
      <td>14.600000</td>
      <td>17.400000</td>
      <td>12.216938</td>
      <td>16.400000</td>
      <td>97.800000</td>
      <td>96.471630</td>
      <td>95.333333</td>
      <td>96.002072</td>
      <td>2.479119</td>
      <td>2.372085</td>
      <td>2.039958</td>
      <td>2.086082</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>137594</th>
      <td>87</td>
      <td>1</td>
      <td>170.425096</td>
      <td>72.50</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>65.800000</td>
      <td>68.222222</td>
      <td>66.500000</td>
      <td>65.428571</td>
      <td>84.000000</td>
      <td>79.843976</td>
      <td>80.927647</td>
      <td>82.065406</td>
      <td>151.000000</td>
      <td>118.389228</td>
      <td>120.700257</td>
      <td>123.362393</td>
      <td>55.000000</td>
      <td>59.625315</td>
      <td>60.037273</td>
      <td>60.544665</td>
      <td>75.142857</td>
      <td>97.777778</td>
      <td>244.000000</td>
      <td>152.500000</td>
      <td>72.500000</td>
      <td>72.5</td>
      <td>72.5</td>
      <td>72.500000</td>
      <td>135.000000</td>
      <td>142.777778</td>
      <td>136.833333</td>
      <td>134.142857</td>
      <td>56.684211</td>
      <td>49.555556</td>
      <td>51.333333</td>
      <td>42.285714</td>
      <td>82.792105</td>
      <td>80.630000</td>
      <td>79.833333</td>
      <td>72.905714</td>
      <td>36.625000</td>
      <td>37.050000</td>
      <td>36.900000</td>
      <td>36.300000</td>
      <td>13.142857</td>
      <td>12.777778</td>
      <td>11.142857</td>
      <td>12.142857</td>
      <td>14.421053</td>
      <td>15.444444</td>
      <td>14.333333</td>
      <td>15.000000</td>
      <td>0.591333</td>
      <td>0.516785</td>
      <td>0.520443</td>
      <td>0.506305</td>
      <td>7.366538</td>
      <td>7.388928</td>
      <td>7.397845</td>
      <td>7.399595</td>
      <td>160.449835</td>
      <td>125.811658</td>
      <td>117.976143</td>
      <td>113.725108</td>
      <td>40.434075</td>
      <td>39.343704</td>
      <td>39.350542</td>
      <td>40.04247</td>
      <td>31.900000</td>
      <td>34.700000</td>
      <td>36.100000</td>
      <td>36.300000</td>
      <td>4.122413</td>
      <td>3.500000</td>
      <td>3.900000</td>
      <td>3.800000</td>
      <td>1.316559</td>
      <td>0.800000</td>
      <td>0.900000</td>
      <td>0.800000</td>
      <td>210.566486</td>
      <td>251.000000</td>
      <td>241.00000</td>
      <td>248.000000</td>
      <td>26.151555</td>
      <td>18.000000</td>
      <td>18.000000</td>
      <td>16.000000</td>
      <td>22.950528</td>
      <td>26.00000</td>
      <td>27.000000</td>
      <td>27.00</td>
      <td>1.980247</td>
      <td>1.800000</td>
      <td>1.90000</td>
      <td>1.800000</td>
      <td>139.093356</td>
      <td>138.000000</td>
      <td>142.000000</td>
      <td>140.000000</td>
      <td>148.218042</td>
      <td>101.000000</td>
      <td>115.000000</td>
      <td>92.000000</td>
      <td>13.251714</td>
      <td>11.600000</td>
      <td>10.500000</td>
      <td>11.100000</td>
      <td>96.384891</td>
      <td>96.471630</td>
      <td>96.133857</td>
      <td>96.002072</td>
      <td>2.479119</td>
      <td>2.372085</td>
      <td>2.039958</td>
      <td>2.086082</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>137595</th>
      <td>73</td>
      <td>0</td>
      <td>165.100000</td>
      <td>77.27</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>83.275862</td>
      <td>89.600000</td>
      <td>92.166667</td>
      <td>87.909091</td>
      <td>77.250000</td>
      <td>83.600000</td>
      <td>87.250000</td>
      <td>80.454545</td>
      <td>105.413793</td>
      <td>125.066667</td>
      <td>136.166667</td>
      <td>119.727273</td>
      <td>60.482759</td>
      <td>65.333333</td>
      <td>66.166667</td>
      <td>59.363636</td>
      <td>261.071429</td>
      <td>225.000000</td>
      <td>164.583333</td>
      <td>218.636364</td>
      <td>82.704345</td>
      <td>92.7</td>
      <td>92.7</td>
      <td>91.609091</td>
      <td>88.000000</td>
      <td>116.953677</td>
      <td>118.307963</td>
      <td>120.750541</td>
      <td>48.000000</td>
      <td>57.120246</td>
      <td>57.762168</td>
      <td>57.936709</td>
      <td>61.330000</td>
      <td>76.099618</td>
      <td>76.978834</td>
      <td>77.800938</td>
      <td>35.655172</td>
      <td>37.775000</td>
      <td>38.358333</td>
      <td>37.763636</td>
      <td>3.000000</td>
      <td>7.666667</td>
      <td>10.000000</td>
      <td>11.000000</td>
      <td>19.736448</td>
      <td>19.336067</td>
      <td>19.339250</td>
      <td>19.523698</td>
      <td>0.780000</td>
      <td>0.650000</td>
      <td>0.516667</td>
      <td>0.487500</td>
      <td>7.340909</td>
      <td>7.391667</td>
      <td>7.416667</td>
      <td>7.435000</td>
      <td>200.545455</td>
      <td>95.833333</td>
      <td>91.000000</td>
      <td>91.500000</td>
      <td>38.727273</td>
      <td>34.333333</td>
      <td>35.000000</td>
      <td>34.50000</td>
      <td>27.275000</td>
      <td>36.050000</td>
      <td>30.446855</td>
      <td>32.400000</td>
      <td>4.122413</td>
      <td>5.100000</td>
      <td>4.048392</td>
      <td>4.300000</td>
      <td>0.550000</td>
      <td>0.600000</td>
      <td>1.407454</td>
      <td>0.800000</td>
      <td>118.000000</td>
      <td>94.000000</td>
      <td>194.35375</td>
      <td>67.000000</td>
      <td>9.000000</td>
      <td>8.000000</td>
      <td>27.093275</td>
      <td>12.000000</td>
      <td>19.500000</td>
      <td>20.00000</td>
      <td>23.755556</td>
      <td>23.00</td>
      <td>1.500000</td>
      <td>2.000000</td>
      <td>2.04915</td>
      <td>1.900000</td>
      <td>139.093356</td>
      <td>140.000000</td>
      <td>139.313275</td>
      <td>137.000000</td>
      <td>148.218042</td>
      <td>123.000000</td>
      <td>131.240753</td>
      <td>162.000000</td>
      <td>9.833333</td>
      <td>7.000000</td>
      <td>12.216938</td>
      <td>9.200000</td>
      <td>97.666667</td>
      <td>97.333333</td>
      <td>97.333333</td>
      <td>97.500000</td>
      <td>4.066667</td>
      <td>1.850000</td>
      <td>2.039958</td>
      <td>2.086082</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>137598</th>
      <td>72</td>
      <td>0</td>
      <td>170.425096</td>
      <td>131.80</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>85.200000</td>
      <td>105.312500</td>
      <td>95.000000</td>
      <td>100.727273</td>
      <td>80.628753</td>
      <td>79.843976</td>
      <td>80.927647</td>
      <td>82.065406</td>
      <td>116.470913</td>
      <td>118.389228</td>
      <td>120.700257</td>
      <td>123.362393</td>
      <td>59.560763</td>
      <td>59.625315</td>
      <td>60.037273</td>
      <td>60.544665</td>
      <td>165.860895</td>
      <td>127.358947</td>
      <td>133.163638</td>
      <td>142.398650</td>
      <td>131.800000</td>
      <td>131.8</td>
      <td>131.8</td>
      <td>131.800000</td>
      <td>89.368421</td>
      <td>95.000000</td>
      <td>113.083333</td>
      <td>102.090909</td>
      <td>36.473684</td>
      <td>37.062500</td>
      <td>39.250000</td>
      <td>29.818182</td>
      <td>48.444444</td>
      <td>54.375000</td>
      <td>56.416667</td>
      <td>47.363636</td>
      <td>36.666667</td>
      <td>37.233333</td>
      <td>36.866667</td>
      <td>37.075000</td>
      <td>9.000000</td>
      <td>9.000000</td>
      <td>9.666667</td>
      <td>11.000000</td>
      <td>19.736448</td>
      <td>19.336067</td>
      <td>19.339250</td>
      <td>19.523698</td>
      <td>0.460000</td>
      <td>0.400000</td>
      <td>0.400000</td>
      <td>0.400000</td>
      <td>7.290000</td>
      <td>7.388928</td>
      <td>7.397845</td>
      <td>7.399595</td>
      <td>98.000000</td>
      <td>125.811658</td>
      <td>117.976143</td>
      <td>113.725108</td>
      <td>42.000000</td>
      <td>39.343704</td>
      <td>39.350542</td>
      <td>40.04247</td>
      <td>24.550000</td>
      <td>30.832459</td>
      <td>23.200000</td>
      <td>29.934552</td>
      <td>4.200000</td>
      <td>4.178664</td>
      <td>4.400000</td>
      <td>4.087976</td>
      <td>3.900000</td>
      <td>1.378978</td>
      <td>4.300000</td>
      <td>1.360591</td>
      <td>629.500000</td>
      <td>191.870901</td>
      <td>625.00000</td>
      <td>170.852684</td>
      <td>49.500000</td>
      <td>25.842534</td>
      <td>57.000000</td>
      <td>25.798932</td>
      <td>16.000000</td>
      <td>23.19059</td>
      <td>17.000000</td>
      <td>24.27</td>
      <td>2.300000</td>
      <td>2.057708</td>
      <td>2.20000</td>
      <td>2.063583</td>
      <td>127.000000</td>
      <td>139.052282</td>
      <td>130.000000</td>
      <td>138.436415</td>
      <td>87.000000</td>
      <td>133.889028</td>
      <td>207.000000</td>
      <td>131.223025</td>
      <td>26.150000</td>
      <td>12.555877</td>
      <td>26.000000</td>
      <td>12.180153</td>
      <td>96.384891</td>
      <td>96.471630</td>
      <td>96.133857</td>
      <td>96.002072</td>
      <td>2.600000</td>
      <td>2.372085</td>
      <td>2.039958</td>
      <td>2.086082</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>137600</th>
      <td>76</td>
      <td>1</td>
      <td>172.700000</td>
      <td>86.10</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>88.000000</td>
      <td>88.000000</td>
      <td>88.000000</td>
      <td>87.928571</td>
      <td>77.833333</td>
      <td>67.272727</td>
      <td>67.285714</td>
      <td>71.000000</td>
      <td>113.000000</td>
      <td>101.545455</td>
      <td>109.857143</td>
      <td>113.214286</td>
      <td>58.666667</td>
      <td>49.272727</td>
      <td>47.214286</td>
      <td>49.714286</td>
      <td>192.000000</td>
      <td>94.166667</td>
      <td>105.000000</td>
      <td>101.000000</td>
      <td>82.704345</td>
      <td>92.6</td>
      <td>92.6</td>
      <td>93.035714</td>
      <td>116.815217</td>
      <td>116.953677</td>
      <td>118.307963</td>
      <td>120.750541</td>
      <td>58.166331</td>
      <td>57.120246</td>
      <td>57.762168</td>
      <td>57.936709</td>
      <td>76.704493</td>
      <td>76.099618</td>
      <td>76.978834</td>
      <td>77.800938</td>
      <td>36.116667</td>
      <td>37.281818</td>
      <td>38.014286</td>
      <td>37.307143</td>
      <td>10.590699</td>
      <td>15.000000</td>
      <td>15.000000</td>
      <td>14.000000</td>
      <td>19.736448</td>
      <td>19.336067</td>
      <td>19.339250</td>
      <td>19.523698</td>
      <td>0.666667</td>
      <td>0.516785</td>
      <td>0.520443</td>
      <td>0.506305</td>
      <td>7.363333</td>
      <td>7.322500</td>
      <td>7.397845</td>
      <td>7.380000</td>
      <td>301.666667</td>
      <td>137.000000</td>
      <td>117.976143</td>
      <td>96.000000</td>
      <td>39.666667</td>
      <td>37.500000</td>
      <td>39.350542</td>
      <td>40.00000</td>
      <td>31.639291</td>
      <td>27.200000</td>
      <td>24.600000</td>
      <td>25.100000</td>
      <td>4.122413</td>
      <td>5.600000</td>
      <td>5.300000</td>
      <td>4.200000</td>
      <td>1.316559</td>
      <td>1.400000</td>
      <td>1.407454</td>
      <td>1.400000</td>
      <td>271.000000</td>
      <td>276.000000</td>
      <td>194.35375</td>
      <td>184.000000</td>
      <td>26.151555</td>
      <td>24.000000</td>
      <td>27.093275</td>
      <td>23.000000</td>
      <td>22.950528</td>
      <td>20.00000</td>
      <td>23.755556</td>
      <td>23.00</td>
      <td>1.980247</td>
      <td>2.057708</td>
      <td>1.70000</td>
      <td>1.700000</td>
      <td>139.093356</td>
      <td>137.000000</td>
      <td>139.313275</td>
      <td>136.000000</td>
      <td>148.218042</td>
      <td>169.000000</td>
      <td>132.000000</td>
      <td>143.000000</td>
      <td>13.251714</td>
      <td>16.000000</td>
      <td>12.216938</td>
      <td>10.800000</td>
      <td>96.384891</td>
      <td>98.000000</td>
      <td>96.133857</td>
      <td>97.000000</td>
      <td>2.479119</td>
      <td>2.372085</td>
      <td>2.039958</td>
      <td>2.086082</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Gender</th>
      <th>Height</th>
      <th>Weight</th>
      <th>Bilirubin</th>
      <th>AST</th>
      <th>ALT</th>
      <th>ALP</th>
      <th>Albumin</th>
      <th>TroponinT</th>
      <th>TroponinI</th>
      <th>Cholesterol</th>
      <th>MechVent</th>
      <th>HR0</th>
      <th>HR12</th>
      <th>HR24</th>
      <th>HR36</th>
      <th>MAP0</th>
      <th>MAP12</th>
      <th>MAP24</th>
      <th>MAP36</th>
      <th>SysABP0</th>
      <th>SysABP12</th>
      <th>SysABP24</th>
      <th>SysABP36</th>
      <th>DiasABP0</th>
      <th>DiasABP12</th>
      <th>DiasABP24</th>
      <th>DiasABP36</th>
      <th>Urine0</th>
      <th>Urine12</th>
      <th>Urine24</th>
      <th>Urine36</th>
      <th>Weight0</th>
      <th>Weight12</th>
      <th>Weight24</th>
      <th>Weight36</th>
      <th>NISysABP0</th>
      <th>NISysABP12</th>
      <th>NISysABP24</th>
      <th>NISysABP36</th>
      <th>NIDiasABP0</th>
      <th>NIDiasABP12</th>
      <th>NIDiasABP24</th>
      <th>NIDiasABP36</th>
      <th>NIMAP0</th>
      <th>NIMAP12</th>
      <th>NIMAP24</th>
      <th>NIMAP36</th>
      <th>Temp0</th>
      <th>Temp12</th>
      <th>Temp24</th>
      <th>Temp36</th>
      <th>GCS0</th>
      <th>GCS12</th>
      <th>GCS24</th>
      <th>GCS36</th>
      <th>RespRate0</th>
      <th>RespRate12</th>
      <th>RespRate24</th>
      <th>RespRate36</th>
      <th>FiO20</th>
      <th>FiO212</th>
      <th>FiO224</th>
      <th>FiO236</th>
      <th>pH0</th>
      <th>pH12</th>
      <th>pH24</th>
      <th>pH36</th>
      <th>PaO20</th>
      <th>PaO212</th>
      <th>PaO224</th>
      <th>PaO236</th>
      <th>PaCO20</th>
      <th>PaCO212</th>
      <th>PaCO224</th>
      <th>PaCO236</th>
      <th>HCT0</th>
      <th>HCT12</th>
      <th>HCT24</th>
      <th>HCT36</th>
      <th>K0</th>
      <th>K12</th>
      <th>K24</th>
      <th>K36</th>
      <th>Creatinine0</th>
      <th>Creatinine12</th>
      <th>Creatinine24</th>
      <th>Creatinine36</th>
      <th>Platelets0</th>
      <th>Platelets12</th>
      <th>Platelets24</th>
      <th>Platelets36</th>
      <th>BUN0</th>
      <th>BUN12</th>
      <th>BUN24</th>
      <th>BUN36</th>
      <th>HCO30</th>
      <th>HCO312</th>
      <th>HCO324</th>
      <th>HCO336</th>
      <th>Mg0</th>
      <th>Mg12</th>
      <th>Mg24</th>
      <th>Mg36</th>
      <th>Na0</th>
      <th>Na12</th>
      <th>Na24</th>
      <th>Na36</th>
      <th>Glucose0</th>
      <th>Glucose12</th>
      <th>Glucose24</th>
      <th>Glucose36</th>
      <th>WBC0</th>
      <th>WBC12</th>
      <th>WBC24</th>
      <th>WBC36</th>
      <th>SaO20</th>
      <th>SaO212</th>
      <th>SaO224</th>
      <th>SaO236</th>
      <th>Lactate0</th>
      <th>Lactate12</th>
      <th>Lactate24</th>
      <th>Lactate36</th>
      <th>Coronary Care Unit</th>
      <th>Cardiac Surgery Recovery Unit</th>
      <th>Medical ICU</th>
      <th>Surgical ICU</th>
    </tr>
    <tr>
      <th>recordid</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>140101</th>
      <td>39</td>
      <td>0</td>
      <td>170.200000</td>
      <td>253.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>110.142857</td>
      <td>108.833333</td>
      <td>97.250000</td>
      <td>97.636364</td>
      <td>80.355020</td>
      <td>80.313226</td>
      <td>80.628562</td>
      <td>81.518378</td>
      <td>115.676644</td>
      <td>119.538003</td>
      <td>120.858617</td>
      <td>122.740704</td>
      <td>59.269006</td>
      <td>59.575281</td>
      <td>59.575021</td>
      <td>59.960344</td>
      <td>60.000000</td>
      <td>53.636364</td>
      <td>50.000000</td>
      <td>72.222222</td>
      <td>253.000000</td>
      <td>253.000000</td>
      <td>253.000000</td>
      <td>253.000000</td>
      <td>99.571429</td>
      <td>108.333333</td>
      <td>111.666667</td>
      <td>103.818182</td>
      <td>42.857143</td>
      <td>50.916667</td>
      <td>52.250000</td>
      <td>45.000000</td>
      <td>61.761429</td>
      <td>70.055000</td>
      <td>72.055000</td>
      <td>64.605455</td>
      <td>38.000000</td>
      <td>37.733333</td>
      <td>37.533333</td>
      <td>37.766667</td>
      <td>10.000000</td>
      <td>10.000000</td>
      <td>10.000000</td>
      <td>10.000000</td>
      <td>19.920446</td>
      <td>19.896906</td>
      <td>19.816753</td>
      <td>20.011274</td>
      <td>0.540000</td>
      <td>0.400000</td>
      <td>0.40000</td>
      <td>0.400000</td>
      <td>7.385000</td>
      <td>8.406452</td>
      <td>7.360000</td>
      <td>7.398353</td>
      <td>109.500000</td>
      <td>123.874998</td>
      <td>145.000000</td>
      <td>115.412815</td>
      <td>66.500000</td>
      <td>39.931142</td>
      <td>66.000000</td>
      <td>40.44737</td>
      <td>32.90</td>
      <td>30.964831</td>
      <td>32.9</td>
      <td>29.951503</td>
      <td>4.200000</td>
      <td>4.173645</td>
      <td>3.9</td>
      <td>4.071882</td>
      <td>0.60</td>
      <td>1.453197</td>
      <td>0.5</td>
      <td>1.487078</td>
      <td>149.000000</td>
      <td>197.180099</td>
      <td>179.0</td>
      <td>176.738114</td>
      <td>14.0</td>
      <td>25.731611</td>
      <td>13.0</td>
      <td>25.921424</td>
      <td>34.0</td>
      <td>23.411007</td>
      <td>33.0</td>
      <td>24.214374</td>
      <td>1.500000</td>
      <td>2.055607</td>
      <td>1.9</td>
      <td>2.051946</td>
      <td>140.000000</td>
      <td>139.053094</td>
      <td>142.0</td>
      <td>138.633454</td>
      <td>103.000000</td>
      <td>131.87701</td>
      <td>86.0</td>
      <td>128.530985</td>
      <td>15.800000</td>
      <td>12.416399</td>
      <td>10.7</td>
      <td>12.242523</td>
      <td>96.793083</td>
      <td>96.8269</td>
      <td>96.412421</td>
      <td>96.634002</td>
      <td>1.300000</td>
      <td>2.182108</td>
      <td>2.039218</td>
      <td>1.832424</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>140102</th>
      <td>70</td>
      <td>0</td>
      <td>169.337684</td>
      <td>123.5</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>76.444444</td>
      <td>76.769231</td>
      <td>78.636364</td>
      <td>84.000000</td>
      <td>80.355020</td>
      <td>83.600000</td>
      <td>87.545455</td>
      <td>95.272727</td>
      <td>115.676644</td>
      <td>109.200000</td>
      <td>119.181818</td>
      <td>132.500000</td>
      <td>59.269006</td>
      <td>61.000000</td>
      <td>69.272727</td>
      <td>73.916667</td>
      <td>39.285714</td>
      <td>128.125000</td>
      <td>27.818182</td>
      <td>107.625000</td>
      <td>123.500000</td>
      <td>123.500000</td>
      <td>123.500000</td>
      <td>123.500000</td>
      <td>102.777778</td>
      <td>99.142857</td>
      <td>117.967196</td>
      <td>119.822643</td>
      <td>46.666667</td>
      <td>47.142857</td>
      <td>56.889398</td>
      <td>57.930466</td>
      <td>65.368889</td>
      <td>64.475714</td>
      <td>76.214687</td>
      <td>77.475260</td>
      <td>36.350000</td>
      <td>36.633333</td>
      <td>36.766667</td>
      <td>36.933333</td>
      <td>10.500000</td>
      <td>11.666667</td>
      <td>11.000000</td>
      <td>10.666667</td>
      <td>19.920446</td>
      <td>19.896906</td>
      <td>19.816753</td>
      <td>20.011274</td>
      <td>0.500000</td>
      <td>0.500000</td>
      <td>0.50000</td>
      <td>0.560000</td>
      <td>7.390000</td>
      <td>7.430000</td>
      <td>7.561206</td>
      <td>7.398353</td>
      <td>85.000000</td>
      <td>88.000000</td>
      <td>118.137949</td>
      <td>115.412815</td>
      <td>58.000000</td>
      <td>53.000000</td>
      <td>39.565064</td>
      <td>40.44737</td>
      <td>28.90</td>
      <td>30.964831</td>
      <td>29.4</td>
      <td>29.951503</td>
      <td>4.500000</td>
      <td>4.173645</td>
      <td>4.1</td>
      <td>4.071882</td>
      <td>0.60</td>
      <td>1.453197</td>
      <td>0.5</td>
      <td>1.487078</td>
      <td>162.000000</td>
      <td>197.180099</td>
      <td>185.0</td>
      <td>176.738114</td>
      <td>25.0</td>
      <td>25.731611</td>
      <td>26.0</td>
      <td>25.921424</td>
      <td>34.0</td>
      <td>23.411007</td>
      <td>30.0</td>
      <td>24.214374</td>
      <td>2.200000</td>
      <td>2.055607</td>
      <td>2.2</td>
      <td>2.051946</td>
      <td>141.000000</td>
      <td>139.053094</td>
      <td>142.0</td>
      <td>138.633454</td>
      <td>134.000000</td>
      <td>131.87701</td>
      <td>144.0</td>
      <td>128.530985</td>
      <td>13.600000</td>
      <td>12.416399</td>
      <td>11.8</td>
      <td>12.242523</td>
      <td>96.793083</td>
      <td>96.8269</td>
      <td>96.412421</td>
      <td>96.634002</td>
      <td>2.700000</td>
      <td>2.182108</td>
      <td>2.039218</td>
      <td>1.832424</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>140104</th>
      <td>61</td>
      <td>1</td>
      <td>188.000000</td>
      <td>80.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>100.906250</td>
      <td>88.357143</td>
      <td>82.333333</td>
      <td>93.125000</td>
      <td>78.750000</td>
      <td>65.428571</td>
      <td>74.000000</td>
      <td>81.518378</td>
      <td>101.812500</td>
      <td>89.714286</td>
      <td>95.727273</td>
      <td>122.740704</td>
      <td>58.750000</td>
      <td>49.428571</td>
      <td>58.909091</td>
      <td>59.960344</td>
      <td>143.461538</td>
      <td>114.363636</td>
      <td>76.666667</td>
      <td>158.333333</td>
      <td>84.205934</td>
      <td>85.353485</td>
      <td>85.883694</td>
      <td>86.183251</td>
      <td>116.137862</td>
      <td>97.833333</td>
      <td>96.250000</td>
      <td>99.125000</td>
      <td>57.564944</td>
      <td>43.333333</td>
      <td>45.583333</td>
      <td>43.875000</td>
      <td>75.933999</td>
      <td>61.500000</td>
      <td>62.471667</td>
      <td>62.292500</td>
      <td>36.771429</td>
      <td>37.200000</td>
      <td>37.250000</td>
      <td>37.082040</td>
      <td>8.500000</td>
      <td>15.000000</td>
      <td>15.000000</td>
      <td>15.000000</td>
      <td>19.920446</td>
      <td>19.896906</td>
      <td>19.816753</td>
      <td>20.011274</td>
      <td>0.442857</td>
      <td>0.508116</td>
      <td>0.50586</td>
      <td>0.507407</td>
      <td>7.331429</td>
      <td>7.390000</td>
      <td>7.400000</td>
      <td>7.398353</td>
      <td>156.428571</td>
      <td>153.000000</td>
      <td>118.137949</td>
      <td>115.412815</td>
      <td>44.857143</td>
      <td>42.500000</td>
      <td>39.565064</td>
      <td>40.44737</td>
      <td>28.15</td>
      <td>30.964831</td>
      <td>28.8</td>
      <td>29.951503</td>
      <td>5.100000</td>
      <td>4.173645</td>
      <td>4.1</td>
      <td>4.071882</td>
      <td>0.75</td>
      <td>1.453197</td>
      <td>0.9</td>
      <td>1.487078</td>
      <td>223.500000</td>
      <td>197.180099</td>
      <td>221.0</td>
      <td>176.738114</td>
      <td>11.0</td>
      <td>25.731611</td>
      <td>18.0</td>
      <td>25.921424</td>
      <td>22.5</td>
      <td>23.411007</td>
      <td>30.0</td>
      <td>24.214374</td>
      <td>2.000000</td>
      <td>2.055607</td>
      <td>2.0</td>
      <td>2.051946</td>
      <td>138.000000</td>
      <td>139.053094</td>
      <td>138.0</td>
      <td>138.633454</td>
      <td>161.000000</td>
      <td>131.87701</td>
      <td>99.0</td>
      <td>128.530985</td>
      <td>17.100000</td>
      <td>12.416399</td>
      <td>17.3</td>
      <td>12.242523</td>
      <td>97.857143</td>
      <td>98.0000</td>
      <td>96.412421</td>
      <td>96.634002</td>
      <td>2.581794</td>
      <td>2.182108</td>
      <td>2.039218</td>
      <td>1.832424</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>140106</th>
      <td>64</td>
      <td>1</td>
      <td>162.600000</td>
      <td>80.0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>72.269231</td>
      <td>73.642857</td>
      <td>97.200000</td>
      <td>103.181818</td>
      <td>69.517241</td>
      <td>61.850000</td>
      <td>65.611111</td>
      <td>69.230769</td>
      <td>101.275862</td>
      <td>95.950000</td>
      <td>101.944444</td>
      <td>107.615385</td>
      <td>52.103448</td>
      <td>46.000000</td>
      <td>49.944444</td>
      <td>53.615385</td>
      <td>190.384615</td>
      <td>55.416667</td>
      <td>85.000000</td>
      <td>120.909091</td>
      <td>84.205934</td>
      <td>85.353485</td>
      <td>85.883694</td>
      <td>95.600000</td>
      <td>106.000000</td>
      <td>92.875000</td>
      <td>104.000000</td>
      <td>119.822643</td>
      <td>48.000000</td>
      <td>36.750000</td>
      <td>35.000000</td>
      <td>57.930466</td>
      <td>68.000000</td>
      <td>56.000000</td>
      <td>59.000000</td>
      <td>77.475260</td>
      <td>36.950000</td>
      <td>38.454545</td>
      <td>37.133333</td>
      <td>36.933333</td>
      <td>8.000000</td>
      <td>13.750000</td>
      <td>15.000000</td>
      <td>15.000000</td>
      <td>19.920446</td>
      <td>19.896906</td>
      <td>19.816753</td>
      <td>20.011274</td>
      <td>0.740000</td>
      <td>0.500000</td>
      <td>0.50586</td>
      <td>0.507407</td>
      <td>7.350000</td>
      <td>7.427500</td>
      <td>7.443333</td>
      <td>7.470000</td>
      <td>215.875000</td>
      <td>93.000000</td>
      <td>69.000000</td>
      <td>74.000000</td>
      <td>40.375000</td>
      <td>32.750000</td>
      <td>31.500000</td>
      <td>28.00000</td>
      <td>29.60</td>
      <td>29.600000</td>
      <td>26.3</td>
      <td>29.951503</td>
      <td>4.109867</td>
      <td>4.173645</td>
      <td>4.5</td>
      <td>4.071882</td>
      <td>1.00</td>
      <td>1.200000</td>
      <td>1.0</td>
      <td>1.487078</td>
      <td>86.666667</td>
      <td>140.000000</td>
      <td>108.0</td>
      <td>176.738114</td>
      <td>19.0</td>
      <td>18.000000</td>
      <td>14.0</td>
      <td>25.921424</td>
      <td>22.0</td>
      <td>21.000000</td>
      <td>22.0</td>
      <td>24.214374</td>
      <td>1.952595</td>
      <td>2.000000</td>
      <td>2.4</td>
      <td>2.051946</td>
      <td>138.840093</td>
      <td>136.000000</td>
      <td>135.0</td>
      <td>138.633454</td>
      <td>149.939522</td>
      <td>131.87701</td>
      <td>157.0</td>
      <td>128.530985</td>
      <td>6.366667</td>
      <td>8.900000</td>
      <td>8.4</td>
      <td>12.242523</td>
      <td>95.666667</td>
      <td>96.5000</td>
      <td>92.666667</td>
      <td>94.000000</td>
      <td>0.900000</td>
      <td>2.733333</td>
      <td>2.039218</td>
      <td>1.832424</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>140107</th>
      <td>45</td>
      <td>1</td>
      <td>169.337684</td>
      <td>105.5</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>97.181818</td>
      <td>84.466667</td>
      <td>86.350000</td>
      <td>84.363636</td>
      <td>80.355020</td>
      <td>80.313226</td>
      <td>96.272727</td>
      <td>81.000000</td>
      <td>115.676644</td>
      <td>119.538003</td>
      <td>135.818182</td>
      <td>122.818182</td>
      <td>59.269006</td>
      <td>59.575281</td>
      <td>78.000000</td>
      <td>64.363636</td>
      <td>86.500000</td>
      <td>56.875000</td>
      <td>71.250000</td>
      <td>59.375000</td>
      <td>105.500000</td>
      <td>105.500000</td>
      <td>105.500000</td>
      <td>105.500000</td>
      <td>123.090909</td>
      <td>122.923077</td>
      <td>121.636364</td>
      <td>119.822643</td>
      <td>68.181818</td>
      <td>66.538462</td>
      <td>69.000000</td>
      <td>57.930466</td>
      <td>80.500000</td>
      <td>79.461538</td>
      <td>79.600000</td>
      <td>77.475260</td>
      <td>39.600000</td>
      <td>37.700000</td>
      <td>38.300000</td>
      <td>38.300000</td>
      <td>8.666667</td>
      <td>9.666667</td>
      <td>9.333333</td>
      <td>10.000000</td>
      <td>19.920446</td>
      <td>19.896906</td>
      <td>19.816753</td>
      <td>20.011274</td>
      <td>0.575000</td>
      <td>0.500000</td>
      <td>0.62000</td>
      <td>0.450000</td>
      <td>7.430000</td>
      <td>8.406452</td>
      <td>7.460000</td>
      <td>7.495000</td>
      <td>104.000000</td>
      <td>123.874998</td>
      <td>116.000000</td>
      <td>95.500000</td>
      <td>50.500000</td>
      <td>39.931142</td>
      <td>39.500000</td>
      <td>41.00000</td>
      <td>30.90</td>
      <td>29.100000</td>
      <td>27.6</td>
      <td>29.951503</td>
      <td>3.900000</td>
      <td>3.900000</td>
      <td>3.3</td>
      <td>4.071882</td>
      <td>0.90</td>
      <td>1.000000</td>
      <td>0.6</td>
      <td>1.487078</td>
      <td>384.000000</td>
      <td>374.000000</td>
      <td>411.0</td>
      <td>176.738114</td>
      <td>21.0</td>
      <td>24.000000</td>
      <td>22.0</td>
      <td>25.921424</td>
      <td>31.0</td>
      <td>32.000000</td>
      <td>28.0</td>
      <td>24.214374</td>
      <td>2.600000</td>
      <td>2.700000</td>
      <td>2.5</td>
      <td>2.051946</td>
      <td>145.000000</td>
      <td>146.000000</td>
      <td>140.0</td>
      <td>138.633454</td>
      <td>125.000000</td>
      <td>135.00000</td>
      <td>139.0</td>
      <td>128.530985</td>
      <td>12.100000</td>
      <td>12.900000</td>
      <td>10.8</td>
      <td>12.242523</td>
      <td>97.000000</td>
      <td>96.8269</td>
      <td>96.412421</td>
      <td>96.634002</td>
      <td>1.200000</td>
      <td>2.182108</td>
      <td>0.850000</td>
      <td>1.000000</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4>Regressor Task</h4><p><p>Steps:</p></p>
<ol>
    <li><b>Standard Scaler</b></li>
    <li><b>Dimensionality Reducer with Variance Threshold</b></li>
    <li><b>Dimensionality Reducer with PCA</b></li>
    <li><b>Classifier</b>
        <br>We compare four different classifier models based on the mean squared error. The three classifiers are Linear Regression, DecisionTreeRegressor and MLPRegressor.</br>
    </li>
</ol>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[31]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Running CV</span>

<span class="c1">#Create a matrix to store the results where row represnets the model and column represents the result tested on fold i</span>
<span class="n">w</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span>
<span class="n">matrix</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">w</span><span class="p">)]</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">h</span><span class="p">)]</span> 

<span class="c1"># keep track of the best score when trying out different hyperparameters in a fold</span>
<span class="n">best</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">maxsize</span>
<span class="n">param</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>

<span class="n">all_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Testing on Fold&quot;</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">x_train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">y_train_linear_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">y_train_class_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">x_test_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">y_test_linear_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">y_test_class_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

    <span class="c1"># Getting train data set up</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;# Getting train data set up&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">all_list</span> <span class="k">if</span> <span class="n">x</span> <span class="o">!=</span> <span class="n">i</span><span class="p">]:</span>
        <span class="n">x_train_df</span> <span class="o">=</span> <span class="n">x_train_df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">folds_x</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
        <span class="n">y_train_linear_df</span> <span class="o">=</span> <span class="n">y_train_linear_df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">folds_y_linear</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">y_train_class_df</span> <span class="o">=</span> <span class="n">y_train_class_df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">folds_y_class</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

    <span class="c1"># Getting test data set up </span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;# Getting test data set up&quot;</span><span class="p">)</span>
    <span class="n">x_test_df</span> <span class="o">=</span> <span class="n">folds_x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">y_test_linear_df</span><span class="o">=</span> <span class="n">folds_y_linear</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">y_test_class_df</span> <span class="o">=</span> <span class="n">folds_y_class</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

    <span class="c1"># Linear</span>
    <span class="n">best</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">maxsize</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;f_selecter__threshold&#39;</span><span class="p">:[</span><span class="mf">0.5</span><span class="p">],</span> 
                  <span class="s1">&#39;dim_reducer__n_components&#39;</span><span class="p">:[</span><span class="mi">60</span><span class="p">,</span><span class="mi">70</span><span class="p">]}</span>
    <span class="n">est</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[</span>
            <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">scaler</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;f_selecter&#39;</span><span class="p">,</span> <span class="n">VarianceThreshold</span><span class="p">()),</span>
            <span class="p">(</span><span class="s1">&#39;dim_reducer&#39;</span><span class="p">,</span> <span class="n">PCA</span><span class="p">()),</span>
            <span class="p">(</span><span class="s1">&#39;classifier&#39;</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">())])</span>
    <span class="n">estimator</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">est</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">)</span>
    <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train_df</span><span class="p">,</span> <span class="n">y_train_linear_df</span><span class="p">)</span><span class="c1"># gridsearch obejct has scanned thorugh all the best parameters to set it</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test_df</span><span class="p">)</span>
    <span class="n">matrix</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test_linear_df</span><span class="p">,</span> <span class="n">prediction</span><span class="p">))</span>
    
    
    <span class="c1"># DecisionTreeRegressor</span>
    <span class="n">best</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">maxsize</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;f_selecter__threshold&#39;</span><span class="p">:[</span><span class="mf">0.5</span><span class="p">],</span> 
                  <span class="s1">&#39;dim_reducer__n_components&#39;</span><span class="p">:[</span><span class="mi">60</span><span class="p">,</span><span class="mi">70</span><span class="p">],</span>
                  <span class="s1">&#39;classifier__min_samples_leaf&#39;</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span>
                 <span class="p">}</span>
    <span class="n">est</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[</span>
            <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">scaler</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;f_selecter&#39;</span><span class="p">,</span> <span class="n">VarianceThreshold</span><span class="p">()),</span>
            <span class="p">(</span><span class="s1">&#39;dim_reducer&#39;</span><span class="p">,</span> <span class="n">PCA</span><span class="p">()),</span>
            <span class="p">(</span><span class="s1">&#39;classifier&#39;</span><span class="p">,</span> <span class="n">DecisionTreeRegressor</span><span class="p">())])</span>
    <span class="n">estimator</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">est</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">)</span>
    <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train_df</span><span class="p">,</span> <span class="n">y_train_linear_df</span><span class="p">)</span><span class="c1"># gridsearch obejct has scanned thorugh all the best parameters to set it</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test_df</span><span class="p">)</span>
    <span class="n">matrix</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test_linear_df</span><span class="p">,</span> <span class="n">prediction</span><span class="p">))</span>
    
    
    <span class="c1"># MLPRegressor</span>
    <span class="n">best</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">maxsize</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;f_selecter__threshold&#39;</span><span class="p">:[</span><span class="mf">0.5</span><span class="p">],</span> 
                  <span class="s1">&#39;dim_reducer__n_components&#39;</span><span class="p">:[</span><span class="mi">60</span><span class="p">,</span><span class="mi">70</span><span class="p">],</span>
                  <span class="s1">&#39;classifier__hidden_layer_sizes&#39;</span><span class="p">:[(</span><span class="mi">130</span><span class="p">,</span> <span class="mi">110</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="mi">130</span><span class="p">,</span><span class="mi">100</span><span class="p">)],</span>
                  <span class="s1">&#39;classifier__learning_rate_init&#39;</span><span class="p">:[</span><span class="mf">0.9</span><span class="p">,</span><span class="mf">0.105</span><span class="p">]</span>
                 <span class="p">}</span>
    <span class="n">est</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[</span>
            <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">scaler</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;f_selecter&#39;</span><span class="p">,</span> <span class="n">VarianceThreshold</span><span class="p">()),</span>
            <span class="p">(</span><span class="s1">&#39;dim_reducer&#39;</span><span class="p">,</span> <span class="n">PCA</span><span class="p">()),</span>
            <span class="p">(</span><span class="s1">&#39;classifier&#39;</span><span class="p">,</span> <span class="n">MLPRegressor</span><span class="p">())])</span>
    <span class="n">estimator</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">est</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">)</span>
    <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train_df</span><span class="p">,</span> <span class="n">y_train_linear_df</span><span class="p">)</span><span class="c1"># gridsearch obejct has scanned thorugh all the best parameters to set it</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test_df</span><span class="p">)</span>
    <span class="n">matrix</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test_linear_df</span><span class="p">,</span> <span class="n">prediction</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Testing on Fold 1
# Getting train data set up
# Getting test data set up
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  % self.max_iter, ConvergenceWarning)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Testing on Fold 2
# Getting train data set up
# Getting test data set up
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Testing on Fold 3
# Getting train data set up
# Getting test data set up
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  % self.max_iter, ConvergenceWarning)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Testing on Fold 4
# Getting train data set up
# Getting test data set up
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  % self.max_iter, ConvergenceWarning)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  % self.max_iter, ConvergenceWarning)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  % self.max_iter, ConvergenceWarning)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  % self.max_iter, ConvergenceWarning)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
C:\Users\samue\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4> Result</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[32]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">resultnumber_of_learners</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">score_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Linear&quot;</span><span class="p">,</span> <span class="s2">&quot;DecisionTree&quot;</span><span class="p">,</span> <span class="s2">&quot;MLPRegressor&quot;</span><span class="p">,</span><span class="s2">&quot;&quot;</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">number_of_learners</span><span class="p">):</span>
    <span class="n">score_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">score_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">4</span><span class="p">,</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
<span class="n">display</span><span class="p">(</span><span class="n">score_df</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Linear</th>
      <th>DecisionTree</th>
      <th>MLPRegressor</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>204.702878</td>
      <td>340.127821</td>
      <td>199.545426</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>140.603207</td>
      <td>267.058755</td>
      <td>137.440724</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>142.208207</td>
      <td>248.519249</td>
      <td>138.764072</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1338.477124</td>
      <td>417.637773</td>
      <td>118.310005</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>21.365810</td>
      <td>17.841970</td>
      <td>12.186675</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><h4>Classifier Task</h4></p>
<p><p>Steps:</p></p>
<ol>
    <li><b>SMOTE</b></li>
    <li><b>Standard Scaler</b></li>
    <li><b>Dimensionality Reducer with PCA</b></li>
    <li><b>Classifier</b>
        <br>We compare four different classifier models based on the ROC AUC score. The four classifiers are Logistic Regression, Random Forest, K Nearest Neighbors, and MLP.</br>
    </li>
</ol>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># import all folds</span>
<span class="c1"># Modify into Design Matrix 2</span>
<span class="n">x_fold</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">y_fold</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">non_bin_feat</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">string</span> <span class="o">=</span> <span class="s2">&quot;../Project_Data/Fold&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="n">y_file</span> <span class="o">=</span> <span class="s2">&quot;../Project_Data/Fold&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">+</span><span class="s2">&quot;_Outcomes.csv&quot;</span>
    <span class="n">x_fold</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">preprocess_x_for_design_matrix_2</span><span class="p">(</span><span class="n">put_single_into_dataframe</span><span class="p">(</span><span class="n">read_text</span><span class="p">(</span><span class="n">string</span><span class="p">)))</span>
    <span class="n">y_fold</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">y_file</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[20]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#For MODEL 2</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">OneHotEncoder</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="k">import</span> <span class="n">sqrt</span>

<span class="n">result</span> <span class="o">=</span> <span class="p">{}</span> <span class="c1">#storing results of each fold</span>
<span class="n">all_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Testing on Fold&quot;</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">x_train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">y_train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    
    <span class="c1"># Getting train data set up</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">all_list</span> <span class="k">if</span> <span class="n">x</span> <span class="o">!=</span> <span class="n">i</span><span class="p">]:</span> 
        <span class="n">x_train_df</span> <span class="o">=</span> <span class="n">x_train_df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_fold</span><span class="p">[</span><span class="n">j</span><span class="p">])</span> <span class="c1">#Inserting X for train data</span>
        <span class="n">y_train_df</span> <span class="o">=</span> <span class="n">y_train_df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_fold</span><span class="p">[</span><span class="n">j</span><span class="p">])</span> <span class="c1">#Inserting Y for train data</span>
    <span class="n">y_train_df</span> <span class="o">=</span> <span class="n">y_train_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;Length_of_stay&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y_train_df</span> <span class="o">=</span> <span class="n">y_train_df</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1">#use -1 to indicate missing values</span>
    
    <span class="n">train_df</span> <span class="o">=</span> <span class="n">x_train_df</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">y_train_df</span><span class="p">,</span> <span class="n">left_on</span><span class="o">=</span><span class="s2">&quot;recordid&quot;</span><span class="p">,</span> <span class="n">right_on</span><span class="o">=</span><span class="s2">&quot;RecordID&quot;</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s1">&#39;outer&#39;</span><span class="p">)</span>
    <span class="n">train_df</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">&quot;RecordID&quot;</span><span class="p">)</span>
    <span class="n">train_df</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">train_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">!=</span> <span class="s1">&#39;In-hospital_death&#39;</span><span class="p">]</span>
    <span class="n">Y_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;In-hospital_death&#39;</span><span class="p">]</span>
    
        
    <span class="n">x_test_df</span> <span class="o">=</span> <span class="n">x_train_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">y_test_df</span> <span class="o">=</span> <span class="n">y_train_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># Getting test data set up</span>
    <span class="n">x_test_df</span> <span class="o">=</span> <span class="n">x_test_df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_fold</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">y_test_df</span> <span class="o">=</span> <span class="n">y_test_df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_fold</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">y_test_df</span> <span class="o">=</span> <span class="n">y_test_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;Length_of_stay&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
     <span class="c1"># Replace -1 with NaN</span>
<span class="c1">#     x_test_df = x_test_df.replace(-1, np.nan)</span>
    <span class="c1"># Replace not known length of stay to 2</span>
    <span class="n">y_test_df</span> <span class="o">=</span> <span class="n">y_test_df</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    
    <span class="n">test_df</span> <span class="o">=</span> <span class="n">x_test_df</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">y_test_df</span><span class="p">,</span> <span class="n">left_on</span><span class="o">=</span><span class="s2">&quot;recordid&quot;</span><span class="p">,</span> <span class="n">right_on</span><span class="o">=</span><span class="s2">&quot;RecordID&quot;</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s1">&#39;outer&#39;</span><span class="p">)</span>
    <span class="n">test_df</span> <span class="o">=</span> <span class="n">test_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">&quot;RecordID&quot;</span><span class="p">)</span>
    <span class="n">test_df</span> <span class="o">=</span> <span class="n">test_df</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">test_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">!=</span> <span class="s1">&#39;In-hospital_death&#39;</span><span class="p">]</span>
    <span class="n">Y_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[</span><span class="s1">&#39;In-hospital_death&#39;</span><span class="p">]</span>
    
<span class="c1">#     print(X_train.head())</span>
<span class="c1">#     best = 0</span>
    
    <span class="c1"># Logistic Regression</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;dim_reducer__n_components&#39;</span><span class="p">:[</span><span class="mi">60</span><span class="p">,</span><span class="mi">70</span><span class="p">],</span>
              <span class="s1">&#39;f_selecter__k&#39;</span><span class="p">:[</span><span class="mi">70</span><span class="p">,</span><span class="mi">80</span><span class="p">,</span><span class="mi">90</span><span class="p">]</span>
             <span class="p">}</span>
    
    <span class="n">model</span> <span class="o">=</span> <span class="n">imPipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[</span>
                              <span class="p">(</span><span class="s1">&#39;smote&#39;</span><span class="p">,</span> <span class="n">SMOTE</span><span class="p">()),</span>
                             <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
                            <span class="p">(</span><span class="s1">&#39;f_selecter&#39;</span><span class="p">,</span> <span class="n">SelectKBest</span><span class="p">()),</span>
                            <span class="p">(</span><span class="s1">&#39;dim_reducer&#39;</span><span class="p">,</span> <span class="n">PCA</span><span class="p">()),</span>
                           <span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">LogisticRegression</span><span class="p">())])</span>
    <span class="n">est</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scoring</span> <span class="o">=</span> <span class="s1">&#39;roc_auc&#39;</span><span class="p">)</span>
    <span class="n">est</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
    <span class="n">best_est</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">best_estimator_</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">best_est</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Logistic&quot;</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>            
        
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">result</span><span class="p">[</span><span class="s1">&#39;logistic&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">score</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span><span class="p">[</span><span class="s1">&#39;logistic&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;logistic&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">score</span>
    

    <span class="c1"># Random Forest</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;dim_reducer__n_components&#39;</span><span class="p">:[</span><span class="mi">60</span><span class="p">,</span><span class="mi">70</span><span class="p">],</span>
              <span class="s1">&#39;f_selecter__k&#39;</span><span class="p">:[</span><span class="mi">70</span><span class="p">,</span><span class="mi">80</span><span class="p">,</span><span class="mi">90</span><span class="p">],</span>
              <span class="s1">&#39;rf__max_depth&#39;</span><span class="p">:[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span>
              <span class="s1">&#39;rf__n_estimators&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span><span class="mi">125</span><span class="p">,</span><span class="mi">150</span><span class="p">]</span>
             <span class="p">}</span>
    
    <span class="n">model</span> <span class="o">=</span> <span class="n">imPipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[</span>
                                      <span class="p">(</span><span class="s1">&#39;smote&#39;</span><span class="p">,</span> <span class="n">SMOTE</span><span class="p">()),</span>
                                      <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
                                            <span class="p">(</span><span class="s1">&#39;f_selecter&#39;</span><span class="p">,</span> <span class="n">SelectKBest</span><span class="p">()),</span>
                                            <span class="p">(</span><span class="s1">&#39;dim_reducer&#39;</span><span class="p">,</span> <span class="n">PCA</span><span class="p">()),</span>
                                           <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">RandomForestClassifier</span><span class="p">())])</span>
    
    <span class="n">est</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scoring</span> <span class="o">=</span> <span class="s1">&#39;roc_auc&#39;</span><span class="p">)</span>
    <span class="n">est</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
    <span class="n">best_est</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">best_estimator_</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">best_est</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RF&quot;</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>            
        
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">result</span><span class="p">[</span><span class="s1">&#39;random_forest&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">score</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span><span class="p">[</span><span class="s1">&#39;random_forest&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;random_forest&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">score</span>
    
    <span class="c1"># K-Neighbors</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;dim_reducer__n_components&#39;</span><span class="p">:[</span><span class="mi">60</span><span class="p">,</span><span class="mi">70</span><span class="p">],</span>
              <span class="s1">&#39;f_selecter__k&#39;</span><span class="p">:[</span><span class="mi">70</span><span class="p">,</span><span class="mi">80</span><span class="p">,</span><span class="mi">90</span><span class="p">],</span>
                  <span class="s1">&#39;classifier__n_neighbors&#39;</span><span class="p">:[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">]</span>
             <span class="p">}</span>
    
    <span class="n">model</span> <span class="o">=</span> <span class="n">imPipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[</span>
                                      <span class="p">(</span><span class="s1">&#39;smote&#39;</span><span class="p">,</span> <span class="n">SMOTE</span><span class="p">()),</span>
                                      <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
                                            <span class="p">(</span><span class="s1">&#39;f_selecter&#39;</span><span class="p">,</span> <span class="n">SelectKBest</span><span class="p">()),</span>
                                            <span class="p">(</span><span class="s1">&#39;dim_reducer&#39;</span><span class="p">,</span> <span class="n">PCA</span><span class="p">()),</span>
                                           <span class="p">(</span><span class="s1">&#39;classifier&#39;</span><span class="p">,</span> <span class="n">KNeighborsClassifier</span><span class="p">())])</span>
    
    <span class="n">est</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scoring</span> <span class="o">=</span> <span class="s1">&#39;roc_auc&#39;</span><span class="p">)</span>
    <span class="n">est</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
    <span class="n">best_est</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">best_estimator_</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">best_est</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;KNeighbors&quot;</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">result</span><span class="p">[</span><span class="s1">&#39;k_neighbors&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">score</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span><span class="p">[</span><span class="s1">&#39;k_neighbors&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;k_neighbors&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">score</span>   
    
    <span class="c1"># MLP Classifier</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;dim_reducer__n_components&#39;</span><span class="p">:[</span><span class="mi">60</span><span class="p">,</span><span class="mi">70</span><span class="p">],</span>
              <span class="s1">&#39;f_selecter__k&#39;</span><span class="p">:[</span><span class="mi">70</span><span class="p">,</span><span class="mi">80</span><span class="p">,</span><span class="mi">90</span><span class="p">],</span>
                  <span class="s1">&#39;MLP__hidden_layer_sizes&#39;</span><span class="p">:[(</span><span class="mi">130</span><span class="p">,</span><span class="mi">100</span><span class="p">,),</span> <span class="p">(</span><span class="mi">100</span><span class="p">,)],</span>
                  <span class="s1">&#39;MLP__learning_rate_init&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">]</span>
             <span class="p">}</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">imPipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[</span>
                              <span class="p">(</span><span class="s1">&#39;smote&#39;</span><span class="p">,</span> <span class="n">SMOTE</span><span class="p">()),</span>
                              <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
                              <span class="p">(</span><span class="s1">&#39;f_selecter&#39;</span><span class="p">,</span> <span class="n">SelectKBest</span><span class="p">()),</span>
                              <span class="p">(</span><span class="s1">&#39;dim_reducer&#39;</span><span class="p">,</span> <span class="n">PCA</span><span class="p">()),</span>
                              <span class="p">(</span><span class="s1">&#39;MLP&#39;</span><span class="p">,</span> <span class="n">MLPClassifier</span><span class="p">())])</span>
    <span class="n">est</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scoring</span> <span class="o">=</span> <span class="s1">&#39;roc_auc&#39;</span><span class="p">)</span>
    <span class="n">est</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
    <span class="n">best_est</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">best_estimator_</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">best_est</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MLP&quot;</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>            
        
                
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">result</span><span class="p">[</span><span class="s1">&#39;MLP&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">score</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span><span class="p">[</span><span class="s1">&#39;MLP&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;MLP&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">score</span>            
                
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">result</span><span class="p">:</span>
    <span class="n">result</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">/</span><span class="mi">4</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Testing on Fold 1
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\Asus\Anaconda3\lib\site-packages\pandas\core\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version
of pandas will change to not sort by default.

To accept the future behavior, pass &#39;sort=False&#39;.

To retain the current behavior and silence the warning, pass &#39;sort=True&#39;.

  sort=sort,
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;dim_reducer__n_components&#39;: 60, &#39;f_selecter__k&#39;: 70}
Logistic 0.7155160675381262
{&#39;dim_reducer__n_components&#39;: 60, &#39;f_selecter__k&#39;: 80, &#39;rf__max_depth&#39;: 4, &#39;rf__n_estimators&#39;: 150}
RF 0.7026824618736384
{&#39;classifier__n_neighbors&#39;: 5, &#39;dim_reducer__n_components&#39;: 70, &#39;f_selecter__k&#39;: 90}
KNeighbors 0.7191244553376906
{&#39;MLP__hidden_layer_sizes&#39;: (100,), &#39;MLP__learning_rate_init&#39;: 0.01, &#39;dim_reducer__n_components&#39;: 70, &#39;f_selecter__k&#39;: 80}
MLP 0.6681304466230937
Testing on Fold 2
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\Asus\Anaconda3\lib\site-packages\pandas\core\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version
of pandas will change to not sort by default.

To accept the future behavior, pass &#39;sort=False&#39;.

To retain the current behavior and silence the warning, pass &#39;sort=True&#39;.

  sort=sort,
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;dim_reducer__n_components&#39;: 60, &#39;f_selecter__k&#39;: 90}
Logistic 0.7536480142113944
{&#39;dim_reducer__n_components&#39;: 70, &#39;f_selecter__k&#39;: 70, &#39;rf__max_depth&#39;: 5, &#39;rf__n_estimators&#39;: 100}
RF 0.7057797233853572
{&#39;classifier__n_neighbors&#39;: 5, &#39;dim_reducer__n_components&#39;: 60, &#39;f_selecter__k&#39;: 70}
KNeighbors 0.7197690648394873
{&#39;MLP__hidden_layer_sizes&#39;: (130, 100), &#39;MLP__learning_rate_init&#39;: 0.01, &#39;dim_reducer__n_components&#39;: 70, &#39;f_selecter__k&#39;: 70}
MLP 0.6005107219895952
Testing on Fold 3
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\Asus\Anaconda3\lib\site-packages\pandas\core\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version
of pandas will change to not sort by default.

To accept the future behavior, pass &#39;sort=False&#39;.

To retain the current behavior and silence the warning, pass &#39;sort=True&#39;.

  sort=sort,
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;dim_reducer__n_components&#39;: 60, &#39;f_selecter__k&#39;: 70}
Logistic 0.7285449292491546
{&#39;dim_reducer__n_components&#39;: 60, &#39;f_selecter__k&#39;: 90, &#39;rf__max_depth&#39;: 5, &#39;rf__n_estimators&#39;: 150}
RF 0.6920450441577203
{&#39;classifier__n_neighbors&#39;: 5, &#39;dim_reducer__n_components&#39;: 70, &#39;f_selecter__k&#39;: 80}
KNeighbors 0.6349354870481632
{&#39;MLP__hidden_layer_sizes&#39;: (100,), &#39;MLP__learning_rate_init&#39;: 0.01, &#39;dim_reducer__n_components&#39;: 70, &#39;f_selecter__k&#39;: 90}
MLP 0.6012672773236154
Testing on Fold 4
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\Asus\Anaconda3\lib\site-packages\pandas\core\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version
of pandas will change to not sort by default.

To accept the future behavior, pass &#39;sort=False&#39;.

To retain the current behavior and silence the warning, pass &#39;sort=True&#39;.

  sort=sort,
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;dim_reducer__n_components&#39;: 60, &#39;f_selecter__k&#39;: 70}
Logistic 0.7213661123853211
{&#39;dim_reducer__n_components&#39;: 60, &#39;f_selecter__k&#39;: 90, &#39;rf__max_depth&#39;: 4, &#39;rf__n_estimators&#39;: 100}
RF 0.6829845183486238
{&#39;classifier__n_neighbors&#39;: 5, &#39;dim_reducer__n_components&#39;: 60, &#39;f_selecter__k&#39;: 90}
KNeighbors 0.6539205848623854
{&#39;MLP__hidden_layer_sizes&#39;: (100,), &#39;MLP__learning_rate_init&#39;: 0.01, &#39;dim_reducer__n_components&#39;: 70, &#39;f_selecter__k&#39;: 70}
MLP 0.5767990252293578
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4>Result</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[21]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">result</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[21]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>{&#39;logistic&#39;: 0.7297687808459992,
 &#39;random_forest&#39;: 0.6958729369413349,
 &#39;k_neighbors&#39;: 0.6819373980219317,
 &#39;MLP&#39;: 0.6116768677914155}</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><u>Logistic Regression</u> will be used for <b>M2</b> as it yields higher results. For this design matrix the <b>M1</b> model will be <u>Random Forest</u> as the second best.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><h3>Third Model</h3></p>
<ul> 
    <li>We make use of the second Design matrix described above on the same parameters that we tried in the Model building 1.</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># import all folds</span>
<span class="c1"># Modify into Design Matrix 3</span>
<span class="n">x_fold</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">y_fold</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">string</span> <span class="o">=</span> <span class="s2">&quot;../Project_Data/Fold&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="n">y_file</span> <span class="o">=</span> <span class="s2">&quot;../Project_Data/Fold&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">+</span><span class="s2">&quot;_Outcomes.csv&quot;</span>
    <span class="n">x_fold</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">preprocess_x_for_design_matrix_3</span><span class="p">(</span><span class="n">put_single_into_dataframe</span><span class="p">(</span><span class="n">read_text</span><span class="p">(</span><span class="n">string</span><span class="p">)))</span>
    <span class="n">y_fold</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">y_file</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><h4>Classifier Task</h4></p>
<p><p>We have several steps for classifier task of this design matrix.
    <ol>
        <li><b>SMOTE</b>
            <br>SMOTE is used to balanced the data. From intuition, we believe that living patients should outnumber the rate of mortalities. And based on our observation of data it's true. Only around a tenth of the patients actually passed away in the hospital. SMOTE will oversample the mortality data to achieve balance.</br>
        </li>
        <li><b>Classifier Model</b>
            <br>We select our classifier model empirically based on which model has the best results on average across all folds. There are three classifiers that we tested which are Logistic Regression, Random Forest, and MLP.</br>
        </li>
    </ol></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[23]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#For MODEL 3</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">OneHotEncoder</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="k">import</span> <span class="n">sqrt</span>

<span class="n">num_feat</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">normal_feat</span> <span class="o">=</span> <span class="n">nor_feat</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">normal_feat</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s2">&quot;MechVent&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">normal_feat</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">hour</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">49</span><span class="p">):</span>
        <span class="n">num_feat</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">feat</span> <span class="o">+</span> <span class="s1">&#39;_&#39;</span> <span class="o">+</span>  <span class="nb">str</span><span class="p">(</span><span class="n">hour</span><span class="p">))</span>
<span class="n">cat_feat</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;ICUType&quot;</span><span class="p">]</span>

<span class="n">cat_transformer</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;imputer&#39;</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;most_frequent&#39;</span><span class="p">)),</span>
                                 <span class="p">(</span><span class="s1">&#39;OneHotEncoder&#39;</span><span class="p">,</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">handle_unknown</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">))])</span>

<span class="n">stat_feat_cp</span> <span class="o">=</span> <span class="n">stat_feat</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">stat_feat_cp</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s2">&quot;RecordID&quot;</span><span class="p">)</span>
<span class="n">stat_feat_cp</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s2">&quot;ICUType&quot;</span><span class="p">)</span>
<span class="n">prepro</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">(</span>
    <span class="n">remainder</span> <span class="o">=</span> <span class="s1">&#39;passthrough&#39;</span><span class="p">,</span>
    <span class="n">transformers</span><span class="o">=</span><span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="n">cat_transformer</span><span class="p">,</span> <span class="n">cat_feat</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;num&#39;</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">),</span> <span class="n">num_feat</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;stat&#39;</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">),</span> <span class="n">stat_feat_cp</span><span class="p">)])</span>

<span class="n">result</span> <span class="o">=</span> <span class="p">{}</span> <span class="c1">#storing results of each fold</span>
<span class="n">all_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Testing on Fold&quot;</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">x_train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">y_train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    
    <span class="c1"># Getting train data set up</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">all_list</span> <span class="k">if</span> <span class="n">x</span> <span class="o">!=</span> <span class="n">i</span><span class="p">]:</span> 
        <span class="n">x_train_df</span> <span class="o">=</span> <span class="n">x_train_df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_fold</span><span class="p">[</span><span class="n">j</span><span class="p">])</span> <span class="c1">#Inserting X for train data</span>
        <span class="n">y_train_df</span> <span class="o">=</span> <span class="n">y_train_df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_fold</span><span class="p">[</span><span class="n">j</span><span class="p">])</span> <span class="c1">#Inserting Y for train data</span>
    <span class="n">y_train_df</span> <span class="o">=</span> <span class="n">y_train_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;Length_of_stay&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y_train_df</span> <span class="o">=</span> <span class="n">y_train_df</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1">#use -1 to indicate missing values</span>
    
    <span class="n">train_df</span> <span class="o">=</span> <span class="n">x_train_df</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">y_train_df</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s2">&quot;RecordID&quot;</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s1">&#39;outer&#39;</span><span class="p">)</span>
    <span class="n">train_df</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">&quot;RecordID&quot;</span><span class="p">)</span>
    <span class="n">train_df</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">train_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">!=</span> <span class="s1">&#39;In-hospital_death&#39;</span><span class="p">]</span>
    <span class="n">Y_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;In-hospital_death&#39;</span><span class="p">]</span>
    
        
    <span class="n">x_test_df</span> <span class="o">=</span> <span class="n">x_train_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">y_test_df</span> <span class="o">=</span> <span class="n">y_train_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># Getting test data set up</span>
    <span class="n">x_test_df</span> <span class="o">=</span> <span class="n">x_test_df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_fold</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">y_test_df</span> <span class="o">=</span> <span class="n">y_test_df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_fold</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">y_test_df</span> <span class="o">=</span> <span class="n">y_test_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;Length_of_stay&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
     <span class="c1"># Replace -1 with NaN</span>
<span class="c1">#     x_test_df = x_test_df.replace(-1, np.nan)</span>
    <span class="c1"># Replace not known length of stay to 2</span>
    <span class="n">y_test_df</span> <span class="o">=</span> <span class="n">y_test_df</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    
    <span class="n">test_df</span> <span class="o">=</span> <span class="n">x_test_df</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">y_test_df</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s2">&quot;RecordID&quot;</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s1">&#39;outer&#39;</span><span class="p">)</span>
    <span class="n">test_df</span> <span class="o">=</span> <span class="n">test_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">&quot;RecordID&quot;</span><span class="p">)</span>
    <span class="n">test_df</span> <span class="o">=</span> <span class="n">test_df</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">test_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">!=</span> <span class="s1">&#39;In-hospital_death&#39;</span><span class="p">]</span>
    <span class="n">Y_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[</span><span class="s1">&#39;In-hospital_death&#39;</span><span class="p">]</span>
    
<span class="c1">#     print(X_train.head())</span>
    <span class="n">best</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="c1"># Logistic Regression</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;dim_reducer__n_components&#39;</span><span class="p">:[</span><span class="mi">60</span><span class="p">,</span><span class="mi">70</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">200</span><span class="p">],</span>
              <span class="s1">&#39;f_selecter__k&#39;</span><span class="p">:[</span><span class="mi">350</span><span class="p">,</span><span class="mi">400</span><span class="p">]</span>
             <span class="p">}</span>
    
    <span class="n">model</span> <span class="o">=</span> <span class="n">imPipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;imputer&#39;</span><span class="p">,</span> <span class="n">prepro</span><span class="p">),</span>
                              <span class="p">(</span><span class="s1">&#39;smote&#39;</span><span class="p">,</span> <span class="n">SMOTE</span><span class="p">()),</span>
                             <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
                            <span class="p">(</span><span class="s1">&#39;f_selecter&#39;</span><span class="p">,</span> <span class="n">SelectKBest</span><span class="p">()),</span>
                            <span class="p">(</span><span class="s1">&#39;dim_reducer&#39;</span><span class="p">,</span> <span class="n">PCA</span><span class="p">()),</span>
                           <span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">LogisticRegression</span><span class="p">())])</span>
    <span class="n">est</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scoring</span> <span class="o">=</span> <span class="s1">&#39;roc_auc&#39;</span><span class="p">)</span>
    <span class="n">est</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
    <span class="n">best_est</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">best_estimator_</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">best_est</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Logistic&quot;</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>            
        
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">result</span><span class="p">[</span><span class="s1">&#39;logistic&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">score</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span><span class="p">[</span><span class="s1">&#39;logistic&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;logistic&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">score</span>
    

    <span class="c1"># Random Forest</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;dim_reducer__n_components&#39;</span><span class="p">:[</span><span class="mi">60</span><span class="p">,</span><span class="mi">70</span><span class="p">],</span>
              <span class="s1">&#39;f_selecter__k&#39;</span><span class="p">:[</span><span class="mi">350</span><span class="p">,</span><span class="mi">400</span><span class="p">],</span>
              <span class="s1">&#39;rf__max_depth&#39;</span><span class="p">:[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span>
              <span class="s1">&#39;rf__n_estimators&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span><span class="mi">125</span><span class="p">,</span><span class="mi">150</span><span class="p">]</span>
             <span class="p">}</span>
    
    <span class="n">model</span> <span class="o">=</span> <span class="n">imPipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;imputer&#39;</span><span class="p">,</span> <span class="n">prepro</span><span class="p">),</span>
                                      <span class="p">(</span><span class="s1">&#39;smote&#39;</span><span class="p">,</span> <span class="n">SMOTE</span><span class="p">()),</span>
                                      <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
                                            <span class="p">(</span><span class="s1">&#39;f_selecter&#39;</span><span class="p">,</span> <span class="n">SelectKBest</span><span class="p">()),</span>
                                            <span class="p">(</span><span class="s1">&#39;dim_reducer&#39;</span><span class="p">,</span> <span class="n">PCA</span><span class="p">()),</span>
                                           <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">RandomForestClassifier</span><span class="p">())])</span>
    
    <span class="n">est</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scoring</span> <span class="o">=</span> <span class="s1">&#39;roc_auc&#39;</span><span class="p">)</span>
    <span class="n">est</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
    <span class="n">best_est</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">best_estimator_</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">best_est</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RF&quot;</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>            
        
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">result</span><span class="p">[</span><span class="s1">&#39;random_forest&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">score</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span><span class="p">[</span><span class="s1">&#39;random_forest&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;random_forest&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">score</span>
    
    <span class="c1"># MLP Classifier</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;dim_reducer__n_components&#39;</span><span class="p">:[</span><span class="mi">60</span><span class="p">,</span><span class="mi">70</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">200</span><span class="p">],</span>
              <span class="s1">&#39;f_selecter__k&#39;</span><span class="p">:[</span><span class="mi">350</span><span class="p">,</span><span class="mi">400</span><span class="p">],</span>
                  <span class="s1">&#39;MLP__hidden_layer_sizes&#39;</span><span class="p">:[(</span><span class="mi">130</span><span class="p">,</span> <span class="mi">110</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),(</span><span class="mi">130</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)],</span>
                  <span class="s1">&#39;MLP__learning_rate_init&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">]</span>
             <span class="p">}</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">imPipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;imputer&#39;</span><span class="p">,</span> <span class="n">prepro</span><span class="p">),</span>
                              <span class="p">(</span><span class="s1">&#39;smote&#39;</span><span class="p">,</span> <span class="n">SMOTE</span><span class="p">()),</span>
                              <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
                              <span class="p">(</span><span class="s1">&#39;f_selecter&#39;</span><span class="p">,</span> <span class="n">SelectKBest</span><span class="p">()),</span>
                              <span class="p">(</span><span class="s1">&#39;dim_reducer&#39;</span><span class="p">,</span> <span class="n">PCA</span><span class="p">()),</span>
                              <span class="p">(</span><span class="s1">&#39;MLP&#39;</span><span class="p">,</span> <span class="n">MLPClassifier</span><span class="p">())])</span>
    <span class="n">est</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scoring</span> <span class="o">=</span> <span class="s1">&#39;roc_auc&#39;</span><span class="p">)</span>
    <span class="n">est</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
    <span class="n">best_est</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">best_estimator_</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">best_est</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MLP&quot;</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>            
        
                
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">result</span><span class="p">[</span><span class="s1">&#39;MLP&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">score</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span><span class="p">[</span><span class="s1">&#39;MLP&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;MLP&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">score</span>            
                
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">result</span><span class="p">:</span>
    <span class="n">result</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">/</span><span class="mi">4</span>
    
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Testing on Fold 1
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\Asus\Anaconda3\lib\site-packages\pandas\core\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version
of pandas will change to not sort by default.

To accept the future behavior, pass &#39;sort=False&#39;.

To retain the current behavior and silence the warning, pass &#39;sort=True&#39;.

  sort=sort,
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;dim_reducer__n_components&#39;: 70, &#39;f_selecter__k&#39;: 350}
Logistic 0.6797385620915033
{&#39;dim_reducer__n_components&#39;: 70, &#39;f_selecter__k&#39;: 350, &#39;rf__max_depth&#39;: 5, &#39;rf__n_estimators&#39;: 125}
RF 0.6773556644880174
{&#39;MLP__hidden_layer_sizes&#39;: (130, 110, 90, 70, 50, 30, 10, 5), &#39;MLP__learning_rate_init&#39;: 0.01, &#39;dim_reducer__n_components&#39;: 70, &#39;f_selecter__k&#39;: 400}
MLP 0.616489651416122
Testing on Fold 2
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\Asus\Anaconda3\lib\site-packages\pandas\core\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version
of pandas will change to not sort by default.

To accept the future behavior, pass &#39;sort=False&#39;.

To retain the current behavior and silence the warning, pass &#39;sort=True&#39;.

  sort=sort,
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;dim_reducer__n_components&#39;: 70, &#39;f_selecter__k&#39;: 400}
Logistic 0.6906166730110394
{&#39;dim_reducer__n_components&#39;: 70, &#39;f_selecter__k&#39;: 400, &#39;rf__max_depth&#39;: 5, &#39;rf__n_estimators&#39;: 150}
RF 0.6552309351605127
{&#39;MLP__hidden_layer_sizes&#39;: (130, 100, 100, 100, 100, 100), &#39;MLP__learning_rate_init&#39;: 0.01, &#39;dim_reducer__n_components&#39;: 70, &#39;f_selecter__k&#39;: 350}
MLP 0.6094880091358965
Testing on Fold 3
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\Asus\Anaconda3\lib\site-packages\pandas\core\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version
of pandas will change to not sort by default.

To accept the future behavior, pass &#39;sort=False&#39;.

To retain the current behavior and silence the warning, pass &#39;sort=True&#39;.

  sort=sort,
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;dim_reducer__n_components&#39;: 60, &#39;f_selecter__k&#39;: 350}
Logistic 0.7172510588003547
{&#39;dim_reducer__n_components&#39;: 70, &#39;f_selecter__k&#39;: 350, &#39;rf__max_depth&#39;: 4, &#39;rf__n_estimators&#39;: 150}
RF 0.6415098985521521
{&#39;MLP__hidden_layer_sizes&#39;: (130, 110, 90, 70, 50, 30, 10, 5), &#39;MLP__learning_rate_init&#39;: 0.01, &#39;dim_reducer__n_components&#39;: 60, &#39;f_selecter__k&#39;: 400}
MLP 0.592452148790177
Testing on Fold 4
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\Asus\Anaconda3\lib\site-packages\pandas\core\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version
of pandas will change to not sort by default.

To accept the future behavior, pass &#39;sort=False&#39;.

To retain the current behavior and silence the warning, pass &#39;sort=True&#39;.

  sort=sort,
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;dim_reducer__n_components&#39;: 60, &#39;f_selecter__k&#39;: 400}
Logistic 0.6945599197247706
{&#39;dim_reducer__n_components&#39;: 70, &#39;f_selecter__k&#39;: 400, &#39;rf__max_depth&#39;: 5, &#39;rf__n_estimators&#39;: 150}
RF 0.6663919151376148
{&#39;MLP__hidden_layer_sizes&#39;: (130, 110, 90, 70, 50, 30, 10, 5), &#39;MLP__learning_rate_init&#39;: 0.01, &#39;dim_reducer__n_components&#39;: 150, &#39;f_selecter__k&#39;: 400}
MLP 0.5927823967889908
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4>Result</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[24]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">result</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[24]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>{&#39;logistic&#39;: 0.695541553406917,
 &#39;random_forest&#39;: 0.6601221033345742,
 &#39;MLP&#39;: 0.6028030515327966}</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Logistic Regression yields better result than other methods. The data on this table is really sparsed before imputation and after the imputation the values might not represent the right condition of each patient. Logistic regression able to generalize the data more in this condition.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><h4>Regression Task</h4></p>
<p><p>Steps:</p></p>
<ol>
    <li><b>Imputer</b></li>
    <li><b>One Hot Encoder</b> <i>(only for categorical features)</i>
    <li><b>Standard Scaler</b></li>
    <li><b>Dimensionality Reducer with PCA</b></li>
    <li><b>Classifier</b>
        <br>We compare four different classifier models based on the ROC AUC score. The four classifiers are Logistic Regression, Random Forest, K Nearest Neighbors, and MLP.</br>
    </li>
</ol>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#For MODEL 3</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LogisticRegression</span><span class="p">,</span> <span class="n">Lasso</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">OneHotEncoder</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="k">import</span> <span class="n">sqrt</span>
<span class="n">num_feat</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">normal_feat</span> <span class="o">=</span> <span class="n">nor_feat</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">normal_feat</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s2">&quot;MechVent&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">normal_feat</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">hour</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">49</span><span class="p">):</span>
        <span class="n">num_feat</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">feat</span> <span class="o">+</span> <span class="s1">&#39;_&#39;</span> <span class="o">+</span>  <span class="nb">str</span><span class="p">(</span><span class="n">hour</span><span class="p">))</span>
<span class="n">cat_feat</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;ICUType&quot;</span><span class="p">]</span>

<span class="n">cat_transformer</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;imputer&#39;</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;most_frequent&#39;</span><span class="p">)),</span>
                                 <span class="p">(</span><span class="s1">&#39;OneHotEncoder&#39;</span><span class="p">,</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">handle_unknown</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">))])</span>

<span class="n">stat_feat_cp</span> <span class="o">=</span> <span class="n">stat_feat</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">stat_feat_cp</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s2">&quot;RecordID&quot;</span><span class="p">)</span>
<span class="n">stat_feat_cp</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s2">&quot;ICUType&quot;</span><span class="p">)</span>
<span class="n">prepro</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">(</span>
    <span class="n">remainder</span> <span class="o">=</span> <span class="s1">&#39;passthrough&#39;</span><span class="p">,</span>
    <span class="n">transformers</span><span class="o">=</span><span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="n">cat_transformer</span><span class="p">,</span> <span class="n">cat_feat</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;num&#39;</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">),</span> <span class="n">num_feat</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;stat&#39;</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">),</span> <span class="n">stat_feat_cp</span><span class="p">)])</span>

<span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;VarianceThreshold&#39;</span><span class="p">:[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="s1">&#39;PCA&#39;</span><span class="p">:[</span><span class="mi">50</span><span class="p">,</span><span class="mi">55</span><span class="p">,</span><span class="mi">60</span><span class="p">,</span><span class="mi">70</span><span class="p">],</span>
              <span class="s1">&#39;SelectKBest&#39;</span><span class="p">:[</span><span class="mi">350</span><span class="p">,</span><span class="mi">400</span><span class="p">],</span>
              <span class="s1">&#39;RFDepth&#39;</span><span class="p">:[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span>
              <span class="s1">&#39;RFEst&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span><span class="mi">125</span><span class="p">,</span><span class="mi">150</span><span class="p">],</span>
              <span class="s1">&#39;KNeighbours&#39;</span><span class="p">:[</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">],</span>
              <span class="s1">&#39;DT_min_samples_split&#39;</span><span class="p">:[</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">8</span><span class="p">],</span>
              <span class="s1">&#39;MLPClassifier_hiddenLayer&#39;</span><span class="p">:[(</span><span class="mi">45</span><span class="p">,</span><span class="mi">45</span><span class="p">,),</span> <span class="p">(</span><span class="mi">100</span><span class="p">,),</span> <span class="p">(</span><span class="mi">30</span><span class="p">,</span><span class="mi">30</span><span class="p">,</span><span class="mi">30</span><span class="p">)],</span>
              <span class="s1">&#39;DecisionTreeRegressor_min_samples_leaf&#39;</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span>
              <span class="s1">&#39;MLPRegressor_hiddenLayer&#39;</span><span class="p">:[(</span><span class="mi">130</span><span class="p">,</span> <span class="mi">110</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),(</span><span class="mi">130</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)]</span>
             <span class="p">}</span>
    
<span class="n">result</span> <span class="o">=</span> <span class="p">{}</span>    
<span class="n">all_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Testing on Fold&quot;</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">x_train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">y_train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    
    <span class="c1"># Getting train data set up</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">all_list</span> <span class="k">if</span> <span class="n">x</span> <span class="o">!=</span> <span class="n">i</span><span class="p">]:</span> 
        <span class="n">x_train_df</span> <span class="o">=</span> <span class="n">x_train_df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_fold</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
        <span class="n">y_train_df</span> <span class="o">=</span> <span class="n">y_train_df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_fold</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
    <span class="n">y_train_df</span> <span class="o">=</span> <span class="n">y_train_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;In-hospital_death&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># Replace not known length of stay to 2</span>
    <span class="n">y_train_df</span> <span class="o">=</span> <span class="n">y_train_df</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    
    <span class="n">train_df</span> <span class="o">=</span> <span class="n">x_train_df</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">y_train_df</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s2">&quot;RecordID&quot;</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s1">&#39;outer&#39;</span><span class="p">)</span>
    <span class="n">train_df</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">&quot;RecordID&quot;</span><span class="p">)</span>
    <span class="n">train_df</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">train_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">!=</span> <span class="s1">&#39;Length_of_stay&#39;</span><span class="p">]</span>
    <span class="n">Y_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;Length_of_stay&#39;</span><span class="p">]</span>
    
        
    <span class="n">x_test_df</span> <span class="o">=</span> <span class="n">x_train_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">y_test_df</span> <span class="o">=</span> <span class="n">y_train_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># Getting test data set up</span>
    <span class="n">x_test_df</span> <span class="o">=</span> <span class="n">x_test_df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_fold</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">y_test_df</span> <span class="o">=</span> <span class="n">y_test_df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_fold</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">y_test_df</span> <span class="o">=</span> <span class="n">y_test_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;In-hospital_death&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># Replace not known length of stay to 2</span>
    <span class="n">y_test_df</span> <span class="o">=</span> <span class="n">y_test_df</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    
    <span class="n">test_df</span> <span class="o">=</span> <span class="n">x_test_df</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">y_test_df</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s2">&quot;RecordID&quot;</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s1">&#39;outer&#39;</span><span class="p">)</span>
    <span class="n">test_df</span> <span class="o">=</span> <span class="n">test_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">&quot;RecordID&quot;</span><span class="p">)</span>
    <span class="n">test_df</span> <span class="o">=</span> <span class="n">test_df</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">test_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">!=</span> <span class="s1">&#39;Length_of_stay&#39;</span><span class="p">]</span>
    <span class="n">Y_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[</span><span class="s1">&#39;Length_of_stay&#39;</span><span class="p">]</span>
               
<span class="c1">#     Creating regression model and parameters to try out</span>
    <span class="c1">#Linear Regression</span>
    
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;f_selecter__threshold&#39;</span><span class="p">:[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> 
                  <span class="s1">&#39;dim_reducer__n_components&#39;</span><span class="p">:[</span><span class="mi">50</span><span class="p">,</span><span class="mi">55</span><span class="p">,</span><span class="mi">60</span><span class="p">,</span><span class="mi">70</span><span class="p">]</span>
             <span class="p">}</span>
    
    <span class="n">model</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[</span>
            <span class="p">(</span><span class="s1">&#39;prepro&#39;</span><span class="p">,</span> <span class="n">prepro</span><span class="p">),</span>
                <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
            <span class="p">(</span><span class="s1">&#39;f_selecter&#39;</span><span class="p">,</span> <span class="n">VarianceThreshold</span><span class="p">()),</span>
            <span class="p">(</span><span class="s1">&#39;dim_reducer&#39;</span><span class="p">,</span> <span class="n">PCA</span><span class="p">()),</span>
            <span class="p">(</span><span class="s1">&#39;classifier&#39;</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">())])</span>
            
    <span class="n">est</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scoring</span> <span class="o">=</span> <span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">)</span>
    <span class="n">est</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
    <span class="n">best_est</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">best_estimator_</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">best_est</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Linear&quot;</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>   
    
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">result</span><span class="p">[</span><span class="s1">&#39;linear&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">score</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span><span class="p">[</span><span class="s1">&#39;linear&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;linear&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">score</span>  
        
    
    <span class="c1">#Decision Tree</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;f_selecter__threshold&#39;</span><span class="p">:[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> 
                  <span class="s1">&#39;dim_reducer__n_components&#39;</span><span class="p">:[</span><span class="mi">50</span><span class="p">,</span><span class="mi">55</span><span class="p">,</span><span class="mi">60</span><span class="p">,</span><span class="mi">70</span><span class="p">],</span>
              <span class="s1">&#39;classifier__min_samples_split&#39;</span><span class="p">:[</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">8</span><span class="p">],</span>
              <span class="s1">&#39;classifier__min_samples_leaf&#39;</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span>
             <span class="p">}</span>
    
    <span class="n">model</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;prepro&#39;</span><span class="p">,</span> <span class="n">prepro</span><span class="p">),</span>
                          <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
                          <span class="p">(</span><span class="s1">&#39;f_selecter&#39;</span><span class="p">,</span> <span class="n">VarianceThreshold</span><span class="p">()),</span>
                          <span class="p">(</span><span class="s1">&#39;dim_reducer&#39;</span><span class="p">,</span> <span class="n">PCA</span><span class="p">()),</span>
                          <span class="p">(</span><span class="s1">&#39;classifier&#39;</span><span class="p">,</span> <span class="n">DecisionTreeRegressor</span><span class="p">())])</span>
    <span class="n">est</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scoring</span> <span class="o">=</span> <span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">)</span>
    <span class="n">est</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
    <span class="n">best_est</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">best_estimator_</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">best_est</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Decision Tree&quot;</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">result</span><span class="p">[</span><span class="s1">&#39;decision_tree&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">score</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span><span class="p">[</span><span class="s1">&#39;decision_tree&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;decision_tree&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">score</span>
    
    <span class="c1">#Lasso</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;f_selecter__threshold&#39;</span><span class="p">:[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> 
                  <span class="s1">&#39;dim_reducer__n_components&#39;</span><span class="p">:[</span><span class="mi">50</span><span class="p">,</span><span class="mi">55</span><span class="p">,</span><span class="mi">60</span><span class="p">,</span><span class="mi">70</span><span class="p">]</span>
             <span class="p">}</span>
    
    <span class="n">model</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;prepro&#39;</span><span class="p">,</span> <span class="n">prepro</span><span class="p">),</span>
                <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
                <span class="p">(</span><span class="s1">&#39;f_selecter&#39;</span><span class="p">,</span> <span class="n">VarianceThreshold</span><span class="p">()),</span>
                <span class="p">(</span><span class="s1">&#39;dim_reducer&#39;</span><span class="p">,</span> <span class="n">PCA</span><span class="p">()),</span>
                <span class="p">(</span><span class="s1">&#39;classifier&#39;</span><span class="p">,</span> <span class="n">Lasso</span><span class="p">())])</span>
        
    <span class="n">est</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scoring</span> <span class="o">=</span> <span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">)</span>
    <span class="n">est</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
    <span class="n">best_est</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">best_estimator_</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">best_est</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Lasso&quot;</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">result</span><span class="p">[</span><span class="s1">&#39;lasso&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">score</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span><span class="p">[</span><span class="s1">&#39;lasso&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;lasso&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">score</span>
    
    <span class="c1">#MLP</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span> <span class="s1">&#39;dim_reducer__n_components&#39;</span><span class="p">:[</span><span class="mi">50</span><span class="p">,</span><span class="mi">60</span><span class="p">,</span><span class="mi">70</span><span class="p">],</span>
              <span class="s1">&#39;f_selecter__k&#39;</span><span class="p">:[</span><span class="mi">350</span><span class="p">,</span><span class="mi">400</span><span class="p">],</span>
              <span class="s1">&#39;classifier__hidden_layer_sizes&#39;</span><span class="p">:[(</span><span class="mi">130</span><span class="p">,</span> <span class="mi">110</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),(</span><span class="mi">130</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)]</span>
             <span class="p">}</span>
    
    <span class="n">model</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[</span>
                <span class="p">(</span><span class="s1">&#39;prepro&#39;</span><span class="p">,</span> <span class="n">prepro</span><span class="p">),</span>
                <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
                <span class="p">(</span><span class="s1">&#39;f_selecter&#39;</span><span class="p">,</span> <span class="n">SelectKBest</span><span class="p">()),</span>
                <span class="p">(</span><span class="s1">&#39;dim_reducer&#39;</span><span class="p">,</span> <span class="n">PCA</span><span class="p">()),</span>
                <span class="p">(</span><span class="s1">&#39;classifier&#39;</span><span class="p">,</span> <span class="n">MLPRegressor</span><span class="p">(</span><span class="n">learning_rate_init</span> <span class="o">=</span> <span class="mf">0.105</span><span class="p">))])</span>
    <span class="n">est</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scoring</span> <span class="o">=</span> <span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">)</span>
    <span class="n">est</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
    <span class="n">best_est</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">best_estimator_</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">best_est</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MLP&quot;</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>    
    
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">result</span><span class="p">[</span><span class="s1">&#39;MLP&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">score</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span><span class="p">[</span><span class="s1">&#39;MLP&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;MLP&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">score</span>
    
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">result</span><span class="p">:</span>
    <span class="n">result</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">/</span><span class="mi">4</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Testing on Fold 1
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\Asus\Anaconda3\lib\site-packages\pandas\core\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version
of pandas will change to not sort by default.

To accept the future behavior, pass &#39;sort=False&#39;.

To retain the current behavior and silence the warning, pass &#39;sort=True&#39;.

  sort=sort,
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;dim_reducer__n_components&#39;: 60, &#39;f_selecter__threshold&#39;: 0}
Linear 13.79711067167092
{&#39;classifier__min_samples_leaf&#39;: 3, &#39;classifier__min_samples_split&#39;: 8, &#39;dim_reducer__n_components&#39;: 70, &#39;f_selecter__threshold&#39;: 0.25}
Decision Tree 16.953649951500253
{&#39;dim_reducer__n_components&#39;: 70, &#39;f_selecter__threshold&#39;: 0.5}
Lasso 13.816103812644338
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  % self.max_iter, ConvergenceWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  % self.max_iter, ConvergenceWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  % self.max_iter, ConvergenceWarning)
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;classifier__hidden_layer_sizes&#39;: (130, 110, 90, 70, 50, 30, 10, 5), &#39;dim_reducer__n_components&#39;: 50, &#39;f_selecter__k&#39;: 350}
MLP 14.13199767158594
Testing on Fold 2
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\Asus\Anaconda3\lib\site-packages\pandas\core\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version
of pandas will change to not sort by default.

To accept the future behavior, pass &#39;sort=False&#39;.

To retain the current behavior and silence the warning, pass &#39;sort=True&#39;.

  sort=sort,
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;dim_reducer__n_components&#39;: 70, &#39;f_selecter__threshold&#39;: 0}
Linear 11.41485528892256
{&#39;classifier__min_samples_leaf&#39;: 3, &#39;classifier__min_samples_split&#39;: 2, &#39;dim_reducer__n_components&#39;: 55, &#39;f_selecter__threshold&#39;: 0.25}
Decision Tree 15.62640993638654
{&#39;dim_reducer__n_components&#39;: 60, &#39;f_selecter__threshold&#39;: 0.25}
Lasso 11.416677627429614
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  % self.max_iter, ConvergenceWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  % self.max_iter, ConvergenceWarning)
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;classifier__hidden_layer_sizes&#39;: (130, 110, 90, 70, 50, 30, 10, 5), &#39;dim_reducer__n_components&#39;: 50, &#39;f_selecter__k&#39;: 350}
MLP 11.73174594932755
Testing on Fold 3
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\Asus\Anaconda3\lib\site-packages\pandas\core\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version
of pandas will change to not sort by default.

To accept the future behavior, pass &#39;sort=False&#39;.

To retain the current behavior and silence the warning, pass &#39;sort=True&#39;.

  sort=sort,
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;dim_reducer__n_components&#39;: 50, &#39;f_selecter__threshold&#39;: 0.5}
Linear 11.542117343810261
{&#39;classifier__min_samples_leaf&#39;: 3, &#39;classifier__min_samples_split&#39;: 8, &#39;dim_reducer__n_components&#39;: 60, &#39;f_selecter__threshold&#39;: 0.25}
Decision Tree 14.548822282677218
{&#39;dim_reducer__n_components&#39;: 50, &#39;f_selecter__threshold&#39;: 0}
Lasso 11.537678741383548
{&#39;classifier__hidden_layer_sizes&#39;: (130, 100, 100, 100, 100, 100), &#39;dim_reducer__n_components&#39;: 70, &#39;f_selecter__k&#39;: 400}
MLP 11.792209151253871
Testing on Fold 4
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\Asus\Anaconda3\lib\site-packages\pandas\core\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version
of pandas will change to not sort by default.

To accept the future behavior, pass &#39;sort=False&#39;.

To retain the current behavior and silence the warning, pass &#39;sort=True&#39;.

  sort=sort,
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;dim_reducer__n_components&#39;: 60, &#39;f_selecter__threshold&#39;: 0}
Linear 10.853264272470186
{&#39;classifier__min_samples_leaf&#39;: 3, &#39;classifier__min_samples_split&#39;: 8, &#39;dim_reducer__n_components&#39;: 60, &#39;f_selecter__threshold&#39;: 0}
Decision Tree 13.870632463096406
{&#39;dim_reducer__n_components&#39;: 60, &#39;f_selecter__threshold&#39;: 0.25}
Lasso 10.860807620318155
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  % self.max_iter, ConvergenceWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  % self.max_iter, ConvergenceWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  % self.max_iter, ConvergenceWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  % self.max_iter, ConvergenceWarning)
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;classifier__hidden_layer_sizes&#39;: (130, 100, 100, 100, 100, 100), &#39;dim_reducer__n_components&#39;: 60, &#39;f_selecter__k&#39;: 350}
MLP 10.877115240369134
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4>Result</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">result</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[12]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>{&#39;linear&#39;: 11.901836894218482,
 &#39;decision_tree&#39;: 15.249878658415104,
 &#39;lasso&#39;: 11.907816950443912,
 &#39;MLP&#39;: 12.133267003134124}</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Linear Regression will be used as M1 for this design matrix as it yields the lowest result.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><h2>Model Building for M2</h2></p>
<p>We tried out various models and the ones listed below is the best we found empirically.</p><p><b>Set up</b></p>
<p>IMPT: Please ensure that you have 2 files called "best_model_1.h5" and "best_model_2.h5" in the same folder as this file</p>
<p>IMPT: Please ensure you have Tensorflow installed.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow</span> <span class="k">import</span> <span class="n">keras</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="k">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.callbacks</span> <span class="k">import</span> <span class="n">EarlyStopping</span><span class="p">,</span> <span class="n">ModelCheckpoint</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="k">import</span> <span class="n">load_model</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[52]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow</span> <span class="k">import</span> <span class="n">keras</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="k">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.callbacks</span> <span class="k">import</span> <span class="n">EarlyStopping</span><span class="p">,</span> <span class="n">ModelCheckpoint</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="k">import</span> <span class="n">load_model</span>

<span class="c1"># preprocess y dataframe for use in linear regression</span>
<span class="k">def</span> <span class="nf">preprocess_y_linear</span><span class="p">(</span><span class="n">temp_df</span><span class="p">):</span>
    <span class="n">temp_df</span> <span class="o">=</span> <span class="n">temp_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;recordid&#39;</span><span class="p">,</span><span class="s1">&#39;mortality&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">temp_df</span><span class="p">[</span><span class="s1">&#39;days_in_hospital&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">temp_df</span><span class="p">[</span><span class="s1">&#39;days_in_hospital&#39;</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">temp_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;days_in_hospital&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="n">row</span><span class="p">[</span><span class="s1">&#39;days_in_hospital&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>   
    <span class="k">return</span> <span class="n">temp_df</span>

<span class="c1"># preprocess y dataframe for use in classification</span>
<span class="k">def</span> <span class="nf">preprocess_y_class</span><span class="p">(</span><span class="n">temp_df</span><span class="p">):</span>
    <span class="n">temp_df</span> <span class="o">=</span> <span class="n">temp_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;recordid&#39;</span><span class="p">,</span><span class="s1">&#39;days_in_hospital&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">temp_df</span><span class="p">[</span><span class="s1">&#39;mortality&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">temp_df</span><span class="p">[</span><span class="s1">&#39;mortality&#39;</span><span class="p">])</span>  
    <span class="k">return</span> <span class="n">temp_df</span>

<span class="n">bin_feat</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Gender&quot;</span><span class="p">,</span> <span class="s2">&quot;Coronary Care Unit&quot;</span><span class="p">,</span> <span class="s2">&quot;Cardiac Surgery Recovery Unit&quot;</span><span class="p">,</span> <span class="s2">&quot;Medical ICU&quot;</span><span class="p">,</span> <span class="s2">&quot;Surgical ICU&quot;</span><span class="p">]</span>
<span class="n">non_bin_feat</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">(</span>
    <span class="n">remainder</span> <span class="o">=</span> <span class="s1">&#39;passthrough&#39;</span><span class="p">,</span>
    <span class="n">transformers</span><span class="o">=</span><span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;num&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">non_bin_feat</span><span class="p">)])</span>
<span class="n">all_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>


<span class="c1"># Creating an array containing the preprocessed folds</span>
<span class="n">folds_x</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">folds_y_linear</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">folds_y_class</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">all_list</span><span class="p">:</span>
    <span class="n">x_string</span> <span class="o">=</span> <span class="s2">&quot;../Project_Data/Fold&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="n">y_string</span> <span class="o">=</span> <span class="s2">&quot;../Project_Data/Fold&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">+</span><span class="s2">&quot;_Outcomes.csv&quot;</span>
    <span class="n">temp_df_y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">read_ans</span><span class="p">(</span><span class="n">y_string</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;recordid&#39;</span><span class="p">,</span> <span class="s1">&#39;days_in_hospital&#39;</span><span class="p">,</span> <span class="s1">&#39;mortality&#39;</span><span class="p">])</span>
    <span class="n">temp_df_x</span> <span class="o">=</span> <span class="n">put_single_into_dataframe</span><span class="p">(</span><span class="n">read_text</span><span class="p">(</span><span class="n">x_string</span><span class="p">))</span>
    <span class="n">folds_x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">preprocess_x_for_design_matrix_1</span><span class="p">(</span><span class="n">temp_df_x</span><span class="p">)</span>
    <span class="n">folds_y_linear</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">preprocess_y_linear</span><span class="p">(</span><span class="n">temp_df_y</span><span class="p">)</span>
    <span class="n">folds_y_class</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">preprocess_y_class</span><span class="p">(</span><span class="n">temp_df_y</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3>Layer Selection</h3><p><b>Regressor Task</b>   
For the regressor model, we decided to go with a fairly deep network as we have the benefit of working with the ReLU activation function for all layers, mitigating the Vanishing/Exploding gradidents problem. We have added dropout layers, as this a fairly complex netowrk. The dropout layers essentially block off certain neurons during the training process i.e. during the prediction and when backprop flows through the network, the weights of those neuron "dropped" are not used / affected. We have also included Early Stopping and Model Checkpointing as meta algorithms to prevent overfitting of the network.  </p><p>If you do not have / want to have the h5 files, please comment out lines 42 . And in line 41 remove "mc1" </p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[53]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Create a matrix to store the results where row represnets the model and column represents the result tested on fold i</span>
<span class="n">w</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span>
<span class="n">matrix</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">w</span><span class="p">)]</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">h</span><span class="p">)]</span> 

<span class="c1"># Running CV with Tensorflow</span>
<span class="n">es</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">mc1</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="s1">&#39;best_model_1.h5&#39;</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Testing on Fold&quot;</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">x_train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">y_train_linear_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">y_train_class_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">x_test_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">y_test_linear_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">y_test_class_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

    <span class="c1"># Getting train data set up</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;# Getting train data set up&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">all_list</span> <span class="k">if</span> <span class="n">x</span> <span class="o">!=</span> <span class="n">i</span><span class="p">]:</span>
        <span class="n">x_train_df</span> <span class="o">=</span> <span class="n">x_train_df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">folds_x</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
        <span class="n">y_train_linear_df</span> <span class="o">=</span> <span class="n">y_train_linear_df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">folds_y_linear</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
        <span class="n">y_train_class_df</span> <span class="o">=</span> <span class="n">y_train_class_df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">folds_y_class</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>

    <span class="c1"># Getting test data set up </span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;# Getting test data set up&quot;</span><span class="p">)</span>
    <span class="n">x_test_df</span> <span class="o">=</span> <span class="n">folds_x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">y_test_linear_df</span><span class="o">=</span> <span class="n">folds_y_linear</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">y_test_class_df</span> <span class="o">=</span> <span class="n">folds_y_class</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    
    <span class="c1"># Regressor Model</span>
    <span class="n">model_linear</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">121</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,)))</span>
    <span class="n">model_linear</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="mf">0.2</span><span class="p">))</span>
    <span class="n">model_linear</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">146</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">model_linear</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="mf">0.3</span><span class="p">))</span>
    <span class="n">model_linear</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">196</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">model_linear</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="mf">0.3</span><span class="p">))</span>
    <span class="n">model_linear</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">model_linear</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="mf">0.3</span><span class="p">))</span>
    <span class="n">model_linear</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">312</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">model_linear</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="mf">0.3</span><span class="p">))</span>
    <span class="n">model_linear</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">model_linear</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="mf">0.3</span><span class="p">))</span>
    <span class="n">model_linear</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">196</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">model_linear</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="mf">0.3</span><span class="p">))</span>
    <span class="n">model_linear</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">146</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">model_linear</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="mf">0.3</span><span class="p">))</span>
    <span class="n">model_linear</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">96</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">model_linear</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="mf">0.2</span><span class="p">))</span>
    <span class="n">model_linear</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">46</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">model_linear</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="mf">0.2</span><span class="p">))</span>
    <span class="n">model_linear</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">model_linear</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;mse&#39;</span><span class="p">])</span>
    <span class="n">model_linear</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train_df</span><span class="p">,</span> <span class="n">y_train_linear_df</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">validation_split</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">es</span><span class="p">,</span><span class="n">mc1</span><span class="p">])</span>
    <span class="n">model_linear</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s1">&#39;best_model_1.h5&#39;</span><span class="p">)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">model_linear</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test_df</span><span class="p">)</span>
    <span class="n">matrix</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test_linear_df</span><span class="p">,</span> <span class="n">predictions</span><span class="p">))</span>
    
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Testing on Fold 1
# Getting train data set up
# Getting test data set up
WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;, &lt;class &#39;NoneType&#39;&gt;
Train on 2400 samples, validate on 600 samples
Epoch 1/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 164.8710 - mse: 164.8710
Epoch 00001: val_loss improved from inf to 141.52421, saving model to best_model_1.h5
2400/2400 [==============================] - 2s 644us/sample - loss: 162.3914 - mse: 162.3914 - val_loss: 141.5242 - val_mse: 141.5242
Epoch 2/300
2176/2400 [==========================&gt;...] - ETA: 0s - loss: 143.8299 - mse: 143.8299
Epoch 00002: val_loss did not improve from 141.52421
2400/2400 [==============================] - 1s 266us/sample - loss: 143.6615 - mse: 143.6615 - val_loss: 171.6009 - val_mse: 171.6009
Epoch 3/300
2208/2400 [==========================&gt;...] - ETA: 0s - loss: 139.0507 - mse: 139.0508
Epoch 00003: val_loss improved from 141.52421 to 132.23532, saving model to best_model_1.h5
2400/2400 [==============================] - 1s 298us/sample - loss: 142.2663 - mse: 142.2663 - val_loss: 132.2353 - val_mse: 132.2353
Epoch 4/300
2176/2400 [==========================&gt;...] - ETA: 0s - loss: 142.0051 - mse: 142.0051
Epoch 00004: val_loss improved from 132.23532 to 118.33066, saving model to best_model_1.h5
2400/2400 [==============================] - 1s 300us/sample - loss: 140.7435 - mse: 140.7435 - val_loss: 118.3307 - val_mse: 118.3307
Epoch 5/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 133.5620 - mse: 133.5620
Epoch 00005: val_loss did not improve from 118.33066
2400/2400 [==============================] - 1s 271us/sample - loss: 132.4610 - mse: 132.4610 - val_loss: 122.2728 - val_mse: 122.2728
Epoch 6/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 130.6330 - mse: 130.6330
Epoch 00006: val_loss did not improve from 118.33066
2400/2400 [==============================] - 1s 280us/sample - loss: 132.3879 - mse: 132.3879 - val_loss: 119.4514 - val_mse: 119.4514
Epoch 7/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 131.8630 - mse: 131.8629
Epoch 00007: val_loss did not improve from 118.33066
2400/2400 [==============================] - 1s 273us/sample - loss: 133.9364 - mse: 133.9364 - val_loss: 127.1325 - val_mse: 127.1325
Epoch 8/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 132.4788 - mse: 132.4788
Epoch 00008: val_loss did not improve from 118.33066
2400/2400 [==============================] - 1s 273us/sample - loss: 132.2768 - mse: 132.2768 - val_loss: 119.5878 - val_mse: 119.5878
Epoch 9/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 148.2541 - mse: 148.2541
Epoch 00009: val_loss did not improve from 118.33066
2400/2400 [==============================] - 1s 265us/sample - loss: 145.6373 - mse: 145.6373 - val_loss: 121.9616 - val_mse: 121.9616
Epoch 10/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 139.5219 - mse: 139.5219
Epoch 00010: val_loss did not improve from 118.33066
2400/2400 [==============================] - 1s 297us/sample - loss: 140.5865 - mse: 140.5865 - val_loss: 121.9785 - val_mse: 121.9785
Epoch 11/300
2208/2400 [==========================&gt;...] - ETA: 0s - loss: 132.8802 - mse: 132.8802
Epoch 00011: val_loss did not improve from 118.33066
2400/2400 [==============================] - 1s 291us/sample - loss: 134.6081 - mse: 134.6081 - val_loss: 122.4022 - val_mse: 122.4022
Epoch 12/300
2208/2400 [==========================&gt;...] - ETA: 0s - loss: 127.8727 - mse: 127.8727
Epoch 00012: val_loss improved from 118.33066 to 117.94716, saving model to best_model_1.h5
2400/2400 [==============================] - 1s 343us/sample - loss: 137.0318 - mse: 137.0318 - val_loss: 117.9472 - val_mse: 117.9472
Epoch 13/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 133.7355 - mse: 133.7355
Epoch 00013: val_loss improved from 117.94716 to 114.84641, saving model to best_model_1.h5
2400/2400 [==============================] - 1s 323us/sample - loss: 132.9344 - mse: 132.9344 - val_loss: 114.8464 - val_mse: 114.8464
Epoch 14/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 130.3861 - mse: 130.3861
Epoch 00014: val_loss did not improve from 114.84641
2400/2400 [==============================] - 1s 274us/sample - loss: 129.4436 - mse: 129.4436 - val_loss: 126.0768 - val_mse: 126.0768
Epoch 15/300
2176/2400 [==========================&gt;...] - ETA: 0s - loss: 124.7781 - mse: 124.7781
Epoch 00015: val_loss did not improve from 114.84641
2400/2400 [==============================] - 1s 263us/sample - loss: 125.4342 - mse: 125.4342 - val_loss: 120.7424 - val_mse: 120.7424
Epoch 16/300
2176/2400 [==========================&gt;...] - ETA: 0s - loss: 128.0135 - mse: 128.0135
Epoch 00016: val_loss did not improve from 114.84641
2400/2400 [==============================] - 1s 270us/sample - loss: 129.8740 - mse: 129.8740 - val_loss: 116.0668 - val_mse: 116.0668
Epoch 17/300
2176/2400 [==========================&gt;...] - ETA: 0s - loss: 122.4569 - mse: 122.4569
Epoch 00017: val_loss did not improve from 114.84641
2400/2400 [==============================] - 1s 264us/sample - loss: 126.7798 - mse: 126.7798 - val_loss: 122.8133 - val_mse: 122.8133
Epoch 18/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 131.8050 - mse: 131.8051
Epoch 00018: val_loss did not improve from 114.84641
2400/2400 [==============================] - 1s 264us/sample - loss: 129.5982 - mse: 129.5982 - val_loss: 118.8595 - val_mse: 118.8595
Epoch 19/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 130.3034 - mse: 130.3034
Epoch 00019: val_loss did not improve from 114.84641
2400/2400 [==============================] - 1s 276us/sample - loss: 129.8817 - mse: 129.8817 - val_loss: 118.2230 - val_mse: 118.2230
Epoch 20/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 128.7275 - mse: 128.7275
Epoch 00020: val_loss did not improve from 114.84641
2400/2400 [==============================] - 1s 273us/sample - loss: 126.8990 - mse: 126.8990 - val_loss: 120.1250 - val_mse: 120.1251
Epoch 21/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 128.5322 - mse: 128.5322
Epoch 00021: val_loss did not improve from 114.84641
2400/2400 [==============================] - 1s 278us/sample - loss: 128.6115 - mse: 128.6116 - val_loss: 124.3562 - val_mse: 124.3562
Epoch 22/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 128.3039 - mse: 128.3039
Epoch 00022: val_loss did not improve from 114.84641
2400/2400 [==============================] - 1s 353us/sample - loss: 127.8874 - mse: 127.8874 - val_loss: 117.8866 - val_mse: 117.8866
Epoch 23/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 124.9559 - mse: 124.9560- ETA: 0s - loss: 158.6196 - mse:
Epoch 00023: val_loss did not improve from 114.84641
2400/2400 [==============================] - 1s 299us/sample - loss: 125.2068 - mse: 125.2068 - val_loss: 123.7982 - val_mse: 123.7983
Epoch 24/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 120.1965 - mse: 120.1965
Epoch 00024: val_loss did not improve from 114.84641
2400/2400 [==============================] - 1s 231us/sample - loss: 125.1374 - mse: 125.1374 - val_loss: 119.5456 - val_mse: 119.5456
Epoch 25/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 133.6687 - mse: 133.6687
Epoch 00025: val_loss did not improve from 114.84641
2400/2400 [==============================] - 1s 230us/sample - loss: 130.4130 - mse: 130.4130 - val_loss: 118.1579 - val_mse: 118.1579
Epoch 26/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 118.7687 - mse: 118.7687
Epoch 00026: val_loss did not improve from 114.84641
2400/2400 [==============================] - 1s 235us/sample - loss: 124.1631 - mse: 124.1631 - val_loss: 123.8415 - val_mse: 123.8415
Epoch 27/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 121.5146 - mse: 121.5146
Epoch 00027: val_loss did not improve from 114.84641
2400/2400 [==============================] - 1s 251us/sample - loss: 122.5971 - mse: 122.5971 - val_loss: 120.5564 - val_mse: 120.5564
Epoch 28/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 130.5110 - mse: 130.5110
Epoch 00028: val_loss did not improve from 114.84641
2400/2400 [==============================] - 1s 253us/sample - loss: 130.3652 - mse: 130.3652 - val_loss: 120.8951 - val_mse: 120.8951
Epoch 29/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 123.2409 - mse: 123.2409
Epoch 00029: val_loss did not improve from 114.84641
2400/2400 [==============================] - 1s 255us/sample - loss: 124.4319 - mse: 124.4319 - val_loss: 120.9092 - val_mse: 120.9092
Epoch 30/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 129.0192 - mse: 129.0192
Epoch 00030: val_loss did not improve from 114.84641
2400/2400 [==============================] - 1s 256us/sample - loss: 126.1479 - mse: 126.1480 - val_loss: 115.4599 - val_mse: 115.4599
Epoch 31/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 119.1918 - mse: 119.1918
Epoch 00031: val_loss did not improve from 114.84641
2400/2400 [==============================] - 1s 255us/sample - loss: 123.3002 - mse: 123.3002 - val_loss: 122.1458 - val_mse: 122.1458
Epoch 32/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 128.3961 - mse: 128.3961
Epoch 00032: val_loss did not improve from 114.84641
2400/2400 [==============================] - 1s 255us/sample - loss: 128.7600 - mse: 128.7600 - val_loss: 122.0434 - val_mse: 122.0434
Epoch 33/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 128.3043 - mse: 128.3043
Epoch 00033: val_loss did not improve from 114.84641
2400/2400 [==============================] - 1s 256us/sample - loss: 126.6241 - mse: 126.6241 - val_loss: 124.5964 - val_mse: 124.5964
Epoch 34/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 166.7074 - mse: 166.7074
Epoch 00034: val_loss did not improve from 114.84641
2400/2400 [==============================] - 1s 279us/sample - loss: 165.6710 - mse: 165.6710 - val_loss: 121.8104 - val_mse: 121.8104
Epoch 35/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 138.2363 - mse: 138.2363
Epoch 00035: val_loss did not improve from 114.84641
2400/2400 [==============================] - 1s 295us/sample - loss: 136.7408 - mse: 136.7408 - val_loss: 119.4955 - val_mse: 119.4955
Epoch 36/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 130.7666 - mse: 130.7666
Epoch 00036: val_loss did not improve from 114.84641
2400/2400 [==============================] - 1s 286us/sample - loss: 131.4742 - mse: 131.4742 - val_loss: 118.8574 - val_mse: 118.8574
Epoch 37/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 127.3315 - mse: 127.3315
Epoch 00037: val_loss improved from 114.84641 to 113.45613, saving model to best_model_1.h5
2400/2400 [==============================] - 1s 315us/sample - loss: 127.0131 - mse: 127.0131 - val_loss: 113.4561 - val_mse: 113.4561
Epoch 38/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 127.9626 - mse: 127.9626
Epoch 00038: val_loss did not improve from 113.45613
2400/2400 [==============================] - 1s 265us/sample - loss: 126.6597 - mse: 126.6597 - val_loss: 117.7337 - val_mse: 117.7337
Epoch 39/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 126.1965 - mse: 126.1965
Epoch 00039: val_loss did not improve from 113.45613
2400/2400 [==============================] - 1s 283us/sample - loss: 124.4978 - mse: 124.4978 - val_loss: 115.7045 - val_mse: 115.7045
Epoch 40/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 126.6967 - mse: 126.6967
Epoch 00040: val_loss did not improve from 113.45613
2400/2400 [==============================] - 1s 263us/sample - loss: 124.8832 - mse: 124.8832 - val_loss: 116.5119 - val_mse: 116.5119
Epoch 41/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 124.0411 - mse: 124.0411
Epoch 00041: val_loss improved from 113.45613 to 112.79528, saving model to best_model_1.h5
2400/2400 [==============================] - 1s 295us/sample - loss: 123.6786 - mse: 123.6786 - val_loss: 112.7953 - val_mse: 112.7953
Epoch 42/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 120.7204 - mse: 120.7204
Epoch 00042: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 262us/sample - loss: 120.2536 - mse: 120.2536 - val_loss: 123.1326 - val_mse: 123.1327
Epoch 43/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 121.0830 - mse: 121.0830
Epoch 00043: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 264us/sample - loss: 123.8378 - mse: 123.8378 - val_loss: 122.6851 - val_mse: 122.6851
Epoch 44/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 125.3792 - mse: 125.3791
Epoch 00044: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 261us/sample - loss: 127.6727 - mse: 127.6726 - val_loss: 115.3400 - val_mse: 115.3400
Epoch 45/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 124.5467 - mse: 124.5468
Epoch 00045: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 260us/sample - loss: 122.6618 - mse: 122.6618 - val_loss: 122.0379 - val_mse: 122.0379
Epoch 46/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 122.5430 - mse: 122.5430
Epoch 00046: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 260us/sample - loss: 123.7210 - mse: 123.7210 - val_loss: 119.0095 - val_mse: 119.0095
Epoch 47/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 127.9625 - mse: 127.9625
Epoch 00047: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 255us/sample - loss: 125.2684 - mse: 125.2684 - val_loss: 119.3714 - val_mse: 119.3714
Epoch 48/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 122.8069 - mse: 122.8069
Epoch 00048: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 254us/sample - loss: 120.3547 - mse: 120.3547 - val_loss: 126.6672 - val_mse: 126.6672
Epoch 49/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 115.9164 - mse: 115.9164
Epoch 00049: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 254us/sample - loss: 118.2329 - mse: 118.2328 - val_loss: 114.2809 - val_mse: 114.2809
Epoch 50/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 122.8151 - mse: 122.8151
Epoch 00050: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 256us/sample - loss: 121.4919 - mse: 121.4919 - val_loss: 125.5626 - val_mse: 125.5626
Epoch 51/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 123.8852 - mse: 123.8852
Epoch 00051: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 257us/sample - loss: 124.7160 - mse: 124.7160 - val_loss: 114.5009 - val_mse: 114.5009
Epoch 52/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 125.7884 - mse: 125.7885
Epoch 00052: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 265us/sample - loss: 126.9659 - mse: 126.9659 - val_loss: 116.4168 - val_mse: 116.4167
Epoch 53/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 122.3039 - mse: 122.3039
Epoch 00053: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 253us/sample - loss: 122.7692 - mse: 122.7692 - val_loss: 117.9092 - val_mse: 117.9092
Epoch 54/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 123.9693 - mse: 123.9693
Epoch 00054: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 259us/sample - loss: 121.7996 - mse: 121.7996 - val_loss: 119.5678 - val_mse: 119.5678
Epoch 55/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 129.5857 - mse: 129.5857
Epoch 00055: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 255us/sample - loss: 127.9660 - mse: 127.9660 - val_loss: 122.4610 - val_mse: 122.4610
Epoch 56/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 119.8815 - mse: 119.8815
Epoch 00056: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 255us/sample - loss: 121.0262 - mse: 121.0262 - val_loss: 121.1369 - val_mse: 121.1368
Epoch 57/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 121.5164 - mse: 121.5164
Epoch 00057: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 254us/sample - loss: 125.1014 - mse: 125.1014 - val_loss: 119.5230 - val_mse: 119.5230
Epoch 58/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 126.1010 - mse: 126.1010
Epoch 00058: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 257us/sample - loss: 123.9144 - mse: 123.9144 - val_loss: 118.6722 - val_mse: 118.6722
Epoch 59/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 120.8975 - mse: 120.8975
Epoch 00059: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 288us/sample - loss: 121.9327 - mse: 121.9327 - val_loss: 118.8985 - val_mse: 118.8985
Epoch 60/300
2208/2400 [==========================&gt;...] - ETA: 0s - loss: 118.4560 - mse: 118.4560
Epoch 00060: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 293us/sample - loss: 115.5782 - mse: 115.5782 - val_loss: 126.5413 - val_mse: 126.5413
Epoch 61/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 120.2253 - mse: 120.2253
Epoch 00061: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 288us/sample - loss: 118.3552 - mse: 118.3552 - val_loss: 116.1227 - val_mse: 116.1227
Epoch 62/300
2208/2400 [==========================&gt;...] - ETA: 0s - loss: 113.5129 - mse: 113.5129
Epoch 00062: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 267us/sample - loss: 116.5973 - mse: 116.5973 - val_loss: 134.6209 - val_mse: 134.6208
Epoch 63/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 113.4605 - mse: 113.4605
Epoch 00063: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 254us/sample - loss: 115.6827 - mse: 115.6827 - val_loss: 130.7501 - val_mse: 130.7501
Epoch 64/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 121.2945 - mse: 121.2945
Epoch 00064: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 253us/sample - loss: 120.7546 - mse: 120.7546 - val_loss: 115.0489 - val_mse: 115.0489
Epoch 65/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 122.5218 - mse: 122.5218
Epoch 00065: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 254us/sample - loss: 120.5571 - mse: 120.5571 - val_loss: 117.2192 - val_mse: 117.2192
Epoch 66/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 117.5954 - mse: 117.5954
Epoch 00066: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 257us/sample - loss: 120.9928 - mse: 120.9928 - val_loss: 116.6677 - val_mse: 116.6677
Epoch 67/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 122.8924 - mse: 122.8924
Epoch 00067: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 255us/sample - loss: 123.0217 - mse: 123.0218 - val_loss: 114.9485 - val_mse: 114.9485
Epoch 68/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 116.6488 - mse: 116.6488
Epoch 00068: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 256us/sample - loss: 116.8505 - mse: 116.8505 - val_loss: 116.6214 - val_mse: 116.6214
Epoch 69/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 119.7858 - mse: 119.7858
Epoch 00069: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 255us/sample - loss: 117.7845 - mse: 117.7845 - val_loss: 118.0287 - val_mse: 118.0287
Epoch 70/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 113.4125 - mse: 113.4125
Epoch 00070: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 282us/sample - loss: 114.0462 - mse: 114.0462 - val_loss: 121.8457 - val_mse: 121.8457
Epoch 71/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 120.2681 - mse: 120.2681
Epoch 00071: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 263us/sample - loss: 118.0929 - mse: 118.0929 - val_loss: 118.1506 - val_mse: 118.1506
Epoch 72/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 120.1612 - mse: 120.1612
Epoch 00072: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 259us/sample - loss: 119.0806 - mse: 119.0806 - val_loss: 121.0191 - val_mse: 121.0191
Epoch 73/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 116.2180 - mse: 116.2180
Epoch 00073: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 266us/sample - loss: 114.0384 - mse: 114.0385 - val_loss: 119.3464 - val_mse: 119.3464
Epoch 74/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 115.7181 - mse: 115.7181
Epoch 00074: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 254us/sample - loss: 114.5286 - mse: 114.5286 - val_loss: 136.7914 - val_mse: 136.7914
Epoch 75/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 121.1766 - mse: 121.1766
Epoch 00075: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 253us/sample - loss: 121.0926 - mse: 121.0926 - val_loss: 115.2806 - val_mse: 115.2805
Epoch 76/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 116.5562 - mse: 116.5562
Epoch 00076: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 253us/sample - loss: 116.1293 - mse: 116.1293 - val_loss: 116.6159 - val_mse: 116.6159
Epoch 77/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 118.8918 - mse: 118.8918
Epoch 00077: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 263us/sample - loss: 117.7084 - mse: 117.7084 - val_loss: 120.2344 - val_mse: 120.2344
Epoch 78/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 114.1048 - mse: 114.1047
Epoch 00078: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 256us/sample - loss: 117.6752 - mse: 117.6752 - val_loss: 117.8624 - val_mse: 117.8624
Epoch 79/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 120.5463 - mse: 120.5463
Epoch 00079: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 258us/sample - loss: 119.8526 - mse: 119.8525 - val_loss: 121.5190 - val_mse: 121.5190
Epoch 80/300
2176/2400 [==========================&gt;...] - ETA: 0s - loss: 128.0487 - mse: 128.0488
Epoch 00080: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 266us/sample - loss: 127.4292 - mse: 127.4292 - val_loss: 114.8983 - val_mse: 114.8983
Epoch 81/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 120.3131 - mse: 120.3131
Epoch 00081: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 255us/sample - loss: 124.3989 - mse: 124.3988 - val_loss: 117.8965 - val_mse: 117.8965
Epoch 82/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 115.5470 - mse: 115.5470
Epoch 00082: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 258us/sample - loss: 118.2492 - mse: 118.2492 - val_loss: 123.0251 - val_mse: 123.0252
Epoch 83/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 123.6242 - mse: 123.6242
Epoch 00083: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 253us/sample - loss: 125.8704 - mse: 125.8704 - val_loss: 121.0274 - val_mse: 121.0274
Epoch 84/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 122.9302 - mse: 122.9302
Epoch 00084: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 270us/sample - loss: 122.2455 - mse: 122.2455 - val_loss: 123.3198 - val_mse: 123.3198
Epoch 85/300
2208/2400 [==========================&gt;...] - ETA: 0s - loss: 116.3975 - mse: 116.3975
Epoch 00085: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 296us/sample - loss: 115.9414 - mse: 115.9414 - val_loss: 113.7267 - val_mse: 113.7267
Epoch 86/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 114.4688 - mse: 114.4688
Epoch 00086: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 287us/sample - loss: 114.6204 - mse: 114.6204 - val_loss: 121.6378 - val_mse: 121.6378
Epoch 87/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 121.2106 - mse: 121.2106
Epoch 00087: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 282us/sample - loss: 119.4120 - mse: 119.4120 - val_loss: 130.4273 - val_mse: 130.4273
Epoch 88/300
2208/2400 [==========================&gt;...] - ETA: 0s - loss: 113.6389 - mse: 113.6390
Epoch 00088: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 268us/sample - loss: 113.1784 - mse: 113.1784 - val_loss: 120.9204 - val_mse: 120.9204
Epoch 89/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 119.9265 - mse: 119.9265
Epoch 00089: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 261us/sample - loss: 117.2780 - mse: 117.2780 - val_loss: 117.9797 - val_mse: 117.9797
Epoch 90/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 112.6109 - mse: 112.6109
Epoch 00090: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 261us/sample - loss: 114.7137 - mse: 114.7137 - val_loss: 141.2917 - val_mse: 141.2917
Epoch 91/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 118.0495 - mse: 118.0495
Epoch 00091: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 257us/sample - loss: 116.7717 - mse: 116.7717 - val_loss: 118.6819 - val_mse: 118.6819
Epoch 92/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 110.5201 - mse: 110.5201
Epoch 00092: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 257us/sample - loss: 110.6833 - mse: 110.6833 - val_loss: 154.6758 - val_mse: 154.6758
Epoch 93/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 122.3064 - mse: 122.3064
Epoch 00093: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 257us/sample - loss: 121.5099 - mse: 121.5099 - val_loss: 134.5619 - val_mse: 134.5619
Epoch 94/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 114.8619 - mse: 114.8619
Epoch 00094: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 261us/sample - loss: 115.1866 - mse: 115.1867 - val_loss: 122.2507 - val_mse: 122.2507
Epoch 95/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 111.9695 - mse: 111.9695
Epoch 00095: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 258us/sample - loss: 113.1175 - mse: 113.1175 - val_loss: 117.3280 - val_mse: 117.3280
Epoch 96/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 115.4691 - mse: 115.4691
Epoch 00096: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 258us/sample - loss: 114.2552 - mse: 114.2552 - val_loss: 127.1972 - val_mse: 127.1972
Epoch 97/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 120.1135 - mse: 120.1135
Epoch 00097: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 256us/sample - loss: 118.5987 - mse: 118.5987 - val_loss: 120.7310 - val_mse: 120.7310
Epoch 98/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 123.9872 - mse: 123.9872
Epoch 00098: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 256us/sample - loss: 122.7277 - mse: 122.7277 - val_loss: 117.9232 - val_mse: 117.9232
Epoch 99/300
2208/2400 [==========================&gt;...] - ETA: 0s - loss: 116.2656 - mse: 116.2655
Epoch 00099: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 265us/sample - loss: 118.8402 - mse: 118.8402 - val_loss: 115.1516 - val_mse: 115.1516
Epoch 100/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 112.8224 - mse: 112.8224
Epoch 00100: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 257us/sample - loss: 117.4148 - mse: 117.4148 - val_loss: 122.1039 - val_mse: 122.1039
Epoch 101/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 117.1969 - mse: 117.1970
Epoch 00101: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 260us/sample - loss: 116.8373 - mse: 116.8373 - val_loss: 139.0960 - val_mse: 139.0960
Epoch 102/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 113.0144 - mse: 113.0144
Epoch 00102: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 256us/sample - loss: 113.4211 - mse: 113.4211 - val_loss: 132.5585 - val_mse: 132.5585
Epoch 103/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 109.8102 - mse: 109.8102
Epoch 00103: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 256us/sample - loss: 110.5784 - mse: 110.5784 - val_loss: 129.8334 - val_mse: 129.8334
Epoch 104/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 119.0166 - mse: 119.0165
Epoch 00104: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 259us/sample - loss: 118.9971 - mse: 118.9971 - val_loss: 116.5273 - val_mse: 116.5273
Epoch 105/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 116.2415 - mse: 116.2415
Epoch 00105: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 259us/sample - loss: 113.5553 - mse: 113.5553 - val_loss: 116.0129 - val_mse: 116.0128
Epoch 106/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 117.2355 - mse: 117.2355
Epoch 00106: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 255us/sample - loss: 115.0748 - mse: 115.0748 - val_loss: 116.3593 - val_mse: 116.3593
Epoch 107/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 119.4657 - mse: 119.4657
Epoch 00107: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 265us/sample - loss: 119.4913 - mse: 119.4913 - val_loss: 118.0379 - val_mse: 118.0380
Epoch 108/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 123.0065 - mse: 123.0065
Epoch 00108: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 260us/sample - loss: 120.5422 - mse: 120.5421 - val_loss: 115.7271 - val_mse: 115.7271
Epoch 109/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 117.2541 - mse: 117.2541
Epoch 00109: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 266us/sample - loss: 116.1874 - mse: 116.1874 - val_loss: 124.6098 - val_mse: 124.6097
Epoch 110/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 117.2431 - mse: 117.2431
Epoch 00110: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 296us/sample - loss: 120.8075 - mse: 120.8075 - val_loss: 135.8392 - val_mse: 135.8392
Epoch 111/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 123.9666 - mse: 123.9666- ETA: 0s - loss: 127.3325 - mse:
Epoch 00111: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 297us/sample - loss: 124.2528 - mse: 124.2527 - val_loss: 117.5120 - val_mse: 117.5120
Epoch 112/300
2208/2400 [==========================&gt;...] - ETA: 0s - loss: 114.8347 - mse: 114.8347
Epoch 00112: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 298us/sample - loss: 112.6422 - mse: 112.6422 - val_loss: 119.3087 - val_mse: 119.3087
Epoch 113/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 108.5884 - mse: 108.5884
Epoch 00113: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 268us/sample - loss: 109.1243 - mse: 109.1243 - val_loss: 125.8933 - val_mse: 125.8933
Epoch 114/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 108.2185 - mse: 108.2185
Epoch 00114: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 265us/sample - loss: 110.0576 - mse: 110.0576 - val_loss: 125.1322 - val_mse: 125.1322
Epoch 115/300
2208/2400 [==========================&gt;...] - ETA: 0s - loss: 113.2103 - mse: 113.2103
Epoch 00115: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 268us/sample - loss: 111.2375 - mse: 111.2375 - val_loss: 119.5266 - val_mse: 119.5266
Epoch 116/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 110.4400 - mse: 110.4400
Epoch 00116: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 276us/sample - loss: 109.6759 - mse: 109.6759 - val_loss: 119.3578 - val_mse: 119.3578
Epoch 117/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 107.5278 - mse: 107.5278
Epoch 00117: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 263us/sample - loss: 108.1306 - mse: 108.1306 - val_loss: 144.9584 - val_mse: 144.9584
Epoch 118/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 109.0788 - mse: 109.0788
Epoch 00118: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 263us/sample - loss: 107.8112 - mse: 107.8112 - val_loss: 125.4921 - val_mse: 125.4921
Epoch 119/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 109.4369 - mse: 109.4368
Epoch 00119: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 263us/sample - loss: 107.9002 - mse: 107.9002 - val_loss: 158.4112 - val_mse: 158.4112
Epoch 120/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 105.6424 - mse: 105.6424
Epoch 00120: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 264us/sample - loss: 104.9744 - mse: 104.9744 - val_loss: 133.7110 - val_mse: 133.7110
Epoch 121/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 107.1830 - mse: 107.1830
Epoch 00121: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 260us/sample - loss: 106.0692 - mse: 106.0692 - val_loss: 119.2843 - val_mse: 119.2843
Epoch 122/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 104.7061 - mse: 104.7061
Epoch 00122: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 259us/sample - loss: 107.0763 - mse: 107.0763 - val_loss: 127.7036 - val_mse: 127.7036
Epoch 123/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 114.9302 - mse: 114.9303
Epoch 00123: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 276us/sample - loss: 114.7806 - mse: 114.7806 - val_loss: 120.2735 - val_mse: 120.2735
Epoch 124/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 117.2540 - mse: 117.2540
Epoch 00124: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 260us/sample - loss: 115.7299 - mse: 115.7300 - val_loss: 120.8538 - val_mse: 120.8538
Epoch 125/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 108.8379 - mse: 108.8379
Epoch 00125: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 273us/sample - loss: 108.2928 - mse: 108.2928 - val_loss: 116.8169 - val_mse: 116.8169
Epoch 126/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 109.0638 - mse: 109.0638
Epoch 00126: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 258us/sample - loss: 110.7132 - mse: 110.7132 - val_loss: 129.2570 - val_mse: 129.2570
Epoch 127/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 114.1130 - mse: 114.1130
Epoch 00127: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 263us/sample - loss: 114.9050 - mse: 114.9050 - val_loss: 120.8689 - val_mse: 120.8689
Epoch 128/300
2208/2400 [==========================&gt;...] - ETA: 0s - loss: 108.4050 - mse: 108.4051
Epoch 00128: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 273us/sample - loss: 107.8723 - mse: 107.8723 - val_loss: 116.5260 - val_mse: 116.5260
Epoch 129/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 107.3705 - mse: 107.3705
Epoch 00129: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 265us/sample - loss: 112.3845 - mse: 112.3846 - val_loss: 135.4605 - val_mse: 135.4605
Epoch 130/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 128.7333 - mse: 128.7333
Epoch 00130: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 269us/sample - loss: 126.1998 - mse: 126.1998 - val_loss: 127.3549 - val_mse: 127.3549
Epoch 131/300
2208/2400 [==========================&gt;...] - ETA: 0s - loss: 127.2152 - mse: 127.2152
Epoch 00131: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 271us/sample - loss: 125.4479 - mse: 125.4479 - val_loss: 145.0553 - val_mse: 145.0553
Epoch 132/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 120.5118 - mse: 120.5118
Epoch 00132: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 258us/sample - loss: 125.7407 - mse: 125.7407 - val_loss: 129.0017 - val_mse: 129.0017
Epoch 133/300
2176/2400 [==========================&gt;...] - ETA: 0s - loss: 113.3958 - mse: 113.3958
Epoch 00133: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 300us/sample - loss: 120.3936 - mse: 120.3936 - val_loss: 164.1217 - val_mse: 164.1217
Epoch 134/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 118.4704 - mse: 118.4704
Epoch 00134: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 292us/sample - loss: 117.9292 - mse: 117.9292 - val_loss: 115.7531 - val_mse: 115.7531
Epoch 135/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 122.5937 - mse: 122.5937
Epoch 00135: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 327us/sample - loss: 122.0675 - mse: 122.0675 - val_loss: 118.7533 - val_mse: 118.7533
Epoch 136/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 109.5747 - mse: 109.5747
Epoch 00136: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 352us/sample - loss: 114.8494 - mse: 114.8494 - val_loss: 114.7719 - val_mse: 114.7719
Epoch 137/300
2144/2400 [=========================&gt;....] - ETA: 0s - loss: 109.9515 - mse: 109.9515
Epoch 00137: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 279us/sample - loss: 110.6702 - mse: 110.6702 - val_loss: 137.1744 - val_mse: 137.1743
Epoch 138/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 110.3411 - mse: 110.3411
Epoch 00138: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 284us/sample - loss: 110.3122 - mse: 110.3122 - val_loss: 152.0226 - val_mse: 152.0225
Epoch 139/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 107.6313 - mse: 107.6313
Epoch 00139: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 269us/sample - loss: 108.2868 - mse: 108.2868 - val_loss: 126.0123 - val_mse: 126.0123
Epoch 140/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 114.0695 - mse: 114.0695
Epoch 00140: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 255us/sample - loss: 114.0815 - mse: 114.0815 - val_loss: 132.8704 - val_mse: 132.8704
Epoch 141/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 114.6026 - mse: 114.6026
Epoch 00141: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 254us/sample - loss: 118.6051 - mse: 118.6051 - val_loss: 116.2750 - val_mse: 116.2750
Epoch 142/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 107.3035 - mse: 107.3035
Epoch 00142: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 265us/sample - loss: 107.4144 - mse: 107.4143 - val_loss: 121.9852 - val_mse: 121.9852
Epoch 143/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 115.5195 - mse: 115.5194
Epoch 00143: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 270us/sample - loss: 115.9718 - mse: 115.9717 - val_loss: 119.0777 - val_mse: 119.0777
Epoch 144/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 123.5784 - mse: 123.5784
Epoch 00144: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 278us/sample - loss: 124.5000 - mse: 124.5000 - val_loss: 118.5302 - val_mse: 118.5302
Epoch 145/300
2208/2400 [==========================&gt;...] - ETA: 0s - loss: 120.2563 - mse: 120.2563
Epoch 00145: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 287us/sample - loss: 116.9244 - mse: 116.9244 - val_loss: 117.2148 - val_mse: 117.2148
Epoch 146/300
2208/2400 [==========================&gt;...] - ETA: 0s - loss: 132.5114 - mse: 132.5114
Epoch 00146: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 255us/sample - loss: 134.9833 - mse: 134.9832 - val_loss: 129.5646 - val_mse: 129.5646
Epoch 147/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 137.5585 - mse: 137.5585
Epoch 00147: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 266us/sample - loss: 134.8671 - mse: 134.8671 - val_loss: 121.3138 - val_mse: 121.3138
Epoch 148/300
2176/2400 [==========================&gt;...] - ETA: 0s - loss: 130.0236 - mse: 130.0236
Epoch 00148: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 267us/sample - loss: 129.0739 - mse: 129.0739 - val_loss: 116.5626 - val_mse: 116.5626
Epoch 149/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 119.9418 - mse: 119.9418
Epoch 00149: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 258us/sample - loss: 124.5878 - mse: 124.5878 - val_loss: 122.6518 - val_mse: 122.6518
Epoch 150/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 125.8149 - mse: 125.8149
Epoch 00150: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 254us/sample - loss: 127.4527 - mse: 127.4527 - val_loss: 119.9838 - val_mse: 119.9838
Epoch 151/300
2208/2400 [==========================&gt;...] - ETA: 0s - loss: 123.6277 - mse: 123.6277
Epoch 00151: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 263us/sample - loss: 121.9317 - mse: 121.9317 - val_loss: 121.3738 - val_mse: 121.3738
Epoch 152/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 124.7144 - mse: 124.7145
Epoch 00152: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 254us/sample - loss: 122.2221 - mse: 122.2221 - val_loss: 113.9750 - val_mse: 113.9750
Epoch 153/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 121.2859 - mse: 121.2859
Epoch 00153: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 257us/sample - loss: 119.9994 - mse: 119.9994 - val_loss: 114.7935 - val_mse: 114.7935
Epoch 154/300
2176/2400 [==========================&gt;...] - ETA: 0s - loss: 117.5786 - mse: 117.5786
Epoch 00154: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 273us/sample - loss: 117.9965 - mse: 117.9965 - val_loss: 117.8807 - val_mse: 117.8807
Epoch 155/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 120.4625 - mse: 120.4625
Epoch 00155: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 267us/sample - loss: 117.9548 - mse: 117.9548 - val_loss: 124.5226 - val_mse: 124.5226
Epoch 156/300
2208/2400 [==========================&gt;...] - ETA: 0s - loss: 119.0826 - mse: 119.0826
Epoch 00156: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 270us/sample - loss: 118.0692 - mse: 118.0692 - val_loss: 121.2026 - val_mse: 121.2026
Epoch 157/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 115.1732 - mse: 115.1732
Epoch 00157: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 277us/sample - loss: 116.3441 - mse: 116.3441 - val_loss: 122.6181 - val_mse: 122.6180
Epoch 158/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 116.9512 - mse: 116.9512
Epoch 00158: val_loss did not improve from 112.79528
2400/2400 [==============================] - 1s 283us/sample - loss: 116.1816 - mse: 116.1816 - val_loss: 117.2767 - val_mse: 117.2767
Epoch 159/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 117.9601 - mse: 117.9601
Epoch 00159: val_loss improved from 112.79528 to 111.88478, saving model to best_model_1.h5
2400/2400 [==============================] - 1s 349us/sample - loss: 116.0338 - mse: 116.0338 - val_loss: 111.8848 - val_mse: 111.8848
Epoch 160/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 114.4618 - mse: 114.4618
Epoch 00160: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 287us/sample - loss: 114.6031 - mse: 114.6031 - val_loss: 125.7753 - val_mse: 125.7753
Epoch 161/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 113.8174 - mse: 113.8175
Epoch 00161: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 284us/sample - loss: 113.0908 - mse: 113.0908 - val_loss: 119.4743 - val_mse: 119.4743
Epoch 162/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 113.6449 - mse: 113.6449
Epoch 00162: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 254us/sample - loss: 112.4984 - mse: 112.4984 - val_loss: 139.3179 - val_mse: 139.3179
Epoch 163/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 113.0072 - mse: 113.0072
Epoch 00163: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 259us/sample - loss: 112.2817 - mse: 112.2817 - val_loss: 145.6286 - val_mse: 145.6286
Epoch 164/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 112.6184 - mse: 112.6184
Epoch 00164: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 265us/sample - loss: 110.8758 - mse: 110.8758 - val_loss: 129.5801 - val_mse: 129.5801
Epoch 165/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 121.1280 - mse: 121.1279
Epoch 00165: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 268us/sample - loss: 120.7710 - mse: 120.7710 - val_loss: 120.5602 - val_mse: 120.5602
Epoch 166/300
2176/2400 [==========================&gt;...] - ETA: 0s - loss: 120.1266 - mse: 120.1266
Epoch 00166: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 272us/sample - loss: 126.7120 - mse: 126.7120 - val_loss: 117.1427 - val_mse: 117.1427
Epoch 167/300
2208/2400 [==========================&gt;...] - ETA: 0s - loss: 107.9516 - mse: 107.9516
Epoch 00167: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 271us/sample - loss: 119.5968 - mse: 119.5968 - val_loss: 132.5210 - val_mse: 132.5210
Epoch 168/300
2208/2400 [==========================&gt;...] - ETA: 0s - loss: 106.9501 - mse: 106.9501
Epoch 00168: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 274us/sample - loss: 117.6744 - mse: 117.6744 - val_loss: 125.9026 - val_mse: 125.9026
Epoch 169/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 116.5261 - mse: 116.5261
Epoch 00169: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 295us/sample - loss: 122.3464 - mse: 122.3465 - val_loss: 132.4685 - val_mse: 132.4685
Epoch 170/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 114.5922 - mse: 114.5922
Epoch 00170: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 263us/sample - loss: 116.0027 - mse: 116.0027 - val_loss: 123.7892 - val_mse: 123.7892
Epoch 171/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 118.2034 - mse: 118.2035
Epoch 00171: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 264us/sample - loss: 114.8318 - mse: 114.8318 - val_loss: 136.6140 - val_mse: 136.6140
Epoch 172/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 111.8500 - mse: 111.8500
Epoch 00172: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 257us/sample - loss: 110.3950 - mse: 110.3950 - val_loss: 114.7723 - val_mse: 114.7723
Epoch 173/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 114.7424 - mse: 114.7424
Epoch 00173: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 267us/sample - loss: 113.4685 - mse: 113.4685 - val_loss: 133.8482 - val_mse: 133.8482
Epoch 174/300
2176/2400 [==========================&gt;...] - ETA: 0s - loss: 111.7294 - mse: 111.7294
Epoch 00174: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 271us/sample - loss: 109.4434 - mse: 109.4434 - val_loss: 114.5436 - val_mse: 114.5436
Epoch 175/300
2176/2400 [==========================&gt;...] - ETA: 0s - loss: 101.4024 - mse: 101.4024
Epoch 00175: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 273us/sample - loss: 108.6159 - mse: 108.6159 - val_loss: 114.0312 - val_mse: 114.0312
Epoch 176/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 111.2202 - mse: 111.2202
Epoch 00176: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 268us/sample - loss: 110.7411 - mse: 110.7411 - val_loss: 125.7672 - val_mse: 125.7672
Epoch 177/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 115.5330 - mse: 115.5330
Epoch 00177: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 263us/sample - loss: 114.2932 - mse: 114.2932 - val_loss: 115.2003 - val_mse: 115.2003
Epoch 178/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 113.2173 - mse: 113.2173
Epoch 00178: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 254us/sample - loss: 112.8251 - mse: 112.8251 - val_loss: 118.6475 - val_mse: 118.6475
Epoch 179/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 114.0503 - mse: 114.0503
Epoch 00179: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 257us/sample - loss: 113.4232 - mse: 113.4233 - val_loss: 115.5322 - val_mse: 115.5322
Epoch 180/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 108.0628 - mse: 108.0628
Epoch 00180: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 257us/sample - loss: 110.3087 - mse: 110.3087 - val_loss: 127.9147 - val_mse: 127.9147
Epoch 181/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 108.7325 - mse: 108.7325
Epoch 00181: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 256us/sample - loss: 109.4155 - mse: 109.4156 - val_loss: 121.0242 - val_mse: 121.0242
Epoch 182/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 110.6160 - mse: 110.6160
Epoch 00182: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 243us/sample - loss: 110.1487 - mse: 110.1487 - val_loss: 119.2664 - val_mse: 119.2664
Epoch 183/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 115.8628 - mse: 115.8628
Epoch 00183: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 287us/sample - loss: 115.6056 - mse: 115.6056 - val_loss: 125.1901 - val_mse: 125.1901
Epoch 184/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 111.6781 - mse: 111.6781
Epoch 00184: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 288us/sample - loss: 111.6269 - mse: 111.6269 - val_loss: 153.7047 - val_mse: 153.7047
Epoch 185/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 117.8747 - mse: 117.8747
Epoch 00185: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 292us/sample - loss: 119.4212 - mse: 119.4212 - val_loss: 169.0949 - val_mse: 169.0949
Epoch 186/300
2208/2400 [==========================&gt;...] - ETA: 0s - loss: 118.0033 - mse: 118.0033
Epoch 00186: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 270us/sample - loss: 116.2677 - mse: 116.2677 - val_loss: 126.2772 - val_mse: 126.2772
Epoch 187/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 111.3057 - mse: 111.3056
Epoch 00187: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 258us/sample - loss: 111.1769 - mse: 111.1769 - val_loss: 117.2089 - val_mse: 117.2089
Epoch 188/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 100.0279 - mse: 100.0279
Epoch 00188: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 256us/sample - loss: 104.4593 - mse: 104.4593 - val_loss: 303.3150 - val_mse: 303.3150
Epoch 189/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 143.6635 - mse: 143.6634
Epoch 00189: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 258us/sample - loss: 140.9885 - mse: 140.9884 - val_loss: 129.1629 - val_mse: 129.1629
Epoch 190/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 131.4510 - mse: 131.4511
Epoch 00190: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 261us/sample - loss: 133.9429 - mse: 133.9430 - val_loss: 122.1782 - val_mse: 122.1782
Epoch 191/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 126.1677 - mse: 126.1677
Epoch 00191: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 258us/sample - loss: 128.1254 - mse: 128.1254 - val_loss: 120.7430 - val_mse: 120.7430
Epoch 192/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 137.4608 - mse: 137.4608
Epoch 00192: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 274us/sample - loss: 136.2870 - mse: 136.2870 - val_loss: 122.6225 - val_mse: 122.6225
Epoch 193/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 131.0422 - mse: 131.0422
Epoch 00193: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 262us/sample - loss: 135.3982 - mse: 135.3982 - val_loss: 121.7305 - val_mse: 121.7305
Epoch 194/300
2208/2400 [==========================&gt;...] - ETA: 0s - loss: 133.8941 - mse: 133.8941
Epoch 00194: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 267us/sample - loss: 133.2698 - mse: 133.2698 - val_loss: 118.9627 - val_mse: 118.9627
Epoch 195/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 132.6601 - mse: 132.6601
Epoch 00195: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 257us/sample - loss: 131.2567 - mse: 131.2567 - val_loss: 125.8389 - val_mse: 125.8390
Epoch 196/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 130.2357 - mse: 130.2357
Epoch 00196: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 255us/sample - loss: 128.7412 - mse: 128.7412 - val_loss: 121.5368 - val_mse: 121.5368
Epoch 197/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 135.5495 - mse: 135.5495
Epoch 00197: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 257us/sample - loss: 134.5914 - mse: 134.5913 - val_loss: 124.2807 - val_mse: 124.2807
Epoch 198/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 129.2341 - mse: 129.2341
Epoch 00198: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 263us/sample - loss: 129.9308 - mse: 129.9308 - val_loss: 126.6245 - val_mse: 126.6245
Epoch 199/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 126.6088 - mse: 126.6088
Epoch 00199: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 255us/sample - loss: 139.8548 - mse: 139.8548 - val_loss: 121.7510 - val_mse: 121.7510
Epoch 200/300
2208/2400 [==========================&gt;...] - ETA: 0s - loss: 126.6067 - mse: 126.6067
Epoch 00200: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 266us/sample - loss: 136.6338 - mse: 136.6338 - val_loss: 123.0822 - val_mse: 123.0822
Epoch 201/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 136.0159 - mse: 136.0160
Epoch 00201: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 258us/sample - loss: 136.3415 - mse: 136.3415 - val_loss: 122.2615 - val_mse: 122.2615
Epoch 202/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 138.1456 - mse: 138.1456
Epoch 00202: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 260us/sample - loss: 135.1965 - mse: 135.1965 - val_loss: 122.4331 - val_mse: 122.4331
Epoch 203/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 137.1257 - mse: 137.1257
Epoch 00203: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 256us/sample - loss: 135.7133 - mse: 135.7133 - val_loss: 121.8794 - val_mse: 121.8794
Epoch 204/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 139.0261 - mse: 139.0261
Epoch 00204: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 254us/sample - loss: 135.3208 - mse: 135.3208 - val_loss: 121.9004 - val_mse: 121.9004
Epoch 205/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 137.9321 - mse: 137.9321ETA: 0s - loss: 95.9407 - mse
Epoch 00205: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 259us/sample - loss: 136.7546 - mse: 136.7546 - val_loss: 122.3572 - val_mse: 122.3572
Epoch 206/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 133.9194 - mse: 133.9193
Epoch 00206: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 261us/sample - loss: 134.8571 - mse: 134.8571 - val_loss: 122.3921 - val_mse: 122.3921
Epoch 207/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 136.7494 - mse: 136.7495
Epoch 00207: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 258us/sample - loss: 134.7608 - mse: 134.7608 - val_loss: 122.1613 - val_mse: 122.1613
Epoch 208/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 137.0868 - mse: 137.0868
Epoch 00208: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 281us/sample - loss: 135.2090 - mse: 135.2090 - val_loss: 122.2836 - val_mse: 122.2836
Epoch 209/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 137.0488 - mse: 137.0488
Epoch 00209: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 315us/sample - loss: 135.4605 - mse: 135.4605 - val_loss: 122.0651 - val_mse: 122.0651
Epoch 210/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 137.6861 - mse: 137.6861
Epoch 00210: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 288us/sample - loss: 136.1931 - mse: 136.1931 - val_loss: 122.4454 - val_mse: 122.4454
Epoch 211/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 135.5934 - mse: 135.5933
Epoch 00211: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 285us/sample - loss: 134.9502 - mse: 134.9501 - val_loss: 121.9749 - val_mse: 121.9749
Epoch 212/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 135.4753 - mse: 135.4753
Epoch 00212: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 269us/sample - loss: 134.5327 - mse: 134.5327 - val_loss: 121.9792 - val_mse: 121.9791
Epoch 213/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 137.9402 - mse: 137.9402
Epoch 00213: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 279us/sample - loss: 135.7340 - mse: 135.7340 - val_loss: 121.9122 - val_mse: 121.9122
Epoch 214/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 134.0298 - mse: 134.0299
Epoch 00214: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 264us/sample - loss: 134.9492 - mse: 134.9492 - val_loss: 122.0313 - val_mse: 122.0313
Epoch 215/300
2208/2400 [==========================&gt;...] - ETA: 0s - loss: 136.5402 - mse: 136.5402
Epoch 00215: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 268us/sample - loss: 135.9715 - mse: 135.9715 - val_loss: 121.7788 - val_mse: 121.7788
Epoch 216/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 136.9594 - mse: 136.9594
Epoch 00216: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 261us/sample - loss: 134.6380 - mse: 134.6380 - val_loss: 121.7310 - val_mse: 121.7310
Epoch 217/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 134.9447 - mse: 134.9447
Epoch 00217: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 262us/sample - loss: 134.5361 - mse: 134.5361 - val_loss: 121.9970 - val_mse: 121.9970
Epoch 218/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 131.9028 - mse: 131.9028
Epoch 00218: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 265us/sample - loss: 135.1731 - mse: 135.1731 - val_loss: 121.7328 - val_mse: 121.7328
Epoch 219/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 130.9341 - mse: 130.9341
Epoch 00219: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 265us/sample - loss: 135.2969 - mse: 135.2969 - val_loss: 122.8347 - val_mse: 122.8347
Epoch 220/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 132.1751 - mse: 132.1752
Epoch 00220: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 258us/sample - loss: 135.2548 - mse: 135.2549 - val_loss: 118.2911 - val_mse: 118.2911
Epoch 221/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 133.1440 - mse: 133.1440
Epoch 00221: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 261us/sample - loss: 131.9049 - mse: 131.9049 - val_loss: 120.1260 - val_mse: 120.1260
Epoch 222/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 130.4689 - mse: 130.4689
Epoch 00222: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 263us/sample - loss: 129.0863 - mse: 129.0863 - val_loss: 121.7603 - val_mse: 121.7603
Epoch 223/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 118.3976 - mse: 118.3976
Epoch 00223: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 261us/sample - loss: 132.5269 - mse: 132.5269 - val_loss: 114.8539 - val_mse: 114.8539
Epoch 224/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 123.6608 - mse: 123.6608
Epoch 00224: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 263us/sample - loss: 126.4851 - mse: 126.4851 - val_loss: 116.8047 - val_mse: 116.8046
Epoch 225/300
2176/2400 [==========================&gt;...] - ETA: 0s - loss: 126.8939 - mse: 126.8939
Epoch 00225: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 275us/sample - loss: 127.4059 - mse: 127.4059 - val_loss: 116.4890 - val_mse: 116.4890
Epoch 226/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 128.1336 - mse: 128.133 - ETA: 0s - loss: 125.8610 - mse: 125.8610
Epoch 00226: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 267us/sample - loss: 125.0385 - mse: 125.0385 - val_loss: 118.8981 - val_mse: 118.8981
Epoch 227/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 128.2156 - mse: 128.2156
Epoch 00227: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 260us/sample - loss: 127.2940 - mse: 127.2941 - val_loss: 123.6489 - val_mse: 123.6489
Epoch 228/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 125.2867 - mse: 125.2867
Epoch 00228: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 261us/sample - loss: 125.7307 - mse: 125.7307 - val_loss: 115.2157 - val_mse: 115.2157
Epoch 229/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 129.0650 - mse: 129.0650
Epoch 00229: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 281us/sample - loss: 128.9945 - mse: 128.9945 - val_loss: 114.6669 - val_mse: 114.6669
Epoch 230/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 125.9864 - mse: 125.9865
Epoch 00230: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 294us/sample - loss: 125.8236 - mse: 125.8236 - val_loss: 114.0912 - val_mse: 114.0912
Epoch 231/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 125.9128 - mse: 125.9128
Epoch 00231: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 269us/sample - loss: 124.1865 - mse: 124.1865 - val_loss: 114.7208 - val_mse: 114.7208
Epoch 232/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 119.8871 - mse: 119.8871
Epoch 00232: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 276us/sample - loss: 121.0395 - mse: 121.0395 - val_loss: 116.4944 - val_mse: 116.4944
Epoch 233/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 113.2893 - mse: 113.2893
Epoch 00233: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 311us/sample - loss: 119.2784 - mse: 119.2784 - val_loss: 117.3015 - val_mse: 117.3015
Epoch 234/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 120.8284 - mse: 120.8284
Epoch 00234: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 302us/sample - loss: 119.5380 - mse: 119.5380 - val_loss: 112.5399 - val_mse: 112.5399
Epoch 235/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 117.2503 - mse: 117.2503
Epoch 00235: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 304us/sample - loss: 119.3597 - mse: 119.3597 - val_loss: 115.1305 - val_mse: 115.1305
Epoch 236/300
2208/2400 [==========================&gt;...] - ETA: 0s - loss: 127.7222 - mse: 127.7222
Epoch 00236: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 268us/sample - loss: 122.5016 - mse: 122.5017 - val_loss: 114.5625 - val_mse: 114.5625
Epoch 237/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 125.3762 - mse: 125.3761
Epoch 00237: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 265us/sample - loss: 121.8710 - mse: 121.8709 - val_loss: 121.0014 - val_mse: 121.0014
Epoch 238/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 125.2869 - mse: 125.2869
Epoch 00238: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 261us/sample - loss: 122.3187 - mse: 122.3187 - val_loss: 114.5268 - val_mse: 114.5268
Epoch 239/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 118.3614 - mse: 118.3614
Epoch 00239: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 263us/sample - loss: 118.1044 - mse: 118.1044 - val_loss: 123.0212 - val_mse: 123.0212
Epoch 240/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 126.5317 - mse: 126.5317
Epoch 00240: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 264us/sample - loss: 125.8463 - mse: 125.8463 - val_loss: 116.6334 - val_mse: 116.6334
Epoch 241/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 124.5689 - mse: 124.5689
Epoch 00241: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 263us/sample - loss: 123.6167 - mse: 123.6167 - val_loss: 114.1533 - val_mse: 114.1533
Epoch 242/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 120.4156 - mse: 120.4156
Epoch 00242: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 262us/sample - loss: 120.7509 - mse: 120.7509 - val_loss: 124.5155 - val_mse: 124.5155
Epoch 243/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 126.1908 - mse: 126.1908
Epoch 00243: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 261us/sample - loss: 122.8314 - mse: 122.8314 - val_loss: 116.9983 - val_mse: 116.9983
Epoch 244/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 119.2125 - mse: 119.2125
Epoch 00244: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 263us/sample - loss: 116.5055 - mse: 116.5055 - val_loss: 119.2393 - val_mse: 119.2392
Epoch 245/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 114.9874 - mse: 114.9874
Epoch 00245: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 263us/sample - loss: 114.1679 - mse: 114.1679 - val_loss: 125.4404 - val_mse: 125.4404
Epoch 246/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 125.5452 - mse: 125.5452
Epoch 00246: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 258us/sample - loss: 129.5106 - mse: 129.5106 - val_loss: 116.5502 - val_mse: 116.5502
Epoch 247/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 128.5364 - mse: 128.5364
Epoch 00247: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 258us/sample - loss: 126.2908 - mse: 126.2908 - val_loss: 112.6790 - val_mse: 112.6790
Epoch 248/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 121.0440 - mse: 121.0440
Epoch 00248: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 261us/sample - loss: 118.7867 - mse: 118.7867 - val_loss: 114.3267 - val_mse: 114.3267
Epoch 249/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 113.1830 - mse: 113.1830
Epoch 00249: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 259us/sample - loss: 113.1628 - mse: 113.1628 - val_loss: 120.1407 - val_mse: 120.1407
Epoch 250/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 117.9820 - mse: 117.9820
Epoch 00250: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 260us/sample - loss: 119.5475 - mse: 119.5475 - val_loss: 118.4736 - val_mse: 118.4736
Epoch 251/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 129.7257 - mse: 129.7258
Epoch 00251: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 277us/sample - loss: 127.7125 - mse: 127.7125 - val_loss: 119.2001 - val_mse: 119.2001
Epoch 252/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 126.4935 - mse: 126.4935
Epoch 00252: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 263us/sample - loss: 127.0426 - mse: 127.0426 - val_loss: 118.4055 - val_mse: 118.4055
Epoch 253/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 122.7434 - mse: 122.7433
Epoch 00253: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 279us/sample - loss: 121.9059 - mse: 121.9059 - val_loss: 123.1296 - val_mse: 123.1296
Epoch 254/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 121.2638 - mse: 121.2638
Epoch 00254: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 262us/sample - loss: 119.7552 - mse: 119.7552 - val_loss: 115.5533 - val_mse: 115.5533
Epoch 255/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 112.9577 - mse: 112.9577
Epoch 00255: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 260us/sample - loss: 117.4978 - mse: 117.4978 - val_loss: 115.9826 - val_mse: 115.9826
Epoch 256/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 118.9176 - mse: 118.9176
Epoch 00256: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 258us/sample - loss: 117.6269 - mse: 117.6269 - val_loss: 120.5093 - val_mse: 120.5093
Epoch 257/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 117.5281 - mse: 117.5281
Epoch 00257: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 271us/sample - loss: 119.0247 - mse: 119.0247 - val_loss: 113.0362 - val_mse: 113.0362
Epoch 258/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 114.3347 - mse: 114.3347
Epoch 00258: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 325us/sample - loss: 114.8851 - mse: 114.8850 - val_loss: 120.2039 - val_mse: 120.2039
Epoch 259/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 109.0917 - mse: 109.0917
Epoch 00259: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 374us/sample - loss: 108.8658 - mse: 108.8658 - val_loss: 121.3099 - val_mse: 121.3099
Epoch 260/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 113.3366 - mse: 113.3366
Epoch 00260: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 367us/sample - loss: 113.2697 - mse: 113.2697 - val_loss: 116.7130 - val_mse: 116.7130
Epoch 261/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 121.2988 - mse: 121.2988
Epoch 00261: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 299us/sample - loss: 121.9709 - mse: 121.9709 - val_loss: 114.7289 - val_mse: 114.7289
Epoch 262/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 115.5911 - mse: 115.5911
Epoch 00262: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 297us/sample - loss: 120.0556 - mse: 120.0556 - val_loss: 113.9171 - val_mse: 113.9171
Epoch 263/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 117.5170 - mse: 117.5170
Epoch 00263: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 296us/sample - loss: 117.2362 - mse: 117.2362 - val_loss: 122.5849 - val_mse: 122.5849
Epoch 264/300
2208/2400 [==========================&gt;...] - ETA: 0s - loss: 119.0808 - mse: 119.0808
Epoch 00264: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 289us/sample - loss: 115.1565 - mse: 115.1565 - val_loss: 121.1444 - val_mse: 121.1444
Epoch 265/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 116.5663 - mse: 116.5663
Epoch 00265: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 310us/sample - loss: 116.3414 - mse: 116.3414 - val_loss: 115.2589 - val_mse: 115.2589
Epoch 266/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 113.7902 - mse: 113.7902
Epoch 00266: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 332us/sample - loss: 113.0474 - mse: 113.0474 - val_loss: 118.0394 - val_mse: 118.0394
Epoch 267/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 114.3475 - mse: 114.3475
Epoch 00267: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 293us/sample - loss: 111.5119 - mse: 111.5119 - val_loss: 114.5516 - val_mse: 114.5515
Epoch 268/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 108.7112 - mse: 108.7112
Epoch 00268: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 288us/sample - loss: 113.5998 - mse: 113.5998 - val_loss: 117.3503 - val_mse: 117.3503
Epoch 269/300
2208/2400 [==========================&gt;...] - ETA: 0s - loss: 114.1234 - mse: 114.1234
Epoch 00269: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 284us/sample - loss: 113.2449 - mse: 113.2449 - val_loss: 119.6913 - val_mse: 119.6913
Epoch 270/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 122.7467 - mse: 122.7467
Epoch 00270: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 288us/sample - loss: 122.9790 - mse: 122.9790 - val_loss: 116.8523 - val_mse: 116.8524
Epoch 271/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 120.8869 - mse: 120.8869
Epoch 00271: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 297us/sample - loss: 120.2588 - mse: 120.2588 - val_loss: 122.3743 - val_mse: 122.3743
Epoch 272/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 119.6640 - mse: 119.6640
Epoch 00272: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 288us/sample - loss: 120.1335 - mse: 120.1335 - val_loss: 117.3625 - val_mse: 117.3625
Epoch 273/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 118.2627 - mse: 118.2627
Epoch 00273: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 284us/sample - loss: 116.3503 - mse: 116.3503 - val_loss: 123.7942 - val_mse: 123.7942
Epoch 274/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 113.4055 - mse: 113.4055
Epoch 00274: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 327us/sample - loss: 113.4853 - mse: 113.4853 - val_loss: 117.1326 - val_mse: 117.1326
Epoch 275/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 113.1150 - mse: 113.1150
Epoch 00275: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 307us/sample - loss: 112.1129 - mse: 112.1129 - val_loss: 118.2038 - val_mse: 118.2038
Epoch 276/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 106.9840 - mse: 106.9840
Epoch 00276: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 287us/sample - loss: 106.8449 - mse: 106.8449 - val_loss: 118.5847 - val_mse: 118.5847
Epoch 277/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 115.4373 - mse: 115.4373
Epoch 00277: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 315us/sample - loss: 113.2616 - mse: 113.2616 - val_loss: 113.2688 - val_mse: 113.2688
Epoch 278/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 115.8706 - mse: 115.8706
Epoch 00278: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 288us/sample - loss: 113.9028 - mse: 113.9028 - val_loss: 120.0541 - val_mse: 120.0541
Epoch 279/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 111.6296 - mse: 111.6296
Epoch 00279: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 319us/sample - loss: 110.7707 - mse: 110.7707 - val_loss: 119.5043 - val_mse: 119.5043
Epoch 280/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 108.1819 - mse: 108.1819
Epoch 00280: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 321us/sample - loss: 107.0616 - mse: 107.0616 - val_loss: 116.8462 - val_mse: 116.8462
Epoch 281/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 118.0562 - mse: 118.0562
Epoch 00281: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 318us/sample - loss: 115.7162 - mse: 115.7162 - val_loss: 122.0517 - val_mse: 122.0517
Epoch 282/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 109.4143 - mse: 109.4143
Epoch 00282: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 333us/sample - loss: 108.2725 - mse: 108.2725 - val_loss: 120.3030 - val_mse: 120.3030
Epoch 283/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 140.3206 - mse: 140.3206
Epoch 00283: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 351us/sample - loss: 139.4050 - mse: 139.4050 - val_loss: 114.6734 - val_mse: 114.6734
Epoch 284/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 124.7774 - mse: 124.7774
Epoch 00284: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 360us/sample - loss: 124.5573 - mse: 124.5574 - val_loss: 120.2946 - val_mse: 120.2946
Epoch 285/300
2208/2400 [==========================&gt;...] - ETA: 0s - loss: 114.8414 - mse: 114.8414
Epoch 00285: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 342us/sample - loss: 114.7185 - mse: 114.7185 - val_loss: 120.4008 - val_mse: 120.4008
Epoch 286/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 115.1586 - mse: 115.1586
Epoch 00286: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 272us/sample - loss: 114.2346 - mse: 114.2346 - val_loss: 121.1875 - val_mse: 121.1875
Epoch 287/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 116.8199 - mse: 116.8199
Epoch 00287: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 259us/sample - loss: 114.8319 - mse: 114.8319 - val_loss: 119.8702 - val_mse: 119.8702
Epoch 288/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 113.1260 - mse: 113.1260
Epoch 00288: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 261us/sample - loss: 114.4666 - mse: 114.4666 - val_loss: 123.5506 - val_mse: 123.5506
Epoch 289/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 108.5903 - mse: 108.5903
Epoch 00289: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 268us/sample - loss: 112.2396 - mse: 112.2396 - val_loss: 121.1215 - val_mse: 121.1215
Epoch 290/300
2240/2400 [===========================&gt;..] - ETA: 0s - loss: 114.9537 - mse: 114.9536
Epoch 00290: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 268us/sample - loss: 112.9438 - mse: 112.9438 - val_loss: 119.7389 - val_mse: 119.7389
Epoch 291/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 108.0757 - mse: 108.0757
Epoch 00291: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 258us/sample - loss: 110.2304 - mse: 110.2304 - val_loss: 115.9459 - val_mse: 115.9459
Epoch 292/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 112.2700 - mse: 112.2700
Epoch 00292: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 258us/sample - loss: 111.0947 - mse: 111.0947 - val_loss: 121.5589 - val_mse: 121.5589
Epoch 293/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 108.3259 - mse: 108.3259
Epoch 00293: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 256us/sample - loss: 106.6022 - mse: 106.6022 - val_loss: 115.1768 - val_mse: 115.1768
Epoch 294/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 106.3683 - mse: 106.3683
Epoch 00294: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 265us/sample - loss: 105.8044 - mse: 105.8044 - val_loss: 119.2716 - val_mse: 119.2716
Epoch 295/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 121.8150 - mse: 121.8150
Epoch 00295: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 256us/sample - loss: 125.3383 - mse: 125.3383 - val_loss: 135.9891 - val_mse: 135.9891
Epoch 296/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 121.3915 - mse: 121.3915
Epoch 00296: val_loss did not improve from 111.88478
2400/2400 [==============================] - 1s 257us/sample - loss: 122.8371 - mse: 122.8371 - val_loss: 147.8820 - val_mse: 147.8820
Epoch 297/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 121.6935 - mse: 121.6935
Epoch 00297: val_loss improved from 111.88478 to 111.46321, saving model to best_model_1.h5
2400/2400 [==============================] - 1s 313us/sample - loss: 121.0217 - mse: 121.0217 - val_loss: 111.4632 - val_mse: 111.4632
Epoch 298/300
2208/2400 [==========================&gt;...] - ETA: 0s - loss: 122.2926 - mse: 122.2926
Epoch 00298: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 266us/sample - loss: 120.1095 - mse: 120.1095 - val_loss: 113.5520 - val_mse: 113.5520
Epoch 299/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 117.8404 - mse: 117.8403
Epoch 00299: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 261us/sample - loss: 118.6401 - mse: 118.6401 - val_loss: 113.0891 - val_mse: 113.0891
Epoch 300/300
2208/2400 [==========================&gt;...] - ETA: 0s - loss: 112.6609 - mse: 112.6609
Epoch 00300: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 265us/sample - loss: 115.4253 - mse: 115.4253 - val_loss: 125.4563 - val_mse: 125.4563
WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;, &lt;class &#39;NoneType&#39;&gt;
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Train on 4131 samples, validate on 1033 samples
Epoch 1/200
3648/4131 [=========================&gt;....] - ETA: 0s - loss: 4.8129 - accuracy: 0.6146
Epoch 00001: val_loss improved from inf to 0.09174, saving model to best_model_2.h5
4131/4131 [==============================] - 1s 254us/sample - loss: 4.4299 - accuracy: 0.6209 - val_loss: 0.0917 - val_accuracy: 0.9545
Epoch 2/200
3424/4131 [=======================&gt;......] - ETA: 0s - loss: 1.4834 - accuracy: 0.6387
Epoch 00002: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 56us/sample - loss: 1.4320 - accuracy: 0.6420 - val_loss: 1.6017 - val_accuracy: 0.4550
Epoch 3/200
3424/4131 [=======================&gt;......] - ETA: 0s - loss: 0.9772 - accuracy: 0.6717
Epoch 00003: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 53us/sample - loss: 0.9742 - accuracy: 0.6720 - val_loss: 3.6402 - val_accuracy: 0.1442
Epoch 4/200
4064/4131 [============================&gt;.] - ETA: 0s - loss: 0.8470 - accuracy: 0.6941
Epoch 00004: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 47us/sample - loss: 0.8423 - accuracy: 0.6957 - val_loss: 0.6616 - val_accuracy: 0.7018
Epoch 5/200
3712/4131 [=========================&gt;....] - ETA: 0s - loss: 0.8133 - accuracy: 0.7026
Epoch 00005: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 51us/sample - loss: 0.7929 - accuracy: 0.7056 - val_loss: 0.4416 - val_accuracy: 0.7938
Epoch 6/200
4064/4131 [============================&gt;.] - ETA: 0s - loss: 0.6618 - accuracy: 0.7249
Epoch 00006: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 47us/sample - loss: 0.6642 - accuracy: 0.7231 - val_loss: 1.7545 - val_accuracy: 0.2314
Epoch 7/200
3808/4131 [==========================&gt;...] - ETA: 0s - loss: 0.6226 - accuracy: 0.7319
Epoch 00007: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 50us/sample - loss: 0.6151 - accuracy: 0.7354 - val_loss: 0.2862 - val_accuracy: 0.9080
Epoch 8/200
3872/4131 [===========================&gt;..] - ETA: 0s - loss: 0.6784 - accuracy: 0.7211
Epoch 00008: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 49us/sample - loss: 0.6998 - accuracy: 0.7175 - val_loss: 0.3496 - val_accuracy: 0.8490
Epoch 9/200
4096/4131 [============================&gt;.] - ETA: 0s - loss: 0.5873 - accuracy: 0.7373
Epoch 00009: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 48us/sample - loss: 0.5871 - accuracy: 0.7374 - val_loss: 0.1937 - val_accuracy: 0.9593
Epoch 10/200
3456/4131 [========================&gt;.....] - ETA: 0s - loss: 0.5333 - accuracy: 0.7564
Epoch 00010: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 55us/sample - loss: 0.5543 - accuracy: 0.7509 - val_loss: 0.2656 - val_accuracy: 0.9071
Epoch 11/200
4032/4131 [============================&gt;.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7478
Epoch 00011: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 47us/sample - loss: 0.5633 - accuracy: 0.7492 - val_loss: 0.7661 - val_accuracy: 0.7919
Epoch 12/200
3872/4131 [===========================&gt;..] - ETA: 0s - loss: 0.5257 - accuracy: 0.7583
Epoch 00012: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 49us/sample - loss: 0.5236 - accuracy: 0.7596 - val_loss: 0.3546 - val_accuracy: 0.8703
Epoch 13/200
4032/4131 [============================&gt;.] - ETA: 0s - loss: 0.5223 - accuracy: 0.7537
Epoch 00013: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 47us/sample - loss: 0.5189 - accuracy: 0.7553 - val_loss: 0.1809 - val_accuracy: 0.9497
Epoch 14/200
4000/4131 [============================&gt;.] - ETA: 0s - loss: 0.5750 - accuracy: 0.7487
Epoch 00014: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 48us/sample - loss: 0.5745 - accuracy: 0.7482 - val_loss: 1.2790 - val_accuracy: 0.3320
Epoch 15/200
4000/4131 [============================&gt;.] - ETA: 0s - loss: 0.6343 - accuracy: 0.7295
Epoch 00015: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 48us/sample - loss: 0.6323 - accuracy: 0.7303 - val_loss: 0.6089 - val_accuracy: 0.7561
Epoch 16/200
4000/4131 [============================&gt;.] - ETA: 0s - loss: 0.5137 - accuracy: 0.7713
Epoch 00016: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 47us/sample - loss: 0.5224 - accuracy: 0.7698 - val_loss: 0.4359 - val_accuracy: 0.8877
Epoch 17/200
3872/4131 [===========================&gt;..] - ETA: 0s - loss: 0.4856 - accuracy: 0.7714
Epoch 00017: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 49us/sample - loss: 0.4859 - accuracy: 0.7698 - val_loss: 0.5097 - val_accuracy: 0.7473
Epoch 18/200
4032/4131 [============================&gt;.] - ETA: 0s - loss: 0.4779 - accuracy: 0.7736
Epoch 00018: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 48us/sample - loss: 0.4798 - accuracy: 0.7729 - val_loss: 0.1774 - val_accuracy: 0.9622
Epoch 19/200
4000/4131 [============================&gt;.] - ETA: 0s - loss: 0.4741 - accuracy: 0.7753
Epoch 00019: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 48us/sample - loss: 0.4777 - accuracy: 0.7729 - val_loss: 1.3835 - val_accuracy: 0.3020
Epoch 20/200
3872/4131 [===========================&gt;..] - ETA: 0s - loss: 0.4718 - accuracy: 0.7740
Epoch 00020: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 49us/sample - loss: 0.4714 - accuracy: 0.7754 - val_loss: 0.5150 - val_accuracy: 0.7425
Epoch 21/200
4032/4131 [============================&gt;.] - ETA: 0s - loss: 0.4220 - accuracy: 0.7996
Epoch 00021: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 47us/sample - loss: 0.4214 - accuracy: 0.8000 - val_loss: 0.2262 - val_accuracy: 0.9284
Epoch 22/200
3808/4131 [==========================&gt;...] - ETA: 0s - loss: 0.5954 - accuracy: 0.7547
Epoch 00022: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 50us/sample - loss: 0.5836 - accuracy: 0.7555 - val_loss: 0.4176 - val_accuracy: 0.8064
Epoch 23/200
4064/4131 [============================&gt;.] - ETA: 0s - loss: 0.4467 - accuracy: 0.7985
Epoch 00023: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 47us/sample - loss: 0.4469 - accuracy: 0.7976 - val_loss: 0.7574 - val_accuracy: 0.6302
Epoch 24/200
4064/4131 [============================&gt;.] - ETA: 0s - loss: 0.4295 - accuracy: 0.8083
Epoch 00024: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 47us/sample - loss: 0.4268 - accuracy: 0.8100 - val_loss: 0.1926 - val_accuracy: 0.9477
Epoch 25/200
3776/4131 [==========================&gt;...] - ETA: 0s - loss: 0.4513 - accuracy: 0.7879
Epoch 00025: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 50us/sample - loss: 0.4501 - accuracy: 0.7896 - val_loss: 0.3339 - val_accuracy: 0.8625
Epoch 26/200
4000/4131 [============================&gt;.] - ETA: 0s - loss: 0.4101 - accuracy: 0.8085
Epoch 00026: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 47us/sample - loss: 0.4092 - accuracy: 0.8088 - val_loss: 0.8129 - val_accuracy: 0.5324
Epoch 27/200
3936/4131 [===========================&gt;..] - ETA: 0s - loss: 0.4788 - accuracy: 0.7815
Epoch 00027: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 49us/sample - loss: 0.4761 - accuracy: 0.7826 - val_loss: 0.3640 - val_accuracy: 0.8548
Epoch 28/200
4000/4131 [============================&gt;.] - ETA: 0s - loss: 0.4302 - accuracy: 0.7985
Epoch 00028: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 48us/sample - loss: 0.4280 - accuracy: 0.8008 - val_loss: 0.8502 - val_accuracy: 0.6244
Epoch 29/200
4064/4131 [============================&gt;.] - ETA: 0s - loss: 0.4404 - accuracy: 0.7970
Epoch 00029: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 48us/sample - loss: 0.4408 - accuracy: 0.7974 - val_loss: 0.8225 - val_accuracy: 0.5479
Epoch 30/200
3936/4131 [===========================&gt;..] - ETA: 0s - loss: 0.4301 - accuracy: 0.8031
Epoch 00030: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 48us/sample - loss: 0.4299 - accuracy: 0.8025 - val_loss: 0.8219 - val_accuracy: 0.5324
Epoch 31/200
4000/4131 [============================&gt;.] - ETA: 0s - loss: 0.3835 - accuracy: 0.8288
Epoch 00031: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 48us/sample - loss: 0.3883 - accuracy: 0.8274 - val_loss: 0.8864 - val_accuracy: 0.5760
Epoch 32/200
3936/4131 [===========================&gt;..] - ETA: 0s - loss: 0.4576 - accuracy: 0.8039
Epoch 00032: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 49us/sample - loss: 0.4564 - accuracy: 0.8027 - val_loss: 1.4027 - val_accuracy: 0.3146
Epoch 33/200
4096/4131 [============================&gt;.] - ETA: 0s - loss: 0.3810 - accuracy: 0.8213
Epoch 00033: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 46us/sample - loss: 0.3816 - accuracy: 0.8214 - val_loss: 0.6717 - val_accuracy: 0.6689
Epoch 34/200
4064/4131 [============================&gt;.] - ETA: 0s - loss: 0.4156 - accuracy: 0.8031
Epoch 00034: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 48us/sample - loss: 0.4137 - accuracy: 0.8046 - val_loss: 0.6693 - val_accuracy: 0.6418
Epoch 35/200
3904/4131 [===========================&gt;..] - ETA: 0s - loss: 0.4000 - accuracy: 0.8189
Epoch 00035: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 50us/sample - loss: 0.4046 - accuracy: 0.8184 - val_loss: 0.5406 - val_accuracy: 0.7241
Epoch 36/200
4128/4131 [============================&gt;.] - ETA: 0s - loss: 0.3848 - accuracy: 0.8268
Epoch 00036: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 46us/sample - loss: 0.3848 - accuracy: 0.8267 - val_loss: 0.5524 - val_accuracy: 0.7173
Epoch 37/200
3776/4131 [==========================&gt;...] - ETA: 0s - loss: 0.4119 - accuracy: 0.8159
Epoch 00037: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 50us/sample - loss: 0.4157 - accuracy: 0.8131 - val_loss: 1.0454 - val_accuracy: 0.4627
Epoch 38/200
4064/4131 [============================&gt;.] - ETA: 0s - loss: 0.3985 - accuracy: 0.8127
Epoch 00038: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 47us/sample - loss: 0.3994 - accuracy: 0.8126 - val_loss: 0.4876 - val_accuracy: 0.7706
Epoch 39/200
4096/4131 [============================&gt;.] - ETA: 0s - loss: 0.3590 - accuracy: 0.8440
Epoch 00039: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 51us/sample - loss: 0.3590 - accuracy: 0.8439 - val_loss: 0.2984 - val_accuracy: 0.9032
Epoch 40/200
3360/4131 [=======================&gt;......] - ETA: 0s - loss: 0.4787 - accuracy: 0.8045
Epoch 00040: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 72us/sample - loss: 0.4820 - accuracy: 0.8013 - val_loss: 1.0667 - val_accuracy: 0.4443
Epoch 41/200
3776/4131 [==========================&gt;...] - ETA: 0s - loss: 0.3555 - accuracy: 0.8456
Epoch 00041: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 68us/sample - loss: 0.3559 - accuracy: 0.8451 - val_loss: 0.7159 - val_accuracy: 0.6002
Epoch 42/200
3840/4131 [==========================&gt;...] - ETA: 0s - loss: 0.4002 - accuracy: 0.8391
Epoch 00042: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 67us/sample - loss: 0.3984 - accuracy: 0.8376 - val_loss: 0.5863 - val_accuracy: 0.7212
Epoch 43/200
3904/4131 [===========================&gt;..] - ETA: 0s - loss: 0.4893 - accuracy: 0.8074
Epoch 00043: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 62us/sample - loss: 0.4805 - accuracy: 0.8092 - val_loss: 0.3317 - val_accuracy: 0.8606
Epoch 44/200
3488/4131 [========================&gt;.....] - ETA: 0s - loss: 0.3718 - accuracy: 0.8306
Epoch 00044: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 57us/sample - loss: 0.3704 - accuracy: 0.8313 - val_loss: 0.2908 - val_accuracy: 0.8945
Epoch 45/200
3360/4131 [=======================&gt;......] - ETA: 0s - loss: 0.3435 - accuracy: 0.8527
Epoch 00045: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 71us/sample - loss: 0.3444 - accuracy: 0.8511 - val_loss: 0.2735 - val_accuracy: 0.8916
Epoch 46/200
3680/4131 [=========================&gt;....] - ETA: 0s - loss: 0.3445 - accuracy: 0.8438
Epoch 00046: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 51us/sample - loss: 0.3461 - accuracy: 0.8429 - val_loss: 0.4283 - val_accuracy: 0.7880
Epoch 47/200
3776/4131 [==========================&gt;...] - ETA: 0s - loss: 0.3169 - accuracy: 0.8631
Epoch 00047: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 52us/sample - loss: 0.3252 - accuracy: 0.8608 - val_loss: 0.6365 - val_accuracy: 0.6786
Epoch 48/200
3872/4131 [===========================&gt;..] - ETA: 0s - loss: 0.4311 - accuracy: 0.8223
Epoch 00048: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 49us/sample - loss: 0.4244 - accuracy: 0.8245 - val_loss: 0.3809 - val_accuracy: 0.8306
Epoch 49/200
3968/4131 [===========================&gt;..] - ETA: 0s - loss: 0.3300 - accuracy: 0.8574
Epoch 00049: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 49us/sample - loss: 0.3342 - accuracy: 0.8560 - val_loss: 0.3128 - val_accuracy: 0.8790
Epoch 50/200
3680/4131 [=========================&gt;....] - ETA: 0s - loss: 0.3475 - accuracy: 0.8519
Epoch 00050: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 51us/sample - loss: 0.3430 - accuracy: 0.8526 - val_loss: 0.3710 - val_accuracy: 0.8412
Epoch 51/200
3936/4131 [===========================&gt;..] - ETA: 0s - loss: 0.3063 - accuracy: 0.8709
Epoch 00051: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 48us/sample - loss: 0.3065 - accuracy: 0.8688 - val_loss: 0.2883 - val_accuracy: 0.8887
Epoch 52/200
3808/4131 [==========================&gt;...] - ETA: 0s - loss: 0.3547 - accuracy: 0.8466
Epoch 00052: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 50us/sample - loss: 0.3525 - accuracy: 0.8477 - val_loss: 1.0212 - val_accuracy: 0.5421
Epoch 53/200
3904/4131 [===========================&gt;..] - ETA: 0s - loss: 0.3274 - accuracy: 0.8635
Epoch 00053: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 49us/sample - loss: 0.3264 - accuracy: 0.8637 - val_loss: 0.7896 - val_accuracy: 0.5808
Epoch 54/200
3872/4131 [===========================&gt;..] - ETA: 0s - loss: 0.3398 - accuracy: 0.8487
Epoch 00054: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 50us/sample - loss: 0.3357 - accuracy: 0.8511 - val_loss: 0.2743 - val_accuracy: 0.8877
Epoch 55/200
3840/4131 [==========================&gt;...] - ETA: 0s - loss: 0.2932 - accuracy: 0.8781
Epoch 00055: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 50us/sample - loss: 0.2947 - accuracy: 0.8770 - val_loss: 0.3167 - val_accuracy: 0.8742
Epoch 56/200
3936/4131 [===========================&gt;..] - ETA: 0s - loss: 0.3085 - accuracy: 0.8727
Epoch 00056: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 49us/sample - loss: 0.3053 - accuracy: 0.8746 - val_loss: 0.1868 - val_accuracy: 0.9400
Epoch 57/200
3584/4131 [=========================&gt;....] - ETA: 0s - loss: 0.2826 - accuracy: 0.8800
Epoch 00057: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 52us/sample - loss: 0.2902 - accuracy: 0.8768 - val_loss: 0.9045 - val_accuracy: 0.6302
Epoch 58/200
3968/4131 [===========================&gt;..] - ETA: 0s - loss: 0.3055 - accuracy: 0.8649
Epoch 00058: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 49us/sample - loss: 0.3071 - accuracy: 0.8640 - val_loss: 0.1830 - val_accuracy: 0.9477
Epoch 59/200
3872/4131 [===========================&gt;..] - ETA: 0s - loss: 0.3154 - accuracy: 0.8667
Epoch 00059: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 50us/sample - loss: 0.3130 - accuracy: 0.8671 - val_loss: 0.2553 - val_accuracy: 0.9109
Epoch 60/200
3776/4131 [==========================&gt;...] - ETA: 0s - loss: 0.3483 - accuracy: 0.8549
Epoch 00060: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 50us/sample - loss: 0.3448 - accuracy: 0.8565 - val_loss: 0.3282 - val_accuracy: 0.8703
Epoch 61/200
3968/4131 [===========================&gt;..] - ETA: 0s - loss: 0.3215 - accuracy: 0.8621
Epoch 00061: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 48us/sample - loss: 0.3255 - accuracy: 0.8601 - val_loss: 0.5143 - val_accuracy: 0.7289
Epoch 62/200
3648/4131 [=========================&gt;....] - ETA: 0s - loss: 0.2833 - accuracy: 0.8802
Epoch 00062: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 51us/sample - loss: 0.2923 - accuracy: 0.8761 - val_loss: 0.2902 - val_accuracy: 0.8867
Epoch 63/200
3904/4131 [===========================&gt;..] - ETA: 0s - loss: 0.3139 - accuracy: 0.8691
Epoch 00063: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 49us/sample - loss: 0.3099 - accuracy: 0.8705 - val_loss: 0.5981 - val_accuracy: 0.6941
Epoch 64/200
3808/4131 [==========================&gt;...] - ETA: 0s - loss: 0.2905 - accuracy: 0.8747
Epoch 00064: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 50us/sample - loss: 0.3023 - accuracy: 0.8695 - val_loss: 0.1370 - val_accuracy: 0.9642
Epoch 65/200
3936/4131 [===========================&gt;..] - ETA: 0s - loss: 0.2786 - accuracy: 0.8796
Epoch 00065: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 48us/sample - loss: 0.2828 - accuracy: 0.8763 - val_loss: 0.2483 - val_accuracy: 0.9051
Epoch 66/200
3968/4131 [===========================&gt;..] - ETA: 0s - loss: 0.3058 - accuracy: 0.8697
Epoch 00066: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 49us/sample - loss: 0.3060 - accuracy: 0.8690 - val_loss: 0.1659 - val_accuracy: 0.9603
Epoch 67/200
3680/4131 [=========================&gt;....] - ETA: 0s - loss: 0.2790 - accuracy: 0.8878
Epoch 00067: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 51us/sample - loss: 0.2776 - accuracy: 0.8891 - val_loss: 0.7107 - val_accuracy: 0.6244
Epoch 68/200
3456/4131 [========================&gt;.....] - ETA: 0s - loss: 0.2637 - accuracy: 0.8825
Epoch 00068: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 53us/sample - loss: 0.2648 - accuracy: 0.8819 - val_loss: 0.6130 - val_accuracy: 0.7057
Epoch 69/200
3424/4131 [=======================&gt;......] - ETA: 0s - loss: 0.2869 - accuracy: 0.8782
Epoch 00069: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 54us/sample - loss: 0.2968 - accuracy: 0.8758 - val_loss: 0.3862 - val_accuracy: 0.8267
Epoch 70/200
3904/4131 [===========================&gt;..] - ETA: 0s - loss: 0.2879 - accuracy: 0.8763
Epoch 00070: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 49us/sample - loss: 0.2856 - accuracy: 0.8773 - val_loss: 0.4332 - val_accuracy: 0.8015
Epoch 71/200
3872/4131 [===========================&gt;..] - ETA: 0s - loss: 0.2783 - accuracy: 0.8799
Epoch 00071: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 50us/sample - loss: 0.2810 - accuracy: 0.8790 - val_loss: 0.2493 - val_accuracy: 0.9080
Epoch 72/200
3904/4131 [===========================&gt;..] - ETA: 0s - loss: 0.2610 - accuracy: 0.8852
Epoch 00072: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 49us/sample - loss: 0.2619 - accuracy: 0.8862 - val_loss: 0.3393 - val_accuracy: 0.8761
Epoch 73/200
3904/4131 [===========================&gt;..] - ETA: 0s - loss: 0.2807 - accuracy: 0.8753
Epoch 00073: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 49us/sample - loss: 0.2804 - accuracy: 0.8753 - val_loss: 0.1105 - val_accuracy: 0.9748
Epoch 74/200
3776/4131 [==========================&gt;...] - ETA: 0s - loss: 0.2783 - accuracy: 0.8867
Epoch 00074: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 50us/sample - loss: 0.2766 - accuracy: 0.8874 - val_loss: 0.3555 - val_accuracy: 0.8577
Epoch 75/200
3968/4131 [===========================&gt;..] - ETA: 0s - loss: 0.2823 - accuracy: 0.8810
Epoch 00075: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 48us/sample - loss: 0.2835 - accuracy: 0.8802 - val_loss: 0.2674 - val_accuracy: 0.8838
Epoch 76/200
3744/4131 [==========================&gt;...] - ETA: 0s - loss: 0.2495 - accuracy: 0.8913
Epoch 00076: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 51us/sample - loss: 0.2579 - accuracy: 0.8884 - val_loss: 0.1972 - val_accuracy: 0.9400
Epoch 77/200
4032/4131 [============================&gt;.] - ETA: 0s - loss: 0.2414 - accuracy: 0.8983
Epoch 00077: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 47us/sample - loss: 0.2413 - accuracy: 0.8983 - val_loss: 0.2185 - val_accuracy: 0.9119
Epoch 78/200
3968/4131 [===========================&gt;..] - ETA: 0s - loss: 0.2637 - accuracy: 0.8871
Epoch 00078: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 49us/sample - loss: 0.2618 - accuracy: 0.8872 - val_loss: 0.1409 - val_accuracy: 0.9603
Epoch 79/200
3776/4131 [==========================&gt;...] - ETA: 0s - loss: 0.2172 - accuracy: 0.9084
Epoch 00079: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 50us/sample - loss: 0.2212 - accuracy: 0.9075 - val_loss: 0.1114 - val_accuracy: 0.9710
Epoch 80/200
4128/4131 [============================&gt;.] - ETA: 0s - loss: 0.2967 - accuracy: 0.8762
Epoch 00080: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 46us/sample - loss: 0.2966 - accuracy: 0.8763 - val_loss: 0.4111 - val_accuracy: 0.8374
Epoch 81/200
3680/4131 [=========================&gt;....] - ETA: 0s - loss: 0.2390 - accuracy: 0.8957
Epoch 00081: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 52us/sample - loss: 0.2411 - accuracy: 0.8962 - val_loss: 0.5499 - val_accuracy: 0.7348
Epoch 82/200
3936/4131 [===========================&gt;..] - ETA: 0s - loss: 0.2415 - accuracy: 0.9014
Epoch 00082: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 48us/sample - loss: 0.2415 - accuracy: 0.9017 - val_loss: 0.3615 - val_accuracy: 0.8480
Epoch 83/200
3872/4131 [===========================&gt;..] - ETA: 0s - loss: 0.3131 - accuracy: 0.8673
Epoch 00083: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 50us/sample - loss: 0.3077 - accuracy: 0.8690 - val_loss: 0.6296 - val_accuracy: 0.6989
Epoch 84/200
3712/4131 [=========================&gt;....] - ETA: 0s - loss: 0.2526 - accuracy: 0.8952
Epoch 00084: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 51us/sample - loss: 0.2554 - accuracy: 0.8932 - val_loss: 0.4602 - val_accuracy: 0.7803
Epoch 85/200
3936/4131 [===========================&gt;..] - ETA: 0s - loss: 0.2158 - accuracy: 0.9093
Epoch 00085: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 48us/sample - loss: 0.2158 - accuracy: 0.9095 - val_loss: 0.3781 - val_accuracy: 0.8306
Epoch 86/200
3616/4131 [=========================&gt;....] - ETA: 0s - loss: 0.2123 - accuracy: 0.9112
Epoch 00086: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 52us/sample - loss: 0.2117 - accuracy: 0.9114 - val_loss: 0.1481 - val_accuracy: 0.9497
Epoch 87/200
3744/4131 [==========================&gt;...] - ETA: 0s - loss: 0.2311 - accuracy: 0.8988
Epoch 00087: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 50us/sample - loss: 0.2316 - accuracy: 0.8983 - val_loss: 0.5504 - val_accuracy: 0.7299
Epoch 88/200
3840/4131 [==========================&gt;...] - ETA: 0s - loss: 0.2629 - accuracy: 0.8935
Epoch 00088: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 50us/sample - loss: 0.2629 - accuracy: 0.8932 - val_loss: 0.1962 - val_accuracy: 0.9264
Epoch 89/200
3712/4131 [=========================&gt;....] - ETA: 0s - loss: 0.3785 - accuracy: 0.8421
Epoch 00089: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 51us/sample - loss: 0.3636 - accuracy: 0.8489 - val_loss: 0.4696 - val_accuracy: 0.7822
Epoch 90/200
3968/4131 [===========================&gt;..] - ETA: 0s - loss: 0.2574 - accuracy: 0.8931
Epoch 00090: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 48us/sample - loss: 0.2573 - accuracy: 0.8916 - val_loss: 0.2194 - val_accuracy: 0.9197
Epoch 91/200
3712/4131 [=========================&gt;....] - ETA: 0s - loss: 0.2117 - accuracy: 0.9124
Epoch 00091: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 51us/sample - loss: 0.2106 - accuracy: 0.9138 - val_loss: 0.6700 - val_accuracy: 0.6583
Epoch 92/200
3904/4131 [===========================&gt;..] - ETA: 0s - loss: 0.2664 - accuracy: 0.8876
Epoch 00092: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 48us/sample - loss: 0.2627 - accuracy: 0.8901 - val_loss: 0.4857 - val_accuracy: 0.7783
Epoch 93/200
3840/4131 [==========================&gt;...] - ETA: 0s - loss: 0.2077 - accuracy: 0.9146
Epoch 00093: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 50us/sample - loss: 0.2055 - accuracy: 0.9155 - val_loss: 0.2527 - val_accuracy: 0.8993
Epoch 94/200
3936/4131 [===========================&gt;..] - ETA: 0s - loss: 0.2229 - accuracy: 0.9090
Epoch 00094: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 52us/sample - loss: 0.2207 - accuracy: 0.9107 - val_loss: 0.1355 - val_accuracy: 0.9545
Epoch 95/200
3936/4131 [===========================&gt;..] - ETA: 0s - loss: 0.1831 - accuracy: 0.9261
Epoch 00095: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 50us/sample - loss: 0.1830 - accuracy: 0.9269 - val_loss: 0.2402 - val_accuracy: 0.9022
Epoch 96/200
3840/4131 [==========================&gt;...] - ETA: 0s - loss: 0.2025 - accuracy: 0.9164
Epoch 00096: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 50us/sample - loss: 0.2038 - accuracy: 0.9158 - val_loss: 0.3504 - val_accuracy: 0.8354
Epoch 97/200
4096/4131 [============================&gt;.] - ETA: 0s - loss: 0.2037 - accuracy: 0.9172
Epoch 00097: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 47us/sample - loss: 0.2036 - accuracy: 0.9172 - val_loss: 0.3069 - val_accuracy: 0.8616
Epoch 98/200
3936/4131 [===========================&gt;..] - ETA: 0s - loss: 0.2690 - accuracy: 0.8989
Epoch 00098: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 49us/sample - loss: 0.2651 - accuracy: 0.8998 - val_loss: 0.3037 - val_accuracy: 0.8606
Epoch 99/200
4032/4131 [============================&gt;.] - ETA: 0s - loss: 0.2151 - accuracy: 0.9144
Epoch 00099: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 47us/sample - loss: 0.2155 - accuracy: 0.9136 - val_loss: 0.1970 - val_accuracy: 0.9351
Epoch 100/200
4064/4131 [============================&gt;.] - ETA: 0s - loss: 0.2661 - accuracy: 0.8920
Epoch 00100: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 49us/sample - loss: 0.2649 - accuracy: 0.8925 - val_loss: 0.1854 - val_accuracy: 0.9361
Epoch 101/200
3648/4131 [=========================&gt;....] - ETA: 0s - loss: 0.1618 - accuracy: 0.9389
Epoch 00101: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 52us/sample - loss: 0.1631 - accuracy: 0.9383 - val_loss: 0.2692 - val_accuracy: 0.8955
Epoch 102/200
4032/4131 [============================&gt;.] - ETA: 0s - loss: 0.1788 - accuracy: 0.9281
Epoch 00102: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 47us/sample - loss: 0.1770 - accuracy: 0.9291 - val_loss: 0.2900 - val_accuracy: 0.8722
Epoch 103/200
3808/4131 [==========================&gt;...] - ETA: 0s - loss: 0.1795 - accuracy: 0.9259
Epoch 00103: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 50us/sample - loss: 0.1914 - accuracy: 0.9218 - val_loss: 0.8499 - val_accuracy: 0.6108
Epoch 104/200
4064/4131 [============================&gt;.] - ETA: 0s - loss: 0.2315 - accuracy: 0.9001
Epoch 00104: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 47us/sample - loss: 0.2302 - accuracy: 0.9008 - val_loss: 0.2935 - val_accuracy: 0.8751
Epoch 105/200
4032/4131 [============================&gt;.] - ETA: 0s - loss: 0.1469 - accuracy: 0.9422
Epoch 00105: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 48us/sample - loss: 0.1464 - accuracy: 0.9424 - val_loss: 0.3341 - val_accuracy: 0.8587
Epoch 106/200
3968/4131 [===========================&gt;..] - ETA: 0s - loss: 0.1409 - accuracy: 0.9466
Epoch 00106: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 48us/sample - loss: 0.1446 - accuracy: 0.9450 - val_loss: 0.1876 - val_accuracy: 0.9274
Epoch 107/200
4064/4131 [============================&gt;.] - ETA: 0s - loss: 0.3176 - accuracy: 0.8743
Epoch 00107: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 48us/sample - loss: 0.3144 - accuracy: 0.8751 - val_loss: 0.3941 - val_accuracy: 0.8267
Epoch 108/200
3360/4131 [=======================&gt;......] - ETA: 0s - loss: 0.2047 - accuracy: 0.9164
Epoch 00108: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 55us/sample - loss: 0.2007 - accuracy: 0.9189 - val_loss: 0.3063 - val_accuracy: 0.8742
Epoch 109/200
4096/4131 [============================&gt;.] - ETA: 0s - loss: 0.1745 - accuracy: 0.9280
Epoch 00109: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 47us/sample - loss: 0.1751 - accuracy: 0.9276 - val_loss: 0.2652 - val_accuracy: 0.8683
Epoch 110/200
4032/4131 [============================&gt;.] - ETA: 0s - loss: 0.1970 - accuracy: 0.9194
Epoch 00110: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 48us/sample - loss: 0.1981 - accuracy: 0.9191 - val_loss: 0.1894 - val_accuracy: 0.9361
Epoch 111/200
3968/4131 [===========================&gt;..] - ETA: 0s - loss: 0.1873 - accuracy: 0.9211
Epoch 00111: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 48us/sample - loss: 0.1869 - accuracy: 0.9216 - val_loss: 0.2691 - val_accuracy: 0.8703
Epoch 112/200
3936/4131 [===========================&gt;..] - ETA: 0s - loss: 0.1526 - accuracy: 0.9413
Epoch 00112: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 48us/sample - loss: 0.1508 - accuracy: 0.9417 - val_loss: 0.1058 - val_accuracy: 0.9758
Epoch 113/200
3744/4131 [==========================&gt;...] - ETA: 0s - loss: 0.1544 - accuracy: 0.9412
Epoch 00113: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 50us/sample - loss: 0.1534 - accuracy: 0.9409 - val_loss: 0.3641 - val_accuracy: 0.8470
Epoch 114/200
4064/4131 [============================&gt;.] - ETA: 0s - loss: 0.1830 - accuracy: 0.9250
Epoch 00114: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 47us/sample - loss: 0.1816 - accuracy: 0.9252 - val_loss: 0.1539 - val_accuracy: 0.9429
Epoch 115/200
3648/4131 [=========================&gt;....] - ETA: 0s - loss: 0.1764 - accuracy: 0.9265
Epoch 00115: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 56us/sample - loss: 0.1756 - accuracy: 0.9274 - val_loss: 0.2785 - val_accuracy: 0.8761
Epoch 116/200
3840/4131 [==========================&gt;...] - ETA: 0s - loss: 0.1480 - accuracy: 0.9383
Epoch 00116: val_loss did not improve from 0.09174
4131/4131 [==============================] - 0s 70us/sample - loss: 0.1503 - accuracy: 0.9383 - val_loss: 0.2377 - val_accuracy: 0.9061
Epoch 117/200
3584/4131 [=========================&gt;....] - ETA: 0s - loss: 0.1600 - accuracy: 0.9314 ETA: 0s - loss: 0.1735 - accuracy: 0.
Epoch 00117: val_loss improved from 0.09174 to 0.05836, saving model to best_model_2.h5
4131/4131 [==============================] - 0s 77us/sample - loss: 0.1530 - accuracy: 0.9346 - val_loss: 0.0584 - val_accuracy: 0.9864
Epoch 118/200
3872/4131 [===========================&gt;..] - ETA: 0s - loss: 0.1470 - accuracy: 0.9442
Epoch 00118: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 64us/sample - loss: 0.1478 - accuracy: 0.9448 - val_loss: 0.2305 - val_accuracy: 0.9051
Epoch 119/200
3200/4131 [======================&gt;.......] - ETA: 0s - loss: 0.2054 - accuracy: 0.9166
Epoch 00119: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 55us/sample - loss: 0.1932 - accuracy: 0.9201 - val_loss: 0.1681 - val_accuracy: 0.9313
Epoch 120/200
3360/4131 [=======================&gt;......] - ETA: 0s - loss: 0.1369 - accuracy: 0.9449
Epoch 00120: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 60us/sample - loss: 0.1332 - accuracy: 0.9475 - val_loss: 0.1457 - val_accuracy: 0.9468
Epoch 121/200
3904/4131 [===========================&gt;..] - ETA: 0s - loss: 0.1547 - accuracy: 0.9367
Epoch 00121: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 63us/sample - loss: 0.1539 - accuracy: 0.9380 - val_loss: 0.6221 - val_accuracy: 0.7483
Epoch 122/200
3808/4131 [==========================&gt;...] - ETA: 0s - loss: 0.1446 - accuracy: 0.9412
Epoch 00122: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 49us/sample - loss: 0.1444 - accuracy: 0.9412 - val_loss: 0.1232 - val_accuracy: 0.9584
Epoch 123/200
3808/4131 [==========================&gt;...] - ETA: 0s - loss: 0.1887 - accuracy: 0.9194
Epoch 00123: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 50us/sample - loss: 0.1844 - accuracy: 0.9221 - val_loss: 0.1771 - val_accuracy: 0.9303
Epoch 124/200
3968/4131 [===========================&gt;..] - ETA: 0s - loss: 0.2175 - accuracy: 0.9115
Epoch 00124: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 49us/sample - loss: 0.2184 - accuracy: 0.9104 - val_loss: 0.9489 - val_accuracy: 0.6534
Epoch 125/200
4000/4131 [============================&gt;.] - ETA: 0s - loss: 0.1927 - accuracy: 0.9275
Epoch 00125: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 48us/sample - loss: 0.1890 - accuracy: 0.9288 - val_loss: 0.2562 - val_accuracy: 0.8887
Epoch 126/200
3776/4131 [==========================&gt;...] - ETA: 0s - loss: 0.1390 - accuracy: 0.9441
Epoch 00126: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 50us/sample - loss: 0.1373 - accuracy: 0.9448 - val_loss: 0.0855 - val_accuracy: 0.9787
Epoch 127/200
4000/4131 [============================&gt;.] - ETA: 0s - loss: 0.1353 - accuracy: 0.9480
Epoch 00127: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 47us/sample - loss: 0.1361 - accuracy: 0.9477 - val_loss: 0.0916 - val_accuracy: 0.9681
Epoch 128/200
3872/4131 [===========================&gt;..] - ETA: 0s - loss: 0.1462 - accuracy: 0.9411
Epoch 00128: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 49us/sample - loss: 0.1440 - accuracy: 0.9421 - val_loss: 0.1283 - val_accuracy: 0.9555
Epoch 129/200
4000/4131 [============================&gt;.] - ETA: 0s - loss: 0.1833 - accuracy: 0.9315
Epoch 00129: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 48us/sample - loss: 0.1808 - accuracy: 0.9329 - val_loss: 0.0860 - val_accuracy: 0.9700
Epoch 130/200
4064/4131 [============================&gt;.] - ETA: 0s - loss: 0.1464 - accuracy: 0.9459
Epoch 00130: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 48us/sample - loss: 0.1477 - accuracy: 0.9450 - val_loss: 0.3426 - val_accuracy: 0.8606
Epoch 131/200
3808/4131 [==========================&gt;...] - ETA: 0s - loss: 0.1660 - accuracy: 0.9309
Epoch 00131: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 50us/sample - loss: 0.1647 - accuracy: 0.9327 - val_loss: 0.3660 - val_accuracy: 0.8616
Epoch 132/200
4032/4131 [============================&gt;.] - ETA: 0s - loss: 0.1356 - accuracy: 0.9464
Epoch 00132: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 47us/sample - loss: 0.1348 - accuracy: 0.9472 - val_loss: 0.1690 - val_accuracy: 0.9390
Epoch 133/200
3776/4131 [==========================&gt;...] - ETA: 0s - loss: 0.1065 - accuracy: 0.9582
Epoch 00133: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 52us/sample - loss: 0.1095 - accuracy: 0.9562 - val_loss: 0.3017 - val_accuracy: 0.8693
Epoch 134/200
3968/4131 [===========================&gt;..] - ETA: 0s - loss: 0.1013 - accuracy: 0.9647
Epoch 00134: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 48us/sample - loss: 0.1020 - accuracy: 0.9649 - val_loss: 0.1177 - val_accuracy: 0.9535
Epoch 135/200
4032/4131 [============================&gt;.] - ETA: 0s - loss: 0.1584 - accuracy: 0.9392
Epoch 00135: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 48us/sample - loss: 0.1580 - accuracy: 0.9395 - val_loss: 0.0759 - val_accuracy: 0.9768
Epoch 136/200
3936/4131 [===========================&gt;..] - ETA: 0s - loss: 0.1556 - accuracy: 0.9347
Epoch 00136: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 48us/sample - loss: 0.1547 - accuracy: 0.9351 - val_loss: 0.1129 - val_accuracy: 0.9584
Epoch 137/200
2784/4131 [===================&gt;..........] - ETA: 0s - loss: 0.1274 - accuracy: 0.9483
Epoch 00137: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 46us/sample - loss: 0.1198 - accuracy: 0.9526 - val_loss: 0.1198 - val_accuracy: 0.9535
Epoch 138/200
3808/4131 [==========================&gt;...] - ETA: 0s - loss: 0.1023 - accuracy: 0.9619
Epoch 00138: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 50us/sample - loss: 0.1018 - accuracy: 0.9622 - val_loss: 0.1824 - val_accuracy: 0.9322
Epoch 139/200
4032/4131 [============================&gt;.] - ETA: 0s - loss: 0.1406 - accuracy: 0.9437
Epoch 00139: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 47us/sample - loss: 0.1390 - accuracy: 0.9448 - val_loss: 0.2346 - val_accuracy: 0.8935
Epoch 140/200
4064/4131 [============================&gt;.] - ETA: 0s - loss: 0.1527 - accuracy: 0.9432
Epoch 00140: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 48us/sample - loss: 0.1519 - accuracy: 0.9436 - val_loss: 0.2176 - val_accuracy: 0.9080
Epoch 141/200
3872/4131 [===========================&gt;..] - ETA: 0s - loss: 0.1729 - accuracy: 0.9329
Epoch 00141: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 49us/sample - loss: 0.1728 - accuracy: 0.9327 - val_loss: 0.1653 - val_accuracy: 0.9380
Epoch 142/200
3360/4131 [=======================&gt;......] - ETA: 0s - loss: 0.1418 - accuracy: 0.9432
Epoch 00142: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 54us/sample - loss: 0.1342 - accuracy: 0.9475 - val_loss: 0.4580 - val_accuracy: 0.7909
Epoch 143/200
3808/4131 [==========================&gt;...] - ETA: 0s - loss: 0.0997 - accuracy: 0.9567
Epoch 00143: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 51us/sample - loss: 0.0987 - accuracy: 0.9579 - val_loss: 0.6913 - val_accuracy: 0.7164
Epoch 144/200
4032/4131 [============================&gt;.] - ETA: 0s - loss: 0.1153 - accuracy: 0.9571
Epoch 00144: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 47us/sample - loss: 0.1141 - accuracy: 0.9574 - val_loss: 0.1996 - val_accuracy: 0.9187
Epoch 145/200
4032/4131 [============================&gt;.] - ETA: 0s - loss: 0.2417 - accuracy: 0.9055
Epoch 00145: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 48us/sample - loss: 0.2417 - accuracy: 0.9056 - val_loss: 0.1057 - val_accuracy: 0.9652
Epoch 146/200
3936/4131 [===========================&gt;..] - ETA: 0s - loss: 0.1328 - accuracy: 0.9494
Epoch 00146: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 48us/sample - loss: 0.1319 - accuracy: 0.9501 - val_loss: 0.1633 - val_accuracy: 0.9458
Epoch 147/200
4064/4131 [============================&gt;.] - ETA: 0s - loss: 0.0826 - accuracy: 0.9705
Epoch 00147: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 47us/sample - loss: 0.0838 - accuracy: 0.9697 - val_loss: 0.1665 - val_accuracy: 0.9351
Epoch 148/200
3776/4131 [==========================&gt;...] - ETA: 0s - loss: 0.0951 - accuracy: 0.9632
Epoch 00148: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 50us/sample - loss: 0.0961 - accuracy: 0.9637 - val_loss: 0.4107 - val_accuracy: 0.8209
Epoch 149/200
4000/4131 [============================&gt;.] - ETA: 0s - loss: 0.1157 - accuracy: 0.9542
Epoch 00149: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 48us/sample - loss: 0.1132 - accuracy: 0.9557 - val_loss: 0.1009 - val_accuracy: 0.9593
Epoch 150/200
3968/4131 [===========================&gt;..] - ETA: 0s - loss: 0.0911 - accuracy: 0.9652
Epoch 00150: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 48us/sample - loss: 0.0928 - accuracy: 0.9647 - val_loss: 0.1013 - val_accuracy: 0.9652
Epoch 151/200
4096/4131 [============================&gt;.] - ETA: 0s - loss: 0.0846 - accuracy: 0.9666
Epoch 00151: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 46us/sample - loss: 0.0856 - accuracy: 0.9659 - val_loss: 0.2314 - val_accuracy: 0.9071
Epoch 152/200
4064/4131 [============================&gt;.] - ETA: 0s - loss: 0.0948 - accuracy: 0.9599
Epoch 00152: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 47us/sample - loss: 0.0954 - accuracy: 0.9598 - val_loss: 0.1539 - val_accuracy: 0.9351
Epoch 153/200
3872/4131 [===========================&gt;..] - ETA: 0s - loss: 0.1782 - accuracy: 0.9403
Epoch 00153: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 49us/sample - loss: 0.1741 - accuracy: 0.9409 - val_loss: 0.2065 - val_accuracy: 0.9235
Epoch 154/200
2816/4131 [===================&gt;..........] - ETA: 0s - loss: 0.1345 - accuracy: 0.9499
Epoch 00154: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 45us/sample - loss: 0.1409 - accuracy: 0.9475 - val_loss: 0.3549 - val_accuracy: 0.8693
Epoch 155/200
4000/4131 [============================&gt;.] - ETA: 0s - loss: 0.1393 - accuracy: 0.9475
Epoch 00155: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 48us/sample - loss: 0.1379 - accuracy: 0.9477 - val_loss: 0.2475 - val_accuracy: 0.8848
Epoch 156/200
3968/4131 [===========================&gt;..] - ETA: 0s - loss: 0.1363 - accuracy: 0.9493
Epoch 00156: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 48us/sample - loss: 0.1396 - accuracy: 0.9492 - val_loss: 0.1842 - val_accuracy: 0.9351
Epoch 157/200
4064/4131 [============================&gt;.] - ETA: 0s - loss: 0.0952 - accuracy: 0.9631
Epoch 00157: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 47us/sample - loss: 0.0962 - accuracy: 0.9622 - val_loss: 1.0106 - val_accuracy: 0.5934
Epoch 158/200
3968/4131 [===========================&gt;..] - ETA: 0s - loss: 0.2145 - accuracy: 0.9168
Epoch 00158: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 49us/sample - loss: 0.2089 - accuracy: 0.9189 - val_loss: 0.2660 - val_accuracy: 0.8993
Epoch 159/200
4064/4131 [============================&gt;.] - ETA: 0s - loss: 0.0893 - accuracy: 0.9688
Epoch 00159: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 47us/sample - loss: 0.0896 - accuracy: 0.9683 - val_loss: 0.1598 - val_accuracy: 0.9487
Epoch 160/200
4032/4131 [============================&gt;.] - ETA: 0s - loss: 0.0831 - accuracy: 0.9663
Epoch 00160: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 49us/sample - loss: 0.0840 - accuracy: 0.9659 - val_loss: 0.2316 - val_accuracy: 0.9284
Epoch 161/200
3904/4131 [===========================&gt;..] - ETA: 0s - loss: 0.0759 - accuracy: 0.9741
Epoch 00161: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 49us/sample - loss: 0.0753 - accuracy: 0.9743 - val_loss: 0.3121 - val_accuracy: 0.8761
Epoch 162/200
2816/4131 [===================&gt;..........] - ETA: 0s - loss: 0.0984 - accuracy: 0.9624
Epoch 00162: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 45us/sample - loss: 0.1035 - accuracy: 0.9596 - val_loss: 0.2820 - val_accuracy: 0.8838
Epoch 163/200
3936/4131 [===========================&gt;..] - ETA: 0s - loss: 0.1920 - accuracy: 0.9360
Epoch 00163: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 49us/sample - loss: 0.2147 - accuracy: 0.9303 - val_loss: 0.8364 - val_accuracy: 0.6960
Epoch 164/200
4096/4131 [============================&gt;.] - ETA: 0s - loss: 0.2117 - accuracy: 0.9177
Epoch 00164: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 47us/sample - loss: 0.2110 - accuracy: 0.9179 - val_loss: 0.2324 - val_accuracy: 0.9197
Epoch 165/200
2816/4131 [===================&gt;..........] - ETA: 0s - loss: 0.1609 - accuracy: 0.9318
Epoch 00165: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 46us/sample - loss: 0.1533 - accuracy: 0.9388 - val_loss: 0.1948 - val_accuracy: 0.9148
Epoch 166/200
3840/4131 [==========================&gt;...] - ETA: 0s - loss: 0.0914 - accuracy: 0.9667
Epoch 00166: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 49us/sample - loss: 0.0934 - accuracy: 0.9659 - val_loss: 0.2049 - val_accuracy: 0.9206
Epoch 167/200
2816/4131 [===================&gt;..........] - ETA: 0s - loss: 0.1343 - accuracy: 0.9513
Epoch 00167: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 45us/sample - loss: 0.1204 - accuracy: 0.9555 - val_loss: 0.3014 - val_accuracy: 0.8761
Epoch 168/200
3904/4131 [===========================&gt;..] - ETA: 0s - loss: 0.0793 - accuracy: 0.9716
Epoch 00168: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 49us/sample - loss: 0.0798 - accuracy: 0.9714 - val_loss: 0.2400 - val_accuracy: 0.9003
Epoch 169/200
4064/4131 [============================&gt;.] - ETA: 0s - loss: 0.0827 - accuracy: 0.9710
Epoch 00169: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 48us/sample - loss: 0.0821 - accuracy: 0.9712 - val_loss: 0.1242 - val_accuracy: 0.9574
Epoch 170/200
3808/4131 [==========================&gt;...] - ETA: 0s - loss: 0.0806 - accuracy: 0.9714
Epoch 00170: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 53us/sample - loss: 0.0877 - accuracy: 0.9693 - val_loss: 0.2197 - val_accuracy: 0.9177
Epoch 171/200
3808/4131 [==========================&gt;...] - ETA: 0s - loss: 0.0763 - accuracy: 0.9722
Epoch 00171: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 49us/sample - loss: 0.0739 - accuracy: 0.9734 - val_loss: 0.1438 - val_accuracy: 0.9564
Epoch 172/200
2784/4131 [===================&gt;..........] - ETA: 0s - loss: 0.0688 - accuracy: 0.9741
Epoch 00172: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 45us/sample - loss: 0.0744 - accuracy: 0.9705 - val_loss: 0.1949 - val_accuracy: 0.9119
Epoch 173/200
4000/4131 [============================&gt;.] - ETA: 0s - loss: 0.0750 - accuracy: 0.9700
Epoch 00173: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 48us/sample - loss: 0.0777 - accuracy: 0.9688 - val_loss: 0.5107 - val_accuracy: 0.7967
Epoch 174/200
4096/4131 [============================&gt;.] - ETA: 0s - loss: 0.1068 - accuracy: 0.9597
Epoch 00174: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 47us/sample - loss: 0.1062 - accuracy: 0.9601 - val_loss: 0.0620 - val_accuracy: 0.9787
Epoch 175/200
4128/4131 [============================&gt;.] - ETA: 0s - loss: 0.0841 - accuracy: 0.9685
Epoch 00175: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 46us/sample - loss: 0.0841 - accuracy: 0.9685 - val_loss: 0.3643 - val_accuracy: 0.8742
Epoch 176/200
3712/4131 [=========================&gt;....] - ETA: 0s - loss: 0.1532 - accuracy: 0.9397
Epoch 00176: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 51us/sample - loss: 0.1487 - accuracy: 0.9421 - val_loss: 0.1975 - val_accuracy: 0.9206
Epoch 177/200
4032/4131 [============================&gt;.] - ETA: 0s - loss: 0.1459 - accuracy: 0.9430
Epoch 00177: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 48us/sample - loss: 0.1454 - accuracy: 0.9434 - val_loss: 0.1808 - val_accuracy: 0.9419
Epoch 178/200
4000/4131 [============================&gt;.] - ETA: 0s - loss: 0.1443 - accuracy: 0.9488
Epoch 00178: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 48us/sample - loss: 0.1428 - accuracy: 0.9496 - val_loss: 0.2986 - val_accuracy: 0.8790
Epoch 179/200
4000/4131 [============================&gt;.] - ETA: 0s - loss: 0.0659 - accuracy: 0.9730
Epoch 00179: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 47us/sample - loss: 0.0668 - accuracy: 0.9726 - val_loss: 0.1634 - val_accuracy: 0.9439
Epoch 180/200
2848/4131 [===================&gt;..........] - ETA: 0s - loss: 0.1323 - accuracy: 0.9431
Epoch 00180: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 46us/sample - loss: 0.1300 - accuracy: 0.9460 - val_loss: 0.0784 - val_accuracy: 0.9768
Epoch 181/200
3840/4131 [==========================&gt;...] - ETA: 0s - loss: 0.0667 - accuracy: 0.9789
Epoch 00181: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 49us/sample - loss: 0.0657 - accuracy: 0.9792 - val_loss: 0.1706 - val_accuracy: 0.9351
Epoch 182/200
4032/4131 [============================&gt;.] - ETA: 0s - loss: 0.0702 - accuracy: 0.9747
Epoch 00182: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 47us/sample - loss: 0.0703 - accuracy: 0.9746 - val_loss: 0.1114 - val_accuracy: 0.9652
Epoch 183/200
3872/4131 [===========================&gt;..] - ETA: 0s - loss: 0.1007 - accuracy: 0.9602
Epoch 00183: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 50us/sample - loss: 0.1012 - accuracy: 0.9601 - val_loss: 0.1727 - val_accuracy: 0.9342
Epoch 184/200
3904/4131 [===========================&gt;..] - ETA: 0s - loss: 0.1529 - accuracy: 0.9424
Epoch 00184: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 49us/sample - loss: 0.1509 - accuracy: 0.9424 - val_loss: 0.3237 - val_accuracy: 0.8596
Epoch 185/200
4064/4131 [============================&gt;.] - ETA: 0s - loss: 0.0805 - accuracy: 0.9685
Epoch 00185: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 48us/sample - loss: 0.0815 - accuracy: 0.9683 - val_loss: 0.0994 - val_accuracy: 0.9739
Epoch 186/200
3840/4131 [==========================&gt;...] - ETA: 0s - loss: 0.0638 - accuracy: 0.9755
Epoch 00186: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 49us/sample - loss: 0.0623 - accuracy: 0.9763 - val_loss: 0.1016 - val_accuracy: 0.9652
Epoch 187/200
4032/4131 [============================&gt;.] - ETA: 0s - loss: 0.0859 - accuracy: 0.9663
Epoch 00187: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 47us/sample - loss: 0.0844 - accuracy: 0.9671 - val_loss: 0.0964 - val_accuracy: 0.9622
Epoch 188/200
3904/4131 [===========================&gt;..] - ETA: 0s - loss: 0.0810 - accuracy: 0.9688
Epoch 00188: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 49us/sample - loss: 0.0822 - accuracy: 0.9678 - val_loss: 0.3469 - val_accuracy: 0.8645
Epoch 189/200
4096/4131 [============================&gt;.] - ETA: 0s - loss: 0.1196 - accuracy: 0.9531
Epoch 00189: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 47us/sample - loss: 0.1198 - accuracy: 0.9530 - val_loss: 0.3568 - val_accuracy: 0.8345
Epoch 190/200
2816/4131 [===================&gt;..........] - ETA: 0s - loss: 0.1337 - accuracy: 0.9471
Epoch 00190: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 45us/sample - loss: 0.1508 - accuracy: 0.9419 - val_loss: 0.2751 - val_accuracy: 0.9167
Epoch 191/200
3936/4131 [===========================&gt;..] - ETA: 0s - loss: 0.1903 - accuracy: 0.9319
Epoch 00191: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 48us/sample - loss: 0.1853 - accuracy: 0.9337 - val_loss: 0.1512 - val_accuracy: 0.9477
Epoch 192/200
2784/4131 [===================&gt;..........] - ETA: 0s - loss: 0.0485 - accuracy: 0.9842
Epoch 00192: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 45us/sample - loss: 0.0566 - accuracy: 0.9806 - val_loss: 0.2074 - val_accuracy: 0.9187
Epoch 193/200
3552/4131 [========================&gt;.....] - ETA: 0s - loss: 0.0702 - accuracy: 0.9755
Epoch 00193: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 57us/sample - loss: 0.0689 - accuracy: 0.9753 - val_loss: 0.2597 - val_accuracy: 0.8974
Epoch 194/200
4096/4131 [============================&gt;.] - ETA: 0s - loss: 0.0465 - accuracy: 0.9839
Epoch 00194: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 63us/sample - loss: 0.0468 - accuracy: 0.9835 - val_loss: 0.1166 - val_accuracy: 0.9613
Epoch 195/200
3904/4131 [===========================&gt;..] - ETA: 0s - loss: 0.0699 - accuracy: 0.9736
Epoch 00195: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 66us/sample - loss: 0.0683 - accuracy: 0.9741 - val_loss: 0.1673 - val_accuracy: 0.9371
Epoch 196/200
4032/4131 [============================&gt;.] - ETA: 0s - loss: 0.0648 - accuracy: 0.9747
Epoch 00196: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 64us/sample - loss: 0.0653 - accuracy: 0.9746 - val_loss: 0.0806 - val_accuracy: 0.9710
Epoch 197/200
3392/4131 [=======================&gt;......] - ETA: 0s - loss: 0.0684 - accuracy: 0.9767
Epoch 00197: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 53us/sample - loss: 0.0746 - accuracy: 0.9741 - val_loss: 0.1918 - val_accuracy: 0.9264
Epoch 198/200
3552/4131 [========================&gt;.....] - ETA: 0s - loss: 0.2272 - accuracy: 0.9229
Epoch 00198: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 56us/sample - loss: 0.2210 - accuracy: 0.9237 - val_loss: 0.7216 - val_accuracy: 0.7076
Epoch 199/200
3904/4131 [===========================&gt;..] - ETA: 0s - loss: 0.1325 - accuracy: 0.9485
Epoch 00199: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 61us/sample - loss: 0.1300 - accuracy: 0.9501 - val_loss: 0.2420 - val_accuracy: 0.9206
Epoch 200/200
4096/4131 [============================&gt;.] - ETA: 0s - loss: 0.2859 - accuracy: 0.9065
Epoch 00200: val_loss did not improve from 0.05836
4131/4131 [==============================] - 0s 47us/sample - loss: 0.2835 - accuracy: 0.9073 - val_loss: 0.1070 - val_accuracy: 0.9690
Testing on Fold 2
# Getting train data set up
# Getting test data set up
WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;, &lt;class &#39;NoneType&#39;&gt;
Train on 2400 samples, validate on 600 samples
Epoch 1/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 194.0505 - mse: 194.0505
Epoch 00001: val_loss did not improve from 111.46321
2400/2400 [==============================] - 2s 875us/sample - loss: 193.3361 - mse: 193.3361 - val_loss: 126.9743 - val_mse: 126.9743
Epoch 2/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 170.0336 - mse: 170.0336
Epoch 00002: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 435us/sample - loss: 170.8361 - mse: 170.8361 - val_loss: 119.1447 - val_mse: 119.1447
Epoch 3/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 179.0253 - mse: 179.0254
Epoch 00003: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 440us/sample - loss: 177.7054 - mse: 177.7054 - val_loss: 115.2302 - val_mse: 115.2302
Epoch 4/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 174.4036 - mse: 174.4037
Epoch 00004: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 446us/sample - loss: 171.4833 - mse: 171.4833 - val_loss: 151.6120 - val_mse: 151.6120
Epoch 5/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 173.6265 - mse: 173.6265
Epoch 00005: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 509us/sample - loss: 171.1681 - mse: 171.1682 - val_loss: 125.0101 - val_mse: 125.0101
Epoch 6/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 171.0731 - mse: 171.0731
Epoch 00006: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 494us/sample - loss: 169.4743 - mse: 169.4743 - val_loss: 128.3587 - val_mse: 128.3587
Epoch 7/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 169.7647 - mse: 169.7647
Epoch 00007: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 455us/sample - loss: 169.1899 - mse: 169.1899 - val_loss: 118.6707 - val_mse: 118.6707
Epoch 8/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 173.0717 - mse: 173.0717
Epoch 00008: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 452us/sample - loss: 171.5817 - mse: 171.5817 - val_loss: 118.5809 - val_mse: 118.5809
Epoch 9/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 164.4070 - mse: 164.4070
Epoch 00009: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 451us/sample - loss: 162.7711 - mse: 162.7711 - val_loss: 114.5641 - val_mse: 114.5640
Epoch 10/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.3219 - mse: 162.3219
Epoch 00010: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 456us/sample - loss: 166.3480 - mse: 166.3480 - val_loss: 116.2285 - val_mse: 116.2285
Epoch 11/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.2498 - mse: 163.2498
Epoch 00011: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 446us/sample - loss: 160.4799 - mse: 160.4799 - val_loss: 119.3131 - val_mse: 119.3131
Epoch 12/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 160.2601 - mse: 160.2601
Epoch 00012: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 436us/sample - loss: 158.7437 - mse: 158.7437 - val_loss: 115.1717 - val_mse: 115.1717
Epoch 13/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 157.2678 - mse: 157.2678
Epoch 00013: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 490us/sample - loss: 161.9270 - mse: 161.9270 - val_loss: 123.1349 - val_mse: 123.1349
Epoch 14/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 157.3130 - mse: 157.3130
Epoch 00014: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 597us/sample - loss: 157.1816 - mse: 157.1816 - val_loss: 117.6678 - val_mse: 117.6678
Epoch 15/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 157.7279 - mse: 157.7279
Epoch 00015: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 583us/sample - loss: 158.0445 - mse: 158.0445 - val_loss: 114.9847 - val_mse: 114.9847
Epoch 16/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 154.6680 - mse: 154.6680
Epoch 00016: val_loss did not improve from 111.46321
2400/2400 [==============================] - 2s 713us/sample - loss: 154.3836 - mse: 154.3837 - val_loss: 137.5497 - val_mse: 137.5497
Epoch 17/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 158.6229 - mse: 158.6229
Epoch 00017: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 615us/sample - loss: 157.5641 - mse: 157.5641 - val_loss: 127.3403 - val_mse: 127.3403
Epoch 18/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 149.3642 - mse: 149.3642
Epoch 00018: val_loss did not improve from 111.46321
2400/2400 [==============================] - 2s 625us/sample - loss: 155.5678 - mse: 155.5679 - val_loss: 118.6868 - val_mse: 118.6869
Epoch 19/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 155.3561 - mse: 155.3562
Epoch 00019: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 589us/sample - loss: 154.7604 - mse: 154.7604 - val_loss: 120.3399 - val_mse: 120.3399
Epoch 20/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 150.6078 - mse: 150.6077
Epoch 00020: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 564us/sample - loss: 150.8336 - mse: 150.8336 - val_loss: 115.2498 - val_mse: 115.2498
Epoch 21/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 157.9448 - mse: 157.9448
Epoch 00021: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 543us/sample - loss: 156.8648 - mse: 156.8648 - val_loss: 125.8479 - val_mse: 125.8479
Epoch 22/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 164.8523 - mse: 164.8523
Epoch 00022: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 556us/sample - loss: 164.2120 - mse: 164.2120 - val_loss: 125.4184 - val_mse: 125.4184
Epoch 23/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 163.1784 - mse: 163.1784
Epoch 00023: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 535us/sample - loss: 161.6902 - mse: 161.6902 - val_loss: 129.6946 - val_mse: 129.6946
Epoch 24/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 160.9697 - mse: 160.9697
Epoch 00024: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 543us/sample - loss: 162.5911 - mse: 162.5911 - val_loss: 123.8219 - val_mse: 123.8220
Epoch 25/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 161.8500 - mse: 161.8500
Epoch 00025: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 540us/sample - loss: 160.6202 - mse: 160.6202 - val_loss: 118.0388 - val_mse: 118.0388
Epoch 26/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 156.9401 - mse: 156.9401
Epoch 00026: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 562us/sample - loss: 157.1010 - mse: 157.1010 - val_loss: 115.4480 - val_mse: 115.4480
Epoch 27/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 157.1411 - mse: 157.1411
Epoch 00027: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 548us/sample - loss: 153.4082 - mse: 153.4082 - val_loss: 117.7459 - val_mse: 117.7459
Epoch 28/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 159.8561 - mse: 159.8560
Epoch 00028: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 535us/sample - loss: 157.5467 - mse: 157.5466 - val_loss: 138.1171 - val_mse: 138.1171
Epoch 29/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 150.1600 - mse: 150.1600
Epoch 00029: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 542us/sample - loss: 149.1490 - mse: 149.1489 - val_loss: 161.9053 - val_mse: 161.9053
Epoch 30/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 157.9519 - mse: 157.9519
Epoch 00030: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 597us/sample - loss: 156.9549 - mse: 156.9549 - val_loss: 128.1807 - val_mse: 128.1807
Epoch 31/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 160.4005 - mse: 160.4005
Epoch 00031: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 590us/sample - loss: 158.4233 - mse: 158.4232 - val_loss: 121.2198 - val_mse: 121.2198
Epoch 32/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 152.1921 - mse: 152.1921
Epoch 00032: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 544us/sample - loss: 151.2823 - mse: 151.2823 - val_loss: 116.1060 - val_mse: 116.1060
Epoch 33/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 156.4264 - mse: 156.4264
Epoch 00033: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 556us/sample - loss: 155.0210 - mse: 155.0209 - val_loss: 112.2592 - val_mse: 112.2592
Epoch 34/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 150.9893 - mse: 150.9893
Epoch 00034: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 527us/sample - loss: 150.7855 - mse: 150.7855 - val_loss: 125.0600 - val_mse: 125.0600
Epoch 35/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 156.4937 - mse: 156.4937
Epoch 00035: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 534us/sample - loss: 155.4156 - mse: 155.4157 - val_loss: 114.8519 - val_mse: 114.8519
Epoch 36/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 147.6298 - mse: 147.6298
Epoch 00036: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 536us/sample - loss: 146.9012 - mse: 146.9012 - val_loss: 155.2498 - val_mse: 155.2498
Epoch 37/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 148.7903 - mse: 148.7903
Epoch 00037: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 506us/sample - loss: 148.3255 - mse: 148.3255 - val_loss: 135.2886 - val_mse: 135.2885
Epoch 38/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 149.1226 - mse: 149.1226
Epoch 00038: val_loss did not improve from 111.46321
2400/2400 [==============================] - 1s 514us/sample - loss: 148.5695 - mse: 148.5695 - val_loss: 117.7340 - val_mse: 117.7340
Epoch 39/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 148.4758 - mse: 148.4758
Epoch 00039: val_loss improved from 111.46321 to 111.12089, saving model to best_model_1.h5
2400/2400 [==============================] - 1s 584us/sample - loss: 148.5528 - mse: 148.5528 - val_loss: 111.1209 - val_mse: 111.1209
Epoch 40/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 155.8493 - mse: 155.8492
Epoch 00040: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 520us/sample - loss: 155.0432 - mse: 155.0432 - val_loss: 121.4552 - val_mse: 121.4552
Epoch 41/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 158.7395 - mse: 158.7395
Epoch 00041: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 554us/sample - loss: 163.8427 - mse: 163.8426 - val_loss: 123.8722 - val_mse: 123.8722
Epoch 42/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 161.4651 - mse: 161.4651
Epoch 00042: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 575us/sample - loss: 160.2501 - mse: 160.2501 - val_loss: 119.7790 - val_mse: 119.7790
Epoch 43/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 155.4984 - mse: 155.4984
Epoch 00043: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 578us/sample - loss: 156.5608 - mse: 156.5608 - val_loss: 128.5139 - val_mse: 128.5139
Epoch 44/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 155.2218 - mse: 155.2217
Epoch 00044: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 584us/sample - loss: 153.7968 - mse: 153.7968 - val_loss: 124.3979 - val_mse: 124.3979
Epoch 45/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 142.1069 - mse: 142.1069
Epoch 00045: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 583us/sample - loss: 147.3003 - mse: 147.3003 - val_loss: 161.0579 - val_mse: 161.0579
Epoch 46/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 157.8771 - mse: 157.8771
Epoch 00046: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 542us/sample - loss: 159.1585 - mse: 159.1585 - val_loss: 124.8010 - val_mse: 124.8009
Epoch 47/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 158.9427 - mse: 158.9427
Epoch 00047: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 555us/sample - loss: 156.2802 - mse: 156.2802 - val_loss: 119.8621 - val_mse: 119.8621
Epoch 48/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 153.4071 - mse: 153.4071- ETA: 0s - loss: 160.4880 - m
Epoch 00048: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 540us/sample - loss: 152.1936 - mse: 152.1936 - val_loss: 119.2066 - val_mse: 119.2066
Epoch 49/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 154.4561 - mse: 154.4561- ETA: 0s - loss: 180.3755 - m
Epoch 00049: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 547us/sample - loss: 152.4613 - mse: 152.4613 - val_loss: 127.1597 - val_mse: 127.1597
Epoch 50/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 150.7993 - mse: 150.7993
Epoch 00050: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 544us/sample - loss: 148.4081 - mse: 148.4081 - val_loss: 111.3116 - val_mse: 111.3116
Epoch 51/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 148.4239 - mse: 148.4239
Epoch 00051: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 542us/sample - loss: 147.0998 - mse: 147.0999 - val_loss: 113.2510 - val_mse: 113.2510
Epoch 52/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 147.0492 - mse: 147.0492
Epoch 00052: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 541us/sample - loss: 145.4368 - mse: 145.4368 - val_loss: 115.5228 - val_mse: 115.5228
Epoch 53/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 149.0092 - mse: 149.0092
Epoch 00053: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 542us/sample - loss: 146.9744 - mse: 146.9744 - val_loss: 118.3983 - val_mse: 118.3983
Epoch 54/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 149.8591 - mse: 149.8591
Epoch 00054: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 589us/sample - loss: 147.3734 - mse: 147.3734 - val_loss: 122.3478 - val_mse: 122.3478
Epoch 55/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 152.6943 - mse: 152.6943
Epoch 00055: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 593us/sample - loss: 152.9336 - mse: 152.9336 - val_loss: 113.0367 - val_mse: 113.0367
Epoch 56/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 140.1629 - mse: 140.1629
Epoch 00056: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 553us/sample - loss: 143.4164 - mse: 143.4164 - val_loss: 146.8151 - val_mse: 146.8151
Epoch 57/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 175.7833 - mse: 175.7833
Epoch 00057: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 540us/sample - loss: 171.6074 - mse: 171.6074 - val_loss: 125.6576 - val_mse: 125.6576
Epoch 58/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 164.8429 - mse: 164.8429
Epoch 00058: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 535us/sample - loss: 164.1664 - mse: 164.1664 - val_loss: 121.1389 - val_mse: 121.1389
Epoch 59/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 161.7513 - mse: 161.7513
Epoch 00059: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 536us/sample - loss: 160.0232 - mse: 160.0232 - val_loss: 120.0125 - val_mse: 120.0125
Epoch 60/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 163.2457 - mse: 163.2457
Epoch 00060: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 526us/sample - loss: 160.2424 - mse: 160.2424 - val_loss: 118.2523 - val_mse: 118.2523
Epoch 61/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 158.5077 - mse: 158.5077
Epoch 00061: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 521us/sample - loss: 157.8307 - mse: 157.8307 - val_loss: 117.3606 - val_mse: 117.3606
Epoch 62/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 156.4288 - mse: 156.4288
Epoch 00062: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 528us/sample - loss: 155.9499 - mse: 155.9499 - val_loss: 120.1090 - val_mse: 120.1090
Epoch 63/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 159.0725 - mse: 159.0726
Epoch 00063: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 505us/sample - loss: 160.6246 - mse: 160.6247 - val_loss: 120.8187 - val_mse: 120.8187
Epoch 64/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 149.8646 - mse: 149.8645
Epoch 00064: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 521us/sample - loss: 156.3101 - mse: 156.3100 - val_loss: 117.7493 - val_mse: 117.7493
Epoch 65/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 154.0479 - mse: 154.0479
Epoch 00065: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 529us/sample - loss: 153.5325 - mse: 153.5325 - val_loss: 113.9790 - val_mse: 113.9790
Epoch 66/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 149.9257 - mse: 149.9257
Epoch 00066: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 567us/sample - loss: 150.6917 - mse: 150.6917 - val_loss: 140.1847 - val_mse: 140.1847
Epoch 67/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 152.4930 - mse: 152.4930
Epoch 00067: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 575us/sample - loss: 152.6527 - mse: 152.6527 - val_loss: 168.9097 - val_mse: 168.9097
Epoch 68/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 167.7208 - mse: 167.7208
Epoch 00068: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 584us/sample - loss: 167.2970 - mse: 167.2969 - val_loss: 121.9282 - val_mse: 121.9282
Epoch 69/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.4296 - mse: 163.4296
Epoch 00069: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 542us/sample - loss: 163.2575 - mse: 163.2575 - val_loss: 121.6315 - val_mse: 121.6315
Epoch 70/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.8334 - mse: 163.8334
Epoch 00070: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 532us/sample - loss: 161.8560 - mse: 161.8559 - val_loss: 121.7501 - val_mse: 121.7501
Epoch 71/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.6056 - mse: 162.6056
Epoch 00071: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 551us/sample - loss: 161.3103 - mse: 161.3103 - val_loss: 119.0371 - val_mse: 119.0371
Epoch 72/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.5737 - mse: 162.5736
Epoch 00072: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 546us/sample - loss: 161.0468 - mse: 161.0468 - val_loss: 117.0002 - val_mse: 117.0002
Epoch 73/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 158.7395 - mse: 158.7396
Epoch 00073: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 523us/sample - loss: 160.4332 - mse: 160.4332 - val_loss: 124.2412 - val_mse: 124.2412
Epoch 74/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 164.6470 - mse: 164.6470
Epoch 00074: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 528us/sample - loss: 163.1906 - mse: 163.1907 - val_loss: 123.1151 - val_mse: 123.1151
Epoch 75/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 165.5733 - mse: 165.5733
Epoch 00075: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 527us/sample - loss: 162.9216 - mse: 162.9216 - val_loss: 120.8163 - val_mse: 120.8163
Epoch 76/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 160.2883 - mse: 160.2883
Epoch 00076: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 521us/sample - loss: 158.1875 - mse: 158.1875 - val_loss: 119.8974 - val_mse: 119.8974
Epoch 77/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 160.9122 - mse: 160.9122
Epoch 00077: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 528us/sample - loss: 159.2752 - mse: 159.2752 - val_loss: 119.3733 - val_mse: 119.3733
Epoch 78/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 155.7533 - mse: 155.7533
Epoch 00078: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 582us/sample - loss: 155.2736 - mse: 155.2736 - val_loss: 119.4269 - val_mse: 119.4269
Epoch 79/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 156.6164 - mse: 156.6164
Epoch 00079: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 572us/sample - loss: 154.5005 - mse: 154.5005 - val_loss: 112.3408 - val_mse: 112.3408
Epoch 80/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 154.1310 - mse: 154.1310
Epoch 00080: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 591us/sample - loss: 153.5483 - mse: 153.5484 - val_loss: 111.5639 - val_mse: 111.5639
Epoch 81/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 154.6469 - mse: 154.6469
Epoch 00081: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 555us/sample - loss: 153.8026 - mse: 153.8026 - val_loss: 121.3529 - val_mse: 121.3529
Epoch 82/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 156.0881 - mse: 156.0881
Epoch 00082: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 542us/sample - loss: 153.7877 - mse: 153.7877 - val_loss: 120.6002 - val_mse: 120.6002
Epoch 83/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 152.5734 - mse: 152.5734
Epoch 00083: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 561us/sample - loss: 152.1806 - mse: 152.1806 - val_loss: 115.9661 - val_mse: 115.9661
Epoch 84/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 162.5057 - mse: 162.5057
Epoch 00084: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 562us/sample - loss: 161.4977 - mse: 161.4977 - val_loss: 126.6235 - val_mse: 126.6235
Epoch 85/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 161.7441 - mse: 161.7440
Epoch 00085: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 549us/sample - loss: 164.5371 - mse: 164.5371 - val_loss: 122.0885 - val_mse: 122.0885
Epoch 86/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 158.7357 - mse: 158.7357
Epoch 00086: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 534us/sample - loss: 162.5544 - mse: 162.5544 - val_loss: 123.1490 - val_mse: 123.1490
Epoch 87/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 157.9478 - mse: 157.9478
Epoch 00087: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 541us/sample - loss: 164.2446 - mse: 164.2446 - val_loss: 122.0689 - val_mse: 122.0689
Epoch 88/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 164.4492 - mse: 164.4492
Epoch 00088: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 528us/sample - loss: 163.1261 - mse: 163.1261 - val_loss: 121.7797 - val_mse: 121.7797
Epoch 89/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 162.2918 - mse: 162.2919
Epoch 00089: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 528us/sample - loss: 161.2982 - mse: 161.2982 - val_loss: 122.1283 - val_mse: 122.1283
Epoch 90/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.8678 - mse: 162.8678
Epoch 00090: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 562us/sample - loss: 161.7174 - mse: 161.7174 - val_loss: 122.2159 - val_mse: 122.2159
Epoch 91/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 160.3820 - mse: 160.3820
Epoch 00091: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 577us/sample - loss: 161.2071 - mse: 161.2071 - val_loss: 121.9292 - val_mse: 121.9292
Epoch 92/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 164.4433 - mse: 164.4433
Epoch 00092: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 587us/sample - loss: 162.8932 - mse: 162.8932 - val_loss: 122.2915 - val_mse: 122.2914
Epoch 93/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 159.9876 - mse: 159.9876
Epoch 00093: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 543us/sample - loss: 161.8053 - mse: 161.8053 - val_loss: 122.3063 - val_mse: 122.3063
Epoch 94/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.2263 - mse: 162.2263
Epoch 00094: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 557us/sample - loss: 161.4534 - mse: 161.4534 - val_loss: 121.3556 - val_mse: 121.3556
Epoch 95/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 160.2748 - mse: 160.2748
Epoch 00095: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 547us/sample - loss: 161.8325 - mse: 161.8325 - val_loss: 126.8565 - val_mse: 126.8565
Epoch 96/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 161.3209 - mse: 161.3209
Epoch 00096: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 543us/sample - loss: 162.5076 - mse: 162.5076 - val_loss: 120.3805 - val_mse: 120.3805
Epoch 97/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.5879 - mse: 163.5880
Epoch 00097: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 560us/sample - loss: 161.3110 - mse: 161.3111 - val_loss: 120.5712 - val_mse: 120.5712
Epoch 98/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 159.9130 - mse: 159.9131
Epoch 00098: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 540us/sample - loss: 159.9363 - mse: 159.9363 - val_loss: 122.9417 - val_mse: 122.9416
Epoch 99/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 164.9485 - mse: 164.9485- ETA: 0s - loss: 168.2193 -
Epoch 00099: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 528us/sample - loss: 162.1106 - mse: 162.1106 - val_loss: 121.7878 - val_mse: 121.7878
Epoch 100/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 164.5757 - mse: 164.5756
Epoch 00100: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 543us/sample - loss: 162.6761 - mse: 162.6761 - val_loss: 122.2034 - val_mse: 122.2034
Epoch 101/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.0354 - mse: 162.0354
Epoch 00101: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 543us/sample - loss: 162.5973 - mse: 162.5973 - val_loss: 122.4193 - val_mse: 122.4193
Epoch 102/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.6565 - mse: 162.6565
Epoch 00102: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 573us/sample - loss: 162.0272 - mse: 162.0271 - val_loss: 122.2786 - val_mse: 122.2786
Epoch 103/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 161.6974 - mse: 161.6974
Epoch 00103: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 611us/sample - loss: 162.1941 - mse: 162.1940 - val_loss: 122.9910 - val_mse: 122.9910
Epoch 104/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.7477 - mse: 162.7477
Epoch 00104: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 575us/sample - loss: 162.5191 - mse: 162.5191 - val_loss: 122.1937 - val_mse: 122.1937
Epoch 105/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 164.0561 - mse: 164.0561
Epoch 00105: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 541us/sample - loss: 162.2705 - mse: 162.2705 - val_loss: 122.1211 - val_mse: 122.1211
Epoch 106/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 164.5929 - mse: 164.5929
Epoch 00106: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 521us/sample - loss: 162.2156 - mse: 162.2156 - val_loss: 122.6798 - val_mse: 122.6798
Epoch 107/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 164.4566 - mse: 164.4566
Epoch 00107: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 542us/sample - loss: 163.7637 - mse: 163.7637 - val_loss: 121.8474 - val_mse: 121.8474
Epoch 108/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 161.4438 - mse: 161.4438
Epoch 00108: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 534us/sample - loss: 161.6711 - mse: 161.6711 - val_loss: 123.2160 - val_mse: 123.2160
Epoch 109/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.7522 - mse: 162.7522
Epoch 00109: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 521us/sample - loss: 162.0017 - mse: 162.0017 - val_loss: 121.8375 - val_mse: 121.8375
Epoch 110/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.8906 - mse: 162.8906
Epoch 00110: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 527us/sample - loss: 162.3715 - mse: 162.3715 - val_loss: 123.4423 - val_mse: 123.4423
Epoch 111/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 160.0470 - mse: 160.0470
Epoch 00111: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 528us/sample - loss: 161.4323 - mse: 161.4323 - val_loss: 124.6003 - val_mse: 124.6003
Epoch 112/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 155.1757 - mse: 155.1757
Epoch 00112: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 585us/sample - loss: 160.9643 - mse: 160.9643 - val_loss: 123.1811 - val_mse: 123.1811
Epoch 113/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 165.7644 - mse: 165.7644
Epoch 00113: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 688us/sample - loss: 162.7923 - mse: 162.7922 - val_loss: 121.6431 - val_mse: 121.6431
Epoch 114/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 161.9558 - mse: 161.9558
Epoch 00114: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 516us/sample - loss: 161.6942 - mse: 161.6942 - val_loss: 124.3803 - val_mse: 124.3803
Epoch 115/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 160.2127 - mse: 160.2126
Epoch 00115: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 549us/sample - loss: 162.0730 - mse: 162.0730 - val_loss: 122.8409 - val_mse: 122.8409
Epoch 116/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 163.4510 - mse: 163.4511
Epoch 00116: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 536us/sample - loss: 162.0649 - mse: 162.0649 - val_loss: 121.9802 - val_mse: 121.9803
Epoch 117/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 162.6053 - mse: 162.6053
Epoch 00117: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 534us/sample - loss: 161.9886 - mse: 161.9886 - val_loss: 121.9719 - val_mse: 121.9719
Epoch 118/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 164.1119 - mse: 164.1119
Epoch 00118: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 563us/sample - loss: 163.2736 - mse: 163.2736 - val_loss: 123.3133 - val_mse: 123.3133
Epoch 119/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 164.8023 - mse: 164.8024
Epoch 00119: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 524us/sample - loss: 162.6596 - mse: 162.6596 - val_loss: 121.7683 - val_mse: 121.7682
Epoch 120/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.1171 - mse: 163.1171
Epoch 00120: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 516us/sample - loss: 162.1446 - mse: 162.1446 - val_loss: 122.0818 - val_mse: 122.0818
Epoch 121/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 166.1436 - mse: 166.1437
Epoch 00121: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 513us/sample - loss: 163.4604 - mse: 163.4605 - val_loss: 122.8549 - val_mse: 122.8549
Epoch 122/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 156.9200 - mse: 156.9200
Epoch 00122: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 507us/sample - loss: 162.1365 - mse: 162.1365 - val_loss: 122.1679 - val_mse: 122.1679
Epoch 123/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 155.8065 - mse: 155.8066
Epoch 00123: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 506us/sample - loss: 163.2599 - mse: 163.2599 - val_loss: 122.1107 - val_mse: 122.1107
Epoch 124/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 165.2021 - mse: 165.2021
Epoch 00124: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 522us/sample - loss: 162.5160 - mse: 162.5160 - val_loss: 122.2480 - val_mse: 122.2480
Epoch 125/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 163.7581 - mse: 163.7581
Epoch 00125: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 512us/sample - loss: 162.7171 - mse: 162.7171 - val_loss: 122.5197 - val_mse: 122.5197
Epoch 126/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 154.3195 - mse: 154.3194
Epoch 00126: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 514us/sample - loss: 160.8810 - mse: 160.8810 - val_loss: 121.3548 - val_mse: 121.3548
Epoch 127/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.7667 - mse: 163.7667
Epoch 00127: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 577us/sample - loss: 162.9633 - mse: 162.9634 - val_loss: 120.9738 - val_mse: 120.9737
Epoch 128/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.2453 - mse: 162.2453
Epoch 00128: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 575us/sample - loss: 161.1602 - mse: 161.1602 - val_loss: 120.1527 - val_mse: 120.1527
Epoch 129/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 160.1169 - mse: 160.1169
Epoch 00129: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 563us/sample - loss: 161.9153 - mse: 161.9153 - val_loss: 118.1088 - val_mse: 118.1089
Epoch 130/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 159.2702 - mse: 159.2702
Epoch 00130: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 542us/sample - loss: 157.0149 - mse: 157.0148 - val_loss: 118.0665 - val_mse: 118.0665
Epoch 131/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 159.7680 - mse: 159.7681
Epoch 00131: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 535us/sample - loss: 159.8797 - mse: 159.8797 - val_loss: 119.6164 - val_mse: 119.6164
Epoch 132/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 160.0865 - mse: 160.0865
Epoch 00132: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 555us/sample - loss: 157.9598 - mse: 157.9598 - val_loss: 116.0612 - val_mse: 116.0612
Epoch 133/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 163.7100 - mse: 163.7100- ETA: 0s - loss: 143.0556 - 
Epoch 00133: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 534us/sample - loss: 161.6047 - mse: 161.6047 - val_loss: 115.1784 - val_mse: 115.1784
Epoch 134/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 154.2150 - mse: 154.2150
Epoch 00134: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 527us/sample - loss: 158.3268 - mse: 158.3268 - val_loss: 123.6480 - val_mse: 123.6480
Epoch 135/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 165.8999 - mse: 165.8999
Epoch 00135: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 528us/sample - loss: 162.4669 - mse: 162.4669 - val_loss: 119.0358 - val_mse: 119.0358
Epoch 136/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 148.6493 - mse: 148.6493
Epoch 00136: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 535us/sample - loss: 157.8753 - mse: 157.8753 - val_loss: 129.1867 - val_mse: 129.1867
Epoch 137/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 155.5086 - mse: 155.5086
Epoch 00137: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 534us/sample - loss: 162.3453 - mse: 162.3452 - val_loss: 122.3448 - val_mse: 122.3448
Epoch 138/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 163.4413 - mse: 163.4413
Epoch 00138: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 541us/sample - loss: 163.6630 - mse: 163.6630 - val_loss: 121.9074 - val_mse: 121.9074
Epoch 139/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.5725 - mse: 162.5725
Epoch 00139: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 591us/sample - loss: 162.3482 - mse: 162.3482 - val_loss: 122.5069 - val_mse: 122.5069
Epoch 140/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 156.4216 - mse: 156.4216
Epoch 00140: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 582us/sample - loss: 162.9782 - mse: 162.9782 - val_loss: 122.9397 - val_mse: 122.9398
Epoch 141/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 153.0703 - mse: 153.0703
Epoch 00141: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 605us/sample - loss: 162.2502 - mse: 162.2502 - val_loss: 126.5166 - val_mse: 126.5166
Epoch 142/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 165.4366 - mse: 165.4366
Epoch 00142: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 556us/sample - loss: 163.0103 - mse: 163.0103 - val_loss: 121.9221 - val_mse: 121.9221
Epoch 143/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 157.4095 - mse: 157.4095
Epoch 00143: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 541us/sample - loss: 163.4215 - mse: 163.4216 - val_loss: 121.8813 - val_mse: 121.8813
Epoch 144/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 163.8688 - mse: 163.8688
Epoch 00144: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 537us/sample - loss: 163.0243 - mse: 163.0243 - val_loss: 122.0675 - val_mse: 122.0675
Epoch 145/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 162.8712 - mse: 162.8712
Epoch 00145: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 528us/sample - loss: 161.7580 - mse: 161.7580 - val_loss: 122.9566 - val_mse: 122.9566
Epoch 146/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 161.3871 - mse: 161.3871
Epoch 00146: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 514us/sample - loss: 163.7835 - mse: 163.7835 - val_loss: 122.8498 - val_mse: 122.8498
Epoch 147/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 163.3531 - mse: 163.3531
Epoch 00147: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 520us/sample - loss: 163.2784 - mse: 163.2784 - val_loss: 121.9606 - val_mse: 121.9606
Epoch 148/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.2021 - mse: 162.2021
Epoch 00148: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 528us/sample - loss: 161.4003 - mse: 161.4003 - val_loss: 122.8951 - val_mse: 122.8951
Epoch 149/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.1880 - mse: 163.1880
Epoch 00149: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 548us/sample - loss: 163.4632 - mse: 163.4633 - val_loss: 121.8947 - val_mse: 121.8947
Epoch 150/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 164.4932 - mse: 164.4932
Epoch 00150: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 541us/sample - loss: 161.1855 - mse: 161.1855 - val_loss: 121.8593 - val_mse: 121.8593
Epoch 151/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 158.0619 - mse: 158.0619
Epoch 00151: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 556us/sample - loss: 160.9677 - mse: 160.9677 - val_loss: 121.5170 - val_mse: 121.5170
Epoch 152/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 161.1480 - mse: 161.1480- ETA: 0s - loss: 145.4395 - mse: 
Epoch 00152: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 569us/sample - loss: 161.1624 - mse: 161.1624 - val_loss: 122.5161 - val_mse: 122.5161
Epoch 153/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 164.4646 - mse: 164.4646
Epoch 00153: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 561us/sample - loss: 161.7009 - mse: 161.7009 - val_loss: 123.7045 - val_mse: 123.7045
Epoch 154/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 167.4653 - mse: 167.4653
Epoch 00154: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 542us/sample - loss: 163.9572 - mse: 163.9572 - val_loss: 122.4611 - val_mse: 122.4611
Epoch 155/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 161.0990 - mse: 161.0990
Epoch 00155: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 514us/sample - loss: 162.3953 - mse: 162.3953 - val_loss: 121.9033 - val_mse: 121.9033
Epoch 156/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 164.4344 - mse: 164.4344
Epoch 00156: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 521us/sample - loss: 161.9357 - mse: 161.9358 - val_loss: 120.6898 - val_mse: 120.6898
Epoch 157/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 157.9460 - mse: 157.9460
Epoch 00157: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 514us/sample - loss: 160.6022 - mse: 160.6022 - val_loss: 118.8534 - val_mse: 118.8534
Epoch 158/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 159.5945 - mse: 159.5945
Epoch 00158: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 520us/sample - loss: 159.3958 - mse: 159.3958 - val_loss: 118.8891 - val_mse: 118.8891
Epoch 159/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 160.1851 - mse: 160.1851- ETA: 0s - loss: 154.0766 - m
Epoch 00159: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 521us/sample - loss: 157.4070 - mse: 157.4070 - val_loss: 121.2597 - val_mse: 121.2597
Epoch 160/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 157.2030 - mse: 157.2030
Epoch 00160: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 535us/sample - loss: 162.6173 - mse: 162.6174 - val_loss: 120.1226 - val_mse: 120.1226
Epoch 161/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 164.4357 - mse: 164.4357
Epoch 00161: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 519us/sample - loss: 160.6273 - mse: 160.6273 - val_loss: 120.0774 - val_mse: 120.0774
Epoch 162/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 159.7170 - mse: 159.7170
Epoch 00162: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 529us/sample - loss: 159.0234 - mse: 159.0234 - val_loss: 118.2901 - val_mse: 118.2901
Epoch 163/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.3653 - mse: 162.3653
Epoch 00163: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 548us/sample - loss: 160.4255 - mse: 160.4255 - val_loss: 119.4246 - val_mse: 119.4246
Epoch 164/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 157.7016 - mse: 157.7016
Epoch 00164: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 590us/sample - loss: 158.7651 - mse: 158.7651 - val_loss: 121.3204 - val_mse: 121.3204
Epoch 165/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 161.8062 - mse: 161.8062
Epoch 00165: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 604us/sample - loss: 160.4215 - mse: 160.4215 - val_loss: 121.7569 - val_mse: 121.7569
Epoch 166/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 159.8654 - mse: 159.8654
Epoch 00166: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 560us/sample - loss: 159.6812 - mse: 159.6812 - val_loss: 122.2313 - val_mse: 122.2313
Epoch 167/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 160.6253 - mse: 160.6253
Epoch 00167: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 563us/sample - loss: 161.0555 - mse: 161.0555 - val_loss: 119.2988 - val_mse: 119.2988
Epoch 168/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.2910 - mse: 162.2910
Epoch 00168: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 583us/sample - loss: 161.4213 - mse: 161.4213 - val_loss: 117.3094 - val_mse: 117.3094
Epoch 169/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 158.5820 - mse: 158.5820
Epoch 00169: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 567us/sample - loss: 159.0579 - mse: 159.0579 - val_loss: 115.3656 - val_mse: 115.3656
Epoch 170/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 156.5629 - mse: 156.5630
Epoch 00170: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 543us/sample - loss: 157.9095 - mse: 157.9096 - val_loss: 121.0182 - val_mse: 121.0182
Epoch 171/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.0672 - mse: 162.0672
Epoch 00171: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 536us/sample - loss: 160.9632 - mse: 160.9632 - val_loss: 122.8904 - val_mse: 122.8904
Epoch 172/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 155.1375 - mse: 155.1375
Epoch 00172: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 547us/sample - loss: 161.1525 - mse: 161.1526 - val_loss: 121.8579 - val_mse: 121.8579
Epoch 173/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 162.8030 - mse: 162.8030
Epoch 00173: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 520us/sample - loss: 163.7728 - mse: 163.7728 - val_loss: 121.8392 - val_mse: 121.8392
Epoch 174/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 165.1047 - mse: 165.1047
Epoch 00174: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 529us/sample - loss: 164.4483 - mse: 164.4482 - val_loss: 121.8389 - val_mse: 121.8389
Epoch 175/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.8039 - mse: 162.8039
Epoch 00175: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 562us/sample - loss: 161.5233 - mse: 161.5234 - val_loss: 122.0563 - val_mse: 122.0563
Epoch 176/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 156.0896 - mse: 156.0897
Epoch 00176: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 580us/sample - loss: 162.4494 - mse: 162.4494 - val_loss: 124.4150 - val_mse: 124.4150
Epoch 177/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 164.5095 - mse: 164.5096
Epoch 00177: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 578us/sample - loss: 163.7861 - mse: 163.7861 - val_loss: 122.0342 - val_mse: 122.0342
Epoch 178/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 161.0481 - mse: 161.0481
Epoch 00178: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 546us/sample - loss: 161.9087 - mse: 161.9087 - val_loss: 123.7338 - val_mse: 123.7338
Epoch 179/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.8158 - mse: 162.8159
Epoch 00179: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 536us/sample - loss: 162.8289 - mse: 162.8290 - val_loss: 121.8566 - val_mse: 121.8567
Epoch 180/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.2051 - mse: 163.2051
Epoch 00180: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 534us/sample - loss: 161.9805 - mse: 161.9805 - val_loss: 121.9409 - val_mse: 121.9409
Epoch 181/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.0148 - mse: 162.0148
Epoch 00181: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 542us/sample - loss: 162.1184 - mse: 162.1184 - val_loss: 122.2163 - val_mse: 122.2163
Epoch 182/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 165.7111 - mse: 165.7112
Epoch 00182: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 513us/sample - loss: 162.7258 - mse: 162.7258 - val_loss: 121.8642 - val_mse: 121.8642
Epoch 183/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.5807 - mse: 163.5807
Epoch 00183: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 520us/sample - loss: 162.9858 - mse: 162.9858 - val_loss: 122.2664 - val_mse: 122.2664
Epoch 184/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.6023 - mse: 163.6023
Epoch 00184: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 521us/sample - loss: 162.2780 - mse: 162.2780 - val_loss: 122.5043 - val_mse: 122.5043
Epoch 185/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.1972 - mse: 162.1971
Epoch 00185: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 520us/sample - loss: 161.7364 - mse: 161.7364 - val_loss: 121.8771 - val_mse: 121.8771
Epoch 186/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 167.0517 - mse: 167.0517
Epoch 00186: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 507us/sample - loss: 161.8939 - mse: 161.8939 - val_loss: 122.6823 - val_mse: 122.6823
Epoch 187/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 160.8064 - mse: 160.8064- ETA: 0s - loss: 213.4528 - 
Epoch 00187: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 513us/sample - loss: 162.7199 - mse: 162.7200 - val_loss: 122.6293 - val_mse: 122.6293
Epoch 188/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 159.2703 - mse: 159.2703
Epoch 00188: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 555us/sample - loss: 161.4625 - mse: 161.4625 - val_loss: 121.9261 - val_mse: 121.9261
Epoch 189/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 165.0095 - mse: 165.0095
Epoch 00189: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 562us/sample - loss: 163.6791 - mse: 163.6791 - val_loss: 121.9360 - val_mse: 121.9360
Epoch 190/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.7199 - mse: 163.7199- ETA: 0s - loss: 160.8145 - mse:
Epoch 00190: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 584us/sample - loss: 162.7738 - mse: 162.7738 - val_loss: 123.7716 - val_mse: 123.7716
Epoch 191/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 164.6512 - mse: 164.6513
Epoch 00191: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 542us/sample - loss: 162.3070 - mse: 162.3070 - val_loss: 122.4042 - val_mse: 122.4042
Epoch 192/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 164.1474 - mse: 164.1474
Epoch 00192: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 548us/sample - loss: 162.9892 - mse: 162.9892 - val_loss: 122.6645 - val_mse: 122.6645
Epoch 193/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.2507 - mse: 163.2507
Epoch 00193: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 529us/sample - loss: 162.5747 - mse: 162.5747 - val_loss: 121.9756 - val_mse: 121.9756
Epoch 194/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 162.8334 - mse: 162.8334
Epoch 00194: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 520us/sample - loss: 162.3112 - mse: 162.3112 - val_loss: 123.2923 - val_mse: 123.2923
Epoch 195/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.7703 - mse: 163.7703
Epoch 00195: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 541us/sample - loss: 162.2181 - mse: 162.2181 - val_loss: 122.2630 - val_mse: 122.2630
Epoch 196/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 164.8546 - mse: 164.8546
Epoch 00196: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 548us/sample - loss: 163.0461 - mse: 163.0461 - val_loss: 122.1331 - val_mse: 122.1331
Epoch 197/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 164.1099 - mse: 164.1099
Epoch 00197: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 520us/sample - loss: 162.8013 - mse: 162.8014 - val_loss: 122.6551 - val_mse: 122.6552
Epoch 198/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 160.1417 - mse: 160.1416
Epoch 00198: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 536us/sample - loss: 162.4865 - mse: 162.4865 - val_loss: 122.7911 - val_mse: 122.7911
Epoch 199/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 159.3879 - mse: 159.3879
Epoch 00199: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 540us/sample - loss: 162.2372 - mse: 162.2372 - val_loss: 122.1152 - val_mse: 122.1152
Epoch 200/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.9817 - mse: 162.9816
Epoch 00200: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 576us/sample - loss: 162.1026 - mse: 162.1026 - val_loss: 122.1232 - val_mse: 122.1232
Epoch 201/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 161.9655 - mse: 161.9655
Epoch 00201: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 577us/sample - loss: 160.6873 - mse: 160.6873 - val_loss: 122.0961 - val_mse: 122.0961
Epoch 202/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 165.0758 - mse: 165.0758
Epoch 00202: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 568us/sample - loss: 163.0604 - mse: 163.0604 - val_loss: 122.3625 - val_mse: 122.3625
Epoch 203/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 162.8402 - mse: 162.8401
Epoch 00203: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 561us/sample - loss: 161.8455 - mse: 161.8455 - val_loss: 123.1477 - val_mse: 123.1477
Epoch 204/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 164.9322 - mse: 164.9322
Epoch 00204: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 537us/sample - loss: 163.4664 - mse: 163.4664 - val_loss: 123.1653 - val_mse: 123.1653
Epoch 205/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 164.5283 - mse: 164.5283- ETA: 0s - loss: 141.2564 - mse:
Epoch 00205: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 548us/sample - loss: 163.9733 - mse: 163.9733 - val_loss: 123.9244 - val_mse: 123.9244
Epoch 206/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 161.0003 - mse: 161.0003
Epoch 00206: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 527us/sample - loss: 161.5004 - mse: 161.5004 - val_loss: 121.8651 - val_mse: 121.8651
Epoch 207/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.2330 - mse: 163.2330
Epoch 00207: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 513us/sample - loss: 162.8155 - mse: 162.8154 - val_loss: 123.2615 - val_mse: 123.2615
Epoch 208/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 165.5893 - mse: 165.5893
Epoch 00208: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 514us/sample - loss: 163.3828 - mse: 163.3828 - val_loss: 122.5167 - val_mse: 122.5167
Epoch 209/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 166.8824 - mse: 166.8824
Epoch 00209: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 527us/sample - loss: 163.6569 - mse: 163.6569 - val_loss: 124.2619 - val_mse: 124.2619
Epoch 210/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 156.9960 - mse: 156.9960
Epoch 00210: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 514us/sample - loss: 161.1725 - mse: 161.1725 - val_loss: 121.8859 - val_mse: 121.8859
Epoch 211/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 161.9799 - mse: 161.9799
Epoch 00211: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 528us/sample - loss: 163.0611 - mse: 163.0611 - val_loss: 122.3354 - val_mse: 122.3354
Epoch 212/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.3244 - mse: 163.3244
Epoch 00212: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 562us/sample - loss: 163.4876 - mse: 163.4875 - val_loss: 124.5917 - val_mse: 124.5917
Epoch 213/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 167.5097 - mse: 167.509 - ETA: 0s - loss: 163.8415 - mse: 163.8415
Epoch 00213: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 590us/sample - loss: 163.0530 - mse: 163.0530 - val_loss: 122.7457 - val_mse: 122.7457
Epoch 214/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 162.7255 - mse: 162.7255
Epoch 00214: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 576us/sample - loss: 161.9047 - mse: 161.9047 - val_loss: 124.0234 - val_mse: 124.0234
Epoch 215/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.2592 - mse: 162.2593
Epoch 00215: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 528us/sample - loss: 162.0898 - mse: 162.0898 - val_loss: 123.1912 - val_mse: 123.1912
Epoch 216/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.5009 - mse: 163.5008
Epoch 00216: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 539us/sample - loss: 162.9329 - mse: 162.9328 - val_loss: 122.1377 - val_mse: 122.1377
Epoch 217/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 163.7631 - mse: 163.7630
Epoch 00217: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 539us/sample - loss: 161.9956 - mse: 161.9956 - val_loss: 123.3363 - val_mse: 123.3363
Epoch 218/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 160.6705 - mse: 160.6705
Epoch 00218: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 549us/sample - loss: 161.1502 - mse: 161.1501 - val_loss: 122.5400 - val_mse: 122.5400
Epoch 219/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 164.4362 - mse: 164.4362
Epoch 00219: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 528us/sample - loss: 162.9381 - mse: 162.9381 - val_loss: 123.7519 - val_mse: 123.7519
Epoch 220/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 161.4978 - mse: 161.4979
Epoch 00220: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 533us/sample - loss: 162.0746 - mse: 162.0746 - val_loss: 124.4472 - val_mse: 124.4472
Epoch 221/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 160.6475 - mse: 160.6475
Epoch 00221: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 535us/sample - loss: 161.5092 - mse: 161.5092 - val_loss: 123.1172 - val_mse: 123.1172
Epoch 222/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.5833 - mse: 162.5833
Epoch 00222: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 542us/sample - loss: 161.7340 - mse: 161.7340 - val_loss: 121.9153 - val_mse: 121.9153
Epoch 223/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.4549 - mse: 162.4549
Epoch 00223: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 533us/sample - loss: 161.7365 - mse: 161.7366 - val_loss: 123.2016 - val_mse: 123.2016
Epoch 224/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.1231 - mse: 163.1231
Epoch 00224: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 579us/sample - loss: 161.7050 - mse: 161.7051 - val_loss: 122.8256 - val_mse: 122.8256
Epoch 225/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 159.2070 - mse: 159.2071
Epoch 00225: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 578us/sample - loss: 162.3203 - mse: 162.3204 - val_loss: 122.3147 - val_mse: 122.3148
Epoch 226/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.5729 - mse: 163.5729
Epoch 00226: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 590us/sample - loss: 162.3441 - mse: 162.3441 - val_loss: 122.0351 - val_mse: 122.0351
Epoch 227/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 163.6614 - mse: 163.6614
Epoch 00227: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 548us/sample - loss: 162.1405 - mse: 162.1405 - val_loss: 122.3168 - val_mse: 122.3168
Epoch 228/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 165.8038 - mse: 165.8038
Epoch 00228: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 521us/sample - loss: 162.1969 - mse: 162.1969 - val_loss: 124.3823 - val_mse: 124.3823
Epoch 229/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 164.2914 - mse: 164.2914
Epoch 00229: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 535us/sample - loss: 162.8864 - mse: 162.8863 - val_loss: 121.9361 - val_mse: 121.9361
Epoch 230/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.5945 - mse: 162.5946- ETA: 0s - loss: 138.6746 -
Epoch 00230: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 570us/sample - loss: 162.0013 - mse: 162.0014 - val_loss: 122.7140 - val_mse: 122.7140
Epoch 231/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.0647 - mse: 162.0647
Epoch 00231: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 554us/sample - loss: 160.6316 - mse: 160.6316 - val_loss: 123.5342 - val_mse: 123.5342
Epoch 232/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 161.8655 - mse: 161.8655
Epoch 00232: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 535us/sample - loss: 162.7017 - mse: 162.7017 - val_loss: 122.7824 - val_mse: 122.7824
Epoch 233/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.3083 - mse: 162.3083- ETA: 0s - loss: 180.0541 - mse: 180.
Epoch 00233: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 527us/sample - loss: 161.8627 - mse: 161.8627 - val_loss: 121.8408 - val_mse: 121.8408
Epoch 234/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 161.8603 - mse: 161.8603
Epoch 00234: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 527us/sample - loss: 162.8617 - mse: 162.8617 - val_loss: 122.7143 - val_mse: 122.7143
Epoch 235/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 160.8152 - mse: 160.8151- ETA: 0s - loss: 154.9877 - mse: 15
Epoch 00235: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 549us/sample - loss: 160.6503 - mse: 160.6502 - val_loss: 123.5148 - val_mse: 123.5148
Epoch 236/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 164.2960 - mse: 164.2960
Epoch 00236: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 548us/sample - loss: 163.0767 - mse: 163.0767 - val_loss: 122.7127 - val_mse: 122.7127
Epoch 237/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 164.1002 - mse: 164.1003
Epoch 00237: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 572us/sample - loss: 161.3123 - mse: 161.3124 - val_loss: 123.0212 - val_mse: 123.0212
Epoch 238/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.2383 - mse: 162.2383
Epoch 00238: val_loss did not improve from 111.12089
2400/2400 [==============================] - 1s 590us/sample - loss: 160.9339 - mse: 160.9339 - val_loss: 123.8634 - val_mse: 123.8634
Epoch 239/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.9713 - mse: 162.9713
Epoch 00239: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 723us/sample - loss: 161.6261 - mse: 161.6261 - val_loss: 123.3278 - val_mse: 123.3278
Epoch 00239: early stopping
WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;, &lt;class &#39;NoneType&#39;&gt;
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Train on 4150 samples, validate on 1038 samples
Epoch 1/200
3232/4150 [======================&gt;.......] - ETA: 0s - loss: 3.2637 - accuracy: 0.6293
Epoch 00001: val_loss did not improve from 0.05836
4150/4150 [==============================] - 1s 191us/sample - loss: 2.9576 - accuracy: 0.6243 - val_loss: 3.1502 - val_accuracy: 0.4162
Epoch 2/200
3456/4150 [=======================&gt;......] - ETA: 0s - loss: 1.2815 - accuracy: 0.6690
Epoch 00002: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 83us/sample - loss: 1.2181 - accuracy: 0.6745 - val_loss: 5.1879 - val_accuracy: 0.2216
Epoch 3/200
3712/4150 [=========================&gt;....] - ETA: 0s - loss: 0.9704 - accuracy: 0.6862
Epoch 00003: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 88us/sample - loss: 0.9495 - accuracy: 0.6923 - val_loss: 1.3121 - val_accuracy: 0.5376
Epoch 4/200
3488/4150 [========================&gt;.....] - ETA: 0s - loss: 0.7942 - accuracy: 0.7162
Epoch 00004: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 84us/sample - loss: 0.8220 - accuracy: 0.7092 - val_loss: 4.5723 - val_accuracy: 0.0568
Epoch 5/200
3904/4150 [===========================&gt;..] - ETA: 0s - loss: 0.8254 - accuracy: 0.7185
Epoch 00005: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 95us/sample - loss: 0.8399 - accuracy: 0.7137 - val_loss: 0.0763 - val_accuracy: 0.9759
Epoch 6/200
3776/4150 [==========================&gt;...] - ETA: 0s - loss: 0.7860 - accuracy: 0.7105
Epoch 00006: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 84us/sample - loss: 0.7702 - accuracy: 0.7125 - val_loss: 0.3208 - val_accuracy: 0.8738
Epoch 7/200
3968/4150 [===========================&gt;..] - ETA: 0s - loss: 0.7079 - accuracy: 0.7238
Epoch 00007: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 93us/sample - loss: 0.7034 - accuracy: 0.7253 - val_loss: 0.1714 - val_accuracy: 0.9566
Epoch 8/200
3520/4150 [========================&gt;.....] - ETA: 0s - loss: 0.6946 - accuracy: 0.7176
Epoch 00008: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 100us/sample - loss: 0.6974 - accuracy: 0.7186 - val_loss: 1.3563 - val_accuracy: 0.4489
Epoch 9/200
4064/4150 [============================&gt;.] - ETA: 0s - loss: 0.6241 - accuracy: 0.7392
Epoch 00009: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 87us/sample - loss: 0.6260 - accuracy: 0.7393 - val_loss: 0.5558 - val_accuracy: 0.7428
Epoch 10/200
3776/4150 [==========================&gt;...] - ETA: 0s - loss: 0.6448 - accuracy: 0.7288
Epoch 00010: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 80us/sample - loss: 0.6266 - accuracy: 0.7366 - val_loss: 0.5310 - val_accuracy: 0.7312
Epoch 11/200
3456/4150 [=======================&gt;......] - ETA: 0s - loss: 0.5281 - accuracy: 0.7593
Epoch 00011: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 68us/sample - loss: 0.5179 - accuracy: 0.7643 - val_loss: 0.7619 - val_accuracy: 0.6089
Epoch 12/200
3584/4150 [========================&gt;.....] - ETA: 0s - loss: 0.4703 - accuracy: 0.7829
Epoch 00012: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 68us/sample - loss: 0.4854 - accuracy: 0.7769 - val_loss: 0.2468 - val_accuracy: 0.8960
Epoch 13/200
3232/4150 [======================&gt;.......] - ETA: 0s - loss: 0.5105 - accuracy: 0.7760
Epoch 00013: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 71us/sample - loss: 0.5021 - accuracy: 0.7778 - val_loss: 0.8206 - val_accuracy: 0.5626
Epoch 14/200
3296/4150 [======================&gt;.......] - ETA: 0s - loss: 0.5612 - accuracy: 0.7627
Epoch 00014: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 82us/sample - loss: 0.5414 - accuracy: 0.7639 - val_loss: 1.0058 - val_accuracy: 0.5896
Epoch 15/200
3328/4150 [=======================&gt;......] - ETA: 0s - loss: 0.4534 - accuracy: 0.7879
Epoch 00015: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 69us/sample - loss: 0.4824 - accuracy: 0.7812 - val_loss: 0.2067 - val_accuracy: 0.9470
Epoch 16/200
3296/4150 [======================&gt;.......] - ETA: 0s - loss: 0.5485 - accuracy: 0.7655
Epoch 00016: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 68us/sample - loss: 0.5333 - accuracy: 0.7651 - val_loss: 0.2669 - val_accuracy: 0.8882
Epoch 17/200
3488/4150 [========================&gt;.....] - ETA: 0s - loss: 0.4799 - accuracy: 0.7738
Epoch 00017: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 80us/sample - loss: 0.4744 - accuracy: 0.7776 - val_loss: 0.2693 - val_accuracy: 0.8950
Epoch 18/200
3520/4150 [========================&gt;.....] - ETA: 0s - loss: 0.4777 - accuracy: 0.7804
Epoch 00018: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 67us/sample - loss: 0.4609 - accuracy: 0.7889 - val_loss: 0.4623 - val_accuracy: 0.7813
Epoch 19/200
3552/4150 [========================&gt;.....] - ETA: 0s - loss: 0.4534 - accuracy: 0.7936
Epoch 00019: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 68us/sample - loss: 0.4546 - accuracy: 0.7940 - val_loss: 0.4439 - val_accuracy: 0.8160
Epoch 20/200
3904/4150 [===========================&gt;..] - ETA: 0s - loss: 0.4315 - accuracy: 0.8056
Epoch 00020: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 64us/sample - loss: 0.4299 - accuracy: 0.8065 - val_loss: 0.7062 - val_accuracy: 0.6830
Epoch 21/200
3904/4150 [===========================&gt;..] - ETA: 0s - loss: 0.4146 - accuracy: 0.8023
Epoch 00021: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 64us/sample - loss: 0.4167 - accuracy: 0.8022 - val_loss: 0.4636 - val_accuracy: 0.7958
Epoch 22/200
3904/4150 [===========================&gt;..] - ETA: 0s - loss: 0.4286 - accuracy: 0.8051
Epoch 00022: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 65us/sample - loss: 0.4227 - accuracy: 0.8070 - val_loss: 0.6757 - val_accuracy: 0.6638
Epoch 23/200
3680/4150 [=========================&gt;....] - ETA: 0s - loss: 0.3976 - accuracy: 0.8136
Epoch 00023: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 65us/sample - loss: 0.3996 - accuracy: 0.8120 - val_loss: 2.4517 - val_accuracy: 0.1898
Epoch 24/200
3552/4150 [========================&gt;.....] - ETA: 0s - loss: 0.4167 - accuracy: 0.8026
Epoch 00024: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 68us/sample - loss: 0.4111 - accuracy: 0.8046 - val_loss: 0.7913 - val_accuracy: 0.5568
Epoch 25/200
3584/4150 [========================&gt;.....] - ETA: 0s - loss: 0.4366 - accuracy: 0.8050
Epoch 00025: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 70us/sample - loss: 0.4355 - accuracy: 0.8036 - val_loss: 0.3938 - val_accuracy: 0.8160
Epoch 26/200
3840/4150 [==========================&gt;...] - ETA: 0s - loss: 0.4478 - accuracy: 0.8013
Epoch 00026: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 65us/sample - loss: 0.4530 - accuracy: 0.7990 - val_loss: 0.3142 - val_accuracy: 0.8776
Epoch 27/200
3680/4150 [=========================&gt;....] - ETA: 0s - loss: 0.5536 - accuracy: 0.7807
Epoch 00027: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 66us/sample - loss: 0.5537 - accuracy: 0.7802 - val_loss: 0.4288 - val_accuracy: 0.8112
Epoch 28/200
4064/4150 [============================&gt;.] - ETA: 0s - loss: 0.3986 - accuracy: 0.8155
Epoch 00028: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 62us/sample - loss: 0.3978 - accuracy: 0.8154 - val_loss: 0.4938 - val_accuracy: 0.7543
Epoch 29/200
3648/4150 [=========================&gt;....] - ETA: 0s - loss: 0.4031 - accuracy: 0.8224
Epoch 00029: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 67us/sample - loss: 0.4038 - accuracy: 0.8214 - val_loss: 0.3194 - val_accuracy: 0.8805
Epoch 30/200
4000/4150 [===========================&gt;..] - ETA: 0s - loss: 0.4146 - accuracy: 0.8080
Epoch 00030: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 64us/sample - loss: 0.4166 - accuracy: 0.8077 - val_loss: 0.4669 - val_accuracy: 0.7707
Epoch 31/200
3840/4150 [==========================&gt;...] - ETA: 0s - loss: 0.3756 - accuracy: 0.8326
Epoch 00031: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 64us/sample - loss: 0.3775 - accuracy: 0.8316 - val_loss: 0.6180 - val_accuracy: 0.6493
Epoch 32/200
4128/4150 [============================&gt;.] - ETA: 0s - loss: 0.4616 - accuracy: 0.7970
Epoch 00032: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 61us/sample - loss: 0.4600 - accuracy: 0.7978 - val_loss: 0.7821 - val_accuracy: 0.5780
Epoch 33/200
3936/4150 [===========================&gt;..] - ETA: 0s - loss: 0.4386 - accuracy: 0.8021
Epoch 00033: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 64us/sample - loss: 0.4440 - accuracy: 0.7986 - val_loss: 1.1512 - val_accuracy: 0.4451
Epoch 34/200
3840/4150 [==========================&gt;...] - ETA: 0s - loss: 0.3751 - accuracy: 0.8250
Epoch 00034: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 64us/sample - loss: 0.3760 - accuracy: 0.8253 - val_loss: 1.0934 - val_accuracy: 0.3545
Epoch 35/200
3136/4150 [=====================&gt;........] - ETA: 0s - loss: 0.3819 - accuracy: 0.8288
Epoch 00035: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 58us/sample - loss: 0.3774 - accuracy: 0.8294 - val_loss: 0.6206 - val_accuracy: 0.6821
Epoch 36/200
3680/4150 [=========================&gt;....] - ETA: 0s - loss: 0.3832 - accuracy: 0.8236
Epoch 00036: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 65us/sample - loss: 0.3840 - accuracy: 0.8241 - val_loss: 0.5233 - val_accuracy: 0.7572
Epoch 37/200
3136/4150 [=====================&gt;........] - ETA: 0s - loss: 0.3945 - accuracy: 0.8198
Epoch 00037: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 55us/sample - loss: 0.3830 - accuracy: 0.8239 - val_loss: 0.3497 - val_accuracy: 0.8584
Epoch 38/200
3808/4150 [==========================&gt;...] - ETA: 0s - loss: 0.3579 - accuracy: 0.8364
Epoch 00038: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 67us/sample - loss: 0.3573 - accuracy: 0.8364 - val_loss: 0.2919 - val_accuracy: 0.8834
Epoch 39/200
3648/4150 [=========================&gt;....] - ETA: 0s - loss: 0.3657 - accuracy: 0.8358
Epoch 00039: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 78us/sample - loss: 0.3648 - accuracy: 0.8359 - val_loss: 0.5954 - val_accuracy: 0.7119
Epoch 40/200
3296/4150 [======================&gt;.......] - ETA: 0s - loss: 0.3994 - accuracy: 0.8249
Epoch 00040: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 67us/sample - loss: 0.3932 - accuracy: 0.8255 - val_loss: 0.3554 - val_accuracy: 0.8401
Epoch 41/200
3616/4150 [=========================&gt;....] - ETA: 0s - loss: 0.3337 - accuracy: 0.8498
Epoch 00041: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 68us/sample - loss: 0.3436 - accuracy: 0.8436 - val_loss: 0.8180 - val_accuracy: 0.5511
Epoch 42/200
3840/4150 [==========================&gt;...] - ETA: 0s - loss: 0.3613 - accuracy: 0.8359
Epoch 00042: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 66us/sample - loss: 0.3772 - accuracy: 0.8306 - val_loss: 0.0948 - val_accuracy: 0.9750
Epoch 43/200
4096/4150 [============================&gt;.] - ETA: 0s - loss: 0.3995 - accuracy: 0.8213
Epoch 00043: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 61us/sample - loss: 0.4000 - accuracy: 0.8207 - val_loss: 0.3151 - val_accuracy: 0.8642
Epoch 44/200
3808/4150 [==========================&gt;...] - ETA: 0s - loss: 0.3662 - accuracy: 0.8406
Epoch 00044: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 65us/sample - loss: 0.3634 - accuracy: 0.8395 - val_loss: 0.5752 - val_accuracy: 0.6985
Epoch 45/200
4000/4150 [===========================&gt;..] - ETA: 0s - loss: 0.3404 - accuracy: 0.8455
Epoch 00045: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 62us/sample - loss: 0.3400 - accuracy: 0.8455 - val_loss: 0.1817 - val_accuracy: 0.9576
Epoch 46/200
3776/4150 [==========================&gt;...] - ETA: 0s - loss: 0.3418 - accuracy: 0.8419
Epoch 00046: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 66us/sample - loss: 0.3389 - accuracy: 0.8427 - val_loss: 0.9198 - val_accuracy: 0.5568
Epoch 47/200
3104/4150 [=====================&gt;........] - ETA: 0s - loss: 0.3211 - accuracy: 0.8547
Epoch 00047: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 55us/sample - loss: 0.3142 - accuracy: 0.8590 - val_loss: 0.2766 - val_accuracy: 0.9162
Epoch 48/200
3360/4150 [=======================&gt;......] - ETA: 0s - loss: 0.3670 - accuracy: 0.8420
Epoch 00048: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 68us/sample - loss: 0.3657 - accuracy: 0.8407 - val_loss: 0.9511 - val_accuracy: 0.5202
Epoch 49/200
3648/4150 [=========================&gt;....] - ETA: 0s - loss: 0.3235 - accuracy: 0.8569
Epoch 00049: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 65us/sample - loss: 0.3250 - accuracy: 0.8571 - val_loss: 0.4125 - val_accuracy: 0.7890
Epoch 50/200
3936/4150 [===========================&gt;..] - ETA: 0s - loss: 0.3415 - accuracy: 0.8481
Epoch 00050: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 102us/sample - loss: 0.3423 - accuracy: 0.8482 - val_loss: 0.3057 - val_accuracy: 0.8719
Epoch 51/200
4032/4150 [============================&gt;.] - ETA: 0s - loss: 0.3244 - accuracy: 0.8537
Epoch 00051: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 89us/sample - loss: 0.3242 - accuracy: 0.8537 - val_loss: 0.7734 - val_accuracy: 0.6224
Epoch 52/200
4000/4150 [===========================&gt;..] - ETA: 0s - loss: 0.3154 - accuracy: 0.8555
Epoch 00052: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 64us/sample - loss: 0.3132 - accuracy: 0.8571 - val_loss: 0.5445 - val_accuracy: 0.7360
Epoch 53/200
3776/4150 [==========================&gt;...] - ETA: 0s - loss: 0.3350 - accuracy: 0.8512
Epoch 00053: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 65us/sample - loss: 0.3636 - accuracy: 0.8467 - val_loss: 1.8672 - val_accuracy: 0.4634
Epoch 54/200
4096/4150 [============================&gt;.] - ETA: 0s - loss: 0.4232 - accuracy: 0.8301
Epoch 00054: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 77us/sample - loss: 0.4219 - accuracy: 0.8306 - val_loss: 0.8686 - val_accuracy: 0.5906
Epoch 55/200
3584/4150 [========================&gt;.....] - ETA: 0s - loss: 0.3269 - accuracy: 0.8552
Epoch 00055: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 66us/sample - loss: 0.3319 - accuracy: 0.8542 - val_loss: 0.6138 - val_accuracy: 0.6830
Epoch 56/200
3936/4150 [===========================&gt;..] - ETA: 0s - loss: 0.3198 - accuracy: 0.8506
Epoch 00056: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 93us/sample - loss: 0.3221 - accuracy: 0.8499 - val_loss: 0.2172 - val_accuracy: 0.9191
Epoch 57/200
3520/4150 [========================&gt;.....] - ETA: 0s - loss: 0.2905 - accuracy: 0.8662
Epoch 00057: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 85us/sample - loss: 0.2974 - accuracy: 0.8631 - val_loss: 0.4433 - val_accuracy: 0.7813
Epoch 58/200
4096/4150 [============================&gt;.] - ETA: 0s - loss: 0.2850 - accuracy: 0.8713
Epoch 00058: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 92us/sample - loss: 0.2868 - accuracy: 0.8701 - val_loss: 0.5250 - val_accuracy: 0.7476
Epoch 59/200
3840/4150 [==========================&gt;...] - ETA: 0s - loss: 0.2969 - accuracy: 0.8714
Epoch 00059: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 80us/sample - loss: 0.2957 - accuracy: 0.8708 - val_loss: 0.4123 - val_accuracy: 0.8015
Epoch 60/200
3072/4150 [=====================&gt;........] - ETA: 0s - loss: 0.3165 - accuracy: 0.8545
Epoch 00060: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 73us/sample - loss: 0.3086 - accuracy: 0.8598 - val_loss: 0.5087 - val_accuracy: 0.7697
Epoch 61/200
3488/4150 [========================&gt;.....] - ETA: 0s - loss: 0.3035 - accuracy: 0.8675
Epoch 00061: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 84us/sample - loss: 0.3045 - accuracy: 0.8667 - val_loss: 0.4694 - val_accuracy: 0.7659
Epoch 62/200
3456/4150 [=======================&gt;......] - ETA: 0s - loss: 0.2977 - accuracy: 0.8686
Epoch 00062: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 92us/sample - loss: 0.3029 - accuracy: 0.8670 - val_loss: 0.4961 - val_accuracy: 0.7601
Epoch 63/200
3648/4150 [=========================&gt;....] - ETA: 0s - loss: 0.2788 - accuracy: 0.8791
Epoch 00063: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 69us/sample - loss: 0.2812 - accuracy: 0.8773 - val_loss: 0.3624 - val_accuracy: 0.8536
Epoch 64/200
3456/4150 [=======================&gt;......] - ETA: 0s - loss: 0.2482 - accuracy: 0.8935
Epoch 00064: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 67us/sample - loss: 0.2505 - accuracy: 0.8923 - val_loss: 0.3286 - val_accuracy: 0.8401
Epoch 65/200
3776/4150 [==========================&gt;...] - ETA: 0s - loss: 0.2946 - accuracy: 0.8657
Epoch 00065: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 66us/sample - loss: 0.2981 - accuracy: 0.8646 - val_loss: 0.2689 - val_accuracy: 0.8902
Epoch 66/200
3680/4150 [=========================&gt;....] - ETA: 0s - loss: 0.2928 - accuracy: 0.8687
Epoch 00066: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 66us/sample - loss: 0.2971 - accuracy: 0.8677 - val_loss: 0.3566 - val_accuracy: 0.8401
Epoch 67/200
3552/4150 [========================&gt;.....] - ETA: 0s - loss: 0.2886 - accuracy: 0.8680
Epoch 00067: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 65us/sample - loss: 0.2793 - accuracy: 0.8735 - val_loss: 0.2513 - val_accuracy: 0.8931
Epoch 68/200
3552/4150 [========================&gt;.....] - ETA: 0s - loss: 0.2549 - accuracy: 0.8899
Epoch 00068: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 71us/sample - loss: 0.2561 - accuracy: 0.8899 - val_loss: 0.1798 - val_accuracy: 0.9403
Epoch 69/200
3264/4150 [======================&gt;.......] - ETA: 0s - loss: 0.2284 - accuracy: 0.9007
Epoch 00069: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 69us/sample - loss: 0.2317 - accuracy: 0.8988 - val_loss: 0.2064 - val_accuracy: 0.9268
Epoch 70/200
3232/4150 [======================&gt;.......] - ETA: 0s - loss: 0.2376 - accuracy: 0.8936
Epoch 00070: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 68us/sample - loss: 0.2443 - accuracy: 0.8913 - val_loss: 1.1152 - val_accuracy: 0.5800
Epoch 71/200
3648/4150 [=========================&gt;....] - ETA: 0s - loss: 0.2809 - accuracy: 0.8777
Epoch 00071: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 68us/sample - loss: 0.2754 - accuracy: 0.8798 - val_loss: 0.4841 - val_accuracy: 0.7592
Epoch 72/200
3584/4150 [========================&gt;.....] - ETA: 0s - loss: 0.2537 - accuracy: 0.8959
Epoch 00072: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 67us/sample - loss: 0.2546 - accuracy: 0.8961 - val_loss: 0.3350 - val_accuracy: 0.8565
Epoch 73/200
3680/4150 [=========================&gt;....] - ETA: 0s - loss: 0.2335 - accuracy: 0.8995
Epoch 00073: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 67us/sample - loss: 0.2300 - accuracy: 0.9014 - val_loss: 0.1903 - val_accuracy: 0.9249
Epoch 74/200
4000/4150 [===========================&gt;..] - ETA: 0s - loss: 0.2097 - accuracy: 0.9082
Epoch 00074: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 62us/sample - loss: 0.2100 - accuracy: 0.9084 - val_loss: 0.1718 - val_accuracy: 0.9383
Epoch 75/200
4032/4150 [============================&gt;.] - ETA: 0s - loss: 0.2397 - accuracy: 0.8951
Epoch 00075: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 63us/sample - loss: 0.2446 - accuracy: 0.8935 - val_loss: 0.4491 - val_accuracy: 0.7919
Epoch 76/200
3424/4150 [=======================&gt;......] - ETA: 0s - loss: 0.2607 - accuracy: 0.8922
Epoch 00076: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 68us/sample - loss: 0.2558 - accuracy: 0.8930 - val_loss: 0.3227 - val_accuracy: 0.8796
Epoch 77/200
4128/4150 [============================&gt;.] - ETA: 0s - loss: 0.3486 - accuracy: 0.8631
Epoch 00077: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 75us/sample - loss: 0.3492 - accuracy: 0.8631 - val_loss: 0.3973 - val_accuracy: 0.8237
Epoch 78/200
3520/4150 [========================&gt;.....] - ETA: 0s - loss: 0.2366 - accuracy: 0.8920
Epoch 00078: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 73us/sample - loss: 0.2287 - accuracy: 0.8966 - val_loss: 0.0869 - val_accuracy: 0.9865
Epoch 79/200
4064/4150 [============================&gt;.] - ETA: 0s - loss: 0.2072 - accuracy: 0.9109
Epoch 00079: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 76us/sample - loss: 0.2068 - accuracy: 0.9108 - val_loss: 0.3633 - val_accuracy: 0.8574
Epoch 80/200
3488/4150 [========================&gt;.....] - ETA: 0s - loss: 0.2444 - accuracy: 0.8982
Epoch 00080: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 83us/sample - loss: 0.2391 - accuracy: 0.9002 - val_loss: 0.3194 - val_accuracy: 0.8661
Epoch 81/200
4032/4150 [============================&gt;.] - ETA: 0s - loss: 0.3194 - accuracy: 0.8596
Epoch 00081: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 79us/sample - loss: 0.3161 - accuracy: 0.8610 - val_loss: 0.2523 - val_accuracy: 0.9017
Epoch 82/200
3648/4150 [=========================&gt;....] - ETA: 0s - loss: 0.1991 - accuracy: 0.9191
Epoch 00082: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 66us/sample - loss: 0.1943 - accuracy: 0.9205 - val_loss: 0.2605 - val_accuracy: 0.9008
Epoch 83/200
3328/4150 [=======================&gt;......] - ETA: 0s - loss: 0.1912 - accuracy: 0.9174
Epoch 00083: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 71us/sample - loss: 0.2083 - accuracy: 0.9118 - val_loss: 0.4604 - val_accuracy: 0.7640
Epoch 84/200
3968/4150 [===========================&gt;..] - ETA: 0s - loss: 0.2044 - accuracy: 0.9168
Epoch 00084: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 79us/sample - loss: 0.2084 - accuracy: 0.9145 - val_loss: 0.0801 - val_accuracy: 0.9778
Epoch 85/200
3776/4150 [==========================&gt;...] - ETA: 0s - loss: 0.1974 - accuracy: 0.9198
Epoch 00085: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 67us/sample - loss: 0.1943 - accuracy: 0.9210 - val_loss: 0.1945 - val_accuracy: 0.9229
Epoch 86/200
3616/4150 [=========================&gt;....] - ETA: 0s - loss: 0.1886 - accuracy: 0.9259
Epoch 00086: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 66us/sample - loss: 0.1959 - accuracy: 0.9224 - val_loss: 0.2835 - val_accuracy: 0.8748
Epoch 87/200
3136/4150 [=====================&gt;........] - ETA: 0s - loss: 0.1832 - accuracy: 0.9238
Epoch 00087: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 58us/sample - loss: 0.1803 - accuracy: 0.9258 - val_loss: 0.3129 - val_accuracy: 0.8661
Epoch 88/200
3712/4150 [=========================&gt;....] - ETA: 0s - loss: 0.1638 - accuracy: 0.9337
Epoch 00088: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 73us/sample - loss: 0.1774 - accuracy: 0.9275 - val_loss: 0.2522 - val_accuracy: 0.9056
Epoch 89/200
3424/4150 [=======================&gt;......] - ETA: 0s - loss: 0.1815 - accuracy: 0.9209
Epoch 00089: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 67us/sample - loss: 0.1858 - accuracy: 0.9205 - val_loss: 0.3043 - val_accuracy: 0.8738
Epoch 90/200
3840/4150 [==========================&gt;...] - ETA: 0s - loss: 0.2214 - accuracy: 0.9081
Epoch 00090: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 65us/sample - loss: 0.2303 - accuracy: 0.9046 - val_loss: 0.4757 - val_accuracy: 0.7755
Epoch 91/200
3840/4150 [==========================&gt;...] - ETA: 0s - loss: 0.2354 - accuracy: 0.9000
Epoch 00091: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 64us/sample - loss: 0.2316 - accuracy: 0.9012 - val_loss: 0.2926 - val_accuracy: 0.8661
Epoch 92/200
3936/4150 [===========================&gt;..] - ETA: 0s - loss: 0.1659 - accuracy: 0.9352
Epoch 00092: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 94us/sample - loss: 0.1648 - accuracy: 0.9354 - val_loss: 0.2035 - val_accuracy: 0.9056
Epoch 93/200
4000/4150 [===========================&gt;..] - ETA: 0s - loss: 0.1665 - accuracy: 0.9293
Epoch 00093: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 96us/sample - loss: 0.1669 - accuracy: 0.9301 - val_loss: 0.5794 - val_accuracy: 0.7553
Epoch 94/200
3648/4150 [=========================&gt;....] - ETA: 0s - loss: 0.2635 - accuracy: 0.8953
Epoch 00094: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 66us/sample - loss: 0.2592 - accuracy: 0.8961 - val_loss: 0.2072 - val_accuracy: 0.9056
Epoch 95/200
4128/4150 [============================&gt;.] - ETA: 0s - loss: 0.1995 - accuracy: 0.9201
Epoch 00095: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 60us/sample - loss: 0.2028 - accuracy: 0.9195 - val_loss: 0.3215 - val_accuracy: 0.8622
Epoch 96/200
3776/4150 [==========================&gt;...] - ETA: 0s - loss: 0.1707 - accuracy: 0.9309
Epoch 00096: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 70us/sample - loss: 0.1723 - accuracy: 0.9294 - val_loss: 0.4003 - val_accuracy: 0.8247
Epoch 97/200
3520/4150 [========================&gt;.....] - ETA: 0s - loss: 0.1772 - accuracy: 0.9276
Epoch 00097: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 68us/sample - loss: 0.1707 - accuracy: 0.9294 - val_loss: 0.2002 - val_accuracy: 0.9200
Epoch 98/200
3360/4150 [=======================&gt;......] - ETA: 0s - loss: 0.1465 - accuracy: 0.9455
Epoch 00098: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 68us/sample - loss: 0.1562 - accuracy: 0.9400 - val_loss: 0.2154 - val_accuracy: 0.9191
Epoch 99/200
3456/4150 [=======================&gt;......] - ETA: 0s - loss: 0.1285 - accuracy: 0.9531
Epoch 00099: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 68us/sample - loss: 0.1329 - accuracy: 0.9504 - val_loss: 0.1291 - val_accuracy: 0.9509
Epoch 100/200
3776/4150 [==========================&gt;...] - ETA: 0s - loss: 0.1636 - accuracy: 0.9338
Epoch 00100: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 64us/sample - loss: 0.1713 - accuracy: 0.9316 - val_loss: 0.0757 - val_accuracy: 0.9778
Epoch 101/200
3424/4150 [=======================&gt;......] - ETA: 0s - loss: 0.1823 - accuracy: 0.9305
Epoch 00101: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 68us/sample - loss: 0.1816 - accuracy: 0.9287 - val_loss: 0.0785 - val_accuracy: 0.9740
Epoch 102/200
3648/4150 [=========================&gt;....] - ETA: 0s - loss: 0.2401 - accuracy: 0.9104
Epoch 00102: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 66us/sample - loss: 0.2464 - accuracy: 0.9060 - val_loss: 0.4533 - val_accuracy: 0.8179
Epoch 103/200
4096/4150 [============================&gt;.] - ETA: 0s - loss: 0.1977 - accuracy: 0.9243
Epoch 00103: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 62us/sample - loss: 0.1957 - accuracy: 0.9251 - val_loss: 0.2030 - val_accuracy: 0.9152
Epoch 104/200
3648/4150 [=========================&gt;....] - ETA: 0s - loss: 0.1221 - accuracy: 0.9534
Epoch 00104: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 67us/sample - loss: 0.1290 - accuracy: 0.9501 - val_loss: 0.1589 - val_accuracy: 0.9422
Epoch 105/200
3968/4150 [===========================&gt;..] - ETA: 0s - loss: 0.1240 - accuracy: 0.9529
Epoch 00105: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 63us/sample - loss: 0.1253 - accuracy: 0.9518 - val_loss: 0.1045 - val_accuracy: 0.9711
Epoch 106/200
4128/4150 [============================&gt;.] - ETA: 0s - loss: 0.2017 - accuracy: 0.9254
Epoch 00106: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 62us/sample - loss: 0.2022 - accuracy: 0.9251 - val_loss: 0.5831 - val_accuracy: 0.7225
Epoch 107/200
3776/4150 [==========================&gt;...] - ETA: 0s - loss: 0.1887 - accuracy: 0.9232
Epoch 00107: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 65us/sample - loss: 0.1873 - accuracy: 0.9239 - val_loss: 0.1243 - val_accuracy: 0.9538
Epoch 108/200
4096/4150 [============================&gt;.] - ETA: 0s - loss: 0.1412 - accuracy: 0.9470
Epoch 00108: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 62us/sample - loss: 0.1414 - accuracy: 0.9467 - val_loss: 0.2015 - val_accuracy: 0.9297
Epoch 109/200
3104/4150 [=====================&gt;........] - ETA: 0s - loss: 0.1846 - accuracy: 0.9246
Epoch 00109: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 55us/sample - loss: 0.1794 - accuracy: 0.9258 - val_loss: 0.3453 - val_accuracy: 0.8507
Epoch 110/200
3552/4150 [========================&gt;.....] - ETA: 0s - loss: 0.1780 - accuracy: 0.9282
Epoch 00110: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 88us/sample - loss: 0.1760 - accuracy: 0.9284 - val_loss: 0.4662 - val_accuracy: 0.8073
Epoch 111/200
3552/4150 [========================&gt;.....] - ETA: 0s - loss: 0.1931 - accuracy: 0.9178
Epoch 00111: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 81us/sample - loss: 0.1832 - accuracy: 0.9219 - val_loss: 0.2865 - val_accuracy: 0.8690
Epoch 112/200
3968/4150 [===========================&gt;..] - ETA: 0s - loss: 0.1441 - accuracy: 0.9415
Epoch 00112: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 92us/sample - loss: 0.1435 - accuracy: 0.9412 - val_loss: 0.2499 - val_accuracy: 0.9066
Epoch 113/200
3744/4150 [==========================&gt;...] - ETA: 0s - loss: 0.1163 - accuracy: 0.9570
Epoch 00113: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 81us/sample - loss: 0.1176 - accuracy: 0.9557 - val_loss: 0.1226 - val_accuracy: 0.9586
Epoch 114/200
3744/4150 [==========================&gt;...] - ETA: 0s - loss: 0.2035 - accuracy: 0.9279
Epoch 00114: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 80us/sample - loss: 0.2155 - accuracy: 0.9229 - val_loss: 0.5924 - val_accuracy: 0.8006
Epoch 115/200
3904/4150 [===========================&gt;..] - ETA: 0s - loss: 0.3437 - accuracy: 0.8768
Epoch 00115: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 82us/sample - loss: 0.3334 - accuracy: 0.8802 - val_loss: 0.1821 - val_accuracy: 0.9258
Epoch 116/200
3552/4150 [========================&gt;.....] - ETA: 0s - loss: 0.1581 - accuracy: 0.9383
Epoch 00116: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 82us/sample - loss: 0.1530 - accuracy: 0.9405 - val_loss: 0.2858 - val_accuracy: 0.8902
Epoch 117/200
3168/4150 [=====================&gt;........] - ETA: 0s - loss: 0.1240 - accuracy: 0.9508
Epoch 00117: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 68us/sample - loss: 0.1262 - accuracy: 0.9484 - val_loss: 0.3326 - val_accuracy: 0.8526
Epoch 118/200
3360/4150 [=======================&gt;......] - ETA: 0s - loss: 0.1031 - accuracy: 0.9628
Epoch 00118: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 72us/sample - loss: 0.1081 - accuracy: 0.9607 - val_loss: 0.1880 - val_accuracy: 0.9287
Epoch 119/200
3712/4150 [=========================&gt;....] - ETA: 0s - loss: 0.1292 - accuracy: 0.9488
Epoch 00119: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 66us/sample - loss: 0.1311 - accuracy: 0.9484 - val_loss: 0.2186 - val_accuracy: 0.9162
Epoch 120/200
3936/4150 [===========================&gt;..] - ETA: 0s - loss: 0.1137 - accuracy: 0.9560
Epoch 00120: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 63us/sample - loss: 0.1126 - accuracy: 0.9564 - val_loss: 0.4034 - val_accuracy: 0.8198
Epoch 121/200
3648/4150 [=========================&gt;....] - ETA: 0s - loss: 0.1928 - accuracy: 0.9241
Epoch 00121: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 66us/sample - loss: 0.1910 - accuracy: 0.9239 - val_loss: 0.4124 - val_accuracy: 0.8603
Epoch 122/200
3712/4150 [=========================&gt;....] - ETA: 0s - loss: 0.1715 - accuracy: 0.9294
Epoch 00122: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 66us/sample - loss: 0.1715 - accuracy: 0.9289 - val_loss: 0.3277 - val_accuracy: 0.8699
Epoch 123/200
4032/4150 [============================&gt;.] - ETA: 0s - loss: 0.1327 - accuracy: 0.9462
Epoch 00123: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 62us/sample - loss: 0.1345 - accuracy: 0.9453 - val_loss: 0.2165 - val_accuracy: 0.9133
Epoch 124/200
3712/4150 [=========================&gt;....] - ETA: 0s - loss: 0.1518 - accuracy: 0.9450
Epoch 00124: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 66us/sample - loss: 0.1470 - accuracy: 0.9465 - val_loss: 0.1113 - val_accuracy: 0.9595
Epoch 125/200
3936/4150 [===========================&gt;..] - ETA: 0s - loss: 0.1125 - accuracy: 0.9520
Epoch 00125: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 64us/sample - loss: 0.1128 - accuracy: 0.9518 - val_loss: 0.1098 - val_accuracy: 0.9566
Epoch 126/200
3808/4150 [==========================&gt;...] - ETA: 0s - loss: 0.1841 - accuracy: 0.9357
Epoch 00126: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 64us/sample - loss: 0.1846 - accuracy: 0.9342 - val_loss: 0.3587 - val_accuracy: 0.8459
Epoch 127/200
3968/4150 [===========================&gt;..] - ETA: 0s - loss: 0.1433 - accuracy: 0.9458
Epoch 00127: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 63us/sample - loss: 0.1469 - accuracy: 0.9424 - val_loss: 0.2850 - val_accuracy: 0.8699
Epoch 128/200
4064/4150 [============================&gt;.] - ETA: 0s - loss: 0.2197 - accuracy: 0.9178
Epoch 00128: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 62us/sample - loss: 0.2175 - accuracy: 0.9188 - val_loss: 0.3444 - val_accuracy: 0.8410
Epoch 129/200
3648/4150 [=========================&gt;....] - ETA: 0s - loss: 0.1032 - accuracy: 0.9608
Epoch 00129: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 67us/sample - loss: 0.1015 - accuracy: 0.9619 - val_loss: 0.1084 - val_accuracy: 0.9586
Epoch 130/200
3584/4150 [========================&gt;.....] - ETA: 0s - loss: 0.0893 - accuracy: 0.9665
Epoch 00130: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 66us/sample - loss: 0.0899 - accuracy: 0.9670 - val_loss: 0.1240 - val_accuracy: 0.9595
Epoch 131/200
3648/4150 [=========================&gt;....] - ETA: 0s - loss: 0.1136 - accuracy: 0.9581
Epoch 00131: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 68us/sample - loss: 0.1146 - accuracy: 0.9559 - val_loss: 0.2438 - val_accuracy: 0.9181
Epoch 132/200
3456/4150 [=======================&gt;......] - ETA: 0s - loss: 0.1288 - accuracy: 0.9494
Epoch 00132: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 67us/sample - loss: 0.1293 - accuracy: 0.9508 - val_loss: 0.2368 - val_accuracy: 0.9104
Epoch 133/200
3584/4150 [========================&gt;.....] - ETA: 0s - loss: 0.1355 - accuracy: 0.9467
Epoch 00133: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 67us/sample - loss: 0.1334 - accuracy: 0.9472 - val_loss: 0.3295 - val_accuracy: 0.8603
Epoch 134/200
4032/4150 [============================&gt;.] - ETA: 0s - loss: 0.0934 - accuracy: 0.9630
Epoch 00134: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 63us/sample - loss: 0.0933 - accuracy: 0.9627 - val_loss: 0.2893 - val_accuracy: 0.8863
Epoch 135/200
3520/4150 [========================&gt;.....] - ETA: 0s - loss: 0.1395 - accuracy: 0.9435
Epoch 00135: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 67us/sample - loss: 0.1337 - accuracy: 0.9453 - val_loss: 0.1470 - val_accuracy: 0.9480
Epoch 136/200
3616/4150 [=========================&gt;....] - ETA: 0s - loss: 0.0970 - accuracy: 0.9621
Epoch 00136: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 66us/sample - loss: 0.1045 - accuracy: 0.9583 - val_loss: 0.4538 - val_accuracy: 0.8227
Epoch 137/200
4096/4150 [============================&gt;.] - ETA: 0s - loss: 0.1278 - accuracy: 0.9490
Epoch 00137: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 62us/sample - loss: 0.1276 - accuracy: 0.9487 - val_loss: 0.1505 - val_accuracy: 0.9518
Epoch 138/200
3680/4150 [=========================&gt;....] - ETA: 0s - loss: 0.0871 - accuracy: 0.9696
Epoch 00138: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 65us/sample - loss: 0.0865 - accuracy: 0.9699 - val_loss: 0.3979 - val_accuracy: 0.8304
Epoch 139/200
4032/4150 [============================&gt;.] - ETA: 0s - loss: 0.1231 - accuracy: 0.9544
Epoch 00139: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 62us/sample - loss: 0.1249 - accuracy: 0.9530 - val_loss: 0.2698 - val_accuracy: 0.8940
Epoch 140/200
3712/4150 [=========================&gt;....] - ETA: 0s - loss: 0.2216 - accuracy: 0.9165
Epoch 00140: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 65us/sample - loss: 0.2178 - accuracy: 0.9178 - val_loss: 0.2856 - val_accuracy: 0.8911
Epoch 141/200
4128/4150 [============================&gt;.] - ETA: 0s - loss: 0.1315 - accuracy: 0.9462
Epoch 00141: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 62us/sample - loss: 0.1318 - accuracy: 0.9458 - val_loss: 0.2807 - val_accuracy: 0.8979
Epoch 142/200
3584/4150 [========================&gt;.....] - ETA: 0s - loss: 0.0925 - accuracy: 0.9648
Epoch 00142: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 67us/sample - loss: 0.0972 - accuracy: 0.9629 - val_loss: 0.1922 - val_accuracy: 0.9383
Epoch 143/200
3776/4150 [==========================&gt;...] - ETA: 0s - loss: 0.1394 - accuracy: 0.9473
Epoch 00143: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 64us/sample - loss: 0.1378 - accuracy: 0.9487 - val_loss: 0.1834 - val_accuracy: 0.9412
Epoch 144/200
3136/4150 [=====================&gt;........] - ETA: 0s - loss: 0.1069 - accuracy: 0.9589
Epoch 00144: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 60us/sample - loss: 0.1043 - accuracy: 0.9600 - val_loss: 0.1969 - val_accuracy: 0.9258
Epoch 145/200
3552/4150 [========================&gt;.....] - ETA: 0s - loss: 0.1387 - accuracy: 0.9454
Epoch 00145: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 65us/sample - loss: 0.1360 - accuracy: 0.9467 - val_loss: 0.2960 - val_accuracy: 0.8931
Epoch 146/200
3104/4150 [=====================&gt;........] - ETA: 0s - loss: 0.1051 - accuracy: 0.9613
Epoch 00146: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 55us/sample - loss: 0.1018 - accuracy: 0.9631 - val_loss: 0.1542 - val_accuracy: 0.9461
Epoch 147/200
3872/4150 [==========================&gt;...] - ETA: 0s - loss: 0.0661 - accuracy: 0.9749
Epoch 00147: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 67us/sample - loss: 0.0660 - accuracy: 0.9754 - val_loss: 0.0882 - val_accuracy: 0.9778
Epoch 148/200
3744/4150 [==========================&gt;...] - ETA: 0s - loss: 0.0946 - accuracy: 0.9642
Epoch 00148: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 65us/sample - loss: 0.0927 - accuracy: 0.9648 - val_loss: 0.2689 - val_accuracy: 0.8969
Epoch 149/200
3136/4150 [=====================&gt;........] - ETA: 0s - loss: 0.1031 - accuracy: 0.9589
Epoch 00149: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 56us/sample - loss: 0.1249 - accuracy: 0.9537 - val_loss: 1.0692 - val_accuracy: 0.7100
Epoch 150/200
3488/4150 [========================&gt;.....] - ETA: 0s - loss: 0.1848 - accuracy: 0.9286
Epoch 00150: val_loss did not improve from 0.05836
4150/4150 [==============================] - 0s 68us/sample - loss: 0.1708 - accuracy: 0.9352 - val_loss: 0.2344 - val_accuracy: 0.8979
Epoch 151/200
3392/4150 [=======================&gt;......] - ETA: 0s - loss: 0.0707 - accuracy: 0.9741
Epoch 00151: val_loss improved from 0.05836 to 0.04715, saving model to best_model_2.h5
4150/4150 [==============================] - 0s 68us/sample - loss: 0.0712 - accuracy: 0.9742 - val_loss: 0.0471 - val_accuracy: 0.9836
Epoch 152/200
3424/4150 [=======================&gt;......] - ETA: 0s - loss: 0.1431 - accuracy: 0.9492
Epoch 00152: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 68us/sample - loss: 0.1849 - accuracy: 0.9376 - val_loss: 0.8206 - val_accuracy: 0.8064
Epoch 153/200
3648/4150 [=========================&gt;....] - ETA: 0s - loss: 0.3627 - accuracy: 0.8838
Epoch 00153: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 67us/sample - loss: 0.3399 - accuracy: 0.8896 - val_loss: 0.1947 - val_accuracy: 0.9133
Epoch 154/200
4096/4150 [============================&gt;.] - ETA: 0s - loss: 0.1524 - accuracy: 0.9390
Epoch 00154: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 61us/sample - loss: 0.1521 - accuracy: 0.9390 - val_loss: 0.1608 - val_accuracy: 0.9528
Epoch 155/200
3072/4150 [=====================&gt;........] - ETA: 0s - loss: 0.0961 - accuracy: 0.9609
Epoch 00155: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 70us/sample - loss: 0.0910 - accuracy: 0.9646 - val_loss: 0.1316 - val_accuracy: 0.9576
Epoch 156/200
3680/4150 [=========================&gt;....] - ETA: 0s - loss: 0.0662 - accuracy: 0.9758
Epoch 00156: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 67us/sample - loss: 0.0658 - accuracy: 0.9759 - val_loss: 0.1433 - val_accuracy: 0.9499
Epoch 157/200
4064/4150 [============================&gt;.] - ETA: 0s - loss: 0.0548 - accuracy: 0.9813
Epoch 00157: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 63us/sample - loss: 0.0554 - accuracy: 0.9810 - val_loss: 0.1461 - val_accuracy: 0.9432
Epoch 158/200
3744/4150 [==========================&gt;...] - ETA: 0s - loss: 0.0723 - accuracy: 0.9706
Epoch 00158: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 65us/sample - loss: 0.0771 - accuracy: 0.9689 - val_loss: 0.0820 - val_accuracy: 0.9730
Epoch 159/200
3840/4150 [==========================&gt;...] - ETA: 0s - loss: 0.0952 - accuracy: 0.9620
Epoch 00159: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 65us/sample - loss: 0.0958 - accuracy: 0.9624 - val_loss: 0.1850 - val_accuracy: 0.9306
Epoch 160/200
3072/4150 [=====================&gt;........] - ETA: 0s - loss: 0.0881 - accuracy: 0.9652
Epoch 00160: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 57us/sample - loss: 0.0772 - accuracy: 0.9704 - val_loss: 0.1663 - val_accuracy: 0.9374
Epoch 161/200
3520/4150 [========================&gt;.....] - ETA: 0s - loss: 0.0640 - accuracy: 0.9778
Epoch 00161: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 69us/sample - loss: 0.0668 - accuracy: 0.9759 - val_loss: 0.1676 - val_accuracy: 0.9316
Epoch 162/200
3904/4150 [===========================&gt;..] - ETA: 0s - loss: 0.0626 - accuracy: 0.9759
Epoch 00162: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 64us/sample - loss: 0.0625 - accuracy: 0.9761 - val_loss: 0.1481 - val_accuracy: 0.9538
Epoch 163/200
4032/4150 [============================&gt;.] - ETA: 0s - loss: 0.0625 - accuracy: 0.9777
Epoch 00163: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 62us/sample - loss: 0.0618 - accuracy: 0.9783 - val_loss: 0.2916 - val_accuracy: 0.8960
Epoch 164/200
3648/4150 [=========================&gt;....] - ETA: 0s - loss: 0.0721 - accuracy: 0.9737
Epoch 00164: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 66us/sample - loss: 0.0701 - accuracy: 0.9745 - val_loss: 0.1719 - val_accuracy: 0.9374
Epoch 165/200
4064/4150 [============================&gt;.] - ETA: 0s - loss: 0.1060 - accuracy: 0.9604
Epoch 00165: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 62us/sample - loss: 0.1059 - accuracy: 0.9602 - val_loss: 0.1278 - val_accuracy: 0.9595
Epoch 166/200
3712/4150 [=========================&gt;....] - ETA: 0s - loss: 0.1628 - accuracy: 0.9421
Epoch 00166: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 66us/sample - loss: 0.1557 - accuracy: 0.9443 - val_loss: 0.4369 - val_accuracy: 0.8333
Epoch 167/200
4096/4150 [============================&gt;.] - ETA: 0s - loss: 0.1795 - accuracy: 0.9348
Epoch 00167: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 64us/sample - loss: 0.1776 - accuracy: 0.9354 - val_loss: 0.1298 - val_accuracy: 0.9518
Epoch 168/200
4032/4150 [============================&gt;.] - ETA: 0s - loss: 0.0874 - accuracy: 0.9675
Epoch 00168: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 78us/sample - loss: 0.0856 - accuracy: 0.9682 - val_loss: 0.1216 - val_accuracy: 0.9624
Epoch 169/200
3520/4150 [========================&gt;.....] - ETA: 0s - loss: 0.0924 - accuracy: 0.9628
Epoch 00169: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 85us/sample - loss: 0.1031 - accuracy: 0.9600 - val_loss: 0.2214 - val_accuracy: 0.9123
Epoch 170/200
3552/4150 [========================&gt;.....] - ETA: 0s - loss: 0.2123 - accuracy: 0.9369
Epoch 00170: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 84us/sample - loss: 0.1987 - accuracy: 0.9393 - val_loss: 0.1301 - val_accuracy: 0.9624
Epoch 171/200
3680/4150 [=========================&gt;....] - ETA: 0s - loss: 0.0729 - accuracy: 0.9758
Epoch 00171: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 85us/sample - loss: 0.0722 - accuracy: 0.9754 - val_loss: 0.2039 - val_accuracy: 0.9345
Epoch 172/200
3424/4150 [=======================&gt;......] - ETA: 0s - loss: 0.0558 - accuracy: 0.9801
Epoch 00172: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 72us/sample - loss: 0.0570 - accuracy: 0.9786 - val_loss: 0.1378 - val_accuracy: 0.9538
Epoch 173/200
4000/4150 [===========================&gt;..] - ETA: 0s - loss: 0.0442 - accuracy: 0.9852
Epoch 00173: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 92us/sample - loss: 0.0440 - accuracy: 0.9851 - val_loss: 0.0923 - val_accuracy: 0.9711
Epoch 174/200
3232/4150 [======================&gt;.......] - ETA: 0s - loss: 0.0422 - accuracy: 0.9889
Epoch 00174: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 68us/sample - loss: 0.0432 - accuracy: 0.9884 - val_loss: 0.1566 - val_accuracy: 0.9412
Epoch 175/200
3488/4150 [========================&gt;.....] - ETA: 0s - loss: 0.0517 - accuracy: 0.9811
Epoch 00175: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 71us/sample - loss: 0.0539 - accuracy: 0.9805 - val_loss: 0.2207 - val_accuracy: 0.9326
Epoch 176/200
4032/4150 [============================&gt;.] - ETA: 0s - loss: 0.0820 - accuracy: 0.9680
Epoch 00176: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 63us/sample - loss: 0.0848 - accuracy: 0.9672 - val_loss: 0.6232 - val_accuracy: 0.8035
Epoch 177/200
3968/4150 [===========================&gt;..] - ETA: 0s - loss: 0.1181 - accuracy: 0.9569
Epoch 00177: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 63us/sample - loss: 0.1158 - accuracy: 0.9573 - val_loss: 0.1609 - val_accuracy: 0.9547
Epoch 178/200
3936/4150 [===========================&gt;..] - ETA: 0s - loss: 0.0983 - accuracy: 0.9627
Epoch 00178: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 63us/sample - loss: 0.0980 - accuracy: 0.9631 - val_loss: 0.0912 - val_accuracy: 0.9740
Epoch 179/200
4032/4150 [============================&gt;.] - ETA: 0s - loss: 0.0680 - accuracy: 0.9772
Epoch 00179: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 63us/sample - loss: 0.0686 - accuracy: 0.9769 - val_loss: 0.2009 - val_accuracy: 0.9412
Epoch 180/200
3616/4150 [=========================&gt;....] - ETA: 0s - loss: 0.1006 - accuracy: 0.9616
Epoch 00180: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 67us/sample - loss: 0.0963 - accuracy: 0.9631 - val_loss: 0.0852 - val_accuracy: 0.9701
Epoch 181/200
3904/4150 [===========================&gt;..] - ETA: 0s - loss: 0.0531 - accuracy: 0.9810
Epoch 00181: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 64us/sample - loss: 0.0531 - accuracy: 0.9812 - val_loss: 0.0864 - val_accuracy: 0.9740
Epoch 182/200
3648/4150 [=========================&gt;....] - ETA: 0s - loss: 0.0855 - accuracy: 0.9646
Epoch 00182: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 80us/sample - loss: 0.0861 - accuracy: 0.9643 - val_loss: 0.0859 - val_accuracy: 0.9624
Epoch 183/200
4000/4150 [===========================&gt;..] - ETA: 0s - loss: 0.1033 - accuracy: 0.9617
Epoch 00183: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 77us/sample - loss: 0.1011 - accuracy: 0.9627 - val_loss: 0.1194 - val_accuracy: 0.9586
Epoch 184/200
3904/4150 [===========================&gt;..] - ETA: 0s - loss: 0.1102 - accuracy: 0.9618
Epoch 00184: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 64us/sample - loss: 0.1144 - accuracy: 0.9602 - val_loss: 0.2139 - val_accuracy: 0.9181
Epoch 185/200
3424/4150 [=======================&gt;......] - ETA: 0s - loss: 0.0740 - accuracy: 0.9734
Epoch 00185: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 70us/sample - loss: 0.0769 - accuracy: 0.9725 - val_loss: 0.0643 - val_accuracy: 0.9798
Epoch 186/200
4096/4150 [============================&gt;.] - ETA: 0s - loss: 0.0870 - accuracy: 0.9651
Epoch 00186: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 74us/sample - loss: 0.0887 - accuracy: 0.9651 - val_loss: 0.1136 - val_accuracy: 0.9576
Epoch 187/200
4096/4150 [============================&gt;.] - ETA: 0s - loss: 0.1201 - accuracy: 0.9548
Epoch 00187: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 75us/sample - loss: 0.1200 - accuracy: 0.9549 - val_loss: 0.2044 - val_accuracy: 0.9268
Epoch 188/200
3776/4150 [==========================&gt;...] - ETA: 0s - loss: 0.0883 - accuracy: 0.9674
Epoch 00188: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 65us/sample - loss: 0.0880 - accuracy: 0.9667 - val_loss: 0.1785 - val_accuracy: 0.9374
Epoch 189/200
3616/4150 [=========================&gt;....] - ETA: 0s - loss: 0.0437 - accuracy: 0.9851
Epoch 00189: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 68us/sample - loss: 0.0430 - accuracy: 0.9853 - val_loss: 0.0579 - val_accuracy: 0.9778
Epoch 190/200
3392/4150 [=======================&gt;......] - ETA: 0s - loss: 0.0516 - accuracy: 0.9832
Epoch 00190: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 70us/sample - loss: 0.0489 - accuracy: 0.9841 - val_loss: 0.0757 - val_accuracy: 0.9692
Epoch 191/200
3776/4150 [==========================&gt;...] - ETA: 0s - loss: 0.0453 - accuracy: 0.9865
Epoch 00191: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 84us/sample - loss: 0.0479 - accuracy: 0.9855 - val_loss: 0.1297 - val_accuracy: 0.9538
Epoch 192/200
4032/4150 [============================&gt;.] - ETA: 0s - loss: 0.1762 - accuracy: 0.9435
Epoch 00192: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 77us/sample - loss: 0.1823 - accuracy: 0.9414 - val_loss: 1.0624 - val_accuracy: 0.6753
Epoch 193/200
4032/4150 [============================&gt;.] - ETA: 0s - loss: 0.4002 - accuracy: 0.8772
Epoch 00193: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 78us/sample - loss: 0.3930 - accuracy: 0.8790 - val_loss: 0.2197 - val_accuracy: 0.9277
Epoch 194/200
4096/4150 [============================&gt;.] - ETA: 0s - loss: 0.1055 - accuracy: 0.9604
Epoch 00194: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 107us/sample - loss: 0.1047 - accuracy: 0.9607 - val_loss: 0.2236 - val_accuracy: 0.9229
Epoch 195/200
3680/4150 [=========================&gt;....] - ETA: 0s - loss: 0.0665 - accuracy: 0.9793
Epoch 00195: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 103us/sample - loss: 0.0640 - accuracy: 0.9798 - val_loss: 0.0716 - val_accuracy: 0.9778
Epoch 196/200
3616/4150 [=========================&gt;....] - ETA: 0s - loss: 0.0378 - accuracy: 0.9884
Epoch 00196: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 52us/sample - loss: 0.0400 - accuracy: 0.9875 - val_loss: 0.1890 - val_accuracy: 0.9345
Epoch 197/200
3360/4150 [=======================&gt;......] - ETA: 0s - loss: 0.0698 - accuracy: 0.9762
Epoch 00197: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 56us/sample - loss: 0.0634 - accuracy: 0.9783 - val_loss: 0.1437 - val_accuracy: 0.9595
Epoch 198/200
3264/4150 [======================&gt;.......] - ETA: 0s - loss: 0.0643 - accuracy: 0.9727
Epoch 00198: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 56us/sample - loss: 0.0665 - accuracy: 0.9733 - val_loss: 0.0508 - val_accuracy: 0.9827
Epoch 199/200
3808/4150 [==========================&gt;...] - ETA: 0s - loss: 0.0722 - accuracy: 0.9745
Epoch 00199: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 50us/sample - loss: 0.0712 - accuracy: 0.9749 - val_loss: 0.0954 - val_accuracy: 0.9701
Epoch 200/200
3904/4150 [===========================&gt;..] - ETA: 0s - loss: 0.0638 - accuracy: 0.9793
Epoch 00200: val_loss did not improve from 0.04715
4150/4150 [==============================] - 0s 48us/sample - loss: 0.0619 - accuracy: 0.9802 - val_loss: 0.2645 - val_accuracy: 0.9027
Testing on Fold 3
# Getting train data set up
# Getting test data set up
WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;, &lt;class &#39;NoneType&#39;&gt;
Train on 2400 samples, validate on 600 samples
Epoch 1/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 192.6307 - mse: 192.6307
Epoch 00001: val_loss did not improve from 111.12089
2400/2400 [==============================] - 4s 2ms/sample - loss: 191.3546 - mse: 191.3546 - val_loss: 136.3155 - val_mse: 136.3155
Epoch 2/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 169.8871 - mse: 169.8871
Epoch 00002: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 917us/sample - loss: 173.8672 - mse: 173.8672 - val_loss: 112.7022 - val_mse: 112.7022
Epoch 3/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 165.7134 - mse: 165.7134
Epoch 00003: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 674us/sample - loss: 166.4826 - mse: 166.4826 - val_loss: 117.2998 - val_mse: 117.2998
Epoch 4/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 158.1730 - mse: 158.1730- ETA: 0s - loss: 150.5263 - ms
Epoch 00004: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 814us/sample - loss: 159.6364 - mse: 159.6364 - val_loss: 114.3195 - val_mse: 114.3195
Epoch 5/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 160.5405 - mse: 160.5405
Epoch 00005: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 767us/sample - loss: 158.4694 - mse: 158.4694 - val_loss: 138.6964 - val_mse: 138.6965
Epoch 6/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 161.1521 - mse: 161.1521
Epoch 00006: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 785us/sample - loss: 161.0121 - mse: 161.0121 - val_loss: 120.2608 - val_mse: 120.2608
Epoch 7/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 163.1197 - mse: 163.1197
Epoch 00007: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 811us/sample - loss: 161.9584 - mse: 161.9583 - val_loss: 115.6909 - val_mse: 115.6909
Epoch 8/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 154.2139 - mse: 154.2139
Epoch 00008: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 764us/sample - loss: 153.0801 - mse: 153.0801 - val_loss: 118.8275 - val_mse: 118.8275
Epoch 9/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 152.4071 - mse: 152.4071
Epoch 00009: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 730us/sample - loss: 151.2422 - mse: 151.2422 - val_loss: 115.0790 - val_mse: 115.0790
Epoch 10/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 149.2932 - mse: 149.2932
Epoch 00010: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 751us/sample - loss: 148.5256 - mse: 148.5256 - val_loss: 130.9828 - val_mse: 130.9828
Epoch 11/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 151.8965 - mse: 151.8965
Epoch 00011: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 721us/sample - loss: 151.5511 - mse: 151.5511 - val_loss: 141.0099 - val_mse: 141.0099
Epoch 12/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 155.6764 - mse: 155.6764
Epoch 00012: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 713us/sample - loss: 153.6630 - mse: 153.6630 - val_loss: 114.1198 - val_mse: 114.1197
Epoch 13/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 147.8707 - mse: 147.8708
Epoch 00013: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 736us/sample - loss: 149.7029 - mse: 149.7029 - val_loss: 124.4326 - val_mse: 124.4326
Epoch 14/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 151.0645 - mse: 151.0645
Epoch 00014: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 759us/sample - loss: 151.7724 - mse: 151.7724 - val_loss: 120.0350 - val_mse: 120.0350
Epoch 15/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 158.6493 - mse: 158.6493- ETA: 1s - loss: 155.0
Epoch 00015: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 758us/sample - loss: 159.1556 - mse: 159.1556 - val_loss: 119.5833 - val_mse: 119.5833
Epoch 16/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 160.2747 - mse: 160.2747
Epoch 00016: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 830us/sample - loss: 165.0587 - mse: 165.0587 - val_loss: 122.1984 - val_mse: 122.1983
Epoch 17/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 167.8514 - mse: 167.8514
Epoch 00017: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 737us/sample - loss: 165.4476 - mse: 165.4476 - val_loss: 122.6537 - val_mse: 122.6537
Epoch 18/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 168.0877 - mse: 168.0877
Epoch 00018: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 712us/sample - loss: 166.1790 - mse: 166.1790 - val_loss: 122.7420 - val_mse: 122.7420
Epoch 19/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.6933 - mse: 162.6933
Epoch 00019: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 722us/sample - loss: 161.8603 - mse: 161.8603 - val_loss: 134.6883 - val_mse: 134.6883
Epoch 20/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 160.0676 - mse: 160.0677
Epoch 00020: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 743us/sample - loss: 159.3702 - mse: 159.3703 - val_loss: 120.8831 - val_mse: 120.8831
Epoch 21/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 153.8686 - mse: 153.8686
Epoch 00021: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 763us/sample - loss: 152.3709 - mse: 152.3709 - val_loss: 120.5568 - val_mse: 120.5568
Epoch 22/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 159.7181 - mse: 159.7181
Epoch 00022: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 735us/sample - loss: 156.9708 - mse: 156.9708 - val_loss: 117.8036 - val_mse: 117.8036
Epoch 23/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 157.6622 - mse: 157.6622
Epoch 00023: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 730us/sample - loss: 156.9800 - mse: 156.9799 - val_loss: 123.4298 - val_mse: 123.4298
Epoch 24/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 159.5453 - mse: 159.5453
Epoch 00024: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 776us/sample - loss: 158.8699 - mse: 158.8699 - val_loss: 116.4963 - val_mse: 116.4963
Epoch 25/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 159.6052 - mse: 159.6053
Epoch 00025: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 761us/sample - loss: 157.4734 - mse: 157.4734 - val_loss: 123.1884 - val_mse: 123.1884
Epoch 26/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 155.2619 - mse: 155.2619
Epoch 00026: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 735us/sample - loss: 154.2339 - mse: 154.2339 - val_loss: 113.8526 - val_mse: 113.8526
Epoch 27/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 160.7607 - mse: 160.7607
Epoch 00027: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 752us/sample - loss: 159.6284 - mse: 159.6284 - val_loss: 120.4541 - val_mse: 120.4541
Epoch 28/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 164.0873 - mse: 164.0873
Epoch 00028: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 742us/sample - loss: 161.7522 - mse: 161.7522 - val_loss: 120.5592 - val_mse: 120.5592
Epoch 29/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 157.4029 - mse: 157.4029
Epoch 00029: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 735us/sample - loss: 157.5194 - mse: 157.5194 - val_loss: 143.0723 - val_mse: 143.0723
Epoch 30/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 151.1254 - mse: 151.1255
Epoch 00030: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 710us/sample - loss: 158.5314 - mse: 158.5314 - val_loss: 128.8830 - val_mse: 128.8830
Epoch 31/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 154.5881 - mse: 154.5882
Epoch 00031: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 720us/sample - loss: 155.3672 - mse: 155.3673 - val_loss: 118.0738 - val_mse: 118.0738
Epoch 32/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 153.2976 - mse: 153.2976
Epoch 00032: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 717us/sample - loss: 155.1495 - mse: 155.1495 - val_loss: 129.6623 - val_mse: 129.6623
Epoch 33/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 154.5584 - mse: 154.5584
Epoch 00033: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 785us/sample - loss: 153.2643 - mse: 153.2643 - val_loss: 115.4780 - val_mse: 115.4780
Epoch 34/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 157.5263 - mse: 157.5262
Epoch 00034: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 806us/sample - loss: 156.0715 - mse: 156.0715 - val_loss: 121.3532 - val_mse: 121.3532
Epoch 35/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 155.3591 - mse: 155.3591
Epoch 00035: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 749us/sample - loss: 154.8772 - mse: 154.8772 - val_loss: 116.0185 - val_mse: 116.0185
Epoch 36/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 155.0509 - mse: 155.0509
Epoch 00036: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 734us/sample - loss: 154.6574 - mse: 154.6574 - val_loss: 131.8214 - val_mse: 131.8214
Epoch 37/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 148.2815 - mse: 148.2816
Epoch 00037: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 742us/sample - loss: 148.5246 - mse: 148.5247 - val_loss: 153.9786 - val_mse: 153.9786
Epoch 38/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 151.4648 - mse: 151.4648
Epoch 00038: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 713us/sample - loss: 152.4253 - mse: 152.4253 - val_loss: 115.8447 - val_mse: 115.8447
Epoch 39/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 147.5286 - mse: 147.5286
Epoch 00039: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 722us/sample - loss: 146.8517 - mse: 146.8517 - val_loss: 113.3951 - val_mse: 113.3951
Epoch 40/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 143.2057 - mse: 143.2057
Epoch 00040: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 787us/sample - loss: 147.3209 - mse: 147.3209 - val_loss: 116.1533 - val_mse: 116.1533
Epoch 41/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 151.0403 - mse: 151.0403
Epoch 00041: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 798us/sample - loss: 150.1480 - mse: 150.1480 - val_loss: 123.3701 - val_mse: 123.3701
Epoch 42/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 151.2900 - mse: 151.2900
Epoch 00042: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 827us/sample - loss: 148.5727 - mse: 148.5727 - val_loss: 112.9895 - val_mse: 112.9895
Epoch 43/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 153.4153 - mse: 153.4153
Epoch 00043: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 807us/sample - loss: 151.8392 - mse: 151.8393 - val_loss: 121.1201 - val_mse: 121.1201
Epoch 44/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 141.2375 - mse: 141.2374
Epoch 00044: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 757us/sample - loss: 145.7192 - mse: 145.7192 - val_loss: 137.3977 - val_mse: 137.3977
Epoch 45/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 146.3049 - mse: 146.3049
Epoch 00045: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 798us/sample - loss: 146.1531 - mse: 146.1532 - val_loss: 126.3879 - val_mse: 126.3878
Epoch 46/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 153.7506 - mse: 153.7506
Epoch 00046: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 871us/sample - loss: 152.5062 - mse: 152.5062 - val_loss: 116.5572 - val_mse: 116.5572
Epoch 47/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 149.9180 - mse: 149.9181
Epoch 00047: val_loss did not improve from 111.12089
2400/2400 [==============================] - 2s 806us/sample - loss: 146.1334 - mse: 146.1334 - val_loss: 130.9172 - val_mse: 130.9172
Epoch 48/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 143.6968 - mse: 143.6967
Epoch 00048: val_loss improved from 111.12089 to 110.39956, saving model to best_model_1.h5
2400/2400 [==============================] - 2s 814us/sample - loss: 144.3967 - mse: 144.3967 - val_loss: 110.3996 - val_mse: 110.3996
Epoch 49/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 149.7406 - mse: 149.7406
Epoch 00049: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 757us/sample - loss: 146.2418 - mse: 146.2419 - val_loss: 111.6435 - val_mse: 111.6435
Epoch 50/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 148.6887 - mse: 148.6887
Epoch 00050: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 834us/sample - loss: 147.9548 - mse: 147.9548 - val_loss: 116.8457 - val_mse: 116.8457
Epoch 51/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 148.4739 - mse: 148.4738
Epoch 00051: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 876us/sample - loss: 146.7057 - mse: 146.7057 - val_loss: 157.7446 - val_mse: 157.7446
Epoch 52/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 149.6472 - mse: 149.6472
Epoch 00052: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 778us/sample - loss: 148.6020 - mse: 148.6020 - val_loss: 121.4780 - val_mse: 121.4781
Epoch 53/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 146.1411 - mse: 146.1411
Epoch 00053: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 769us/sample - loss: 143.8441 - mse: 143.8441 - val_loss: 116.6677 - val_mse: 116.6676
Epoch 54/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 152.6870 - mse: 152.6870
Epoch 00054: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 767us/sample - loss: 149.3820 - mse: 149.3820 - val_loss: 114.5603 - val_mse: 114.5603
Epoch 55/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 148.3033 - mse: 148.3032
Epoch 00055: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 750us/sample - loss: 147.2483 - mse: 147.2483 - val_loss: 122.1218 - val_mse: 122.1218
Epoch 56/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 143.5421 - mse: 143.5421
Epoch 00056: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 757us/sample - loss: 141.2698 - mse: 141.2698 - val_loss: 140.7268 - val_mse: 140.7268
Epoch 57/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 145.4928 - mse: 145.4928
Epoch 00057: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 736us/sample - loss: 146.5114 - mse: 146.5114 - val_loss: 120.0278 - val_mse: 120.0278
Epoch 58/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 143.9574 - mse: 143.9575
Epoch 00058: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 732us/sample - loss: 143.8667 - mse: 143.8667 - val_loss: 111.7701 - val_mse: 111.7701
Epoch 59/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 148.6362 - mse: 148.6363
Epoch 00059: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 805us/sample - loss: 146.3318 - mse: 146.3318 - val_loss: 126.4993 - val_mse: 126.4993
Epoch 60/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 137.5600 - mse: 137.5600
Epoch 00060: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 817us/sample - loss: 144.5059 - mse: 144.5059 - val_loss: 112.2204 - val_mse: 112.2204
Epoch 61/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 142.0960 - mse: 142.0959
Epoch 00061: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 739us/sample - loss: 141.4831 - mse: 141.4831 - val_loss: 115.6952 - val_mse: 115.6952
Epoch 62/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 154.5630 - mse: 154.5630
Epoch 00062: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 740us/sample - loss: 155.3604 - mse: 155.3604 - val_loss: 118.5203 - val_mse: 118.5203
Epoch 63/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 153.3305 - mse: 153.3305
Epoch 00063: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 730us/sample - loss: 154.5571 - mse: 154.5571 - val_loss: 122.0733 - val_mse: 122.0733
Epoch 64/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 149.2290 - mse: 149.2290
Epoch 00064: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 726us/sample - loss: 148.4587 - mse: 148.4587 - val_loss: 118.7998 - val_mse: 118.7998
Epoch 65/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 153.6803 - mse: 153.6803
Epoch 00065: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 725us/sample - loss: 154.1697 - mse: 154.1697 - val_loss: 138.7107 - val_mse: 138.7107
Epoch 66/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 147.9063 - mse: 147.9063
Epoch 00066: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 714us/sample - loss: 146.0730 - mse: 146.0730 - val_loss: 120.7645 - val_mse: 120.7645
Epoch 67/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 147.2893 - mse: 147.2893
Epoch 00067: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 756us/sample - loss: 144.2487 - mse: 144.2487 - val_loss: 113.0510 - val_mse: 113.0510
Epoch 68/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 150.6673 - mse: 150.6674
Epoch 00068: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 786us/sample - loss: 152.9712 - mse: 152.9713 - val_loss: 121.3711 - val_mse: 121.3711
Epoch 69/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 154.2982 - mse: 154.2982
Epoch 00069: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 763us/sample - loss: 153.7900 - mse: 153.7900 - val_loss: 113.5062 - val_mse: 113.5062
Epoch 70/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 153.0432 - mse: 153.0432
Epoch 00070: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 732us/sample - loss: 152.5674 - mse: 152.5675 - val_loss: 119.5019 - val_mse: 119.5019
Epoch 71/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 154.8558 - mse: 154.8557
Epoch 00071: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 728us/sample - loss: 157.9043 - mse: 157.9043 - val_loss: 123.5979 - val_mse: 123.5980
Epoch 72/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 151.2457 - mse: 151.2457
Epoch 00072: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 711us/sample - loss: 148.8029 - mse: 148.8029 - val_loss: 119.8979 - val_mse: 119.8979
Epoch 73/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 159.2156 - mse: 159.2155
Epoch 00073: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 723us/sample - loss: 158.0978 - mse: 158.0978 - val_loss: 124.8871 - val_mse: 124.8871
Epoch 74/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 150.3655 - mse: 150.3655
Epoch 00074: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 733us/sample - loss: 152.2037 - mse: 152.2037 - val_loss: 117.6859 - val_mse: 117.6858
Epoch 75/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 141.6773 - mse: 141.6773- ETA: 0s - loss: 146.5514 - mse: 1
Epoch 00075: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 712us/sample - loss: 149.8399 - mse: 149.8399 - val_loss: 149.9904 - val_mse: 149.9904
Epoch 76/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 148.6871 - mse: 148.6871
Epoch 00076: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 756us/sample - loss: 150.2035 - mse: 150.2035 - val_loss: 114.7978 - val_mse: 114.7979
Epoch 77/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 148.4699 - mse: 148.4699
Epoch 00077: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 799us/sample - loss: 148.6866 - mse: 148.6866 - val_loss: 115.1095 - val_mse: 115.1095
Epoch 78/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 158.8564 - mse: 158.8564
Epoch 00078: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 756us/sample - loss: 157.1416 - mse: 157.1416 - val_loss: 119.6581 - val_mse: 119.6581
Epoch 79/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 148.4305 - mse: 148.4305
Epoch 00079: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 736us/sample - loss: 151.7426 - mse: 151.7426 - val_loss: 134.3863 - val_mse: 134.3863
Epoch 80/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 157.2984 - mse: 157.2985
Epoch 00080: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 791us/sample - loss: 163.7030 - mse: 163.7030 - val_loss: 121.9656 - val_mse: 121.9655
Epoch 81/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 160.7296 - mse: 160.7296
Epoch 00081: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 749us/sample - loss: 161.7699 - mse: 161.7699 - val_loss: 122.3215 - val_mse: 122.3215
Epoch 82/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 161.8762 - mse: 161.8763
Epoch 00082: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 743us/sample - loss: 161.0405 - mse: 161.0406 - val_loss: 122.8214 - val_mse: 122.8214
Epoch 83/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.3253 - mse: 163.3254
Epoch 00083: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 736us/sample - loss: 163.5822 - mse: 163.5823 - val_loss: 124.0508 - val_mse: 124.0508
Epoch 84/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 164.3565 - mse: 164.3565
Epoch 00084: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 735us/sample - loss: 162.0731 - mse: 162.0731 - val_loss: 121.8425 - val_mse: 121.8425
Epoch 85/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.2153 - mse: 162.2153
Epoch 00085: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 791us/sample - loss: 162.2437 - mse: 162.2437 - val_loss: 122.2163 - val_mse: 122.2162
Epoch 86/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 161.8802 - mse: 161.8802
Epoch 00086: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 799us/sample - loss: 161.4293 - mse: 161.4292 - val_loss: 123.0474 - val_mse: 123.0474
Epoch 87/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 158.6352 - mse: 158.6352
Epoch 00087: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 763us/sample - loss: 162.0617 - mse: 162.0617 - val_loss: 122.9458 - val_mse: 122.9458
Epoch 88/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 163.8172 - mse: 163.8172
Epoch 00088: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 728us/sample - loss: 161.5960 - mse: 161.5960 - val_loss: 123.9715 - val_mse: 123.9715
Epoch 89/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 162.9292 - mse: 162.9293
Epoch 00089: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 750us/sample - loss: 161.9531 - mse: 161.9531 - val_loss: 123.9798 - val_mse: 123.9798
Epoch 90/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 159.7805 - mse: 159.7805
Epoch 00090: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 737us/sample - loss: 161.5877 - mse: 161.5877 - val_loss: 122.1079 - val_mse: 122.1079
Epoch 91/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 161.1375 - mse: 161.1375
Epoch 00091: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 723us/sample - loss: 161.1506 - mse: 161.1506 - val_loss: 122.1910 - val_mse: 122.1910
Epoch 92/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 150.4468 - mse: 150.4468
Epoch 00092: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 729us/sample - loss: 161.8952 - mse: 161.8951 - val_loss: 122.1108 - val_mse: 122.1107
Epoch 93/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 165.3767 - mse: 165.3767
Epoch 00093: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 730us/sample - loss: 162.9641 - mse: 162.9641 - val_loss: 121.9889 - val_mse: 121.9889
Epoch 94/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.3411 - mse: 162.3411
Epoch 00094: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 871us/sample - loss: 162.2691 - mse: 162.2691 - val_loss: 122.5973 - val_mse: 122.5973
Epoch 95/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 157.4548 - mse: 157.4548
Epoch 00095: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 831us/sample - loss: 162.2722 - mse: 162.2722 - val_loss: 122.3185 - val_mse: 122.3185
Epoch 96/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 152.2161 - mse: 152.2160
Epoch 00096: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 784us/sample - loss: 161.8770 - mse: 161.8770 - val_loss: 122.5204 - val_mse: 122.5204
Epoch 97/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 161.3765 - mse: 161.3764
Epoch 00097: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 775us/sample - loss: 161.5567 - mse: 161.5567 - val_loss: 123.3071 - val_mse: 123.3071
Epoch 98/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 161.0318 - mse: 161.0318
Epoch 00098: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 761us/sample - loss: 160.3924 - mse: 160.3924 - val_loss: 122.8008 - val_mse: 122.8008
Epoch 99/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 161.6506 - mse: 161.6506
Epoch 00099: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 810us/sample - loss: 162.2801 - mse: 162.2801 - val_loss: 122.1737 - val_mse: 122.1736
Epoch 100/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 161.2713 - mse: 161.2713
Epoch 00100: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 761us/sample - loss: 160.3106 - mse: 160.3106 - val_loss: 125.5760 - val_mse: 125.5760
Epoch 101/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 152.1364 - mse: 152.1364
Epoch 00101: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 757us/sample - loss: 161.7631 - mse: 161.7631 - val_loss: 122.6623 - val_mse: 122.6623
Epoch 102/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 153.4149 - mse: 153.4149
Epoch 00102: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 784us/sample - loss: 159.7265 - mse: 159.7265 - val_loss: 133.9704 - val_mse: 133.9704
Epoch 103/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.6715 - mse: 162.6715
Epoch 00103: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 823us/sample - loss: 161.9713 - mse: 161.9713 - val_loss: 122.3260 - val_mse: 122.3260
Epoch 104/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.8145 - mse: 162.8145
Epoch 00104: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 806us/sample - loss: 162.1449 - mse: 162.1449 - val_loss: 123.3441 - val_mse: 123.3441
Epoch 105/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.8906 - mse: 162.8906
Epoch 00105: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 786us/sample - loss: 161.3503 - mse: 161.3503 - val_loss: 122.4601 - val_mse: 122.4601
Epoch 106/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 163.2329 - mse: 163.2329
Epoch 00106: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 817us/sample - loss: 161.2815 - mse: 161.2815 - val_loss: 127.8604 - val_mse: 127.8604
Epoch 107/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 160.3777 - mse: 160.3777
Epoch 00107: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 831us/sample - loss: 162.1235 - mse: 162.1235 - val_loss: 119.2348 - val_mse: 119.2348
Epoch 108/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 161.4083 - mse: 161.4084
Epoch 00108: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 762us/sample - loss: 161.2598 - mse: 161.2598 - val_loss: 120.2087 - val_mse: 120.2087
Epoch 109/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 153.5475 - mse: 153.5475
Epoch 00109: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 776us/sample - loss: 159.6175 - mse: 159.6175 - val_loss: 124.5270 - val_mse: 124.5270
Epoch 110/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.3769 - mse: 163.3769
Epoch 00110: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 750us/sample - loss: 161.8730 - mse: 161.8730 - val_loss: 120.8874 - val_mse: 120.8875
Epoch 111/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 159.1663 - mse: 159.1663
Epoch 00111: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 826us/sample - loss: 158.4585 - mse: 158.4585 - val_loss: 115.8164 - val_mse: 115.8164
Epoch 112/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 150.5148 - mse: 150.5147
Epoch 00112: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 860us/sample - loss: 153.7660 - mse: 153.7659 - val_loss: 129.7612 - val_mse: 129.7612
Epoch 113/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.2003 - mse: 163.2003
Epoch 00113: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 799us/sample - loss: 162.4703 - mse: 162.4703 - val_loss: 122.1032 - val_mse: 122.1032
Epoch 114/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 152.6086 - mse: 152.6087
Epoch 00114: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 1ms/sample - loss: 161.2837 - mse: 161.2837 - val_loss: 121.8822 - val_mse: 121.8822
Epoch 115/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 154.2459 - mse: 154.2459
Epoch 00115: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 894us/sample - loss: 161.9190 - mse: 161.9190 - val_loss: 123.3431 - val_mse: 123.3431
Epoch 116/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.5706 - mse: 162.5706
Epoch 00116: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 730us/sample - loss: 161.0975 - mse: 161.0974 - val_loss: 122.1165 - val_mse: 122.1165
Epoch 117/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 163.9637 - mse: 163.9637
Epoch 00117: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 749us/sample - loss: 162.5955 - mse: 162.5955 - val_loss: 122.2734 - val_mse: 122.2734
Epoch 118/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.1993 - mse: 162.1992
Epoch 00118: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 899us/sample - loss: 162.2191 - mse: 162.2191 - val_loss: 122.2860 - val_mse: 122.2860
Epoch 119/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.4500 - mse: 162.4500
Epoch 00119: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 856us/sample - loss: 161.6528 - mse: 161.6528 - val_loss: 122.5222 - val_mse: 122.5222
Epoch 120/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 160.6885 - mse: 160.6884
Epoch 00120: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 877us/sample - loss: 160.0061 - mse: 160.0060 - val_loss: 123.1411 - val_mse: 123.1411
Epoch 121/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 161.7727 - mse: 161.7727
Epoch 00121: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 860us/sample - loss: 161.0013 - mse: 161.0013 - val_loss: 121.9591 - val_mse: 121.9591
Epoch 122/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.0271 - mse: 163.0271
Epoch 00122: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 788us/sample - loss: 162.2753 - mse: 162.2753 - val_loss: 123.3613 - val_mse: 123.3613
Epoch 123/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.8158 - mse: 162.8159
Epoch 00123: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 777us/sample - loss: 162.2121 - mse: 162.2121 - val_loss: 122.2249 - val_mse: 122.2249
Epoch 124/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.8962 - mse: 162.8962
Epoch 00124: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 763us/sample - loss: 160.7758 - mse: 160.7757 - val_loss: 123.9710 - val_mse: 123.9710
Epoch 125/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 163.7377 - mse: 163.7377
Epoch 00125: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 757us/sample - loss: 162.7602 - mse: 162.7602 - val_loss: 122.6537 - val_mse: 122.6537
Epoch 126/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 163.3932 - mse: 163.3932
Epoch 00126: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 725us/sample - loss: 162.1775 - mse: 162.1775 - val_loss: 121.8611 - val_mse: 121.8611
Epoch 127/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 161.7090 - mse: 161.7090
Epoch 00127: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 847us/sample - loss: 160.7203 - mse: 160.7203 - val_loss: 125.2014 - val_mse: 125.2014
Epoch 128/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 158.9366 - mse: 158.9366- ETA: 0s - loss: 162.5576 - mse: 162.5
Epoch 00128: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 883us/sample - loss: 162.4535 - mse: 162.4534 - val_loss: 122.7687 - val_mse: 122.7687
Epoch 129/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 165.5431 - mse: 165.5431
Epoch 00129: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 822us/sample - loss: 162.3376 - mse: 162.3376 - val_loss: 122.4492 - val_mse: 122.4492
Epoch 130/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 159.2016 - mse: 159.2016
Epoch 00130: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 772us/sample - loss: 162.5258 - mse: 162.5258 - val_loss: 122.3755 - val_mse: 122.3755
Epoch 131/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 157.4146 - mse: 157.4146
Epoch 00131: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 784us/sample - loss: 161.1104 - mse: 161.1105 - val_loss: 122.4530 - val_mse: 122.4530
Epoch 132/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 159.8484 - mse: 159.8484
Epoch 00132: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 771us/sample - loss: 160.2634 - mse: 160.2633 - val_loss: 124.1118 - val_mse: 124.1118
Epoch 133/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 164.8484 - mse: 164.8484
Epoch 00133: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 759us/sample - loss: 161.4080 - mse: 161.4079 - val_loss: 121.9657 - val_mse: 121.9657
Epoch 134/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 161.7594 - mse: 161.7595
Epoch 00134: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 771us/sample - loss: 161.8885 - mse: 161.8885 - val_loss: 121.9712 - val_mse: 121.9712
Epoch 135/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.4813 - mse: 163.4814
Epoch 00135: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 816us/sample - loss: 162.3658 - mse: 162.3658 - val_loss: 124.0332 - val_mse: 124.0332
Epoch 136/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.4035 - mse: 163.4035
Epoch 00136: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 892us/sample - loss: 162.1426 - mse: 162.1426 - val_loss: 121.9289 - val_mse: 121.9289
Epoch 137/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 160.0978 - mse: 160.0979
Epoch 00137: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 868us/sample - loss: 161.4588 - mse: 161.4588 - val_loss: 122.9381 - val_mse: 122.9381
Epoch 138/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 161.7991 - mse: 161.7991
Epoch 00138: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 773us/sample - loss: 161.8598 - mse: 161.8598 - val_loss: 122.1675 - val_mse: 122.1675
Epoch 139/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 161.5361 - mse: 161.5361
Epoch 00139: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 764us/sample - loss: 161.3477 - mse: 161.3477 - val_loss: 123.3557 - val_mse: 123.3557
Epoch 140/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.5871 - mse: 162.5871
Epoch 00140: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 829us/sample - loss: 160.5244 - mse: 160.5244 - val_loss: 121.8887 - val_mse: 121.8887
Epoch 141/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 160.8458 - mse: 160.8458
Epoch 00141: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 781us/sample - loss: 161.3683 - mse: 161.3683 - val_loss: 122.8270 - val_mse: 122.8270
Epoch 142/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.3829 - mse: 163.3829
Epoch 00142: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 748us/sample - loss: 162.4782 - mse: 162.4781 - val_loss: 122.4967 - val_mse: 122.4967
Epoch 143/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 165.1351 - mse: 165.1351
Epoch 00143: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 773us/sample - loss: 162.3066 - mse: 162.3065 - val_loss: 123.2172 - val_mse: 123.2172
Epoch 144/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 159.6737 - mse: 159.6737
Epoch 00144: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 816us/sample - loss: 161.4608 - mse: 161.4608 - val_loss: 122.8128 - val_mse: 122.8127
Epoch 145/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.9130 - mse: 162.9131
Epoch 00145: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 757us/sample - loss: 160.9058 - mse: 160.9059 - val_loss: 122.6009 - val_mse: 122.6009
Epoch 146/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.5635 - mse: 162.5635
Epoch 00146: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 750us/sample - loss: 161.3498 - mse: 161.3498 - val_loss: 122.7338 - val_mse: 122.7338
Epoch 147/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.9928 - mse: 162.9928
Epoch 00147: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 729us/sample - loss: 161.5866 - mse: 161.5866 - val_loss: 122.3934 - val_mse: 122.3934
Epoch 148/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 160.1537 - mse: 160.1537- ETA: 1s - loss: 11
Epoch 00148: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 714us/sample - loss: 162.2561 - mse: 162.2561 - val_loss: 121.9696 - val_mse: 121.9696
Epoch 149/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 164.3860 - mse: 164.3860
Epoch 00149: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 722us/sample - loss: 162.2064 - mse: 162.2063 - val_loss: 123.6518 - val_mse: 123.6517
Epoch 150/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.1812 - mse: 162.1811
Epoch 00150: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 719us/sample - loss: 162.6898 - mse: 162.6898 - val_loss: 122.6571 - val_mse: 122.6571
Epoch 151/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 161.7733 - mse: 161.7733
Epoch 00151: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 709us/sample - loss: 162.1570 - mse: 162.1570 - val_loss: 123.3002 - val_mse: 123.3002
Epoch 152/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 161.1716 - mse: 161.1716
Epoch 00152: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 825us/sample - loss: 160.9635 - mse: 160.9635 - val_loss: 122.4839 - val_mse: 122.4839
Epoch 153/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 159.9045 - mse: 159.9045
Epoch 00153: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 689us/sample - loss: 160.7166 - mse: 160.7166 - val_loss: 122.0758 - val_mse: 122.0758
Epoch 154/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.4193 - mse: 162.4194
Epoch 00154: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 633us/sample - loss: 161.3798 - mse: 161.3798 - val_loss: 123.3201 - val_mse: 123.3201
Epoch 155/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 164.0479 - mse: 164.0479- ETA: 0s - loss: 157.7343 - mse: 
Epoch 00155: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 625us/sample - loss: 161.3545 - mse: 161.3545 - val_loss: 123.1140 - val_mse: 123.1141
Epoch 156/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.7688 - mse: 163.7688
Epoch 00156: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 623us/sample - loss: 162.0235 - mse: 162.0235 - val_loss: 122.6317 - val_mse: 122.6317
Epoch 157/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.5199 - mse: 162.5199
Epoch 00157: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 620us/sample - loss: 161.6088 - mse: 161.6088 - val_loss: 123.4609 - val_mse: 123.4609
Epoch 158/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 155.9190 - mse: 155.9190
Epoch 00158: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 614us/sample - loss: 161.7269 - mse: 161.7269 - val_loss: 122.2418 - val_mse: 122.2418
Epoch 159/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.5525 - mse: 163.5525
Epoch 00159: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 611us/sample - loss: 161.5896 - mse: 161.5896 - val_loss: 121.8529 - val_mse: 121.8529
Epoch 160/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 160.2329 - mse: 160.2329
Epoch 00160: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 619us/sample - loss: 161.4254 - mse: 161.4254 - val_loss: 122.1749 - val_mse: 122.1749
Epoch 161/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 162.3277 - mse: 162.3277
Epoch 00161: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 612us/sample - loss: 162.3428 - mse: 162.3428 - val_loss: 124.5507 - val_mse: 124.5507
Epoch 162/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 163.2881 - mse: 163.2881
Epoch 00162: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 625us/sample - loss: 160.6756 - mse: 160.6756 - val_loss: 122.3954 - val_mse: 122.3954
Epoch 163/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 166.1376 - mse: 166.1376
Epoch 00163: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 678us/sample - loss: 161.7916 - mse: 161.7916 - val_loss: 122.0196 - val_mse: 122.0196
Epoch 164/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 160.3151 - mse: 160.3151
Epoch 00164: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 658us/sample - loss: 161.0316 - mse: 161.0316 - val_loss: 123.9245 - val_mse: 123.9245
Epoch 165/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.3543 - mse: 162.3543
Epoch 00165: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 618us/sample - loss: 160.7330 - mse: 160.7330 - val_loss: 123.4851 - val_mse: 123.4851
Epoch 166/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 164.9920 - mse: 164.9920
Epoch 00166: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 625us/sample - loss: 161.3115 - mse: 161.3116 - val_loss: 122.6246 - val_mse: 122.6246
Epoch 167/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 161.9385 - mse: 161.9385
Epoch 00167: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 621us/sample - loss: 161.7727 - mse: 161.7727 - val_loss: 122.7061 - val_mse: 122.7061
Epoch 168/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 164.8029 - mse: 164.8029
Epoch 00168: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 608us/sample - loss: 161.1582 - mse: 161.1582 - val_loss: 122.2443 - val_mse: 122.2443
Epoch 169/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.9382 - mse: 162.9382
Epoch 00169: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 610us/sample - loss: 161.9640 - mse: 161.9640 - val_loss: 122.5553 - val_mse: 122.5553
Epoch 170/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 162.5684 - mse: 162.5684
Epoch 00170: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 621us/sample - loss: 161.1834 - mse: 161.1834 - val_loss: 122.5713 - val_mse: 122.5713
Epoch 171/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.2214 - mse: 162.2214
Epoch 00171: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 614us/sample - loss: 161.4477 - mse: 161.4477 - val_loss: 123.1406 - val_mse: 123.1406
Epoch 172/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 163.2054 - mse: 163.2054
Epoch 00172: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 613us/sample - loss: 162.0616 - mse: 162.0616 - val_loss: 122.2085 - val_mse: 122.2085
Epoch 173/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 160.4053 - mse: 160.4053
Epoch 00173: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 682us/sample - loss: 161.5658 - mse: 161.5658 - val_loss: 122.6811 - val_mse: 122.6811
Epoch 174/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.2046 - mse: 162.2046
Epoch 00174: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 687us/sample - loss: 161.0308 - mse: 161.0308 - val_loss: 121.9696 - val_mse: 121.9696
Epoch 175/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 158.7472 - mse: 158.7471
Epoch 00175: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 621us/sample - loss: 161.5109 - mse: 161.5109 - val_loss: 122.1605 - val_mse: 122.1605
Epoch 176/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 164.5097 - mse: 164.5097
Epoch 00176: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 621us/sample - loss: 161.0226 - mse: 161.0225 - val_loss: 123.0137 - val_mse: 123.0137
Epoch 177/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 163.0348 - mse: 163.0348
Epoch 00177: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 635us/sample - loss: 161.8821 - mse: 161.8820 - val_loss: 122.3230 - val_mse: 122.3230
Epoch 178/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 161.9121 - mse: 161.9121
Epoch 00178: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 619us/sample - loss: 161.4618 - mse: 161.4619 - val_loss: 122.5991 - val_mse: 122.5991
Epoch 179/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.9428 - mse: 163.9429
Epoch 00179: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 605us/sample - loss: 161.8481 - mse: 161.8482 - val_loss: 121.8543 - val_mse: 121.8543
Epoch 180/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 159.7196 - mse: 159.7196
Epoch 00180: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 609us/sample - loss: 160.3283 - mse: 160.3283 - val_loss: 122.0713 - val_mse: 122.0713
Epoch 181/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 152.6636 - mse: 152.6636
Epoch 00181: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 604us/sample - loss: 160.6580 - mse: 160.6580 - val_loss: 122.4141 - val_mse: 122.4142
Epoch 182/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 164.0827 - mse: 164.0826
Epoch 00182: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 609us/sample - loss: 160.7321 - mse: 160.7321 - val_loss: 124.1579 - val_mse: 124.1579
Epoch 183/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.4969 - mse: 163.4969
Epoch 00183: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 603us/sample - loss: 161.2222 - mse: 161.2223 - val_loss: 122.6983 - val_mse: 122.6983
Epoch 184/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.6500 - mse: 163.6500
Epoch 00184: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 673us/sample - loss: 160.8212 - mse: 160.8212 - val_loss: 122.5609 - val_mse: 122.5609
Epoch 185/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 163.2121 - mse: 163.2121
Epoch 00185: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 655us/sample - loss: 160.7172 - mse: 160.7171 - val_loss: 123.1265 - val_mse: 123.1265
Epoch 186/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.4663 - mse: 162.4663
Epoch 00186: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 625us/sample - loss: 161.1903 - mse: 161.1903 - val_loss: 122.6172 - val_mse: 122.6172
Epoch 187/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 163.2898 - mse: 163.2898
Epoch 00187: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 626us/sample - loss: 160.5103 - mse: 160.5103 - val_loss: 121.9442 - val_mse: 121.9442
Epoch 188/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 160.9404 - mse: 160.9404
Epoch 00188: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 623us/sample - loss: 159.9336 - mse: 159.9336 - val_loss: 122.0283 - val_mse: 122.0283
Epoch 189/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 158.4760 - mse: 158.4760
Epoch 00189: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 613us/sample - loss: 161.8883 - mse: 161.8883 - val_loss: 122.1067 - val_mse: 122.1067
Epoch 190/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.7892 - mse: 162.7892
Epoch 00190: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 611us/sample - loss: 162.2857 - mse: 162.2857 - val_loss: 122.3769 - val_mse: 122.3769
Epoch 191/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 164.7451 - mse: 164.7452
Epoch 00191: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 605us/sample - loss: 161.8205 - mse: 161.8205 - val_loss: 122.0153 - val_mse: 122.0153
Epoch 192/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.6723 - mse: 162.6724
Epoch 00192: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 612us/sample - loss: 162.0974 - mse: 162.0975 - val_loss: 122.2715 - val_mse: 122.2715
Epoch 193/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 158.3825 - mse: 158.3825
Epoch 00193: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 606us/sample - loss: 161.2117 - mse: 161.2117 - val_loss: 122.1075 - val_mse: 122.1075
Epoch 194/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.3708 - mse: 162.3708
Epoch 00194: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 634us/sample - loss: 161.3403 - mse: 161.3403 - val_loss: 122.0606 - val_mse: 122.0606
Epoch 195/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 155.4822 - mse: 155.4822
Epoch 00195: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 675us/sample - loss: 161.1165 - mse: 161.1165 - val_loss: 122.2893 - val_mse: 122.2893
Epoch 196/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.3580 - mse: 163.3580
Epoch 00196: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 636us/sample - loss: 161.1660 - mse: 161.1660 - val_loss: 122.5697 - val_mse: 122.5697
Epoch 197/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 163.0513 - mse: 163.0513
Epoch 00197: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 611us/sample - loss: 160.8566 - mse: 160.8566 - val_loss: 123.0661 - val_mse: 123.0661
Epoch 198/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.3847 - mse: 162.3847
Epoch 00198: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 618us/sample - loss: 162.4502 - mse: 162.4502 - val_loss: 123.1018 - val_mse: 123.1018
Epoch 199/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 161.9967 - mse: 161.9967
Epoch 00199: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 624us/sample - loss: 161.1832 - mse: 161.1832 - val_loss: 122.2258 - val_mse: 122.2258
Epoch 200/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 155.2056 - mse: 155.2056
Epoch 00200: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 609us/sample - loss: 161.3115 - mse: 161.3115 - val_loss: 122.2647 - val_mse: 122.2647
Epoch 201/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 164.0913 - mse: 164.0914
Epoch 00201: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 612us/sample - loss: 161.1637 - mse: 161.1638 - val_loss: 122.2895 - val_mse: 122.2895
Epoch 202/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 164.1459 - mse: 164.1459
Epoch 00202: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 618us/sample - loss: 161.3676 - mse: 161.3677 - val_loss: 123.6471 - val_mse: 123.6471
Epoch 203/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.7147 - mse: 162.7147
Epoch 00203: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 616us/sample - loss: 162.1928 - mse: 162.1928 - val_loss: 122.4148 - val_mse: 122.4147
Epoch 204/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 160.6925 - mse: 160.6925
Epoch 00204: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 613us/sample - loss: 161.5389 - mse: 161.5389 - val_loss: 122.3722 - val_mse: 122.3722
Epoch 205/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 161.5169 - mse: 161.5168
Epoch 00205: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 651us/sample - loss: 160.6730 - mse: 160.6729 - val_loss: 124.0831 - val_mse: 124.0830
Epoch 206/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 164.4196 - mse: 164.4196
Epoch 00206: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 690us/sample - loss: 161.6733 - mse: 161.6733 - val_loss: 121.8541 - val_mse: 121.8541
Epoch 207/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.5398 - mse: 163.5398
Epoch 00207: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 610us/sample - loss: 162.1293 - mse: 162.1293 - val_loss: 122.6374 - val_mse: 122.6374
Epoch 208/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.8135 - mse: 162.8135
Epoch 00208: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 615us/sample - loss: 161.5427 - mse: 161.5427 - val_loss: 122.2826 - val_mse: 122.2825
Epoch 209/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 161.4302 - mse: 161.4303
Epoch 00209: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 618us/sample - loss: 161.6287 - mse: 161.6287 - val_loss: 122.0595 - val_mse: 122.0595
Epoch 210/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 164.3133 - mse: 164.3133
Epoch 00210: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 617us/sample - loss: 161.4789 - mse: 161.4789 - val_loss: 123.6789 - val_mse: 123.6789
Epoch 211/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.6753 - mse: 162.6753
Epoch 00211: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 602us/sample - loss: 161.1612 - mse: 161.1612 - val_loss: 122.7577 - val_mse: 122.7577
Epoch 212/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 166.8353 - mse: 166.8353
Epoch 00212: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 602us/sample - loss: 163.4470 - mse: 163.4470 - val_loss: 122.1326 - val_mse: 122.1326
Epoch 213/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 161.0068 - mse: 161.0068
Epoch 00213: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 619us/sample - loss: 161.5009 - mse: 161.5009 - val_loss: 122.2432 - val_mse: 122.2432
Epoch 214/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.0888 - mse: 162.0888- ETA: 0s - loss: 176.1005 -
Epoch 00214: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 596us/sample - loss: 160.9746 - mse: 160.9747 - val_loss: 122.9256 - val_mse: 122.9256
Epoch 215/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 154.8877 - mse: 154.8876
Epoch 00215: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 591us/sample - loss: 160.4467 - mse: 160.4467 - val_loss: 122.2159 - val_mse: 122.2159
Epoch 216/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 166.6184 - mse: 166.6184
Epoch 00216: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 666us/sample - loss: 162.7732 - mse: 162.7732 - val_loss: 123.2452 - val_mse: 123.2452
Epoch 217/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.1259 - mse: 163.1260
Epoch 00217: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 666us/sample - loss: 161.3831 - mse: 161.3832 - val_loss: 122.0237 - val_mse: 122.0237
Epoch 218/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 156.0399 - mse: 156.0399
Epoch 00218: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 615us/sample - loss: 161.8809 - mse: 161.8809 - val_loss: 122.4537 - val_mse: 122.4537
Epoch 219/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.5876 - mse: 163.5876
Epoch 00219: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 651us/sample - loss: 161.1344 - mse: 161.1344 - val_loss: 122.2256 - val_mse: 122.2256
Epoch 220/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 161.7724 - mse: 161.7724
Epoch 00220: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 644us/sample - loss: 160.7480 - mse: 160.7480 - val_loss: 121.8751 - val_mse: 121.8751
Epoch 221/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 163.4481 - mse: 163.4481
Epoch 00221: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 646us/sample - loss: 161.5910 - mse: 161.5910 - val_loss: 122.4919 - val_mse: 122.4919
Epoch 222/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 159.8924 - mse: 159.8924
Epoch 00222: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 648us/sample - loss: 161.1824 - mse: 161.1824 - val_loss: 123.2290 - val_mse: 123.2289
Epoch 223/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 161.4682 - mse: 161.4682
Epoch 00223: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 599us/sample - loss: 161.7644 - mse: 161.7643 - val_loss: 122.2126 - val_mse: 122.2126
Epoch 224/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.7513 - mse: 162.7513
Epoch 00224: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 615us/sample - loss: 160.3953 - mse: 160.3953 - val_loss: 121.9017 - val_mse: 121.9017
Epoch 225/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 161.5509 - mse: 161.5509
Epoch 00225: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 604us/sample - loss: 160.8312 - mse: 160.8313 - val_loss: 123.1079 - val_mse: 123.1078
Epoch 226/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 160.6977 - mse: 160.6977
Epoch 00226: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 634us/sample - loss: 160.4334 - mse: 160.4334 - val_loss: 122.5196 - val_mse: 122.5196
Epoch 227/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 162.2657 - mse: 162.2657
Epoch 00227: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 676us/sample - loss: 160.7724 - mse: 160.7724 - val_loss: 121.9049 - val_mse: 121.9049
Epoch 228/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 160.8730 - mse: 160.8729
Epoch 00228: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 619us/sample - loss: 161.9245 - mse: 161.9245 - val_loss: 122.7761 - val_mse: 122.7761
Epoch 229/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 164.0823 - mse: 164.0824
Epoch 00229: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 603us/sample - loss: 160.4410 - mse: 160.4410 - val_loss: 122.3411 - val_mse: 122.3411
Epoch 230/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 159.9276 - mse: 159.9276
Epoch 00230: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 603us/sample - loss: 161.4301 - mse: 161.4301 - val_loss: 122.6382 - val_mse: 122.6382
Epoch 231/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 160.8424 - mse: 160.8424
Epoch 00231: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 602us/sample - loss: 161.8614 - mse: 161.8614 - val_loss: 122.8005 - val_mse: 122.8005
Epoch 232/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 148.6732 - mse: 148.6732
Epoch 00232: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 603us/sample - loss: 161.5609 - mse: 161.5609 - val_loss: 122.1235 - val_mse: 122.1235
Epoch 233/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 164.4007 - mse: 164.4007
Epoch 00233: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 589us/sample - loss: 162.2822 - mse: 162.2822 - val_loss: 122.8595 - val_mse: 122.8595
Epoch 234/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 165.0020 - mse: 165.0020
Epoch 00234: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 587us/sample - loss: 161.1966 - mse: 161.1966 - val_loss: 121.8835 - val_mse: 121.8835
Epoch 235/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 164.4625 - mse: 164.4624
Epoch 00235: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 600us/sample - loss: 161.8887 - mse: 161.8887 - val_loss: 122.3981 - val_mse: 122.3981
Epoch 236/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 161.5261 - mse: 161.5261
Epoch 00236: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 594us/sample - loss: 161.2679 - mse: 161.2679 - val_loss: 123.2062 - val_mse: 123.2062
Epoch 237/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 164.0981 - mse: 164.0981
Epoch 00237: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 620us/sample - loss: 161.1625 - mse: 161.1625 - val_loss: 122.0134 - val_mse: 122.0134
Epoch 238/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 153.1088 - mse: 153.1088
Epoch 00238: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 673us/sample - loss: 160.8851 - mse: 160.8851 - val_loss: 123.8866 - val_mse: 123.8866
Epoch 239/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 156.7820 - mse: 156.7820
Epoch 00239: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 620us/sample - loss: 161.1909 - mse: 161.1908 - val_loss: 122.9245 - val_mse: 122.9245
Epoch 240/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 163.7796 - mse: 163.7796
Epoch 00240: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 607us/sample - loss: 161.7436 - mse: 161.7436 - val_loss: 122.4829 - val_mse: 122.4829
Epoch 241/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 164.3816 - mse: 164.3816
Epoch 00241: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 606us/sample - loss: 161.5254 - mse: 161.5254 - val_loss: 122.0064 - val_mse: 122.0064
Epoch 242/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 154.6353 - mse: 154.6353
Epoch 00242: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 612us/sample - loss: 161.6825 - mse: 161.6825 - val_loss: 122.4657 - val_mse: 122.4657
Epoch 243/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.9796 - mse: 162.9796
Epoch 00243: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 594us/sample - loss: 161.2686 - mse: 161.2686 - val_loss: 122.2585 - val_mse: 122.2585
Epoch 244/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 153.8836 - mse: 153.8837
Epoch 00244: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 591us/sample - loss: 162.5450 - mse: 162.5450 - val_loss: 123.0547 - val_mse: 123.0547
Epoch 245/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 158.0266 - mse: 158.0266
Epoch 00245: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 597us/sample - loss: 160.3393 - mse: 160.3393 - val_loss: 122.3769 - val_mse: 122.3769
Epoch 246/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 156.8021 - mse: 156.8021
Epoch 00246: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 591us/sample - loss: 161.2476 - mse: 161.2476 - val_loss: 124.1134 - val_mse: 124.1134
Epoch 247/300
2272/2400 [===========================&gt;..] - ETA: 0s - loss: 157.2563 - mse: 157.2563
Epoch 00247: val_loss did not improve from 110.39956
2400/2400 [==============================] - 1s 592us/sample - loss: 160.9479 - mse: 160.9478 - val_loss: 124.1539 - val_mse: 124.1539
Epoch 248/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.2379 - mse: 163.2379
Epoch 00248: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 643us/sample - loss: 161.4348 - mse: 161.4348 - val_loss: 122.6437 - val_mse: 122.6437
Epoch 00248: early stopping
WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;, &lt;class &#39;NoneType&#39;&gt;
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Train on 4140 samples, validate on 1036 samples
Epoch 1/200
4000/4140 [===========================&gt;..] - ETA: 0s - loss: 2.6911 - accuracy: 0.6290
Epoch 00001: val_loss did not improve from 0.04715
4140/4140 [==============================] - 1s 219us/sample - loss: 2.6538 - accuracy: 0.6319 - val_loss: 1.0356 - val_accuracy: 0.8205
Epoch 2/200
3392/4140 [=======================&gt;......] - ETA: 0s - loss: 1.3145 - accuracy: 0.6728
Epoch 00002: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 79us/sample - loss: 1.2912 - accuracy: 0.6722 - val_loss: 0.6485 - val_accuracy: 0.8282
Epoch 3/200
3040/4140 [=====================&gt;........] - ETA: 0s - loss: 1.1586 - accuracy: 0.6852
Epoch 00003: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 1.1798 - accuracy: 0.6804 - val_loss: 0.5785 - val_accuracy: 0.8581
Epoch 4/200
2944/4140 [====================&gt;.........] - ETA: 0s - loss: 1.0308 - accuracy: 0.7045
Epoch 00004: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 1.1329 - accuracy: 0.6920 - val_loss: 1.9356 - val_accuracy: 0.4624
Epoch 5/200
2880/4140 [===================&gt;..........] - ETA: 0s - loss: 0.9164 - accuracy: 0.7076
Epoch 00005: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.9201 - accuracy: 0.7085 - val_loss: 0.6881 - val_accuracy: 0.7278
Epoch 6/200
2912/4140 [====================&gt;.........] - ETA: 0s - loss: 0.8091 - accuracy: 0.7139
Epoch 00006: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.8282 - accuracy: 0.7143 - val_loss: 2.1831 - val_accuracy: 0.4112
Epoch 7/200
4128/4140 [============================&gt;.] - ETA: 0s - loss: 0.6947 - accuracy: 0.7427
Epoch 00007: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 51us/sample - loss: 0.6938 - accuracy: 0.7430 - val_loss: 0.9497 - val_accuracy: 0.6486
Epoch 8/200
3072/4140 [=====================&gt;........] - ETA: 0s - loss: 0.7794 - accuracy: 0.7249
Epoch 00008: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.8002 - accuracy: 0.7234 - val_loss: 1.4806 - val_accuracy: 0.3649
Epoch 9/200
2880/4140 [===================&gt;..........] - ETA: 0s - loss: 0.6339 - accuracy: 0.7417
Epoch 00009: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.6547 - accuracy: 0.7331 - val_loss: 1.8377 - val_accuracy: 0.2876
Epoch 10/200
2848/4140 [===================&gt;..........] - ETA: 0s - loss: 0.7197 - accuracy: 0.7149
Epoch 00010: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.6734 - accuracy: 0.7273 - val_loss: 0.6976 - val_accuracy: 0.7008
Epoch 11/200
2912/4140 [====================&gt;.........] - ETA: 0s - loss: 0.5982 - accuracy: 0.7569
Epoch 00011: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.5963 - accuracy: 0.7556 - val_loss: 0.1155 - val_accuracy: 0.9710
Epoch 12/200
3008/4140 [====================&gt;.........] - ETA: 0s - loss: 0.7352 - accuracy: 0.7224
Epoch 00012: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 50us/sample - loss: 0.6956 - accuracy: 0.7261 - val_loss: 0.4955 - val_accuracy: 0.7790
Epoch 13/200
3200/4140 [======================&gt;.......] - ETA: 0s - loss: 0.5446 - accuracy: 0.7672
Epoch 00013: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 46us/sample - loss: 0.5339 - accuracy: 0.7691 - val_loss: 0.8148 - val_accuracy: 0.5830
Epoch 14/200
3200/4140 [======================&gt;.......] - ETA: 0s - loss: 0.5468 - accuracy: 0.7594
Epoch 00014: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.5496 - accuracy: 0.7589 - val_loss: 0.0631 - val_accuracy: 0.9903
Epoch 15/200
3232/4140 [======================&gt;.......] - ETA: 0s - loss: 0.6043 - accuracy: 0.7503
Epoch 00015: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.6525 - accuracy: 0.7408 - val_loss: 0.4515 - val_accuracy: 0.8156
Epoch 16/200
2816/4140 [===================&gt;..........] - ETA: 0s - loss: 0.6393 - accuracy: 0.7344
Epoch 00016: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.6186 - accuracy: 0.7389 - val_loss: 0.7538 - val_accuracy: 0.6622
Epoch 17/200
4128/4140 [============================&gt;.] - ETA: 0s - loss: 0.5746 - accuracy: 0.7599
Epoch 00017: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 51us/sample - loss: 0.5748 - accuracy: 0.7599 - val_loss: 0.7147 - val_accuracy: 0.6612
Epoch 18/200
3104/4140 [=====================&gt;........] - ETA: 0s - loss: 0.5030 - accuracy: 0.7726
Epoch 00018: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.5052 - accuracy: 0.7783 - val_loss: 1.2098 - val_accuracy: 0.4083
Epoch 19/200
3072/4140 [=====================&gt;........] - ETA: 0s - loss: 0.5670 - accuracy: 0.7575
Epoch 00019: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 45us/sample - loss: 0.5486 - accuracy: 0.7609 - val_loss: 0.4721 - val_accuracy: 0.8069
Epoch 20/200
3168/4140 [=====================&gt;........] - ETA: 0s - loss: 0.4614 - accuracy: 0.7936
Epoch 00020: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 50us/sample - loss: 0.4595 - accuracy: 0.7918 - val_loss: 1.0962 - val_accuracy: 0.4237
Epoch 21/200
3232/4140 [======================&gt;.......] - ETA: 0s - loss: 0.5035 - accuracy: 0.7887
Epoch 00021: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.5475 - accuracy: 0.7742 - val_loss: 0.5993 - val_accuracy: 0.7819
Epoch 22/200
3872/4140 [===========================&gt;..] - ETA: 0s - loss: 0.5356 - accuracy: 0.7745
Epoch 00022: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 51us/sample - loss: 0.5328 - accuracy: 0.7751 - val_loss: 0.8317 - val_accuracy: 0.5840
Epoch 23/200
2784/4140 [===================&gt;..........] - ETA: 0s - loss: 0.4973 - accuracy: 0.7665
Epoch 00023: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.4821 - accuracy: 0.7761 - val_loss: 0.8328 - val_accuracy: 0.5705
Epoch 24/200
2880/4140 [===================&gt;..........] - ETA: 0s - loss: 0.4342 - accuracy: 0.8031
Epoch 00024: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.4650 - accuracy: 0.7903 - val_loss: 0.4071 - val_accuracy: 0.8311
Epoch 25/200
2912/4140 [====================&gt;.........] - ETA: 0s - loss: 0.4580 - accuracy: 0.7916
Epoch 00025: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.4890 - accuracy: 0.7812 - val_loss: 0.7272 - val_accuracy: 0.6554
Epoch 26/200
2944/4140 [====================&gt;.........] - ETA: 0s - loss: 0.4324 - accuracy: 0.7945
Epoch 00026: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.4410 - accuracy: 0.7944 - val_loss: 0.3100 - val_accuracy: 0.8793
Epoch 27/200
3936/4140 [===========================&gt;..] - ETA: 0s - loss: 0.4487 - accuracy: 0.8054
Epoch 00027: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 47us/sample - loss: 0.4507 - accuracy: 0.8036 - val_loss: 0.4405 - val_accuracy: 0.8002
Epoch 28/200
3072/4140 [=====================&gt;........] - ETA: 0s - loss: 0.4669 - accuracy: 0.7923
Epoch 00028: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.4604 - accuracy: 0.7940 - val_loss: 0.2867 - val_accuracy: 0.8967
Epoch 29/200
3136/4140 [=====================&gt;........] - ETA: 0s - loss: 0.4409 - accuracy: 0.7956
Epoch 00029: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 45us/sample - loss: 0.4444 - accuracy: 0.7995 - val_loss: 1.4028 - val_accuracy: 0.4392
Epoch 30/200
2816/4140 [===================&gt;..........] - ETA: 0s - loss: 0.4698 - accuracy: 0.7940
Epoch 00030: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.5630 - accuracy: 0.7684 - val_loss: 1.3650 - val_accuracy: 0.4054
Epoch 31/200
3808/4140 [==========================&gt;...] - ETA: 0s - loss: 0.4525 - accuracy: 0.7973
Epoch 00031: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 60us/sample - loss: 0.4481 - accuracy: 0.7990 - val_loss: 0.7290 - val_accuracy: 0.6062
Epoch 32/200
3584/4140 [========================&gt;.....] - ETA: 0s - loss: 0.5093 - accuracy: 0.7762
Epoch 00032: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 64us/sample - loss: 0.4944 - accuracy: 0.7826 - val_loss: 1.1146 - val_accuracy: 0.3900
Epoch 33/200
3456/4140 [========================&gt;.....] - ETA: 0s - loss: 0.4687 - accuracy: 0.7859
Epoch 00033: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 79us/sample - loss: 0.4774 - accuracy: 0.7843 - val_loss: 0.9182 - val_accuracy: 0.4923
Epoch 34/200
3360/4140 [=======================&gt;......] - ETA: 0s - loss: 0.4040 - accuracy: 0.8152
Epoch 00034: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 64us/sample - loss: 0.4059 - accuracy: 0.8188 - val_loss: 0.3631 - val_accuracy: 0.8822
Epoch 35/200
3520/4140 [========================&gt;.....] - ETA: 0s - loss: 0.4000 - accuracy: 0.8224
Epoch 00035: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 56us/sample - loss: 0.4107 - accuracy: 0.8198 - val_loss: 0.4135 - val_accuracy: 0.8234
Epoch 36/200
3840/4140 [==========================&gt;...] - ETA: 0s - loss: 0.4209 - accuracy: 0.8057
Epoch 00036: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 102us/sample - loss: 0.4171 - accuracy: 0.8070 - val_loss: 0.9188 - val_accuracy: 0.5415
Epoch 37/200
3840/4140 [==========================&gt;...] - ETA: 0s - loss: 0.4095 - accuracy: 0.8245
Epoch 00037: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 118us/sample - loss: 0.4118 - accuracy: 0.8225 - val_loss: 0.4150 - val_accuracy: 0.8137
Epoch 38/200
4000/4140 [===========================&gt;..] - ETA: 0s - loss: 0.4453 - accuracy: 0.8023
Epoch 00038: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 74us/sample - loss: 0.4444 - accuracy: 0.8027 - val_loss: 0.2347 - val_accuracy: 0.9218
Epoch 39/200
3744/4140 [==========================&gt;...] - ETA: 0s - loss: 0.3927 - accuracy: 0.8205
Epoch 00039: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 69us/sample - loss: 0.3854 - accuracy: 0.8234 - val_loss: 0.7453 - val_accuracy: 0.6313
Epoch 40/200
3232/4140 [======================&gt;.......] - ETA: 0s - loss: 0.4383 - accuracy: 0.8131
Epoch 00040: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 57us/sample - loss: 0.4150 - accuracy: 0.8200 - val_loss: 0.5623 - val_accuracy: 0.7423
Epoch 41/200
3200/4140 [======================&gt;.......] - ETA: 0s - loss: 0.3998 - accuracy: 0.8234
Epoch 00041: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 57us/sample - loss: 0.3978 - accuracy: 0.8225 - val_loss: 0.3754 - val_accuracy: 0.8456
Epoch 42/200
3648/4140 [=========================&gt;....] - ETA: 0s - loss: 0.3637 - accuracy: 0.8374
Epoch 00042: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 52us/sample - loss: 0.3691 - accuracy: 0.8365 - val_loss: 1.4027 - val_accuracy: 0.3407
Epoch 43/200
3712/4140 [=========================&gt;....] - ETA: 0s - loss: 0.3556 - accuracy: 0.8367
Epoch 00043: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 51us/sample - loss: 0.3553 - accuracy: 0.8374 - val_loss: 0.6514 - val_accuracy: 0.6515
Epoch 44/200
3808/4140 [==========================&gt;...] - ETA: 0s - loss: 0.3720 - accuracy: 0.8348
Epoch 00044: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 50us/sample - loss: 0.3753 - accuracy: 0.8338 - val_loss: 0.8139 - val_accuracy: 0.5898
Epoch 45/200
3744/4140 [==========================&gt;...] - ETA: 0s - loss: 0.3554 - accuracy: 0.8456
Epoch 00045: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 51us/sample - loss: 0.3542 - accuracy: 0.8461 - val_loss: 0.2964 - val_accuracy: 0.8900
Epoch 46/200
3808/4140 [==========================&gt;...] - ETA: 0s - loss: 0.4134 - accuracy: 0.8251
Epoch 00046: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 51us/sample - loss: 0.4125 - accuracy: 0.8246 - val_loss: 0.1660 - val_accuracy: 0.9624
Epoch 47/200
3712/4140 [=========================&gt;....] - ETA: 0s - loss: 0.3601 - accuracy: 0.8349
Epoch 00047: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 52us/sample - loss: 0.3641 - accuracy: 0.8353 - val_loss: 0.9979 - val_accuracy: 0.5174
Epoch 48/200
3872/4140 [===========================&gt;..] - ETA: 0s - loss: 0.3677 - accuracy: 0.8339
Epoch 00048: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 50us/sample - loss: 0.3653 - accuracy: 0.8357 - val_loss: 0.2474 - val_accuracy: 0.9237
Epoch 49/200
3872/4140 [===========================&gt;..] - ETA: 0s - loss: 0.3641 - accuracy: 0.8427
Epoch 00049: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.3650 - accuracy: 0.8428 - val_loss: 0.9942 - val_accuracy: 0.5174
Epoch 50/200
3744/4140 [==========================&gt;...] - ETA: 0s - loss: 0.3629 - accuracy: 0.8392
Epoch 00050: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 50us/sample - loss: 0.3850 - accuracy: 0.8326 - val_loss: 1.0402 - val_accuracy: 0.4942
Epoch 51/200
3488/4140 [========================&gt;.....] - ETA: 0s - loss: 0.4105 - accuracy: 0.8228
Epoch 00051: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 56us/sample - loss: 0.4014 - accuracy: 0.8261 - val_loss: 0.4141 - val_accuracy: 0.8012
Epoch 52/200
3264/4140 [======================&gt;.......] - ETA: 0s - loss: 0.3690 - accuracy: 0.8361
Epoch 00052: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 56us/sample - loss: 0.3772 - accuracy: 0.8302 - val_loss: 0.5330 - val_accuracy: 0.7375
Epoch 53/200
3808/4140 [==========================&gt;...] - ETA: 0s - loss: 0.3326 - accuracy: 0.8569
Epoch 00053: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 50us/sample - loss: 0.3325 - accuracy: 0.8558 - val_loss: 0.6312 - val_accuracy: 0.6718
Epoch 54/200
3936/4140 [===========================&gt;..] - ETA: 0s - loss: 0.3255 - accuracy: 0.8565
Epoch 00054: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.3271 - accuracy: 0.8546 - val_loss: 0.1859 - val_accuracy: 0.9431
Epoch 55/200
3648/4140 [=========================&gt;....] - ETA: 0s - loss: 0.3503 - accuracy: 0.8490
Epoch 00055: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 51us/sample - loss: 0.3469 - accuracy: 0.8500 - val_loss: 0.5612 - val_accuracy: 0.7365
Epoch 56/200
3840/4140 [==========================&gt;...] - ETA: 0s - loss: 0.3924 - accuracy: 0.8328
Epoch 00056: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 50us/sample - loss: 0.3875 - accuracy: 0.8374 - val_loss: 0.3892 - val_accuracy: 0.8736
Epoch 57/200
3808/4140 [==========================&gt;...] - ETA: 0s - loss: 0.3248 - accuracy: 0.8606
Epoch 00057: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 51us/sample - loss: 0.3249 - accuracy: 0.8609 - val_loss: 0.2307 - val_accuracy: 0.9199
Epoch 58/200
3904/4140 [===========================&gt;..] - ETA: 0s - loss: 0.3143 - accuracy: 0.8591
Epoch 00058: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.3138 - accuracy: 0.8589 - val_loss: 0.8593 - val_accuracy: 0.6670
Epoch 59/200
3616/4140 [=========================&gt;....] - ETA: 0s - loss: 0.3293 - accuracy: 0.8554
Epoch 00059: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 52us/sample - loss: 0.3218 - accuracy: 0.8585 - val_loss: 0.3683 - val_accuracy: 0.8176
Epoch 60/200
3488/4140 [========================&gt;.....] - ETA: 0s - loss: 0.2971 - accuracy: 0.8655
Epoch 00060: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 53us/sample - loss: 0.3013 - accuracy: 0.8664 - val_loss: 0.4019 - val_accuracy: 0.8272
Epoch 61/200
3712/4140 [=========================&gt;....] - ETA: 0s - loss: 0.2602 - accuracy: 0.8901
Epoch 00061: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 51us/sample - loss: 0.2655 - accuracy: 0.8874 - val_loss: 0.7402 - val_accuracy: 0.6795
Epoch 62/200
3776/4140 [==========================&gt;...] - ETA: 0s - loss: 0.3127 - accuracy: 0.8700
Epoch 00062: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 51us/sample - loss: 0.3148 - accuracy: 0.8703 - val_loss: 0.9778 - val_accuracy: 0.5647
Epoch 63/200
3808/4140 [==========================&gt;...] - ETA: 0s - loss: 0.2916 - accuracy: 0.8732
Epoch 00063: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 50us/sample - loss: 0.2936 - accuracy: 0.8703 - val_loss: 0.1412 - val_accuracy: 0.9537
Epoch 64/200
3776/4140 [==========================&gt;...] - ETA: 0s - loss: 0.3233 - accuracy: 0.8581
Epoch 00064: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 50us/sample - loss: 0.3210 - accuracy: 0.8589 - val_loss: 0.2296 - val_accuracy: 0.9122
Epoch 65/200
3648/4140 [=========================&gt;....] - ETA: 0s - loss: 0.2907 - accuracy: 0.8731
Epoch 00065: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 52us/sample - loss: 0.2851 - accuracy: 0.8758 - val_loss: 0.3074 - val_accuracy: 0.8726
Epoch 66/200
3744/4140 [==========================&gt;...] - ETA: 0s - loss: 0.3386 - accuracy: 0.8614
Epoch 00066: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 51us/sample - loss: 0.3378 - accuracy: 0.8623 - val_loss: 0.2757 - val_accuracy: 0.9025
Epoch 67/200
3648/4140 [=========================&gt;....] - ETA: 0s - loss: 0.3087 - accuracy: 0.8627
Epoch 00067: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 51us/sample - loss: 0.3089 - accuracy: 0.8626 - val_loss: 0.3513 - val_accuracy: 0.8610
Epoch 68/200
3872/4140 [===========================&gt;..] - ETA: 0s - loss: 0.3146 - accuracy: 0.8580
Epoch 00068: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 50us/sample - loss: 0.3118 - accuracy: 0.8599 - val_loss: 0.3182 - val_accuracy: 0.8639
Epoch 69/200
3456/4140 [========================&gt;.....] - ETA: 0s - loss: 0.2700 - accuracy: 0.8805
Epoch 00069: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 67us/sample - loss: 0.2762 - accuracy: 0.8800 - val_loss: 0.5230 - val_accuracy: 0.7577
Epoch 70/200
3776/4140 [==========================&gt;...] - ETA: 0s - loss: 0.3075 - accuracy: 0.8689
Epoch 00070: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 50us/sample - loss: 0.3086 - accuracy: 0.8674 - val_loss: 0.1597 - val_accuracy: 0.9469
Epoch 71/200
3840/4140 [==========================&gt;...] - ETA: 0s - loss: 0.2725 - accuracy: 0.8828
Epoch 00071: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 51us/sample - loss: 0.2706 - accuracy: 0.8833 - val_loss: 0.1999 - val_accuracy: 0.9392
Epoch 72/200
3648/4140 [=========================&gt;....] - ETA: 0s - loss: 0.2527 - accuracy: 0.8931
Epoch 00072: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 53us/sample - loss: 0.2531 - accuracy: 0.8937 - val_loss: 0.3834 - val_accuracy: 0.8311
Epoch 73/200
3776/4140 [==========================&gt;...] - ETA: 0s - loss: 0.2811 - accuracy: 0.8837
Epoch 00073: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 50us/sample - loss: 0.2809 - accuracy: 0.8821 - val_loss: 1.4750 - val_accuracy: 0.4083
Epoch 74/200
3744/4140 [==========================&gt;...] - ETA: 0s - loss: 0.3082 - accuracy: 0.8691
Epoch 00074: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 50us/sample - loss: 0.3036 - accuracy: 0.8720 - val_loss: 0.3396 - val_accuracy: 0.8475
Epoch 75/200
3936/4140 [===========================&gt;..] - ETA: 0s - loss: 0.2796 - accuracy: 0.8793
Epoch 00075: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.2745 - accuracy: 0.8824 - val_loss: 0.4122 - val_accuracy: 0.8137
Epoch 76/200
3424/4140 [=======================&gt;......] - ETA: 0s - loss: 0.2652 - accuracy: 0.8852
Epoch 00076: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 54us/sample - loss: 0.2770 - accuracy: 0.8812 - val_loss: 0.3853 - val_accuracy: 0.8523
Epoch 77/200
3808/4140 [==========================&gt;...] - ETA: 0s - loss: 0.3403 - accuracy: 0.8621
Epoch 00077: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.3350 - accuracy: 0.8628 - val_loss: 0.4861 - val_accuracy: 0.7693
Epoch 78/200
3936/4140 [===========================&gt;..] - ETA: 0s - loss: 0.2683 - accuracy: 0.8859
Epoch 00078: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.2659 - accuracy: 0.8865 - val_loss: 0.5889 - val_accuracy: 0.7114
Epoch 79/200
3808/4140 [==========================&gt;...] - ETA: 0s - loss: 0.2572 - accuracy: 0.8934
Epoch 00079: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 50us/sample - loss: 0.2556 - accuracy: 0.8952 - val_loss: 0.1752 - val_accuracy: 0.9363
Epoch 80/200
3808/4140 [==========================&gt;...] - ETA: 0s - loss: 0.2731 - accuracy: 0.8871
Epoch 00080: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 51us/sample - loss: 0.2659 - accuracy: 0.8889 - val_loss: 0.2739 - val_accuracy: 0.9025
Epoch 81/200
3776/4140 [==========================&gt;...] - ETA: 0s - loss: 0.2293 - accuracy: 0.8996
Epoch 00081: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 51us/sample - loss: 0.2426 - accuracy: 0.8947 - val_loss: 0.6472 - val_accuracy: 0.6583
Epoch 82/200
3808/4140 [==========================&gt;...] - ETA: 0s - loss: 0.2337 - accuracy: 0.9005
Epoch 00082: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.2321 - accuracy: 0.9019 - val_loss: 0.3527 - val_accuracy: 0.8446
Epoch 83/200
3840/4140 [==========================&gt;...] - ETA: 0s - loss: 0.2555 - accuracy: 0.8971
Epoch 00083: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.2528 - accuracy: 0.8986 - val_loss: 0.2484 - val_accuracy: 0.8938
Epoch 84/200
3616/4140 [=========================&gt;....] - ETA: 0s - loss: 0.2339 - accuracy: 0.8996
Epoch 00084: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 52us/sample - loss: 0.2393 - accuracy: 0.8981 - val_loss: 0.5690 - val_accuracy: 0.7519
Epoch 85/200
3232/4140 [======================&gt;.......] - ETA: 0s - loss: 0.2615 - accuracy: 0.8911
Epoch 00085: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 57us/sample - loss: 0.2650 - accuracy: 0.8889 - val_loss: 0.2485 - val_accuracy: 0.8958
Epoch 86/200
3680/4140 [=========================&gt;....] - ETA: 0s - loss: 0.2175 - accuracy: 0.9082
Epoch 00086: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 53us/sample - loss: 0.2171 - accuracy: 0.9099 - val_loss: 0.2265 - val_accuracy: 0.9189
Epoch 87/200
3104/4140 [=====================&gt;........] - ETA: 0s - loss: 0.2462 - accuracy: 0.8943
Epoch 00087: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 59us/sample - loss: 0.2354 - accuracy: 0.8988 - val_loss: 0.3345 - val_accuracy: 0.8542
Epoch 88/200
3936/4140 [===========================&gt;..] - ETA: 0s - loss: 0.2363 - accuracy: 0.8938
Epoch 00088: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 63us/sample - loss: 0.2357 - accuracy: 0.8935 - val_loss: 0.5483 - val_accuracy: 0.7539
Epoch 89/200
3872/4140 [===========================&gt;..] - ETA: 0s - loss: 0.2167 - accuracy: 0.9044
Epoch 00089: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.2177 - accuracy: 0.9051 - val_loss: 0.2518 - val_accuracy: 0.8736
Epoch 90/200
3776/4140 [==========================&gt;...] - ETA: 0s - loss: 0.2162 - accuracy: 0.9081
Epoch 00090: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.2139 - accuracy: 0.9097 - val_loss: 0.1386 - val_accuracy: 0.9556
Epoch 91/200
3840/4140 [==========================&gt;...] - ETA: 0s - loss: 0.2087 - accuracy: 0.9112
Epoch 00091: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.2124 - accuracy: 0.9106 - val_loss: 0.4346 - val_accuracy: 0.7963
Epoch 92/200
3936/4140 [===========================&gt;..] - ETA: 0s - loss: 0.2190 - accuracy: 0.9121
Epoch 00092: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.2182 - accuracy: 0.9109 - val_loss: 0.4317 - val_accuracy: 0.7809
Epoch 93/200
3776/4140 [==========================&gt;...] - ETA: 0s - loss: 0.2104 - accuracy: 0.9121
Epoch 00093: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 50us/sample - loss: 0.2124 - accuracy: 0.9126 - val_loss: 0.2112 - val_accuracy: 0.9180
Epoch 94/200
3776/4140 [==========================&gt;...] - ETA: 0s - loss: 0.2108 - accuracy: 0.9118
Epoch 00094: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 50us/sample - loss: 0.2165 - accuracy: 0.9101 - val_loss: 0.5090 - val_accuracy: 0.7529
Epoch 95/200
3392/4140 [=======================&gt;......] - ETA: 0s - loss: 0.2862 - accuracy: 0.8900
Epoch 00095: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 73us/sample - loss: 0.2768 - accuracy: 0.8923 - val_loss: 0.6930 - val_accuracy: 0.7181
Epoch 96/200
3392/4140 [=======================&gt;......] - ETA: 0s - loss: 0.2257 - accuracy: 0.9071
Epoch 00096: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 56us/sample - loss: 0.2156 - accuracy: 0.9111 - val_loss: 0.3098 - val_accuracy: 0.8851
Epoch 97/200
3872/4140 [===========================&gt;..] - ETA: 0s - loss: 0.1944 - accuracy: 0.9220
Epoch 00097: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 83us/sample - loss: 0.1987 - accuracy: 0.9186 - val_loss: 0.6350 - val_accuracy: 0.7259
Epoch 98/200
3040/4140 [=====================&gt;........] - ETA: 0s - loss: 0.1879 - accuracy: 0.9243
Epoch 00098: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 73us/sample - loss: 0.1915 - accuracy: 0.9215 - val_loss: 0.2828 - val_accuracy: 0.8832
Epoch 99/200
3328/4140 [=======================&gt;......] - ETA: 0s - loss: 0.1845 - accuracy: 0.9210
Epoch 00099: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 59us/sample - loss: 0.1856 - accuracy: 0.9188 - val_loss: 0.1595 - val_accuracy: 0.9450
Epoch 100/200
3360/4140 [=======================&gt;......] - ETA: 0s - loss: 0.1562 - accuracy: 0.9330
Epoch 00100: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 73us/sample - loss: 0.1648 - accuracy: 0.9312 - val_loss: 0.1399 - val_accuracy: 0.9595
Epoch 101/200
3488/4140 [========================&gt;.....] - ETA: 0s - loss: 0.1915 - accuracy: 0.9212
Epoch 00101: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 73us/sample - loss: 0.1906 - accuracy: 0.9217 - val_loss: 0.3521 - val_accuracy: 0.8562
Epoch 102/200
3584/4140 [========================&gt;.....] - ETA: 0s - loss: 0.2164 - accuracy: 0.9110
Epoch 00102: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 72us/sample - loss: 0.2194 - accuracy: 0.9114 - val_loss: 0.1328 - val_accuracy: 0.9498
Epoch 103/200
3744/4140 [==========================&gt;...] - ETA: 0s - loss: 0.2036 - accuracy: 0.9177
Epoch 00103: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 68us/sample - loss: 0.2097 - accuracy: 0.9164 - val_loss: 0.3723 - val_accuracy: 0.8292
Epoch 104/200
3488/4140 [========================&gt;.....] - ETA: 0s - loss: 0.1863 - accuracy: 0.9249
Epoch 00104: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 56us/sample - loss: 0.1846 - accuracy: 0.9251 - val_loss: 0.1447 - val_accuracy: 0.9440
Epoch 105/200
3776/4140 [==========================&gt;...] - ETA: 0s - loss: 0.1871 - accuracy: 0.9285
Epoch 00105: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 67us/sample - loss: 0.1847 - accuracy: 0.9283 - val_loss: 0.3760 - val_accuracy: 0.8253
Epoch 106/200
3648/4140 [=========================&gt;....] - ETA: 0s - loss: 0.1555 - accuracy: 0.9334
Epoch 00106: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 98us/sample - loss: 0.1530 - accuracy: 0.9355 - val_loss: 0.2306 - val_accuracy: 0.9064
Epoch 107/200
3680/4140 [=========================&gt;....] - ETA: 0s - loss: 0.1501 - accuracy: 0.9391
Epoch 00107: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 66us/sample - loss: 0.1507 - accuracy: 0.9386 - val_loss: 0.7672 - val_accuracy: 0.6766
Epoch 108/200
3296/4140 [======================&gt;.......] - ETA: 0s - loss: 0.2155 - accuracy: 0.9053
Epoch 00108: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 59us/sample - loss: 0.2142 - accuracy: 0.9065 - val_loss: 0.3414 - val_accuracy: 0.8552
Epoch 109/200
3136/4140 [=====================&gt;........] - ETA: 0s - loss: 0.1760 - accuracy: 0.9283
Epoch 00109: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 60us/sample - loss: 0.2020 - accuracy: 0.9171 - val_loss: 0.1136 - val_accuracy: 0.9720
Epoch 110/200
3712/4140 [=========================&gt;....] - ETA: 0s - loss: 0.2243 - accuracy: 0.9054
Epoch 00110: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 52us/sample - loss: 0.2158 - accuracy: 0.9092 - val_loss: 0.1227 - val_accuracy: 0.9614
Epoch 111/200
3648/4140 [=========================&gt;....] - ETA: 0s - loss: 0.2000 - accuracy: 0.9194
Epoch 00111: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 68us/sample - loss: 0.1981 - accuracy: 0.9193 - val_loss: 0.3897 - val_accuracy: 0.8263
Epoch 112/200
3104/4140 [=====================&gt;........] - ETA: 0s - loss: 0.1856 - accuracy: 0.9253
Epoch 00112: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 59us/sample - loss: 0.1799 - accuracy: 0.9290 - val_loss: 0.2418 - val_accuracy: 0.9015
Epoch 113/200
3584/4140 [========================&gt;.....] - ETA: 0s - loss: 0.1661 - accuracy: 0.9297
Epoch 00113: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 52us/sample - loss: 0.1661 - accuracy: 0.9309 - val_loss: 0.2394 - val_accuracy: 0.9015
Epoch 114/200
3616/4140 [=========================&gt;....] - ETA: 0s - loss: 0.1920 - accuracy: 0.9143
Epoch 00114: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 52us/sample - loss: 0.1840 - accuracy: 0.9193 - val_loss: 0.0559 - val_accuracy: 0.9903
Epoch 115/200
3680/4140 [=========================&gt;....] - ETA: 0s - loss: 0.1515 - accuracy: 0.9424
Epoch 00115: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 51us/sample - loss: 0.1534 - accuracy: 0.9415 - val_loss: 0.6353 - val_accuracy: 0.8311
Epoch 116/200
3712/4140 [=========================&gt;....] - ETA: 0s - loss: 0.1686 - accuracy: 0.9367
Epoch 00116: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 51us/sample - loss: 0.1747 - accuracy: 0.9345 - val_loss: 0.3946 - val_accuracy: 0.8533
Epoch 117/200
3744/4140 [==========================&gt;...] - ETA: 0s - loss: 0.2153 - accuracy: 0.9132
Epoch 00117: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 51us/sample - loss: 0.2261 - accuracy: 0.9114 - val_loss: 0.2065 - val_accuracy: 0.9450
Epoch 118/200
3840/4140 [==========================&gt;...] - ETA: 0s - loss: 0.1822 - accuracy: 0.9247
Epoch 00118: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 50us/sample - loss: 0.1800 - accuracy: 0.9251 - val_loss: 0.1068 - val_accuracy: 0.9701
Epoch 119/200
3744/4140 [==========================&gt;...] - ETA: 0s - loss: 0.1568 - accuracy: 0.9316
Epoch 00119: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 51us/sample - loss: 0.1586 - accuracy: 0.9316 - val_loss: 0.1607 - val_accuracy: 0.9469
Epoch 120/200
3808/4140 [==========================&gt;...] - ETA: 0s - loss: 0.1491 - accuracy: 0.9378
Epoch 00120: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 50us/sample - loss: 0.1530 - accuracy: 0.9355 - val_loss: 0.1068 - val_accuracy: 0.9614
Epoch 121/200
3744/4140 [==========================&gt;...] - ETA: 0s - loss: 0.1803 - accuracy: 0.9199
Epoch 00121: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 51us/sample - loss: 0.1846 - accuracy: 0.9191 - val_loss: 0.2697 - val_accuracy: 0.8851
Epoch 122/200
3776/4140 [==========================&gt;...] - ETA: 0s - loss: 0.1402 - accuracy: 0.9425
Epoch 00122: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 50us/sample - loss: 0.1377 - accuracy: 0.9437 - val_loss: 0.3621 - val_accuracy: 0.8475
Epoch 123/200
3808/4140 [==========================&gt;...] - ETA: 0s - loss: 0.1209 - accuracy: 0.9538
Epoch 00123: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 50us/sample - loss: 0.1238 - accuracy: 0.9512 - val_loss: 0.1915 - val_accuracy: 0.9257
Epoch 124/200
3744/4140 [==========================&gt;...] - ETA: 0s - loss: 0.1625 - accuracy: 0.9391
Epoch 00124: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 51us/sample - loss: 0.1632 - accuracy: 0.9384 - val_loss: 0.1811 - val_accuracy: 0.9276
Epoch 125/200
3776/4140 [==========================&gt;...] - ETA: 0s - loss: 0.1392 - accuracy: 0.9420
Epoch 00125: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 50us/sample - loss: 0.1441 - accuracy: 0.9399 - val_loss: 0.3002 - val_accuracy: 0.8803
Epoch 126/200
3744/4140 [==========================&gt;...] - ETA: 0s - loss: 0.1708 - accuracy: 0.9351
Epoch 00126: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 51us/sample - loss: 0.1651 - accuracy: 0.9372 - val_loss: 0.3203 - val_accuracy: 0.8871
Epoch 127/200
3776/4140 [==========================&gt;...] - ETA: 0s - loss: 0.1436 - accuracy: 0.9399
Epoch 00127: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 50us/sample - loss: 0.1433 - accuracy: 0.9399 - val_loss: 0.0858 - val_accuracy: 0.9720
Epoch 128/200
3392/4140 [=======================&gt;......] - ETA: 0s - loss: 0.1276 - accuracy: 0.9496
Epoch 00128: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 56us/sample - loss: 0.1286 - accuracy: 0.9490 - val_loss: 0.0959 - val_accuracy: 0.9749
Epoch 129/200
3360/4140 [=======================&gt;......] - ETA: 0s - loss: 0.1245 - accuracy: 0.9545
Epoch 00129: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 56us/sample - loss: 0.1862 - accuracy: 0.9357 - val_loss: 0.7678 - val_accuracy: 0.8388
Epoch 130/200
3712/4140 [=========================&gt;....] - ETA: 0s - loss: 0.2579 - accuracy: 0.9011
Epoch 00130: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 51us/sample - loss: 0.2495 - accuracy: 0.9041 - val_loss: 0.5962 - val_accuracy: 0.7394
Epoch 131/200
3872/4140 [===========================&gt;..] - ETA: 0s - loss: 0.1642 - accuracy: 0.9362
Epoch 00131: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.1618 - accuracy: 0.9374 - val_loss: 0.1811 - val_accuracy: 0.9392
Epoch 132/200
3840/4140 [==========================&gt;...] - ETA: 0s - loss: 0.1203 - accuracy: 0.9563
Epoch 00132: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 50us/sample - loss: 0.1231 - accuracy: 0.9558 - val_loss: 0.1006 - val_accuracy: 0.9720
Epoch 133/200
3904/4140 [===========================&gt;..] - ETA: 0s - loss: 0.1259 - accuracy: 0.9480
Epoch 00133: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 48us/sample - loss: 0.1284 - accuracy: 0.9481 - val_loss: 0.3347 - val_accuracy: 0.8832
Epoch 134/200
3808/4140 [==========================&gt;...] - ETA: 0s - loss: 0.1189 - accuracy: 0.9527
Epoch 00134: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.1180 - accuracy: 0.9534 - val_loss: 0.1561 - val_accuracy: 0.9373
Epoch 135/200
3776/4140 [==========================&gt;...] - ETA: 0s - loss: 0.1492 - accuracy: 0.9428
Epoch 00135: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 50us/sample - loss: 0.1453 - accuracy: 0.9437 - val_loss: 0.3010 - val_accuracy: 0.8803
Epoch 136/200
3904/4140 [===========================&gt;..] - ETA: 0s - loss: 0.1055 - accuracy: 0.9603
Epoch 00136: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.1048 - accuracy: 0.9609 - val_loss: 0.1149 - val_accuracy: 0.9807
Epoch 137/200
3872/4140 [===========================&gt;..] - ETA: 0s - loss: 0.1150 - accuracy: 0.9530
Epoch 00137: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 50us/sample - loss: 0.1159 - accuracy: 0.9527 - val_loss: 0.1380 - val_accuracy: 0.9421
Epoch 138/200
3744/4140 [==========================&gt;...] - ETA: 0s - loss: 0.1094 - accuracy: 0.9594
Epoch 00138: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 50us/sample - loss: 0.1120 - accuracy: 0.9575 - val_loss: 0.6242 - val_accuracy: 0.7722
Epoch 139/200
3840/4140 [==========================&gt;...] - ETA: 0s - loss: 0.1522 - accuracy: 0.9396
Epoch 00139: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.1576 - accuracy: 0.9365 - val_loss: 0.5039 - val_accuracy: 0.7770
Epoch 140/200
3808/4140 [==========================&gt;...] - ETA: 0s - loss: 0.2450 - accuracy: 0.9049
Epoch 00140: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 50us/sample - loss: 0.2394 - accuracy: 0.9072 - val_loss: 0.1868 - val_accuracy: 0.9286
Epoch 141/200
3936/4140 [===========================&gt;..] - ETA: 0s - loss: 0.1348 - accuracy: 0.9459
Epoch 00141: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 49us/sample - loss: 0.1337 - accuracy: 0.9464 - val_loss: 0.1947 - val_accuracy: 0.9189
Epoch 142/200
3904/4140 [===========================&gt;..] - ETA: 0s - loss: 0.1129 - accuracy: 0.9521
Epoch 00142: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 60us/sample - loss: 0.1127 - accuracy: 0.9517 - val_loss: 0.1660 - val_accuracy: 0.9373
Epoch 143/200
4032/4140 [============================&gt;.] - ETA: 0s - loss: 0.1275 - accuracy: 0.9482
Epoch 00143: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 92us/sample - loss: 0.1276 - accuracy: 0.9483 - val_loss: 0.1330 - val_accuracy: 0.9527
Epoch 144/200
3616/4140 [=========================&gt;....] - ETA: 0s - loss: 0.1800 - accuracy: 0.9289
Epoch 00144: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 82us/sample - loss: 0.2026 - accuracy: 0.9203 - val_loss: 0.4387 - val_accuracy: 0.8108
Epoch 145/200
3328/4140 [=======================&gt;......] - ETA: 0s - loss: 0.1419 - accuracy: 0.9438
Epoch 00145: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 68us/sample - loss: 0.1387 - accuracy: 0.9459 - val_loss: 0.0830 - val_accuracy: 0.9768
Epoch 146/200
3232/4140 [======================&gt;.......] - ETA: 0s - loss: 0.1946 - accuracy: 0.9186
Epoch 00146: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 69us/sample - loss: 0.1760 - accuracy: 0.9273 - val_loss: 0.2160 - val_accuracy: 0.9247
Epoch 147/200
3904/4140 [===========================&gt;..] - ETA: 0s - loss: 0.1744 - accuracy: 0.9326
Epoch 00147: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 77us/sample - loss: 0.1721 - accuracy: 0.9329 - val_loss: 0.1443 - val_accuracy: 0.9479
Epoch 148/200
3968/4140 [===========================&gt;..] - ETA: 0s - loss: 0.0943 - accuracy: 0.9642
Epoch 00148: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 65us/sample - loss: 0.0954 - accuracy: 0.9635 - val_loss: 0.0831 - val_accuracy: 0.9691
Epoch 149/200
3872/4140 [===========================&gt;..] - ETA: 0s - loss: 0.1308 - accuracy: 0.9468
Epoch 00149: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 65us/sample - loss: 0.1291 - accuracy: 0.9478 - val_loss: 0.1960 - val_accuracy: 0.9295
Epoch 150/200
3808/4140 [==========================&gt;...] - ETA: 0s - loss: 0.1050 - accuracy: 0.9603
Epoch 00150: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 76us/sample - loss: 0.1061 - accuracy: 0.9601 - val_loss: 0.0512 - val_accuracy: 0.9884
Epoch 151/200
4000/4140 [===========================&gt;..] - ETA: 0s - loss: 0.1141 - accuracy: 0.9542
Epoch 00151: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 76us/sample - loss: 0.1133 - accuracy: 0.9543 - val_loss: 0.1521 - val_accuracy: 0.9479
Epoch 152/200
3968/4140 [===========================&gt;..] - ETA: 0s - loss: 0.1153 - accuracy: 0.9567
Epoch 00152: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 78us/sample - loss: 0.1139 - accuracy: 0.9563 - val_loss: 0.0682 - val_accuracy: 0.9768
Epoch 153/200
3168/4140 [=====================&gt;........] - ETA: 0s - loss: 0.0761 - accuracy: 0.9725
Epoch 00153: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 68us/sample - loss: 0.0848 - accuracy: 0.9708 - val_loss: 0.2554 - val_accuracy: 0.9064
Epoch 154/200
3456/4140 [========================&gt;.....] - ETA: 0s - loss: 0.2525 - accuracy: 0.9175
Epoch 00154: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 89us/sample - loss: 0.2407 - accuracy: 0.9164 - val_loss: 0.1246 - val_accuracy: 0.9537
Epoch 155/200
4128/4140 [============================&gt;.] - ETA: 0s - loss: 0.1084 - accuracy: 0.9608
Epoch 00155: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 88us/sample - loss: 0.1082 - accuracy: 0.9609 - val_loss: 0.1358 - val_accuracy: 0.9537
Epoch 156/200
3840/4140 [==========================&gt;...] - ETA: 0s - loss: 0.0962 - accuracy: 0.9633
Epoch 00156: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 89us/sample - loss: 0.0959 - accuracy: 0.9630 - val_loss: 0.1424 - val_accuracy: 0.9459
Epoch 157/200
3968/4140 [===========================&gt;..] - ETA: 0s - loss: 0.1050 - accuracy: 0.9597
Epoch 00157: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 77us/sample - loss: 0.1082 - accuracy: 0.9592 - val_loss: 0.1705 - val_accuracy: 0.9411
Epoch 158/200
3904/4140 [===========================&gt;..] - ETA: 0s - loss: 0.0838 - accuracy: 0.9662
Epoch 00158: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 80us/sample - loss: 0.0831 - accuracy: 0.9671 - val_loss: 0.0789 - val_accuracy: 0.9817
Epoch 159/200
3680/4140 [=========================&gt;....] - ETA: 0s - loss: 0.0735 - accuracy: 0.9755
Epoch 00159: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 67us/sample - loss: 0.0721 - accuracy: 0.9758 - val_loss: 0.0657 - val_accuracy: 0.9807
Epoch 160/200
3648/4140 [=========================&gt;....] - ETA: 0s - loss: 0.0635 - accuracy: 0.9770
Epoch 00160: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 67us/sample - loss: 0.0682 - accuracy: 0.9763 - val_loss: 0.1127 - val_accuracy: 0.9730
Epoch 161/200
3712/4140 [=========================&gt;....] - ETA: 0s - loss: 0.1137 - accuracy: 0.9561
Epoch 00161: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 66us/sample - loss: 0.1127 - accuracy: 0.9565 - val_loss: 0.1813 - val_accuracy: 0.9218
Epoch 162/200
3552/4140 [========================&gt;.....] - ETA: 0s - loss: 0.1239 - accuracy: 0.9530
Epoch 00162: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 85us/sample - loss: 0.1358 - accuracy: 0.9483 - val_loss: 0.1807 - val_accuracy: 0.9537
Epoch 163/200
3680/4140 [=========================&gt;....] - ETA: 0s - loss: 0.2042 - accuracy: 0.9171
Epoch 00163: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 107us/sample - loss: 0.1998 - accuracy: 0.9181 - val_loss: 0.1901 - val_accuracy: 0.9276
Epoch 164/200
4000/4140 [===========================&gt;..] - ETA: 0s - loss: 0.1314 - accuracy: 0.9477
Epoch 00164: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 99us/sample - loss: 0.1383 - accuracy: 0.9464 - val_loss: 0.1117 - val_accuracy: 0.9585
Epoch 165/200
3712/4140 [=========================&gt;....] - ETA: 0s - loss: 0.1591 - accuracy: 0.9351
Epoch 00165: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 97us/sample - loss: 0.1590 - accuracy: 0.9338 - val_loss: 0.1250 - val_accuracy: 0.9469
Epoch 166/200
3648/4140 [=========================&gt;....] - ETA: 0s - loss: 0.1444 - accuracy: 0.9457
Epoch 00166: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 86us/sample - loss: 0.1461 - accuracy: 0.9435 - val_loss: 0.1885 - val_accuracy: 0.9170
Epoch 167/200
3520/4140 [========================&gt;.....] - ETA: 0s - loss: 0.0806 - accuracy: 0.9693
Epoch 00167: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 102us/sample - loss: 0.0810 - accuracy: 0.9686 - val_loss: 0.1395 - val_accuracy: 0.9469
Epoch 168/200
3744/4140 [==========================&gt;...] - ETA: 0s - loss: 0.0852 - accuracy: 0.9685
Epoch 00168: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 100us/sample - loss: 0.0856 - accuracy: 0.9679 - val_loss: 0.2249 - val_accuracy: 0.9131
Epoch 169/200
3392/4140 [=======================&gt;......] - ETA: 0s - loss: 0.0716 - accuracy: 0.9755
Epoch 00169: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 83us/sample - loss: 0.0713 - accuracy: 0.9754 - val_loss: 0.1491 - val_accuracy: 0.9527
Epoch 170/200
4128/4140 [============================&gt;.] - ETA: 0s - loss: 0.0879 - accuracy: 0.9654
Epoch 00170: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 76us/sample - loss: 0.0883 - accuracy: 0.9652 - val_loss: 0.4897 - val_accuracy: 0.8108
Epoch 171/200
3552/4140 [========================&gt;.....] - ETA: 0s - loss: 0.1385 - accuracy: 0.9493
Epoch 00171: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 68us/sample - loss: 0.1393 - accuracy: 0.9486 - val_loss: 0.0748 - val_accuracy: 0.9778
Epoch 172/200
4064/4140 [============================&gt;.] - ETA: 0s - loss: 0.0885 - accuracy: 0.9663
Epoch 00172: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 76us/sample - loss: 0.0907 - accuracy: 0.9657 - val_loss: 0.2342 - val_accuracy: 0.9064
Epoch 173/200
4000/4140 [===========================&gt;..] - ETA: 0s - loss: 0.1241 - accuracy: 0.9528
Epoch 00173: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 78us/sample - loss: 0.1253 - accuracy: 0.9524 - val_loss: 0.2081 - val_accuracy: 0.9247
Epoch 174/200
3776/4140 [==========================&gt;...] - ETA: 0s - loss: 0.1027 - accuracy: 0.9613
Epoch 00174: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 65us/sample - loss: 0.1015 - accuracy: 0.9611 - val_loss: 0.0999 - val_accuracy: 0.9672
Epoch 175/200
3808/4140 [==========================&gt;...] - ETA: 0s - loss: 0.0667 - accuracy: 0.9782
Epoch 00175: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 65us/sample - loss: 0.0678 - accuracy: 0.9768 - val_loss: 0.0996 - val_accuracy: 0.9595
Epoch 176/200
3552/4140 [========================&gt;.....] - ETA: 0s - loss: 0.0956 - accuracy: 0.9657
Epoch 00176: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 68us/sample - loss: 0.0927 - accuracy: 0.9671 - val_loss: 0.1428 - val_accuracy: 0.9527
Epoch 177/200
4064/4140 [============================&gt;.] - ETA: 0s - loss: 0.0695 - accuracy: 0.9766
Epoch 00177: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 75us/sample - loss: 0.0744 - accuracy: 0.9751 - val_loss: 0.2466 - val_accuracy: 0.9064
Epoch 178/200
3520/4140 [========================&gt;.....] - ETA: 0s - loss: 0.1675 - accuracy: 0.9366
Epoch 00178: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 67us/sample - loss: 0.1548 - accuracy: 0.9408 - val_loss: 0.2214 - val_accuracy: 0.9093
Epoch 179/200
3392/4140 [=======================&gt;......] - ETA: 0s - loss: 0.0900 - accuracy: 0.9658
Epoch 00179: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 70us/sample - loss: 0.0870 - accuracy: 0.9679 - val_loss: 0.1303 - val_accuracy: 0.9604
Epoch 180/200
3264/4140 [======================&gt;.......] - ETA: 0s - loss: 0.0455 - accuracy: 0.9874
Epoch 00180: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 68us/sample - loss: 0.0460 - accuracy: 0.9865 - val_loss: 0.0501 - val_accuracy: 0.9875
Epoch 181/200
4128/4140 [============================&gt;.] - ETA: 0s - loss: 0.1273 - accuracy: 0.9542
Epoch 00181: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 75us/sample - loss: 0.1270 - accuracy: 0.9543 - val_loss: 0.6461 - val_accuracy: 0.7770
Epoch 182/200
3904/4140 [===========================&gt;..] - ETA: 0s - loss: 0.2461 - accuracy: 0.9191
Epoch 00182: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 68us/sample - loss: 0.2428 - accuracy: 0.9196 - val_loss: 0.4058 - val_accuracy: 0.8292
Epoch 183/200
3840/4140 [==========================&gt;...] - ETA: 0s - loss: 0.1375 - accuracy: 0.9424
Epoch 00183: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 67us/sample - loss: 0.1362 - accuracy: 0.9435 - val_loss: 0.2473 - val_accuracy: 0.9054
Epoch 184/200
3776/4140 [==========================&gt;...] - ETA: 0s - loss: 0.1006 - accuracy: 0.9650
Epoch 00184: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 68us/sample - loss: 0.1003 - accuracy: 0.9647 - val_loss: 0.1110 - val_accuracy: 0.9701
Epoch 185/200
3456/4140 [========================&gt;.....] - ETA: 0s - loss: 0.0632 - accuracy: 0.9803
Epoch 00185: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 69us/sample - loss: 0.0635 - accuracy: 0.9797 - val_loss: 0.0702 - val_accuracy: 0.9797
Epoch 186/200
3456/4140 [========================&gt;.....] - ETA: 0s - loss: 0.0740 - accuracy: 0.9714
Epoch 00186: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 70us/sample - loss: 0.0718 - accuracy: 0.9729 - val_loss: 0.1360 - val_accuracy: 0.9537
Epoch 187/200
3968/4140 [===========================&gt;..] - ETA: 0s - loss: 0.0529 - accuracy: 0.9829
Epoch 00187: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 75us/sample - loss: 0.0520 - accuracy: 0.9831 - val_loss: 0.1323 - val_accuracy: 0.9546
Epoch 188/200
3520/4140 [========================&gt;.....] - ETA: 0s - loss: 0.0552 - accuracy: 0.9827
Epoch 00188: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 68us/sample - loss: 0.0550 - accuracy: 0.9824 - val_loss: 0.1794 - val_accuracy: 0.9228
Epoch 189/200
3488/4140 [========================&gt;.....] - ETA: 0s - loss: 0.1287 - accuracy: 0.9561
Epoch 00189: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 72us/sample - loss: 0.1314 - accuracy: 0.9558 - val_loss: 0.3231 - val_accuracy: 0.8813
Epoch 190/200
3648/4140 [=========================&gt;....] - ETA: 0s - loss: 0.1265 - accuracy: 0.9512
Epoch 00190: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 67us/sample - loss: 0.1207 - accuracy: 0.9519 - val_loss: 0.0896 - val_accuracy: 0.9797
Epoch 191/200
3360/4140 [=======================&gt;......] - ETA: 0s - loss: 0.1393 - accuracy: 0.9467
Epoch 00191: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 69us/sample - loss: 0.1382 - accuracy: 0.9473 - val_loss: 0.1472 - val_accuracy: 0.9382
Epoch 192/200
3584/4140 [========================&gt;.....] - ETA: 0s - loss: 0.1191 - accuracy: 0.9562
Epoch 00192: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 68us/sample - loss: 0.1136 - accuracy: 0.9585 - val_loss: 0.1057 - val_accuracy: 0.9691
Epoch 193/200
3552/4140 [========================&gt;.....] - ETA: 0s - loss: 0.0773 - accuracy: 0.9688
Epoch 00193: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 67us/sample - loss: 0.0787 - accuracy: 0.9691 - val_loss: 0.1199 - val_accuracy: 0.9479
Epoch 194/200
3840/4140 [==========================&gt;...] - ETA: 0s - loss: 0.0550 - accuracy: 0.9828
Epoch 00194: val_loss did not improve from 0.04715
4140/4140 [==============================] - 0s 65us/sample - loss: 0.0567 - accuracy: 0.9819 - val_loss: 0.0984 - val_accuracy: 0.9720
Epoch 195/200
3680/4140 [=========================&gt;....] - ETA: 0s - loss: 0.0427 - accuracy: 0.9859
Epoch 00195: val_loss improved from 0.04715 to 0.04632, saving model to best_model_2.h5
4140/4140 [==============================] - 0s 73us/sample - loss: 0.0421 - accuracy: 0.9865 - val_loss: 0.0463 - val_accuracy: 0.9894
Epoch 196/200
3456/4140 [========================&gt;.....] - ETA: 0s - loss: 0.0380 - accuracy: 0.9881
Epoch 00196: val_loss did not improve from 0.04632
4140/4140 [==============================] - 0s 69us/sample - loss: 0.0419 - accuracy: 0.9874 - val_loss: 0.1898 - val_accuracy: 0.9344
Epoch 197/200
4128/4140 [============================&gt;.] - ETA: 0s - loss: 0.0635 - accuracy: 0.9797
Epoch 00197: val_loss did not improve from 0.04632
4140/4140 [==============================] - 0s 76us/sample - loss: 0.0654 - accuracy: 0.9790 - val_loss: 0.3455 - val_accuracy: 0.8832
Epoch 198/200
3488/4140 [========================&gt;.....] - ETA: 0s - loss: 0.4251 - accuracy: 0.8802
Epoch 00198: val_loss did not improve from 0.04632
4140/4140 [==============================] - 0s 68us/sample - loss: 0.3812 - accuracy: 0.8915 - val_loss: 0.1639 - val_accuracy: 0.9373
Epoch 199/200
3552/4140 [========================&gt;.....] - ETA: 0s - loss: 0.1228 - accuracy: 0.9524
Epoch 00199: val_loss did not improve from 0.04632
4140/4140 [==============================] - 0s 71us/sample - loss: 0.1222 - accuracy: 0.9527 - val_loss: 0.2077 - val_accuracy: 0.9180
Epoch 200/200
3616/4140 [=========================&gt;....] - ETA: 0s - loss: 0.1194 - accuracy: 0.9588
Epoch 00200: val_loss did not improve from 0.04632
4140/4140 [==============================] - 0s 66us/sample - loss: 0.1131 - accuracy: 0.9601 - val_loss: 0.3666 - val_accuracy: 0.9170
Testing on Fold 4
# Getting train data set up
# Getting test data set up
WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;, &lt;class &#39;NoneType&#39;&gt;
Train on 2400 samples, validate on 600 samples
Epoch 1/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 184.1818 - mse: 184.1818
Epoch 00001: val_loss did not improve from 110.39956
2400/2400 [==============================] - 4s 2ms/sample - loss: 183.5705 - mse: 183.5704 - val_loss: 138.9953 - val_mse: 138.9953
Epoch 2/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 176.4492 - mse: 176.4492
Epoch 00002: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 819us/sample - loss: 173.8007 - mse: 173.8007 - val_loss: 154.3021 - val_mse: 154.3021
Epoch 3/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 166.6715 - mse: 166.6716
Epoch 00003: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 919us/sample - loss: 165.5025 - mse: 165.5025 - val_loss: 150.2068 - val_mse: 150.2068
Epoch 4/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 176.9716 - mse: 176.9716
Epoch 00004: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 852us/sample - loss: 176.7522 - mse: 176.7523 - val_loss: 140.6176 - val_mse: 140.6176
Epoch 5/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 169.5191 - mse: 169.5191
Epoch 00005: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 831us/sample - loss: 167.8317 - mse: 167.8317 - val_loss: 161.3771 - val_mse: 161.3771
Epoch 6/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 165.2837 - mse: 165.2836
Epoch 00006: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 869us/sample - loss: 162.5147 - mse: 162.5147 - val_loss: 149.8513 - val_mse: 149.8513
Epoch 7/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 158.2568 - mse: 158.2568
Epoch 00007: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 856us/sample - loss: 159.4063 - mse: 159.4063 - val_loss: 131.2538 - val_mse: 131.2538
Epoch 8/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 153.5046 - mse: 153.5045
Epoch 00008: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 823us/sample - loss: 160.1557 - mse: 160.1556 - val_loss: 138.4331 - val_mse: 138.4331
Epoch 9/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 162.2688 - mse: 162.2687
Epoch 00009: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 830us/sample - loss: 159.4637 - mse: 159.4636 - val_loss: 139.8174 - val_mse: 139.8174
Epoch 10/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.3239 - mse: 163.3239
Epoch 00010: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 841us/sample - loss: 162.0970 - mse: 162.0970 - val_loss: 144.6563 - val_mse: 144.6563
Epoch 11/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 155.1871 - mse: 155.1871
Epoch 00011: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 958us/sample - loss: 154.9588 - mse: 154.9588 - val_loss: 138.6792 - val_mse: 138.6792
Epoch 12/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 155.1437 - mse: 155.1438
Epoch 00012: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 1ms/sample - loss: 155.0019 - mse: 155.0019 - val_loss: 136.7385 - val_mse: 136.7385
Epoch 13/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 152.2766 - mse: 152.2766
Epoch 00013: val_loss did not improve from 110.39956
2400/2400 [==============================] - 3s 1ms/sample - loss: 153.9410 - mse: 153.9409 - val_loss: 129.3125 - val_mse: 129.3125
Epoch 14/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 154.4659 - mse: 154.4659
Epoch 00014: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 1ms/sample - loss: 155.9032 - mse: 155.9032 - val_loss: 130.7992 - val_mse: 130.7992
Epoch 15/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 159.3124 - mse: 159.3124
Epoch 00015: val_loss did not improve from 110.39956
2400/2400 [==============================] - 3s 1ms/sample - loss: 159.2853 - mse: 159.2853 - val_loss: 136.6968 - val_mse: 136.6968
Epoch 16/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 152.2254 - mse: 152.2254
Epoch 00016: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 901us/sample - loss: 151.1680 - mse: 151.1681 - val_loss: 134.3406 - val_mse: 134.3406
Epoch 17/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 157.6867 - mse: 157.6867
Epoch 00017: val_loss did not improve from 110.39956
2400/2400 [==============================] - 3s 1ms/sample - loss: 157.2182 - mse: 157.2182 - val_loss: 145.1769 - val_mse: 145.1769
Epoch 18/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 157.9702 - mse: 157.9702
Epoch 00018: val_loss did not improve from 110.39956
2400/2400 [==============================] - 3s 1ms/sample - loss: 157.0063 - mse: 157.0063 - val_loss: 139.9404 - val_mse: 139.9404
Epoch 19/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 142.9835 - mse: 142.9835
Epoch 00019: val_loss did not improve from 110.39956
2400/2400 [==============================] - 3s 1ms/sample - loss: 141.9423 - mse: 141.9422 - val_loss: 139.2667 - val_mse: 139.2667
Epoch 20/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 152.0597 - mse: 152.0597
Epoch 00020: val_loss did not improve from 110.39956
2400/2400 [==============================] - 3s 1ms/sample - loss: 150.6516 - mse: 150.6516 - val_loss: 146.1315 - val_mse: 146.1315
Epoch 21/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 152.9016 - mse: 152.9016
Epoch 00021: val_loss did not improve from 110.39956
2400/2400 [==============================] - 3s 1ms/sample - loss: 151.0313 - mse: 151.0313 - val_loss: 130.0465 - val_mse: 130.0466
Epoch 22/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 145.7136 - mse: 145.7136
Epoch 00022: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 934us/sample - loss: 148.8202 - mse: 148.8202 - val_loss: 150.7541 - val_mse: 150.7541
Epoch 23/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 147.8416 - mse: 147.8416
Epoch 00023: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 898us/sample - loss: 151.2912 - mse: 151.2912 - val_loss: 133.8858 - val_mse: 133.8858
Epoch 24/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 150.1170 - mse: 150.1171
Epoch 00024: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 864us/sample - loss: 148.2634 - mse: 148.2634 - val_loss: 129.5557 - val_mse: 129.5557
Epoch 25/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 152.3665 - mse: 152.3665
Epoch 00025: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 843us/sample - loss: 151.8268 - mse: 151.8268 - val_loss: 146.3968 - val_mse: 146.3968
Epoch 26/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 150.6451 - mse: 150.6451
Epoch 00026: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 850us/sample - loss: 151.6954 - mse: 151.6954 - val_loss: 153.6624 - val_mse: 153.6624
Epoch 27/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 151.8450 - mse: 151.8450
Epoch 00027: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 872us/sample - loss: 149.0981 - mse: 149.0981 - val_loss: 139.5233 - val_mse: 139.5233
Epoch 28/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 153.6217 - mse: 153.6217
Epoch 00028: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 825us/sample - loss: 152.6634 - mse: 152.6634 - val_loss: 190.2684 - val_mse: 190.2684
Epoch 29/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 150.9928 - mse: 150.9929
Epoch 00029: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 816us/sample - loss: 148.8955 - mse: 148.8956 - val_loss: 136.4609 - val_mse: 136.4609
Epoch 30/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 155.7276 - mse: 155.7276
Epoch 00030: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 823us/sample - loss: 154.2517 - mse: 154.2517 - val_loss: 140.6818 - val_mse: 140.6818
Epoch 31/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 145.8787 - mse: 145.8786
Epoch 00031: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 906us/sample - loss: 147.3764 - mse: 147.3764 - val_loss: 146.5970 - val_mse: 146.5970
Epoch 32/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 147.0556 - mse: 147.0557
Epoch 00032: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 887us/sample - loss: 149.7104 - mse: 149.7104 - val_loss: 155.7263 - val_mse: 155.7263
Epoch 33/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 140.3670 - mse: 140.3670
Epoch 00033: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 843us/sample - loss: 149.2022 - mse: 149.2022 - val_loss: 142.2698 - val_mse: 142.2698
Epoch 34/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 145.7461 - mse: 145.7461
Epoch 00034: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 838us/sample - loss: 146.4180 - mse: 146.4180 - val_loss: 151.2092 - val_mse: 151.2092
Epoch 35/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 146.2364 - mse: 146.2364
Epoch 00035: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 815us/sample - loss: 145.4567 - mse: 145.4567 - val_loss: 137.2857 - val_mse: 137.2858
Epoch 36/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 146.2935 - mse: 146.2935
Epoch 00036: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 833us/sample - loss: 144.5490 - mse: 144.5490 - val_loss: 135.8029 - val_mse: 135.8029
Epoch 37/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 146.7544 - mse: 146.7544
Epoch 00037: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 807us/sample - loss: 145.1809 - mse: 145.1809 - val_loss: 133.3424 - val_mse: 133.3424
Epoch 38/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 145.3909 - mse: 145.3909
Epoch 00038: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 812us/sample - loss: 145.1796 - mse: 145.1796 - val_loss: 143.5222 - val_mse: 143.5222
Epoch 39/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 159.0619 - mse: 159.0619
Epoch 00039: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 917us/sample - loss: 156.6705 - mse: 156.6705 - val_loss: 151.8674 - val_mse: 151.8674
Epoch 40/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 164.4521 - mse: 164.4521
Epoch 00040: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 836us/sample - loss: 165.2109 - mse: 165.2109 - val_loss: 152.9455 - val_mse: 152.9455
Epoch 41/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 165.9883 - mse: 165.9883
Epoch 00041: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 813us/sample - loss: 163.7272 - mse: 163.7272 - val_loss: 155.1735 - val_mse: 155.1735
Epoch 42/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.8422 - mse: 162.8423
Epoch 00042: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 817us/sample - loss: 163.4506 - mse: 163.4506 - val_loss: 151.3947 - val_mse: 151.3947
Epoch 43/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 165.2867 - mse: 165.2867
Epoch 00043: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 796us/sample - loss: 163.3056 - mse: 163.3056 - val_loss: 155.5060 - val_mse: 155.5060
Epoch 44/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 165.0995 - mse: 165.0995
Epoch 00044: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 800us/sample - loss: 164.0090 - mse: 164.0090 - val_loss: 153.8619 - val_mse: 153.8619
Epoch 45/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 161.0854 - mse: 161.0854
Epoch 00045: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 800us/sample - loss: 161.4630 - mse: 161.4630 - val_loss: 150.6119 - val_mse: 150.6119
Epoch 46/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 159.0149 - mse: 159.0149
Epoch 00046: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 806us/sample - loss: 161.6402 - mse: 161.6402 - val_loss: 151.5412 - val_mse: 151.5412
Epoch 47/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 161.0336 - mse: 161.0337
Epoch 00047: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 891us/sample - loss: 161.0221 - mse: 161.0221 - val_loss: 147.2764 - val_mse: 147.2764
Epoch 48/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 157.2069 - mse: 157.2069
Epoch 00048: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 850us/sample - loss: 161.0457 - mse: 161.0457 - val_loss: 152.1745 - val_mse: 152.1745
Epoch 49/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 162.6431 - mse: 162.6430
Epoch 00049: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 828us/sample - loss: 160.7451 - mse: 160.7451 - val_loss: 150.5446 - val_mse: 150.5446
Epoch 50/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 161.4620 - mse: 161.4620- ETA: 1s - loss: 168
Epoch 00050: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 832us/sample - loss: 159.5657 - mse: 159.5657 - val_loss: 148.5323 - val_mse: 148.5323
Epoch 51/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 155.5532 - mse: 155.5532
Epoch 00051: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 1ms/sample - loss: 155.4125 - mse: 155.4125 - val_loss: 141.9652 - val_mse: 141.9651
Epoch 52/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 156.7861 - mse: 156.7861
Epoch 00052: val_loss did not improve from 110.39956
2400/2400 [==============================] - 3s 1ms/sample - loss: 156.0023 - mse: 156.0023 - val_loss: 147.9737 - val_mse: 147.9737
Epoch 53/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 156.9909 - mse: 156.9909
Epoch 00053: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 988us/sample - loss: 155.7655 - mse: 155.7655 - val_loss: 147.9940 - val_mse: 147.9940
Epoch 54/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 155.1361 - mse: 155.1360
Epoch 00054: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 1ms/sample - loss: 153.6235 - mse: 153.6235 - val_loss: 143.4306 - val_mse: 143.4306
Epoch 55/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 158.3133 - mse: 158.3133
Epoch 00055: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 887us/sample - loss: 158.9640 - mse: 158.9639 - val_loss: 167.5441 - val_mse: 167.5441
Epoch 56/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 155.4488 - mse: 155.4489
Epoch 00056: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 807us/sample - loss: 157.3005 - mse: 157.3005 - val_loss: 138.0626 - val_mse: 138.0626
Epoch 57/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 151.2003 - mse: 151.2003
Epoch 00057: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 811us/sample - loss: 152.1369 - mse: 152.1369 - val_loss: 139.7301 - val_mse: 139.7301
Epoch 58/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 152.1458 - mse: 152.1459
Epoch 00058: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 796us/sample - loss: 150.9004 - mse: 150.9004 - val_loss: 140.0304 - val_mse: 140.0304
Epoch 59/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 155.0736 - mse: 155.0736
Epoch 00059: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 789us/sample - loss: 153.8528 - mse: 153.8528 - val_loss: 137.2815 - val_mse: 137.2814
Epoch 60/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 151.5273 - mse: 151.5274
Epoch 00060: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 794us/sample - loss: 150.7768 - mse: 150.7768 - val_loss: 136.0360 - val_mse: 136.0360
Epoch 61/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.0547 - mse: 162.0547
Epoch 00061: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 807us/sample - loss: 159.8644 - mse: 159.8644 - val_loss: 146.2613 - val_mse: 146.2613
Epoch 62/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 154.8376 - mse: 154.8376
Epoch 00062: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 880us/sample - loss: 151.5554 - mse: 151.5553 - val_loss: 148.5378 - val_mse: 148.5378
Epoch 63/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 149.3166 - mse: 149.3166
Epoch 00063: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 861us/sample - loss: 148.8562 - mse: 148.8562 - val_loss: 146.9209 - val_mse: 146.9209
Epoch 64/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 151.3440 - mse: 151.3440
Epoch 00064: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 817us/sample - loss: 150.7827 - mse: 150.7827 - val_loss: 143.3267 - val_mse: 143.3267
Epoch 65/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 150.7194 - mse: 150.7193
Epoch 00065: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 813us/sample - loss: 149.6347 - mse: 149.6347 - val_loss: 152.4929 - val_mse: 152.4929
Epoch 66/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 154.4063 - mse: 154.4062
Epoch 00066: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 801us/sample - loss: 154.0662 - mse: 154.0662 - val_loss: 149.9535 - val_mse: 149.9535
Epoch 67/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 154.0452 - mse: 154.0453
Epoch 00067: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 791us/sample - loss: 154.8002 - mse: 154.8002 - val_loss: 161.7441 - val_mse: 161.7441
Epoch 68/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 161.5577 - mse: 161.5577
Epoch 00068: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 796us/sample - loss: 164.0728 - mse: 164.0728 - val_loss: 153.8867 - val_mse: 153.8867
Epoch 69/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 165.1434 - mse: 165.1434
Epoch 00069: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 790us/sample - loss: 164.2307 - mse: 164.2307 - val_loss: 153.1964 - val_mse: 153.1964
Epoch 70/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 164.6930 - mse: 164.6931
Epoch 00070: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 844us/sample - loss: 164.5139 - mse: 164.5139 - val_loss: 152.9049 - val_mse: 152.9049
Epoch 71/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 164.1262 - mse: 164.1263
Epoch 00071: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 860us/sample - loss: 162.8533 - mse: 162.8533 - val_loss: 152.0306 - val_mse: 152.0306
Epoch 72/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.4059 - mse: 162.4059
Epoch 00072: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 816us/sample - loss: 162.4293 - mse: 162.4293 - val_loss: 149.4151 - val_mse: 149.4151
Epoch 73/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 155.2533 - mse: 155.2533
Epoch 00073: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 813us/sample - loss: 161.8948 - mse: 161.8948 - val_loss: 153.0605 - val_mse: 153.0604
Epoch 74/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.8267 - mse: 162.8266
Epoch 00074: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 797us/sample - loss: 161.5550 - mse: 161.5549 - val_loss: 153.3289 - val_mse: 153.3289
Epoch 75/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.9265 - mse: 162.9265
Epoch 00075: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 811us/sample - loss: 162.3175 - mse: 162.3174 - val_loss: 153.9286 - val_mse: 153.9285
Epoch 76/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.9213 - mse: 162.9214
Epoch 00076: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 802us/sample - loss: 162.5397 - mse: 162.5397 - val_loss: 153.4896 - val_mse: 153.4896
Epoch 77/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.6674 - mse: 163.6674
Epoch 00077: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 810us/sample - loss: 162.8961 - mse: 162.8961 - val_loss: 153.2915 - val_mse: 153.2915
Epoch 78/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 164.9338 - mse: 164.9338
Epoch 00078: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 853us/sample - loss: 163.3457 - mse: 163.3457 - val_loss: 153.4901 - val_mse: 153.4901
Epoch 79/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 163.3858 - mse: 163.3858
Epoch 00079: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 861us/sample - loss: 162.9291 - mse: 162.9291 - val_loss: 153.7869 - val_mse: 153.7869
Epoch 80/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 163.8186 - mse: 163.8186
Epoch 00080: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 826us/sample - loss: 162.0190 - mse: 162.0190 - val_loss: 153.2878 - val_mse: 153.2878
Epoch 81/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 161.3367 - mse: 161.3367
Epoch 00081: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 802us/sample - loss: 162.8657 - mse: 162.8656 - val_loss: 153.2828 - val_mse: 153.2828
Epoch 82/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 159.2956 - mse: 159.2955
Epoch 00082: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 815us/sample - loss: 161.7462 - mse: 161.7462 - val_loss: 153.5342 - val_mse: 153.5342
Epoch 83/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.1909 - mse: 163.1909
Epoch 00083: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 789us/sample - loss: 161.9953 - mse: 161.9952 - val_loss: 153.4175 - val_mse: 153.4175
Epoch 84/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.3990 - mse: 163.3991
Epoch 00084: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 804us/sample - loss: 163.1484 - mse: 163.1484 - val_loss: 154.1402 - val_mse: 154.1402
Epoch 85/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.4790 - mse: 162.4790
Epoch 00085: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 782us/sample - loss: 162.2006 - mse: 162.2006 - val_loss: 153.9031 - val_mse: 153.9031
Epoch 86/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.3582 - mse: 163.3582
Epoch 00086: val_loss did not improve from 110.39956
2400/2400 [==============================] - 3s 1ms/sample - loss: 161.9970 - mse: 161.9970 - val_loss: 153.2818 - val_mse: 153.2818
Epoch 87/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.2678 - mse: 163.2678
Epoch 00087: val_loss did not improve from 110.39956
2400/2400 [==============================] - 3s 1ms/sample - loss: 163.1902 - mse: 163.1902 - val_loss: 153.8506 - val_mse: 153.8506
Epoch 88/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.7413 - mse: 163.7413
Epoch 00088: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 870us/sample - loss: 163.8518 - mse: 163.8517 - val_loss: 153.3118 - val_mse: 153.3118
Epoch 89/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.0510 - mse: 163.0510
Epoch 00089: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 812us/sample - loss: 162.9817 - mse: 162.9818 - val_loss: 153.5337 - val_mse: 153.5337
Epoch 90/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.4687 - mse: 162.4688
Epoch 00090: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 799us/sample - loss: 161.6784 - mse: 161.6785 - val_loss: 153.4721 - val_mse: 153.4721
Epoch 91/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 164.6604 - mse: 164.6604
Epoch 00091: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 809us/sample - loss: 163.6362 - mse: 163.6362 - val_loss: 153.9258 - val_mse: 153.9258
Epoch 92/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 164.2426 - mse: 164.2426
Epoch 00092: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 799us/sample - loss: 162.7581 - mse: 162.7580 - val_loss: 153.3036 - val_mse: 153.3036
Epoch 93/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 160.2352 - mse: 160.2353
Epoch 00093: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 813us/sample - loss: 162.7782 - mse: 162.7782 - val_loss: 153.2959 - val_mse: 153.2959
Epoch 94/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.5593 - mse: 163.5593
Epoch 00094: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 885us/sample - loss: 162.9871 - mse: 162.9872 - val_loss: 153.5823 - val_mse: 153.5823
Epoch 95/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 163.5505 - mse: 163.5506
Epoch 00095: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 834us/sample - loss: 162.9659 - mse: 162.9659 - val_loss: 153.7552 - val_mse: 153.7552
Epoch 96/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.5483 - mse: 162.5483
Epoch 00096: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 812us/sample - loss: 162.4544 - mse: 162.4544 - val_loss: 153.5431 - val_mse: 153.5431
Epoch 97/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 164.4220 - mse: 164.4220
Epoch 00097: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 796us/sample - loss: 163.2716 - mse: 163.2716 - val_loss: 153.5554 - val_mse: 153.5554
Epoch 98/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 157.4274 - mse: 157.4274
Epoch 00098: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 796us/sample - loss: 162.7263 - mse: 162.7263 - val_loss: 153.6191 - val_mse: 153.6191
Epoch 99/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 162.9681 - mse: 162.9681
Epoch 00099: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 786us/sample - loss: 162.2528 - mse: 162.2528 - val_loss: 153.2886 - val_mse: 153.2886
Epoch 100/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 166.0086 - mse: 166.0086
Epoch 00100: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 788us/sample - loss: 163.6191 - mse: 163.6191 - val_loss: 153.9248 - val_mse: 153.9248
Epoch 101/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 157.7518 - mse: 157.7518
Epoch 00101: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 808us/sample - loss: 161.1754 - mse: 161.1754 - val_loss: 154.2161 - val_mse: 154.2161
Epoch 102/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 159.8266 - mse: 159.8266
Epoch 00102: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 850us/sample - loss: 161.1224 - mse: 161.1224 - val_loss: 153.4004 - val_mse: 153.4004
Epoch 103/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 161.0574 - mse: 161.0575
Epoch 00103: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 837us/sample - loss: 162.7376 - mse: 162.7377 - val_loss: 154.6819 - val_mse: 154.6819
Epoch 104/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.4351 - mse: 162.4350
Epoch 00104: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 824us/sample - loss: 163.3593 - mse: 163.3593 - val_loss: 153.6022 - val_mse: 153.6022
Epoch 105/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.8833 - mse: 162.8832
Epoch 00105: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 827us/sample - loss: 161.5646 - mse: 161.5646 - val_loss: 153.3116 - val_mse: 153.3116
Epoch 106/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.7337 - mse: 162.7337
Epoch 00106: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 809us/sample - loss: 162.1359 - mse: 162.1359 - val_loss: 153.3103 - val_mse: 153.3103
Epoch 107/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 164.2233 - mse: 164.2234
Epoch 00107: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 793us/sample - loss: 162.0848 - mse: 162.0849 - val_loss: 153.8220 - val_mse: 153.8220
Epoch 108/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 164.1336 - mse: 164.1336
Epoch 00108: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 799us/sample - loss: 163.3096 - mse: 163.3096 - val_loss: 153.5420 - val_mse: 153.5420
Epoch 109/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.5211 - mse: 163.5211
Epoch 00109: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 800us/sample - loss: 162.4757 - mse: 162.4757 - val_loss: 153.4115 - val_mse: 153.4115
Epoch 110/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 161.9327 - mse: 161.9327
Epoch 00110: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 868us/sample - loss: 161.7800 - mse: 161.7800 - val_loss: 153.5372 - val_mse: 153.5372
Epoch 111/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 164.5164 - mse: 164.5164
Epoch 00111: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 851us/sample - loss: 162.4538 - mse: 162.4539 - val_loss: 153.4181 - val_mse: 153.4181
Epoch 112/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 164.1142 - mse: 164.1142
Epoch 00112: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 800us/sample - loss: 163.3934 - mse: 163.3934 - val_loss: 153.4896 - val_mse: 153.4896
Epoch 113/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 155.1724 - mse: 155.1723
Epoch 00113: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 799us/sample - loss: 162.2506 - mse: 162.2506 - val_loss: 153.5039 - val_mse: 153.5039
Epoch 114/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.5607 - mse: 162.5607
Epoch 00114: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 805us/sample - loss: 162.6582 - mse: 162.6581 - val_loss: 153.4300 - val_mse: 153.4300
Epoch 115/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.6531 - mse: 162.6531
Epoch 00115: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 782us/sample - loss: 161.0759 - mse: 161.0759 - val_loss: 153.4949 - val_mse: 153.4950
Epoch 116/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 160.8485 - mse: 160.8485
Epoch 00116: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 790us/sample - loss: 162.9181 - mse: 162.9181 - val_loss: 153.2865 - val_mse: 153.2865
Epoch 117/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 158.7169 - mse: 158.7169
Epoch 00117: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 783us/sample - loss: 162.0975 - mse: 162.0975 - val_loss: 153.3179 - val_mse: 153.3179
Epoch 118/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 161.9024 - mse: 161.9024
Epoch 00118: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 834us/sample - loss: 162.4962 - mse: 162.4961 - val_loss: 153.3260 - val_mse: 153.3260
Epoch 119/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 164.1092 - mse: 164.1092
Epoch 00119: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 863us/sample - loss: 161.6711 - mse: 161.6711 - val_loss: 154.9690 - val_mse: 154.9690
Epoch 120/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.8298 - mse: 162.8298
Epoch 00120: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 810us/sample - loss: 162.6952 - mse: 162.6951 - val_loss: 153.3133 - val_mse: 153.3133
Epoch 121/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.2168 - mse: 163.2169
Epoch 00121: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 810us/sample - loss: 161.0586 - mse: 161.0586 - val_loss: 153.3966 - val_mse: 153.3966
Epoch 122/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.2770 - mse: 163.2770
Epoch 00122: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 810us/sample - loss: 162.1251 - mse: 162.1251 - val_loss: 153.2975 - val_mse: 153.2975
Epoch 123/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 164.3644 - mse: 164.3644
Epoch 00123: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 787us/sample - loss: 162.8085 - mse: 162.8085 - val_loss: 153.3327 - val_mse: 153.3327
Epoch 124/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 165.0566 - mse: 165.0566
Epoch 00124: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 791us/sample - loss: 162.7567 - mse: 162.7567 - val_loss: 153.2822 - val_mse: 153.2822
Epoch 125/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.4056 - mse: 163.4056
Epoch 00125: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 792us/sample - loss: 162.0907 - mse: 162.0907 - val_loss: 153.2913 - val_mse: 153.2913
Epoch 126/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.0566 - mse: 163.0566
Epoch 00126: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 805us/sample - loss: 162.2129 - mse: 162.2129 - val_loss: 153.3050 - val_mse: 153.3049
Epoch 127/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 156.6396 - mse: 156.6396
Epoch 00127: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 893us/sample - loss: 164.3983 - mse: 164.3984 - val_loss: 153.3795 - val_mse: 153.3795
Epoch 128/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 160.5272 - mse: 160.5273
Epoch 00128: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 817us/sample - loss: 161.5173 - mse: 161.5173 - val_loss: 153.3197 - val_mse: 153.3197
Epoch 129/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.8389 - mse: 163.8389
Epoch 00129: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 808us/sample - loss: 162.3269 - mse: 162.3269 - val_loss: 153.6225 - val_mse: 153.6225
Epoch 130/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.1801 - mse: 163.1801
Epoch 00130: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 796us/sample - loss: 162.5188 - mse: 162.5188 - val_loss: 153.3665 - val_mse: 153.3665
Epoch 131/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 165.5365 - mse: 165.5364
Epoch 00131: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 798us/sample - loss: 163.3710 - mse: 163.3710 - val_loss: 153.9880 - val_mse: 153.9880
Epoch 132/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.9780 - mse: 163.9780
Epoch 00132: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 798us/sample - loss: 161.8182 - mse: 161.8182 - val_loss: 153.2938 - val_mse: 153.2938
Epoch 133/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.4939 - mse: 162.4940
Epoch 00133: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 806us/sample - loss: 161.7508 - mse: 161.7508 - val_loss: 153.3849 - val_mse: 153.3849
Epoch 134/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 161.0851 - mse: 161.0852
Epoch 00134: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 806us/sample - loss: 162.0098 - mse: 162.0098 - val_loss: 153.4374 - val_mse: 153.4374
Epoch 135/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.2441 - mse: 163.2441
Epoch 00135: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 888us/sample - loss: 162.3329 - mse: 162.3329 - val_loss: 153.7325 - val_mse: 153.7325
Epoch 136/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.3921 - mse: 163.3921
Epoch 00136: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 847us/sample - loss: 162.3008 - mse: 162.3008 - val_loss: 153.2814 - val_mse: 153.2814
Epoch 137/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.2474 - mse: 162.2474
Epoch 00137: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 1ms/sample - loss: 161.9298 - mse: 161.9297 - val_loss: 153.3450 - val_mse: 153.3450
Epoch 138/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 164.3640 - mse: 164.3639
Epoch 00138: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 1ms/sample - loss: 161.3827 - mse: 161.3827 - val_loss: 153.2807 - val_mse: 153.2806
Epoch 139/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.1617 - mse: 162.1618
Epoch 00139: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 932us/sample - loss: 162.0683 - mse: 162.0684 - val_loss: 153.5880 - val_mse: 153.5880
Epoch 140/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.4976 - mse: 163.4975
Epoch 00140: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 994us/sample - loss: 163.2442 - mse: 163.2442 - val_loss: 153.3783 - val_mse: 153.3783
Epoch 141/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.6768 - mse: 162.6767
Epoch 00141: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 999us/sample - loss: 161.7601 - mse: 161.7601 - val_loss: 154.0077 - val_mse: 154.0076
Epoch 142/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.5235 - mse: 162.5235
Epoch 00142: val_loss did not improve from 110.39956
2400/2400 [==============================] - 3s 1ms/sample - loss: 162.1348 - mse: 162.1349 - val_loss: 153.3982 - val_mse: 153.3982
Epoch 143/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 165.3134 - mse: 165.3134
Epoch 00143: val_loss did not improve from 110.39956
2400/2400 [==============================] - 3s 1ms/sample - loss: 162.6146 - mse: 162.6146 - val_loss: 153.3669 - val_mse: 153.3669
Epoch 144/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.0039 - mse: 162.0039- ETA: 0s - loss: 158.9518 - ms
Epoch 00144: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 1ms/sample - loss: 162.5259 - mse: 162.5259 - val_loss: 153.7353 - val_mse: 153.7353
Epoch 145/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 161.4603 - mse: 161.4604
Epoch 00145: val_loss did not improve from 110.39956
2400/2400 [==============================] - 3s 1ms/sample - loss: 161.4296 - mse: 161.4296 - val_loss: 153.3002 - val_mse: 153.3002
Epoch 146/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.2252 - mse: 162.2253
Epoch 00146: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 1ms/sample - loss: 160.9157 - mse: 160.9158 - val_loss: 153.4458 - val_mse: 153.4458
Epoch 147/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 165.1097 - mse: 165.1097
Epoch 00147: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 1ms/sample - loss: 162.7683 - mse: 162.7682 - val_loss: 153.2972 - val_mse: 153.2972
Epoch 148/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 161.5550 - mse: 161.5550
Epoch 00148: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 886us/sample - loss: 161.4946 - mse: 161.4946 - val_loss: 153.3244 - val_mse: 153.3244
Epoch 149/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.6814 - mse: 163.6814
Epoch 00149: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 860us/sample - loss: 162.5288 - mse: 162.5287 - val_loss: 153.2981 - val_mse: 153.2981
Epoch 150/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.1348 - mse: 163.1348
Epoch 00150: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 837us/sample - loss: 162.5364 - mse: 162.5364 - val_loss: 153.4234 - val_mse: 153.4234
Epoch 151/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.7151 - mse: 163.7151
Epoch 00151: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 944us/sample - loss: 162.8561 - mse: 162.8561 - val_loss: 153.3412 - val_mse: 153.3412
Epoch 152/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 160.8269 - mse: 160.8269
Epoch 00152: val_loss did not improve from 110.39956
2400/2400 [==============================] - 3s 1ms/sample - loss: 162.3588 - mse: 162.3588 - val_loss: 153.2998 - val_mse: 153.2998
Epoch 153/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 165.5284 - mse: 165.5283
Epoch 00153: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 885us/sample - loss: 162.9221 - mse: 162.9220 - val_loss: 153.3055 - val_mse: 153.3055
Epoch 154/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 156.8732 - mse: 156.8733
Epoch 00154: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 836us/sample - loss: 162.1781 - mse: 162.1782 - val_loss: 153.5578 - val_mse: 153.5578
Epoch 155/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 153.6914 - mse: 153.6914
Epoch 00155: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 864us/sample - loss: 161.4638 - mse: 161.4639 - val_loss: 153.3705 - val_mse: 153.3705
Epoch 156/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 164.2358 - mse: 164.2358
Epoch 00156: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 904us/sample - loss: 162.5916 - mse: 162.5916 - val_loss: 153.6433 - val_mse: 153.6433
Epoch 157/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 164.3177 - mse: 164.3177
Epoch 00157: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 832us/sample - loss: 161.9478 - mse: 161.9478 - val_loss: 153.3869 - val_mse: 153.3869
Epoch 158/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 156.3385 - mse: 156.3385
Epoch 00158: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 834us/sample - loss: 161.5443 - mse: 161.5443 - val_loss: 153.3520 - val_mse: 153.3520
Epoch 159/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 160.1449 - mse: 160.1449
Epoch 00159: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 872us/sample - loss: 161.9187 - mse: 161.9187 - val_loss: 153.3749 - val_mse: 153.3749
Epoch 160/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 161.5797 - mse: 161.5797
Epoch 00160: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 804us/sample - loss: 160.9030 - mse: 160.9030 - val_loss: 153.4091 - val_mse: 153.4091
Epoch 161/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.6236 - mse: 163.6236
Epoch 00161: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 868us/sample - loss: 162.3250 - mse: 162.3251 - val_loss: 153.3328 - val_mse: 153.3327
Epoch 162/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.0255 - mse: 162.0254
Epoch 00162: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 817us/sample - loss: 161.5111 - mse: 161.5111 - val_loss: 153.4512 - val_mse: 153.4512
Epoch 163/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 161.7014 - mse: 161.7015
Epoch 00163: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 976us/sample - loss: 162.9198 - mse: 162.9198 - val_loss: 154.0029 - val_mse: 154.0029
Epoch 164/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 155.8573 - mse: 155.8572
Epoch 00164: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 918us/sample - loss: 162.2359 - mse: 162.2359 - val_loss: 153.3199 - val_mse: 153.3199
Epoch 165/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.7637 - mse: 163.7637
Epoch 00165: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 855us/sample - loss: 161.0118 - mse: 161.0118 - val_loss: 153.3467 - val_mse: 153.3466
Epoch 166/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 157.2281 - mse: 157.2280
Epoch 00166: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 809us/sample - loss: 162.4901 - mse: 162.4901 - val_loss: 153.3053 - val_mse: 153.3053
Epoch 167/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 167.9495 - mse: 167.9495
Epoch 00167: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 799us/sample - loss: 163.7514 - mse: 163.7514 - val_loss: 153.3029 - val_mse: 153.3029
Epoch 168/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 160.6592 - mse: 160.6593
Epoch 00168: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 784us/sample - loss: 162.4564 - mse: 162.4564 - val_loss: 153.2823 - val_mse: 153.2823
Epoch 169/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 162.3200 - mse: 162.3200
Epoch 00169: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 783us/sample - loss: 162.2800 - mse: 162.2800 - val_loss: 153.7149 - val_mse: 153.7149
Epoch 170/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.7306 - mse: 162.7306
Epoch 00170: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 779us/sample - loss: 161.6945 - mse: 161.6945 - val_loss: 153.3369 - val_mse: 153.3369
Epoch 171/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.4183 - mse: 162.4184
Epoch 00171: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 834us/sample - loss: 162.3852 - mse: 162.3852 - val_loss: 153.3320 - val_mse: 153.3320
Epoch 172/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.8467 - mse: 163.8467
Epoch 00172: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 858us/sample - loss: 161.4532 - mse: 161.4532 - val_loss: 153.3239 - val_mse: 153.3239
Epoch 173/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 166.3419 - mse: 166.3419
Epoch 00173: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 825us/sample - loss: 163.7618 - mse: 163.7618 - val_loss: 153.9242 - val_mse: 153.9242
Epoch 174/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.4616 - mse: 162.4616
Epoch 00174: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 813us/sample - loss: 161.8179 - mse: 161.8179 - val_loss: 153.6385 - val_mse: 153.6385
Epoch 175/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.4825 - mse: 163.4826
Epoch 00175: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 798us/sample - loss: 160.9094 - mse: 160.9095 - val_loss: 154.1828 - val_mse: 154.1828
Epoch 176/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 165.9631 - mse: 165.9630
Epoch 00176: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 794us/sample - loss: 163.7099 - mse: 163.7099 - val_loss: 153.4928 - val_mse: 153.4928
Epoch 177/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 160.7211 - mse: 160.7211
Epoch 00177: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 811us/sample - loss: 161.7540 - mse: 161.7540 - val_loss: 153.2806 - val_mse: 153.2806
Epoch 178/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 161.5312 - mse: 161.5312
Epoch 00178: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 787us/sample - loss: 161.6905 - mse: 161.6905 - val_loss: 153.3266 - val_mse: 153.3266
Epoch 179/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 161.5878 - mse: 161.5878
Epoch 00179: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 827us/sample - loss: 161.3889 - mse: 161.3889 - val_loss: 153.2805 - val_mse: 153.2805
Epoch 180/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 156.3069 - mse: 156.3070
Epoch 00180: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 896us/sample - loss: 161.9693 - mse: 161.9693 - val_loss: 153.2950 - val_mse: 153.2950
Epoch 181/300
2304/2400 [===========================&gt;..] - ETA: 0s - loss: 164.9608 - mse: 164.9608
Epoch 00181: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 811us/sample - loss: 162.1408 - mse: 162.1408 - val_loss: 153.3219 - val_mse: 153.3219
Epoch 182/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.4587 - mse: 162.4586
Epoch 00182: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 1ms/sample - loss: 162.6205 - mse: 162.6205 - val_loss: 153.3404 - val_mse: 153.3403
Epoch 183/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.4395 - mse: 162.4395
Epoch 00183: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 985us/sample - loss: 162.8368 - mse: 162.8368 - val_loss: 153.3250 - val_mse: 153.3250
Epoch 184/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 151.0317 - mse: 151.0317- ETA: 0s - loss: 149.4047 -
Epoch 00184: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 963us/sample - loss: 161.7119 - mse: 161.7119 - val_loss: 153.2857 - val_mse: 153.2857
Epoch 185/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.3468 - mse: 162.3468
Epoch 00185: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 845us/sample - loss: 161.1097 - mse: 161.1097 - val_loss: 153.3231 - val_mse: 153.3231
Epoch 186/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.1751 - mse: 162.1751
Epoch 00186: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 817us/sample - loss: 161.4515 - mse: 161.4515 - val_loss: 153.3222 - val_mse: 153.3222
Epoch 187/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.2383 - mse: 162.2383
Epoch 00187: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 915us/sample - loss: 161.1726 - mse: 161.1727 - val_loss: 153.2848 - val_mse: 153.2849
Epoch 188/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 150.0116 - mse: 150.0116
Epoch 00188: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 841us/sample - loss: 162.3425 - mse: 162.3425 - val_loss: 153.8068 - val_mse: 153.8068
Epoch 189/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.8345 - mse: 162.8345
Epoch 00189: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 828us/sample - loss: 161.7611 - mse: 161.7611 - val_loss: 153.8133 - val_mse: 153.8133
Epoch 190/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.4979 - mse: 163.4979
Epoch 00190: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 813us/sample - loss: 161.5109 - mse: 161.5109 - val_loss: 153.2867 - val_mse: 153.2867
Epoch 191/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.2633 - mse: 162.2633
Epoch 00191: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 798us/sample - loss: 161.4708 - mse: 161.4708 - val_loss: 153.4569 - val_mse: 153.4569
Epoch 192/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.8745 - mse: 163.8745
Epoch 00192: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 799us/sample - loss: 162.3714 - mse: 162.3714 - val_loss: 153.2854 - val_mse: 153.2854
Epoch 193/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.4954 - mse: 162.4954
Epoch 00193: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 902us/sample - loss: 162.0261 - mse: 162.0261 - val_loss: 153.2818 - val_mse: 153.2818
Epoch 194/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.5586 - mse: 163.5586
Epoch 00194: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 997us/sample - loss: 162.3741 - mse: 162.3741 - val_loss: 153.2904 - val_mse: 153.2904
Epoch 195/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 160.9277 - mse: 160.9276
Epoch 00195: val_loss did not improve from 110.39956
2400/2400 [==============================] - 3s 1ms/sample - loss: 161.6921 - mse: 161.6920 - val_loss: 153.3265 - val_mse: 153.3265
Epoch 196/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 161.5409 - mse: 161.5409
Epoch 00196: val_loss did not improve from 110.39956
2400/2400 [==============================] - 3s 1ms/sample - loss: 162.1320 - mse: 162.1320 - val_loss: 153.2894 - val_mse: 153.2894
Epoch 197/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 161.9486 - mse: 161.9486
Epoch 00197: val_loss did not improve from 110.39956
2400/2400 [==============================] - 3s 1ms/sample - loss: 162.8305 - mse: 162.8305 - val_loss: 153.3012 - val_mse: 153.3012
Epoch 198/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.6116 - mse: 162.6115
Epoch 00198: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 1ms/sample - loss: 161.8719 - mse: 161.8719 - val_loss: 153.4355 - val_mse: 153.4355
Epoch 199/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.2108 - mse: 162.2109
Epoch 00199: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 930us/sample - loss: 161.8830 - mse: 161.8830 - val_loss: 154.0254 - val_mse: 154.0254
Epoch 200/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.0836 - mse: 162.0836
Epoch 00200: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 1ms/sample - loss: 162.6869 - mse: 162.6869 - val_loss: 153.4256 - val_mse: 153.4256
Epoch 201/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.1237 - mse: 163.1237- ETA: 0s - loss: 171.8006
Epoch 00201: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 1ms/sample - loss: 162.1242 - mse: 162.1242 - val_loss: 154.0152 - val_mse: 154.0153
Epoch 202/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 163.0274 - mse: 163.0275
Epoch 00202: val_loss did not improve from 110.39956
2400/2400 [==============================] - 3s 1ms/sample - loss: 161.8458 - mse: 161.8458 - val_loss: 153.7135 - val_mse: 153.7135
Epoch 203/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 164.3129 - mse: 164.3129
Epoch 00203: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 998us/sample - loss: 163.6030 - mse: 163.6030 - val_loss: 153.4596 - val_mse: 153.4596
Epoch 204/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 159.5368 - mse: 159.5368
Epoch 00204: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 938us/sample - loss: 162.4121 - mse: 162.4121 - val_loss: 153.3172 - val_mse: 153.3172
Epoch 205/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 161.9276 - mse: 161.9276
Epoch 00205: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 989us/sample - loss: 162.5113 - mse: 162.5113 - val_loss: 153.4217 - val_mse: 153.4217
Epoch 206/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 162.5796 - mse: 162.5796
Epoch 00206: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 873us/sample - loss: 161.6724 - mse: 161.6725 - val_loss: 153.5524 - val_mse: 153.5524
Epoch 207/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 163.1417 - mse: 163.1416
Epoch 00207: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 926us/sample - loss: 161.7734 - mse: 161.7733 - val_loss: 153.4220 - val_mse: 153.4220
Epoch 208/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 164.1924 - mse: 164.1923
Epoch 00208: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 958us/sample - loss: 162.6448 - mse: 162.6447 - val_loss: 153.4113 - val_mse: 153.4113
Epoch 209/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.8517 - mse: 162.8517
Epoch 00209: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 873us/sample - loss: 161.6634 - mse: 161.6635 - val_loss: 153.3036 - val_mse: 153.3036
Epoch 210/300
2336/2400 [============================&gt;.] - ETA: 0s - loss: 157.9706 - mse: 157.9706
Epoch 00210: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 879us/sample - loss: 162.7997 - mse: 162.7997 - val_loss: 153.2791 - val_mse: 153.2791
Epoch 211/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 161.4807 - mse: 161.4807
Epoch 00211: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 947us/sample - loss: 161.9516 - mse: 161.9516 - val_loss: 153.2941 - val_mse: 153.2941
Epoch 212/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 161.8258 - mse: 161.8258
Epoch 00212: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 812us/sample - loss: 161.7810 - mse: 161.7809 - val_loss: 153.3642 - val_mse: 153.3642
Epoch 213/300
2368/2400 [============================&gt;.] - ETA: 0s - loss: 162.2300 - mse: 162.2300
Epoch 00213: val_loss did not improve from 110.39956
2400/2400 [==============================] - 2s 839us/sample - loss: 162.3335 - mse: 162.3334 - val_loss: 153.6024 - val_mse: 153.6024
Epoch 00213: early stopping
WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;, &lt;class &#39;NoneType&#39;&gt;
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\samue\Anaconda3\lib\site-packages\sklearn\utils\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Train on 4118 samples, validate on 1030 samples
Epoch 1/200
3776/4118 [==========================&gt;...] - ETA: 0s - loss: 3.5421 - accuracy: 0.6224
Epoch 00001: val_loss did not improve from 0.04632
4118/4118 [==============================] - 1s 203us/sample - loss: 3.3556 - accuracy: 0.6272 - val_loss: 1.4110 - val_accuracy: 0.6670
Epoch 2/200
4032/4118 [============================&gt;.] - ETA: 0s - loss: 1.4235 - accuracy: 0.6687
Epoch 00002: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 63us/sample - loss: 1.4194 - accuracy: 0.6685 - val_loss: 1.0019 - val_accuracy: 0.7039
Epoch 3/200
3328/4118 [=======================&gt;......] - ETA: 0s - loss: 1.1847 - accuracy: 0.6944
Epoch 00003: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 54us/sample - loss: 1.1299 - accuracy: 0.6940 - val_loss: 1.7989 - val_accuracy: 0.4184
Epoch 4/200
4096/4118 [============================&gt;.] - ETA: 0s - loss: 0.9744 - accuracy: 0.6975
Epoch 00004: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 61us/sample - loss: 0.9773 - accuracy: 0.6969 - val_loss: 1.0894 - val_accuracy: 0.6233
Epoch 5/200
3200/4118 [======================&gt;.......] - ETA: 0s - loss: 0.9119 - accuracy: 0.7119
Epoch 00005: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 53us/sample - loss: 0.8639 - accuracy: 0.7200 - val_loss: 0.5903 - val_accuracy: 0.7534
Epoch 6/200
4096/4118 [============================&gt;.] - ETA: 0s - loss: 1.1787 - accuracy: 0.6804
Epoch 00006: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 65us/sample - loss: 1.1763 - accuracy: 0.6804 - val_loss: 0.7879 - val_accuracy: 0.6903
Epoch 7/200
3328/4118 [=======================&gt;......] - ETA: 0s - loss: 0.9815 - accuracy: 0.7064
Epoch 00007: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 53us/sample - loss: 0.9769 - accuracy: 0.7064 - val_loss: 0.9436 - val_accuracy: 0.6777
Epoch 8/200
3648/4118 [=========================&gt;....] - ETA: 0s - loss: 0.6672 - accuracy: 0.7390
Epoch 00008: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 67us/sample - loss: 0.6794 - accuracy: 0.7377 - val_loss: 2.5473 - val_accuracy: 0.1874
Epoch 9/200
3168/4118 [======================&gt;.......] - ETA: 0s - loss: 0.8117 - accuracy: 0.7150
Epoch 00009: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 58us/sample - loss: 0.7718 - accuracy: 0.7234 - val_loss: 1.0169 - val_accuracy: 0.5718
Epoch 10/200
3872/4118 [===========================&gt;..] - ETA: 0s - loss: 0.6512 - accuracy: 0.7386
Epoch 00010: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 81us/sample - loss: 0.6501 - accuracy: 0.7368 - val_loss: 1.6394 - val_accuracy: 0.4796
Epoch 11/200
4064/4118 [============================&gt;.] - ETA: 0s - loss: 0.6326 - accuracy: 0.7431
Epoch 00011: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 94us/sample - loss: 0.6315 - accuracy: 0.7436 - val_loss: 0.7757 - val_accuracy: 0.6388
Epoch 12/200
3680/4118 [=========================&gt;....] - ETA: 0s - loss: 0.6738 - accuracy: 0.7486
Epoch 00012: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 83us/sample - loss: 0.6873 - accuracy: 0.7443 - val_loss: 0.1869 - val_accuracy: 0.9359
Epoch 13/200
3616/4118 [=========================&gt;....] - ETA: 0s - loss: 0.5779 - accuracy: 0.7633
Epoch 00013: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 85us/sample - loss: 0.5900 - accuracy: 0.7591 - val_loss: 0.3289 - val_accuracy: 0.8563
Epoch 14/200
3680/4118 [=========================&gt;....] - ETA: 0s - loss: 0.6620 - accuracy: 0.7484
Epoch 00014: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 85us/sample - loss: 0.6694 - accuracy: 0.7472 - val_loss: 0.6006 - val_accuracy: 0.7107
Epoch 15/200
3680/4118 [=========================&gt;....] - ETA: 0s - loss: 0.8016 - accuracy: 0.7261
Epoch 00015: val_loss did not improve from 0.04632
4118/4118 [==============================] - 1s 129us/sample - loss: 0.7692 - accuracy: 0.7329 - val_loss: 0.6347 - val_accuracy: 0.7058
Epoch 16/200
3520/4118 [========================&gt;.....] - ETA: 0s - loss: 0.5629 - accuracy: 0.7594
Epoch 00016: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 81us/sample - loss: 0.5792 - accuracy: 0.7584 - val_loss: 0.3740 - val_accuracy: 0.8505
Epoch 17/200
3328/4118 [=======================&gt;......] - ETA: 0s - loss: 0.5584 - accuracy: 0.7611
Epoch 00017: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 72us/sample - loss: 0.5457 - accuracy: 0.7647 - val_loss: 0.4854 - val_accuracy: 0.7883
Epoch 18/200
4000/4118 [============================&gt;.] - ETA: 0s - loss: 0.4949 - accuracy: 0.7908
Epoch 00018: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 76us/sample - loss: 0.4964 - accuracy: 0.7926 - val_loss: 0.6157 - val_accuracy: 0.7301
Epoch 19/200
3264/4118 [======================&gt;.......] - ETA: 0s - loss: 0.5830 - accuracy: 0.7665
Epoch 00019: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 82us/sample - loss: 0.5630 - accuracy: 0.7739 - val_loss: 0.8675 - val_accuracy: 0.5913
Epoch 20/200
3904/4118 [===========================&gt;..] - ETA: 0s - loss: 0.5734 - accuracy: 0.7702
Epoch 00020: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 76us/sample - loss: 0.5709 - accuracy: 0.7703 - val_loss: 0.6492 - val_accuracy: 0.6398
Epoch 21/200
3392/4118 [=======================&gt;......] - ETA: 0s - loss: 0.5889 - accuracy: 0.7630
Epoch 00021: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 70us/sample - loss: 0.5648 - accuracy: 0.7705 - val_loss: 0.8288 - val_accuracy: 0.5388
Epoch 22/200
3328/4118 [=======================&gt;......] - ETA: 0s - loss: 0.4695 - accuracy: 0.7993
Epoch 00022: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 69us/sample - loss: 0.5020 - accuracy: 0.7839 - val_loss: 0.5611 - val_accuracy: 0.9398
Epoch 23/200
3776/4118 [==========================&gt;...] - ETA: 0s - loss: 0.4894 - accuracy: 0.7871
Epoch 00023: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 87us/sample - loss: 0.4954 - accuracy: 0.7865 - val_loss: 2.5263 - val_accuracy: 0.2544
Epoch 24/200
3296/4118 [=======================&gt;......] - ETA: 0s - loss: 0.5811 - accuracy: 0.7649
Epoch 00024: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 70us/sample - loss: 0.5973 - accuracy: 0.7574 - val_loss: 1.2935 - val_accuracy: 0.6796
Epoch 25/200
3296/4118 [=======================&gt;......] - ETA: 0s - loss: 0.5491 - accuracy: 0.7667
Epoch 00025: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 69us/sample - loss: 0.5334 - accuracy: 0.7712 - val_loss: 0.8058 - val_accuracy: 0.6553
Epoch 26/200
3328/4118 [=======================&gt;......] - ETA: 0s - loss: 0.5040 - accuracy: 0.7951
Epoch 00026: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 73us/sample - loss: 0.5494 - accuracy: 0.7817 - val_loss: 0.1592 - val_accuracy: 0.9631
Epoch 27/200
3392/4118 [=======================&gt;......] - ETA: 0s - loss: 0.4686 - accuracy: 0.7927
Epoch 00027: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 69us/sample - loss: 0.4681 - accuracy: 0.7946 - val_loss: 0.7099 - val_accuracy: 0.6398
Epoch 28/200
4096/4118 [============================&gt;.] - ETA: 0s - loss: 0.4414 - accuracy: 0.8105
Epoch 00028: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 75us/sample - loss: 0.4403 - accuracy: 0.8108 - val_loss: 0.3554 - val_accuracy: 0.8524
Epoch 29/200
3456/4118 [========================&gt;.....] - ETA: 0s - loss: 0.4538 - accuracy: 0.8056
Epoch 00029: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 69us/sample - loss: 0.4511 - accuracy: 0.8082 - val_loss: 0.6013 - val_accuracy: 0.7311
Epoch 30/200
3488/4118 [========================&gt;.....] - ETA: 0s - loss: 0.4566 - accuracy: 0.8157
Epoch 00030: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 66us/sample - loss: 0.4462 - accuracy: 0.8169 - val_loss: 0.3598 - val_accuracy: 0.8398
Epoch 31/200
3168/4118 [======================&gt;.......] - ETA: 0s - loss: 0.4199 - accuracy: 0.8242 ETA: 0s - loss: 0.4434 - accuracy: 0.
Epoch 00031: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 76us/sample - loss: 0.4119 - accuracy: 0.8242 - val_loss: 0.8812 - val_accuracy: 0.5602
Epoch 32/200
3520/4118 [========================&gt;.....] - ETA: 0s - loss: 0.3985 - accuracy: 0.8318
Epoch 00032: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 69us/sample - loss: 0.4058 - accuracy: 0.8283 - val_loss: 1.2407 - val_accuracy: 0.3689
Epoch 33/200
3392/4118 [=======================&gt;......] - ETA: 0s - loss: 0.4411 - accuracy: 0.8110
Epoch 00033: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 70us/sample - loss: 0.4290 - accuracy: 0.8169 - val_loss: 0.4426 - val_accuracy: 0.8398
Epoch 34/200
3520/4118 [========================&gt;.....] - ETA: 0s - loss: 0.3937 - accuracy: 0.8247
Epoch 00034: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 67us/sample - loss: 0.3937 - accuracy: 0.8244 - val_loss: 0.1177 - val_accuracy: 0.9748
Epoch 35/200
3552/4118 [========================&gt;.....] - ETA: 0s - loss: 0.3816 - accuracy: 0.8345
Epoch 00035: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 67us/sample - loss: 0.3863 - accuracy: 0.8310 - val_loss: 0.6827 - val_accuracy: 0.6883
Epoch 36/200
3968/4118 [===========================&gt;..] - ETA: 0s - loss: 0.4017 - accuracy: 0.8309
Epoch 00036: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 62us/sample - loss: 0.4068 - accuracy: 0.8298 - val_loss: 1.0854 - val_accuracy: 0.5204
Epoch 37/200
3552/4118 [========================&gt;.....] - ETA: 0s - loss: 0.4151 - accuracy: 0.8224
Epoch 00037: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 68us/sample - loss: 0.4109 - accuracy: 0.8220 - val_loss: 0.4306 - val_accuracy: 0.7922
Epoch 38/200
4064/4118 [============================&gt;.] - ETA: 0s - loss: 0.4169 - accuracy: 0.8292
Epoch 00038: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 62us/sample - loss: 0.4192 - accuracy: 0.8281 - val_loss: 0.3615 - val_accuracy: 0.8612
Epoch 39/200
3936/4118 [===========================&gt;..] - ETA: 0s - loss: 0.3861 - accuracy: 0.8265
Epoch 00039: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 64us/sample - loss: 0.3833 - accuracy: 0.8273 - val_loss: 0.5888 - val_accuracy: 0.7078
Epoch 40/200
4000/4118 [============================&gt;.] - ETA: 0s - loss: 0.3524 - accuracy: 0.8443
Epoch 00040: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 63us/sample - loss: 0.3542 - accuracy: 0.8443 - val_loss: 0.1049 - val_accuracy: 0.9728
Epoch 41/200
3520/4118 [========================&gt;.....] - ETA: 0s - loss: 0.5031 - accuracy: 0.7869
Epoch 00041: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 67us/sample - loss: 0.5349 - accuracy: 0.7788 - val_loss: 0.0757 - val_accuracy: 0.9942
Epoch 42/200
3904/4118 [===========================&gt;..] - ETA: 0s - loss: 0.4097 - accuracy: 0.8217
Epoch 00042: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 63us/sample - loss: 0.4052 - accuracy: 0.8237 - val_loss: 0.5028 - val_accuracy: 0.7786
Epoch 43/200
3520/4118 [========================&gt;.....] - ETA: 0s - loss: 0.3776 - accuracy: 0.8352
Epoch 00043: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 68us/sample - loss: 0.3752 - accuracy: 0.8361 - val_loss: 0.7382 - val_accuracy: 0.6573
Epoch 44/200
3616/4118 [=========================&gt;....] - ETA: 0s - loss: 0.3458 - accuracy: 0.8515
Epoch 00044: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 66us/sample - loss: 0.3713 - accuracy: 0.8434 - val_loss: 0.6367 - val_accuracy: 0.7155
Epoch 45/200
3936/4118 [===========================&gt;..] - ETA: 0s - loss: 0.3870 - accuracy: 0.8371
Epoch 00045: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 63us/sample - loss: 0.3864 - accuracy: 0.8375 - val_loss: 0.7129 - val_accuracy: 0.6650
Epoch 46/200
3488/4118 [========================&gt;.....] - ETA: 0s - loss: 0.3632 - accuracy: 0.8423
Epoch 00046: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 64us/sample - loss: 0.3655 - accuracy: 0.8407 - val_loss: 0.3367 - val_accuracy: 0.8854
Epoch 47/200
3488/4118 [========================&gt;.....] - ETA: 0s - loss: 0.3932 - accuracy: 0.8340
Epoch 00047: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 71us/sample - loss: 0.3879 - accuracy: 0.8388 - val_loss: 0.7158 - val_accuracy: 0.7175
Epoch 48/200
3712/4118 [==========================&gt;...] - ETA: 0s - loss: 0.3422 - accuracy: 0.8559
Epoch 00048: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 66us/sample - loss: 0.3442 - accuracy: 0.8528 - val_loss: 0.8886 - val_accuracy: 0.6223
Epoch 49/200
3616/4118 [=========================&gt;....] - ETA: 0s - loss: 0.4109 - accuracy: 0.8302
Epoch 00049: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 67us/sample - loss: 0.4102 - accuracy: 0.8293 - val_loss: 0.8188 - val_accuracy: 0.6398
Epoch 50/200
3680/4118 [=========================&gt;....] - ETA: 0s - loss: 0.3823 - accuracy: 0.8250
Epoch 00050: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 67us/sample - loss: 0.3869 - accuracy: 0.8259 - val_loss: 0.5092 - val_accuracy: 0.8466
Epoch 51/200
3840/4118 [==========================&gt;...] - ETA: 0s - loss: 0.3255 - accuracy: 0.8589
Epoch 00051: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 65us/sample - loss: 0.3234 - accuracy: 0.8582 - val_loss: 1.1072 - val_accuracy: 0.6447
Epoch 52/200
3904/4118 [===========================&gt;..] - ETA: 0s - loss: 0.3971 - accuracy: 0.8450
Epoch 00052: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 64us/sample - loss: 0.3937 - accuracy: 0.8446 - val_loss: 0.6523 - val_accuracy: 0.6748
Epoch 53/200
3936/4118 [===========================&gt;..] - ETA: 0s - loss: 0.3296 - accuracy: 0.8610
Epoch 00053: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 64us/sample - loss: 0.3250 - accuracy: 0.8626 - val_loss: 0.7725 - val_accuracy: 0.6748
Epoch 54/200
3520/4118 [========================&gt;.....] - ETA: 0s - loss: 0.3392 - accuracy: 0.8477
Epoch 00054: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 68us/sample - loss: 0.3411 - accuracy: 0.8473 - val_loss: 0.8900 - val_accuracy: 0.6689
Epoch 55/200
3200/4118 [======================&gt;.......] - ETA: 0s - loss: 0.3635 - accuracy: 0.8487
Epoch 00055: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 70us/sample - loss: 0.3712 - accuracy: 0.8475 - val_loss: 0.1990 - val_accuracy: 0.9670
Epoch 56/200
3360/4118 [=======================&gt;......] - ETA: 0s - loss: 0.3909 - accuracy: 0.8351
Epoch 00056: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 84us/sample - loss: 0.3832 - accuracy: 0.8349 - val_loss: 0.4785 - val_accuracy: 0.7942
Epoch 57/200
3488/4118 [========================&gt;.....] - ETA: 0s - loss: 0.3395 - accuracy: 0.8478
Epoch 00057: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 68us/sample - loss: 0.3394 - accuracy: 0.8473 - val_loss: 0.6473 - val_accuracy: 0.6951
Epoch 58/200
3520/4118 [========================&gt;.....] - ETA: 0s - loss: 0.3517 - accuracy: 0.8477
Epoch 00058: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 68us/sample - loss: 0.3676 - accuracy: 0.8470 - val_loss: 0.3613 - val_accuracy: 0.8738
Epoch 59/200
3584/4118 [=========================&gt;....] - ETA: 0s - loss: 0.3326 - accuracy: 0.8502
Epoch 00059: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 70us/sample - loss: 0.3261 - accuracy: 0.8555 - val_loss: 0.7524 - val_accuracy: 0.6350
Epoch 60/200
3968/4118 [===========================&gt;..] - ETA: 0s - loss: 0.3430 - accuracy: 0.8511
Epoch 00060: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 64us/sample - loss: 0.3446 - accuracy: 0.8504 - val_loss: 1.0872 - val_accuracy: 0.5058
Epoch 61/200
3456/4118 [========================&gt;.....] - ETA: 0s - loss: 0.3300 - accuracy: 0.8573
Epoch 00061: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 86us/sample - loss: 0.3393 - accuracy: 0.8536 - val_loss: 0.3852 - val_accuracy: 0.8563
Epoch 62/200
3840/4118 [==========================&gt;...] - ETA: 0s - loss: 0.3329 - accuracy: 0.8565
Epoch 00062: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 68us/sample - loss: 0.3376 - accuracy: 0.8541 - val_loss: 0.3777 - val_accuracy: 0.8184
Epoch 63/200
3360/4118 [=======================&gt;......] - ETA: 0s - loss: 0.3015 - accuracy: 0.8786
Epoch 00063: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 73us/sample - loss: 0.3074 - accuracy: 0.8752 - val_loss: 0.3898 - val_accuracy: 0.8718
Epoch 64/200
3776/4118 [==========================&gt;...] - ETA: 0s - loss: 0.3030 - accuracy: 0.8678
Epoch 00064: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 82us/sample - loss: 0.3089 - accuracy: 0.8657 - val_loss: 0.7315 - val_accuracy: 0.6350
Epoch 65/200
3680/4118 [=========================&gt;....] - ETA: 0s - loss: 0.3300 - accuracy: 0.8524
Epoch 00065: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 83us/sample - loss: 0.3254 - accuracy: 0.8533 - val_loss: 0.6399 - val_accuracy: 0.6612
Epoch 66/200
3584/4118 [=========================&gt;....] - ETA: 0s - loss: 0.3630 - accuracy: 0.8435
Epoch 00066: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 85us/sample - loss: 0.3556 - accuracy: 0.8477 - val_loss: 0.1609 - val_accuracy: 0.9417
Epoch 67/200
3520/4118 [========================&gt;.....] - ETA: 0s - loss: 0.2793 - accuracy: 0.8739
Epoch 00067: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 88us/sample - loss: 0.2765 - accuracy: 0.8786 - val_loss: 0.2037 - val_accuracy: 0.9379
Epoch 68/200
3680/4118 [=========================&gt;....] - ETA: 0s - loss: 0.3450 - accuracy: 0.8655
Epoch 00068: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 102us/sample - loss: 0.3398 - accuracy: 0.8657 - val_loss: 0.3192 - val_accuracy: 0.8990
Epoch 69/200
3840/4118 [==========================&gt;...] - ETA: 0s - loss: 0.3040 - accuracy: 0.8771
Epoch 00069: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 91us/sample - loss: 0.3018 - accuracy: 0.8762 - val_loss: 0.2024 - val_accuracy: 0.9350
Epoch 70/200
3552/4118 [========================&gt;.....] - ETA: 0s - loss: 0.2789 - accuracy: 0.8826
Epoch 00070: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 67us/sample - loss: 0.2851 - accuracy: 0.8815 - val_loss: 0.2878 - val_accuracy: 0.8922
Epoch 71/200
3168/4118 [======================&gt;.......] - ETA: 0s - loss: 0.3062 - accuracy: 0.8703
Epoch 00071: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 72us/sample - loss: 0.3011 - accuracy: 0.8725 - val_loss: 0.0963 - val_accuracy: 0.9883
Epoch 72/200
3520/4118 [========================&gt;.....] - ETA: 0s - loss: 0.3528 - accuracy: 0.8554
Epoch 00072: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 69us/sample - loss: 0.3393 - accuracy: 0.8599 - val_loss: 0.3388 - val_accuracy: 0.8903
Epoch 73/200
3456/4118 [========================&gt;.....] - ETA: 0s - loss: 0.3004 - accuracy: 0.8715
Epoch 00073: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 69us/sample - loss: 0.2968 - accuracy: 0.8703 - val_loss: 0.4265 - val_accuracy: 0.8544
Epoch 74/200
3904/4118 [===========================&gt;..] - ETA: 0s - loss: 0.2613 - accuracy: 0.8942
Epoch 00074: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 82us/sample - loss: 0.2639 - accuracy: 0.8927 - val_loss: 0.5358 - val_accuracy: 0.8777
Epoch 75/200
3584/4118 [=========================&gt;....] - ETA: 0s - loss: 0.2956 - accuracy: 0.8814
Epoch 00075: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 71us/sample - loss: 0.3034 - accuracy: 0.8771 - val_loss: 0.6451 - val_accuracy: 0.7243
Epoch 76/200
3392/4118 [=======================&gt;......] - ETA: 0s - loss: 0.2919 - accuracy: 0.8774
Epoch 00076: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 70us/sample - loss: 0.2842 - accuracy: 0.8800 - val_loss: 0.2642 - val_accuracy: 0.9078
Epoch 77/200
4064/4118 [============================&gt;.] - ETA: 0s - loss: 0.2476 - accuracy: 0.8949
Epoch 00077: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 75us/sample - loss: 0.2475 - accuracy: 0.8946 - val_loss: 0.3001 - val_accuracy: 0.9330
Epoch 78/200
3328/4118 [=======================&gt;......] - ETA: 0s - loss: 0.2524 - accuracy: 0.8882
Epoch 00078: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 70us/sample - loss: 0.2556 - accuracy: 0.8881 - val_loss: 0.2177 - val_accuracy: 0.9214
Epoch 79/200
3904/4118 [===========================&gt;..] - ETA: 0s - loss: 0.2889 - accuracy: 0.8742
Epoch 00079: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 78us/sample - loss: 0.2900 - accuracy: 0.8757 - val_loss: 0.3642 - val_accuracy: 0.8485
Epoch 80/200
3456/4118 [========================&gt;.....] - ETA: 0s - loss: 0.3053 - accuracy: 0.8724
Epoch 00080: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 67us/sample - loss: 0.2932 - accuracy: 0.8759 - val_loss: 0.5350 - val_accuracy: 0.7748
Epoch 81/200
3232/4118 [======================&gt;.......] - ETA: 0s - loss: 0.2276 - accuracy: 0.9038
Epoch 00081: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 73us/sample - loss: 0.2316 - accuracy: 0.9029 - val_loss: 0.3949 - val_accuracy: 0.8495
Epoch 82/200
3360/4118 [=======================&gt;......] - ETA: 0s - loss: 0.2060 - accuracy: 0.9116
Epoch 00082: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 72us/sample - loss: 0.2090 - accuracy: 0.9109 - val_loss: 0.1892 - val_accuracy: 0.9437
Epoch 83/200
3424/4118 [=======================&gt;......] - ETA: 0s - loss: 0.2887 - accuracy: 0.8838
Epoch 00083: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 69us/sample - loss: 0.2892 - accuracy: 0.8844 - val_loss: 0.5144 - val_accuracy: 0.7631
Epoch 84/200
3392/4118 [=======================&gt;......] - ETA: 0s - loss: 0.3228 - accuracy: 0.8688
Epoch 00084: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 68us/sample - loss: 0.3112 - accuracy: 0.8715 - val_loss: 0.3673 - val_accuracy: 0.8563
Epoch 85/200
3200/4118 [======================&gt;.......] - ETA: 0s - loss: 0.2512 - accuracy: 0.8984
Epoch 00085: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 73us/sample - loss: 0.2460 - accuracy: 0.8995 - val_loss: 0.3876 - val_accuracy: 0.9049
Epoch 86/200
3776/4118 [==========================&gt;...] - ETA: 0s - loss: 0.2188 - accuracy: 0.9115
Epoch 00086: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 81us/sample - loss: 0.2192 - accuracy: 0.9111 - val_loss: 0.3462 - val_accuracy: 0.8388
Epoch 87/200
3552/4118 [========================&gt;.....] - ETA: 0s - loss: 0.2022 - accuracy: 0.9192
Epoch 00087: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 68us/sample - loss: 0.2002 - accuracy: 0.9199 - val_loss: 0.3069 - val_accuracy: 0.8524
Epoch 88/200
3520/4118 [========================&gt;.....] - ETA: 0s - loss: 0.2600 - accuracy: 0.8920
Epoch 00088: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 65us/sample - loss: 0.2583 - accuracy: 0.8932 - val_loss: 0.2555 - val_accuracy: 0.9243
Epoch 89/200
3552/4118 [========================&gt;.....] - ETA: 0s - loss: 0.2313 - accuracy: 0.9037
Epoch 00089: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 69us/sample - loss: 0.2440 - accuracy: 0.9007 - val_loss: 0.5955 - val_accuracy: 0.7049
Epoch 90/200
3616/4118 [=========================&gt;....] - ETA: 0s - loss: 0.2348 - accuracy: 0.8982
Epoch 00090: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 69us/sample - loss: 0.2321 - accuracy: 0.8980 - val_loss: 0.1637 - val_accuracy: 0.9408
Epoch 91/200
3552/4118 [========================&gt;.....] - ETA: 0s - loss: 0.2371 - accuracy: 0.9029
Epoch 00091: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 68us/sample - loss: 0.2395 - accuracy: 0.9000 - val_loss: 0.4953 - val_accuracy: 0.8252
Epoch 92/200
3392/4118 [=======================&gt;......] - ETA: 0s - loss: 0.2169 - accuracy: 0.9142
Epoch 00092: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 69us/sample - loss: 0.2125 - accuracy: 0.9153 - val_loss: 0.2175 - val_accuracy: 0.9214
Epoch 93/200
3648/4118 [=========================&gt;....] - ETA: 0s - loss: 0.2583 - accuracy: 0.8958
Epoch 00093: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 67us/sample - loss: 0.2675 - accuracy: 0.8929 - val_loss: 0.7730 - val_accuracy: 0.8019
Epoch 94/200
3584/4118 [=========================&gt;....] - ETA: 0s - loss: 0.2665 - accuracy: 0.8915
Epoch 00094: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 67us/sample - loss: 0.2616 - accuracy: 0.8932 - val_loss: 0.3507 - val_accuracy: 0.8592
Epoch 95/200
3616/4118 [=========================&gt;....] - ETA: 0s - loss: 0.1860 - accuracy: 0.9267
Epoch 00095: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 66us/sample - loss: 0.1867 - accuracy: 0.9269 - val_loss: 0.1854 - val_accuracy: 0.9340
Epoch 96/200
3840/4118 [==========================&gt;...] - ETA: 0s - loss: 0.2394 - accuracy: 0.8992
Epoch 00096: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 64us/sample - loss: 0.2433 - accuracy: 0.8970 - val_loss: 0.4650 - val_accuracy: 0.7650
Epoch 97/200
3520/4118 [========================&gt;.....] - ETA: 0s - loss: 0.2721 - accuracy: 0.8878
Epoch 00097: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 68us/sample - loss: 0.2723 - accuracy: 0.8881 - val_loss: 0.3970 - val_accuracy: 0.8252
Epoch 98/200
3552/4118 [========================&gt;.....] - ETA: 0s - loss: 0.2152 - accuracy: 0.9057
Epoch 00098: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 67us/sample - loss: 0.2286 - accuracy: 0.9038 - val_loss: 0.6154 - val_accuracy: 0.8233
Epoch 99/200
3584/4118 [=========================&gt;....] - ETA: 0s - loss: 0.1984 - accuracy: 0.9180
Epoch 00099: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 69us/sample - loss: 0.1963 - accuracy: 0.9194 - val_loss: 0.1930 - val_accuracy: 0.9233
Epoch 100/200
3552/4118 [========================&gt;.....] - ETA: 0s - loss: 0.1783 - accuracy: 0.9245
Epoch 00100: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 68us/sample - loss: 0.1784 - accuracy: 0.9254 - val_loss: 0.6771 - val_accuracy: 0.7456
Epoch 101/200
3424/4118 [=======================&gt;......] - ETA: 0s - loss: 0.1753 - accuracy: 0.9282
Epoch 00101: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 69us/sample - loss: 0.1847 - accuracy: 0.9233 - val_loss: 0.1150 - val_accuracy: 0.9718
Epoch 102/200
3488/4118 [========================&gt;.....] - ETA: 0s - loss: 0.1881 - accuracy: 0.9260
Epoch 00102: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 71us/sample - loss: 0.1844 - accuracy: 0.9281 - val_loss: 0.2877 - val_accuracy: 0.8767
Epoch 103/200
3872/4118 [===========================&gt;..] - ETA: 0s - loss: 0.1710 - accuracy: 0.9313
Epoch 00103: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 65us/sample - loss: 0.1696 - accuracy: 0.9318 - val_loss: 0.2484 - val_accuracy: 0.8932
Epoch 104/200
3584/4118 [=========================&gt;....] - ETA: 0s - loss: 0.1791 - accuracy: 0.9272
Epoch 00104: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 68us/sample - loss: 0.1715 - accuracy: 0.9313 - val_loss: 0.1336 - val_accuracy: 0.9505
Epoch 105/200
3616/4118 [=========================&gt;....] - ETA: 0s - loss: 0.1652 - accuracy: 0.9311
Epoch 00105: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 67us/sample - loss: 0.1728 - accuracy: 0.9259 - val_loss: 0.2228 - val_accuracy: 0.9117
Epoch 106/200
3456/4118 [========================&gt;.....] - ETA: 0s - loss: 0.2548 - accuracy: 0.8947
Epoch 00106: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 70us/sample - loss: 0.2652 - accuracy: 0.8919 - val_loss: 0.1657 - val_accuracy: 0.9291
Epoch 107/200
3584/4118 [=========================&gt;....] - ETA: 0s - loss: 0.1841 - accuracy: 0.9247
Epoch 00107: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 67us/sample - loss: 0.1793 - accuracy: 0.9267 - val_loss: 0.1417 - val_accuracy: 0.9379
Epoch 108/200
3520/4118 [========================&gt;.....] - ETA: 0s - loss: 0.2062 - accuracy: 0.9145
Epoch 00108: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 68us/sample - loss: 0.2019 - accuracy: 0.9153 - val_loss: 0.0835 - val_accuracy: 0.9806
Epoch 109/200
3936/4118 [===========================&gt;..] - ETA: 0s - loss: 0.1791 - accuracy: 0.9276
Epoch 00109: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 76us/sample - loss: 0.1798 - accuracy: 0.9276 - val_loss: 0.2004 - val_accuracy: 0.9311
Epoch 110/200
3360/4118 [=======================&gt;......] - ETA: 0s - loss: 0.2165 - accuracy: 0.9170
Epoch 00110: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 70us/sample - loss: 0.2155 - accuracy: 0.9182 - val_loss: 0.2395 - val_accuracy: 0.9388
Epoch 111/200
3424/4118 [=======================&gt;......] - ETA: 0s - loss: 0.1900 - accuracy: 0.9252
Epoch 00111: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 69us/sample - loss: 0.2024 - accuracy: 0.9206 - val_loss: 0.2646 - val_accuracy: 0.9262
Epoch 112/200
3264/4118 [======================&gt;.......] - ETA: 0s - loss: 0.1792 - accuracy: 0.9243
Epoch 00112: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 68us/sample - loss: 0.1921 - accuracy: 0.9203 - val_loss: 0.3146 - val_accuracy: 0.8650
Epoch 113/200
3456/4118 [========================&gt;.....] - ETA: 0s - loss: 0.1873 - accuracy: 0.9285
Epoch 00113: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 73us/sample - loss: 0.1837 - accuracy: 0.9291 - val_loss: 0.4013 - val_accuracy: 0.8204
Epoch 114/200
3456/4118 [========================&gt;.....] - ETA: 0s - loss: 0.1356 - accuracy: 0.9476
Epoch 00114: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 68us/sample - loss: 0.1436 - accuracy: 0.9451 - val_loss: 0.4338 - val_accuracy: 0.7942
Epoch 115/200
3424/4118 [=======================&gt;......] - ETA: 0s - loss: 0.2128 - accuracy: 0.9124
Epoch 00115: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 69us/sample - loss: 0.2107 - accuracy: 0.9123 - val_loss: 0.3238 - val_accuracy: 0.8990
Epoch 116/200
3392/4118 [=======================&gt;......] - ETA: 0s - loss: 0.1693 - accuracy: 0.9363
Epoch 00116: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 69us/sample - loss: 0.1628 - accuracy: 0.9390 - val_loss: 0.1157 - val_accuracy: 0.9699
Epoch 117/200
3296/4118 [=======================&gt;......] - ETA: 0s - loss: 0.1279 - accuracy: 0.9493
Epoch 00117: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 69us/sample - loss: 0.1352 - accuracy: 0.9461 - val_loss: 0.2599 - val_accuracy: 0.9078
Epoch 118/200
3584/4118 [=========================&gt;....] - ETA: 0s - loss: 0.1493 - accuracy: 0.9361
Epoch 00118: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 85us/sample - loss: 0.1476 - accuracy: 0.9371 - val_loss: 0.1838 - val_accuracy: 0.9272
Epoch 119/200
3552/4118 [========================&gt;.....] - ETA: 0s - loss: 0.1232 - accuracy: 0.9524
Epoch 00119: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 85us/sample - loss: 0.1282 - accuracy: 0.9495 - val_loss: 0.5061 - val_accuracy: 0.7573
Epoch 120/200
3520/4118 [========================&gt;.....] - ETA: 0s - loss: 0.1774 - accuracy: 0.9310
Epoch 00120: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 89us/sample - loss: 0.1783 - accuracy: 0.9308 - val_loss: 0.2576 - val_accuracy: 0.8942
Epoch 121/200
3520/4118 [========================&gt;.....] - ETA: 0s - loss: 0.1698 - accuracy: 0.9347
Epoch 00121: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 85us/sample - loss: 0.1665 - accuracy: 0.9352 - val_loss: 0.1178 - val_accuracy: 0.9621
Epoch 122/200
3904/4118 [===========================&gt;..] - ETA: 0s - loss: 0.2009 - accuracy: 0.9232
Epoch 00122: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 107us/sample - loss: 0.1968 - accuracy: 0.9240 - val_loss: 0.4285 - val_accuracy: 0.8359
Epoch 123/200
3936/4118 [===========================&gt;..] - ETA: 0s - loss: 0.1492 - accuracy: 0.9405
Epoch 00123: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 94us/sample - loss: 0.1517 - accuracy: 0.9388 - val_loss: 0.1382 - val_accuracy: 0.9515
Epoch 124/200
4000/4118 [============================&gt;.] - ETA: 0s - loss: 0.1974 - accuracy: 0.9197
Epoch 00124: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 76us/sample - loss: 0.1958 - accuracy: 0.9201 - val_loss: 0.1920 - val_accuracy: 0.9350
Epoch 125/200
3552/4118 [========================&gt;.....] - ETA: 0s - loss: 0.1251 - accuracy: 0.9482
Epoch 00125: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 69us/sample - loss: 0.1430 - accuracy: 0.9398 - val_loss: 0.4714 - val_accuracy: 0.7825
Epoch 126/200
3456/4118 [========================&gt;.....] - ETA: 0s - loss: 0.1858 - accuracy: 0.9256
Epoch 00126: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 73us/sample - loss: 0.1799 - accuracy: 0.9286 - val_loss: 0.1668 - val_accuracy: 0.9223
Epoch 127/200
4032/4118 [============================&gt;.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9430
Epoch 00127: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 83us/sample - loss: 0.1279 - accuracy: 0.9429 - val_loss: 0.1056 - val_accuracy: 0.9718
Epoch 128/200
4064/4118 [============================&gt;.] - ETA: 0s - loss: 0.1247 - accuracy: 0.9486
Epoch 00128: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 79us/sample - loss: 0.1249 - accuracy: 0.9483 - val_loss: 0.4219 - val_accuracy: 0.8000
Epoch 129/200
3712/4118 [==========================&gt;...] - ETA: 0s - loss: 0.2223 - accuracy: 0.9246
Epoch 00129: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 67us/sample - loss: 0.2346 - accuracy: 0.9208 - val_loss: 0.5491 - val_accuracy: 0.7854
Epoch 130/200
3808/4118 [==========================&gt;...] - ETA: 0s - loss: 0.2278 - accuracy: 0.9152
Epoch 00130: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 66us/sample - loss: 0.2210 - accuracy: 0.9165 - val_loss: 0.1685 - val_accuracy: 0.9515
Epoch 131/200
3392/4118 [=======================&gt;......] - ETA: 0s - loss: 0.1144 - accuracy: 0.9564
Epoch 00131: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 70us/sample - loss: 0.1148 - accuracy: 0.9560 - val_loss: 0.2519 - val_accuracy: 0.9019
Epoch 132/200
4000/4118 [============================&gt;.] - ETA: 0s - loss: 0.1012 - accuracy: 0.9607
Epoch 00132: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 77us/sample - loss: 0.1026 - accuracy: 0.9604 - val_loss: 0.2396 - val_accuracy: 0.9146
Epoch 133/200
3712/4118 [==========================&gt;...] - ETA: 0s - loss: 0.1199 - accuracy: 0.9539
Epoch 00133: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 65us/sample - loss: 0.1265 - accuracy: 0.9517 - val_loss: 0.2749 - val_accuracy: 0.9107
Epoch 134/200
3808/4118 [==========================&gt;...] - ETA: 0s - loss: 0.1721 - accuracy: 0.9296
Epoch 00134: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 65us/sample - loss: 0.1685 - accuracy: 0.9310 - val_loss: 0.2369 - val_accuracy: 0.9039
Epoch 135/200
3808/4118 [==========================&gt;...] - ETA: 0s - loss: 0.1121 - accuracy: 0.9590
Epoch 00135: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 65us/sample - loss: 0.1125 - accuracy: 0.9592 - val_loss: 0.0950 - val_accuracy: 0.9757
Epoch 136/200
3744/4118 [==========================&gt;...] - ETA: 0s - loss: 0.1307 - accuracy: 0.9447
Epoch 00136: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 66us/sample - loss: 0.1554 - accuracy: 0.9381 - val_loss: 0.3808 - val_accuracy: 0.8437
Epoch 137/200
3488/4118 [========================&gt;.....] - ETA: 0s - loss: 0.3016 - accuracy: 0.8962
Epoch 00137: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 66us/sample - loss: 0.2765 - accuracy: 0.9031 - val_loss: 0.1705 - val_accuracy: 0.9417
Epoch 138/200
3424/4118 [=======================&gt;......] - ETA: 0s - loss: 0.1135 - accuracy: 0.9553
Epoch 00138: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 76us/sample - loss: 0.1150 - accuracy: 0.9541 - val_loss: 0.0964 - val_accuracy: 0.9738
Epoch 139/200
3392/4118 [=======================&gt;......] - ETA: 0s - loss: 0.1671 - accuracy: 0.9407
Epoch 00139: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 69us/sample - loss: 0.1558 - accuracy: 0.9437 - val_loss: 0.2066 - val_accuracy: 0.9359
Epoch 140/200
3392/4118 [=======================&gt;......] - ETA: 0s - loss: 0.0991 - accuracy: 0.9620
Epoch 00140: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 69us/sample - loss: 0.1002 - accuracy: 0.9614 - val_loss: 0.1318 - val_accuracy: 0.9592
Epoch 141/200
4064/4118 [============================&gt;.] - ETA: 0s - loss: 0.0923 - accuracy: 0.9665
Epoch 00141: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 79us/sample - loss: 0.0918 - accuracy: 0.9670 - val_loss: 0.0868 - val_accuracy: 0.9835
Epoch 142/200
3488/4118 [========================&gt;.....] - ETA: 0s - loss: 0.1387 - accuracy: 0.9455
Epoch 00142: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 68us/sample - loss: 0.1481 - accuracy: 0.9415 - val_loss: 0.1645 - val_accuracy: 0.9408
Epoch 143/200
3584/4118 [=========================&gt;....] - ETA: 0s - loss: 0.1578 - accuracy: 0.9397
Epoch 00143: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 68us/sample - loss: 0.1487 - accuracy: 0.9432 - val_loss: 0.1685 - val_accuracy: 0.9417
Epoch 144/200
3808/4118 [==========================&gt;...] - ETA: 0s - loss: 0.0834 - accuracy: 0.9677
Epoch 00144: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 65us/sample - loss: 0.0848 - accuracy: 0.9670 - val_loss: 0.3557 - val_accuracy: 0.8495
Epoch 145/200
3808/4118 [==========================&gt;...] - ETA: 0s - loss: 0.1806 - accuracy: 0.9301
Epoch 00145: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 65us/sample - loss: 0.1731 - accuracy: 0.9330 - val_loss: 0.2528 - val_accuracy: 0.8893
Epoch 146/200
3936/4118 [===========================&gt;..] - ETA: 0s - loss: 0.0954 - accuracy: 0.9609
Epoch 00146: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 64us/sample - loss: 0.0957 - accuracy: 0.9607 - val_loss: 0.2369 - val_accuracy: 0.8932
Epoch 147/200
3872/4118 [===========================&gt;..] - ETA: 0s - loss: 0.1369 - accuracy: 0.9473
Epoch 00147: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 64us/sample - loss: 0.1343 - accuracy: 0.9492 - val_loss: 0.1784 - val_accuracy: 0.9282
Epoch 148/200
3584/4118 [=========================&gt;....] - ETA: 0s - loss: 0.1559 - accuracy: 0.9434
Epoch 00148: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 67us/sample - loss: 0.1495 - accuracy: 0.9444 - val_loss: 0.1611 - val_accuracy: 0.9359
Epoch 149/200
3584/4118 [=========================&gt;....] - ETA: 0s - loss: 0.1219 - accuracy: 0.9562
Epoch 00149: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 68us/sample - loss: 0.1215 - accuracy: 0.9543 - val_loss: 0.0525 - val_accuracy: 0.9835
Epoch 150/200
3648/4118 [=========================&gt;....] - ETA: 0s - loss: 0.1430 - accuracy: 0.9457
Epoch 00150: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 65us/sample - loss: 0.1368 - accuracy: 0.9478 - val_loss: 0.2590 - val_accuracy: 0.9107
Epoch 151/200
3584/4118 [=========================&gt;....] - ETA: 0s - loss: 0.1437 - accuracy: 0.9453
Epoch 00151: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 69us/sample - loss: 0.1438 - accuracy: 0.9458 - val_loss: 0.1326 - val_accuracy: 0.9583
Epoch 152/200
3680/4118 [=========================&gt;....] - ETA: 0s - loss: 0.0926 - accuracy: 0.9639
Epoch 00152: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 71us/sample - loss: 0.0926 - accuracy: 0.9631 - val_loss: 0.2237 - val_accuracy: 0.9058
Epoch 153/200
3840/4118 [==========================&gt;...] - ETA: 0s - loss: 0.0685 - accuracy: 0.9771
Epoch 00153: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 64us/sample - loss: 0.0704 - accuracy: 0.9767 - val_loss: 0.1433 - val_accuracy: 0.9447
Epoch 154/200
3584/4118 [=========================&gt;....] - ETA: 0s - loss: 0.0788 - accuracy: 0.9696
Epoch 00154: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 67us/sample - loss: 0.0824 - accuracy: 0.9689 - val_loss: 0.2121 - val_accuracy: 0.9262
Epoch 155/200
3616/4118 [=========================&gt;....] - ETA: 0s - loss: 0.1205 - accuracy: 0.9563
Epoch 00155: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 68us/sample - loss: 0.1216 - accuracy: 0.9558 - val_loss: 0.2728 - val_accuracy: 0.8845
Epoch 156/200
3904/4118 [===========================&gt;..] - ETA: 0s - loss: 0.1246 - accuracy: 0.9524
Epoch 00156: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 64us/sample - loss: 0.1288 - accuracy: 0.9519 - val_loss: 0.3575 - val_accuracy: 0.8427
Epoch 157/200
3584/4118 [=========================&gt;....] - ETA: 0s - loss: 0.1899 - accuracy: 0.9286
Epoch 00157: val_loss did not improve from 0.04632
4118/4118 [==============================] - 0s 68us/sample - loss: 0.1895 - accuracy: 0.9293 - val_loss: 0.1460 - val_accuracy: 0.9534
Epoch 158/200
3712/4118 [==========================&gt;...] - ETA: 0s - loss: 0.1182 - accuracy: 0.9526
Epoch 00158: val_loss improved from 0.04632 to 0.03695, saving model to best_model_2.h5
4118/4118 [==============================] - 0s 71us/sample - loss: 0.1155 - accuracy: 0.9536 - val_loss: 0.0370 - val_accuracy: 0.9942
Epoch 159/200
3680/4118 [=========================&gt;....] - ETA: 0s - loss: 0.0788 - accuracy: 0.9720
Epoch 00159: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 68us/sample - loss: 0.0762 - accuracy: 0.9733 - val_loss: 0.1653 - val_accuracy: 0.9350
Epoch 160/200
3680/4118 [=========================&gt;....] - ETA: 0s - loss: 0.0563 - accuracy: 0.9815
Epoch 00160: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 67us/sample - loss: 0.0573 - accuracy: 0.9808 - val_loss: 0.0885 - val_accuracy: 0.9738
Epoch 161/200
3776/4118 [==========================&gt;...] - ETA: 0s - loss: 0.0930 - accuracy: 0.9627
Epoch 00161: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 66us/sample - loss: 0.0980 - accuracy: 0.9614 - val_loss: 0.4526 - val_accuracy: 0.8029
Epoch 162/200
3744/4118 [==========================&gt;...] - ETA: 0s - loss: 0.1965 - accuracy: 0.9241
Epoch 00162: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 66us/sample - loss: 0.1989 - accuracy: 0.9235 - val_loss: 0.1931 - val_accuracy: 0.9408
Epoch 163/200
3744/4118 [==========================&gt;...] - ETA: 0s - loss: 0.1915 - accuracy: 0.9356
Epoch 00163: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 69us/sample - loss: 0.1972 - accuracy: 0.9330 - val_loss: 0.8868 - val_accuracy: 0.6563
Epoch 164/200
3776/4118 [==========================&gt;...] - ETA: 0s - loss: 0.1637 - accuracy: 0.9396
Epoch 00164: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 65us/sample - loss: 0.1569 - accuracy: 0.9415 - val_loss: 0.0655 - val_accuracy: 0.9835
Epoch 165/200
3488/4118 [========================&gt;.....] - ETA: 0s - loss: 0.0630 - accuracy: 0.9785
Epoch 00165: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 68us/sample - loss: 0.0672 - accuracy: 0.9760 - val_loss: 0.2271 - val_accuracy: 0.9049
Epoch 166/200
3456/4118 [========================&gt;.....] - ETA: 0s - loss: 0.0555 - accuracy: 0.9800
Epoch 00166: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 69us/sample - loss: 0.0584 - accuracy: 0.9791 - val_loss: 0.1141 - val_accuracy: 0.9631
Epoch 167/200
3328/4118 [=======================&gt;......] - ETA: 0s - loss: 0.1047 - accuracy: 0.9618
Epoch 00167: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 77us/sample - loss: 0.1053 - accuracy: 0.9609 - val_loss: 0.3125 - val_accuracy: 0.8981
Epoch 168/200
3904/4118 [===========================&gt;..] - ETA: 0s - loss: 0.0830 - accuracy: 0.9662
Epoch 00168: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 76us/sample - loss: 0.0830 - accuracy: 0.9655 - val_loss: 0.0499 - val_accuracy: 0.9835
Epoch 169/200
3520/4118 [========================&gt;.....] - ETA: 0s - loss: 0.0646 - accuracy: 0.9767
Epoch 00169: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 69us/sample - loss: 0.0737 - accuracy: 0.9752 - val_loss: 0.1078 - val_accuracy: 0.9709
Epoch 170/200
3424/4118 [=======================&gt;......] - ETA: 0s - loss: 0.0676 - accuracy: 0.9778
Epoch 00170: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 69us/sample - loss: 0.0645 - accuracy: 0.9786 - val_loss: 0.1059 - val_accuracy: 0.9592
Epoch 171/200
3392/4118 [=======================&gt;......] - ETA: 0s - loss: 0.0707 - accuracy: 0.9723
Epoch 00171: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 73us/sample - loss: 0.0797 - accuracy: 0.9682 - val_loss: 0.1743 - val_accuracy: 0.9476
Epoch 172/200
3712/4118 [==========================&gt;...] - ETA: 0s - loss: 0.1953 - accuracy: 0.9289
Epoch 00172: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 85us/sample - loss: 0.1986 - accuracy: 0.9274 - val_loss: 0.2593 - val_accuracy: 0.9146
Epoch 173/200
3968/4118 [===========================&gt;..] - ETA: 0s - loss: 0.3224 - accuracy: 0.9042
Epoch 00173: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 103us/sample - loss: 0.3204 - accuracy: 0.9036 - val_loss: 0.2556 - val_accuracy: 0.9243
Epoch 174/200
3648/4118 [=========================&gt;....] - ETA: 0s - loss: 0.1515 - accuracy: 0.9416
Epoch 00174: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 117us/sample - loss: 0.1431 - accuracy: 0.9439 - val_loss: 0.0757 - val_accuracy: 0.9738
Epoch 175/200
4032/4118 [============================&gt;.] - ETA: 0s - loss: 0.0562 - accuracy: 0.9834
Epoch 00175: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 113us/sample - loss: 0.0557 - accuracy: 0.9837 - val_loss: 0.0655 - val_accuracy: 0.9757
Epoch 176/200
3616/4118 [=========================&gt;....] - ETA: 0s - loss: 0.0515 - accuracy: 0.9815
Epoch 00176: val_loss did not improve from 0.03695
4118/4118 [==============================] - 1s 140us/sample - loss: 0.0511 - accuracy: 0.9825 - val_loss: 0.1446 - val_accuracy: 0.9485
Epoch 177/200
3552/4118 [========================&gt;.....] - ETA: 0s - loss: 0.0549 - accuracy: 0.9825
Epoch 00177: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 94us/sample - loss: 0.0569 - accuracy: 0.9803 - val_loss: 0.0889 - val_accuracy: 0.9660
Epoch 178/200
3648/4118 [=========================&gt;....] - ETA: 0s - loss: 0.1186 - accuracy: 0.9529
Epoch 00178: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 99us/sample - loss: 0.1193 - accuracy: 0.9534 - val_loss: 0.2490 - val_accuracy: 0.8971
Epoch 179/200
3584/4118 [=========================&gt;....] - ETA: 0s - loss: 0.1273 - accuracy: 0.9540
Epoch 00179: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 79us/sample - loss: 0.1222 - accuracy: 0.9558 - val_loss: 0.0762 - val_accuracy: 0.9757
Epoch 180/200
3328/4118 [=======================&gt;......] - ETA: 0s - loss: 0.0531 - accuracy: 0.9829
Epoch 00180: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 69us/sample - loss: 0.0579 - accuracy: 0.9808 - val_loss: 0.0734 - val_accuracy: 0.9777
Epoch 181/200
4096/4118 [============================&gt;.] - ETA: 0s - loss: 0.0955 - accuracy: 0.9656
Epoch 00181: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 77us/sample - loss: 0.0969 - accuracy: 0.9648 - val_loss: 0.3006 - val_accuracy: 0.8796
Epoch 182/200
3392/4118 [=======================&gt;......] - ETA: 0s - loss: 0.1152 - accuracy: 0.9575
Epoch 00182: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 70us/sample - loss: 0.1180 - accuracy: 0.9548 - val_loss: 0.4389 - val_accuracy: 0.8019
Epoch 183/200
3456/4118 [========================&gt;.....] - ETA: 0s - loss: 0.1353 - accuracy: 0.9479
Epoch 00183: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 67us/sample - loss: 0.1258 - accuracy: 0.9526 - val_loss: 0.1068 - val_accuracy: 0.9660
Epoch 184/200
3392/4118 [=======================&gt;......] - ETA: 0s - loss: 0.0635 - accuracy: 0.9761
Epoch 00184: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 70us/sample - loss: 0.0635 - accuracy: 0.9747 - val_loss: 0.1059 - val_accuracy: 0.9738
Epoch 185/200
3520/4118 [========================&gt;.....] - ETA: 0s - loss: 0.0651 - accuracy: 0.9759
Epoch 00185: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 72us/sample - loss: 0.0657 - accuracy: 0.9755 - val_loss: 0.1830 - val_accuracy: 0.9330
Epoch 186/200
4096/4118 [============================&gt;.] - ETA: 0s - loss: 0.0636 - accuracy: 0.9751
Epoch 00186: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 77us/sample - loss: 0.0662 - accuracy: 0.9747 - val_loss: 0.2019 - val_accuracy: 0.9165
Epoch 187/200
3392/4118 [=======================&gt;......] - ETA: 0s - loss: 0.0831 - accuracy: 0.9676
Epoch 00187: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 73us/sample - loss: 0.0909 - accuracy: 0.9658 - val_loss: 0.0690 - val_accuracy: 0.9796
Epoch 188/200
3840/4118 [==========================&gt;...] - ETA: 0s - loss: 0.1029 - accuracy: 0.9648
Epoch 00188: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 97us/sample - loss: 0.1226 - accuracy: 0.9597 - val_loss: 0.3603 - val_accuracy: 0.8340
Epoch 189/200
4032/4118 [============================&gt;.] - ETA: 0s - loss: 0.1622 - accuracy: 0.9373
Epoch 00189: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 110us/sample - loss: 0.1616 - accuracy: 0.9376 - val_loss: 0.2846 - val_accuracy: 0.8806
Epoch 190/200
3808/4118 [==========================&gt;...] - ETA: 0s - loss: 0.1382 - accuracy: 0.9519
Epoch 00190: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 121us/sample - loss: 0.1323 - accuracy: 0.9534 - val_loss: 0.1074 - val_accuracy: 0.9641
Epoch 191/200
3904/4118 [===========================&gt;..] - ETA: 0s - loss: 0.0554 - accuracy: 0.9793
Epoch 00191: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 93us/sample - loss: 0.0548 - accuracy: 0.9798 - val_loss: 0.0625 - val_accuracy: 0.9825
Epoch 192/200
3552/4118 [========================&gt;.....] - ETA: 0s - loss: 0.0428 - accuracy: 0.9842 ETA: 0s - loss: 0.0387 - accuracy
Epoch 00192: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 84us/sample - loss: 0.0428 - accuracy: 0.9847 - val_loss: 0.1222 - val_accuracy: 0.9583
Epoch 193/200
4096/4118 [============================&gt;.] - ETA: 0s - loss: 0.0341 - accuracy: 0.9880
Epoch 00193: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 96us/sample - loss: 0.0342 - accuracy: 0.9881 - val_loss: 0.0561 - val_accuracy: 0.9893
Epoch 194/200
3584/4118 [=========================&gt;....] - ETA: 0s - loss: 0.0350 - accuracy: 0.9891
Epoch 00194: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 82us/sample - loss: 0.0376 - accuracy: 0.9876 - val_loss: 0.1347 - val_accuracy: 0.9544
Epoch 195/200
4096/4118 [============================&gt;.] - ETA: 0s - loss: 0.1153 - accuracy: 0.9578
Epoch 00195: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 89us/sample - loss: 0.1152 - accuracy: 0.9577 - val_loss: 0.0945 - val_accuracy: 0.9728
Epoch 196/200
4032/4118 [============================&gt;.] - ETA: 0s - loss: 0.2589 - accuracy: 0.9144
Epoch 00196: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 77us/sample - loss: 0.2605 - accuracy: 0.9140 - val_loss: 0.5287 - val_accuracy: 0.8097
Epoch 197/200
3360/4118 [=======================&gt;......] - ETA: 0s - loss: 0.1025 - accuracy: 0.9622
Epoch 00197: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 71us/sample - loss: 0.0933 - accuracy: 0.9655 - val_loss: 0.1944 - val_accuracy: 0.9262
Epoch 198/200
3360/4118 [=======================&gt;......] - ETA: 0s - loss: 0.0789 - accuracy: 0.9735
Epoch 00198: val_loss did not improve from 0.03695
4118/4118 [==============================] - 0s 73us/sample - loss: 0.0764 - accuracy: 0.9745 - val_loss: 0.2955 - val_accuracy: 0.8883
Epoch 199/200
4032/4118 [============================&gt;.] - ETA: 0s - loss: 0.0412 - accuracy: 0.9876
Epoch 00199: val_loss improved from 0.03695 to 0.03632, saving model to best_model_2.h5
4118/4118 [==============================] - 0s 81us/sample - loss: 0.0412 - accuracy: 0.9876 - val_loss: 0.0363 - val_accuracy: 0.9903
Epoch 200/200
3392/4118 [=======================&gt;......] - ETA: 0s - loss: 0.0592 - accuracy: 0.9767
Epoch 00200: val_loss did not improve from 0.03632
4118/4118 [==============================] - 0s 73us/sample - loss: 0.0637 - accuracy: 0.9750 - val_loss: 0.0596 - val_accuracy: 0.9825
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4>Result</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[55]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">number_of_learners</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">score_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;MLPRegressor&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">,</span><span class="s2">&quot;&quot;</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">number_of_learners</span><span class="p">):</span>
    <span class="n">score_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">score_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">4</span><span class="p">,</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
<span class="n">display</span><span class="p">(</span><span class="n">score_df</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MLPRegressor</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>186.599299</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>118.379617</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>121.663151</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>104.193848</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>11.519938</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><h3>Classifier Task</h3></p>
<p>The classifier task for M2 is using the second design matrix with logistic regression. The selection is based on the empirical results we have from testing above. Logistic regression able to have a better score probably due to better generalization over the other methods.</p><p><br>Steps:</p>
<p><ol>
    <li><b>SMOTE</b></li>
    <li><b>StandardScaler</b></li>
    <li><b>SelectKBest</b></li>
    <li><b>PCA</b></li>
    <li><b>Lasso Classifier</b></li>
</ol></p>
<p><i>See Model Building for M1 to see the details of each step and the empirical test results</i></p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[22]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># import all folds</span>
<span class="c1"># Modify into Design Matrix 2</span>
<span class="n">x_fold</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">y_fold</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">non_bin_feat</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">string</span> <span class="o">=</span> <span class="s2">&quot;../Project_Data/Fold&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="n">y_file</span> <span class="o">=</span> <span class="s2">&quot;../Project_Data/Fold&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">+</span><span class="s2">&quot;_Outcomes.csv&quot;</span>
    <span class="n">x_fold</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">preprocess_x_for_design_matrix_2</span><span class="p">(</span><span class="n">put_single_into_dataframe</span><span class="p">(</span><span class="n">read_text</span><span class="p">(</span><span class="n">string</span><span class="p">)))</span>
    <span class="n">y_fold</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">y_file</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Gender</th>
      <th>Height</th>
      <th>Weight</th>
      <th>Bilirubin</th>
      <th>AST</th>
      <th>ALT</th>
      <th>ALP</th>
      <th>Albumin</th>
      <th>TroponinT</th>
      <th>TroponinI</th>
      <th>Cholesterol</th>
      <th>MechVent</th>
      <th>HR0</th>
      <th>HR12</th>
      <th>HR24</th>
      <th>HR36</th>
      <th>MAP0</th>
      <th>MAP12</th>
      <th>MAP24</th>
      <th>MAP36</th>
      <th>SysABP0</th>
      <th>SysABP12</th>
      <th>SysABP24</th>
      <th>SysABP36</th>
      <th>DiasABP0</th>
      <th>DiasABP12</th>
      <th>DiasABP24</th>
      <th>DiasABP36</th>
      <th>Urine0</th>
      <th>Urine12</th>
      <th>Urine24</th>
      <th>Urine36</th>
      <th>Weight0</th>
      <th>Weight12</th>
      <th>Weight24</th>
      <th>Weight36</th>
      <th>NISysABP0</th>
      <th>NISysABP12</th>
      <th>NISysABP24</th>
      <th>NISysABP36</th>
      <th>NIDiasABP0</th>
      <th>NIDiasABP12</th>
      <th>NIDiasABP24</th>
      <th>NIDiasABP36</th>
      <th>NIMAP0</th>
      <th>NIMAP12</th>
      <th>NIMAP24</th>
      <th>NIMAP36</th>
      <th>Temp0</th>
      <th>Temp12</th>
      <th>Temp24</th>
      <th>Temp36</th>
      <th>GCS0</th>
      <th>GCS12</th>
      <th>GCS24</th>
      <th>GCS36</th>
      <th>RespRate0</th>
      <th>RespRate12</th>
      <th>RespRate24</th>
      <th>RespRate36</th>
      <th>FiO20</th>
      <th>FiO212</th>
      <th>FiO224</th>
      <th>FiO236</th>
      <th>pH0</th>
      <th>pH12</th>
      <th>pH24</th>
      <th>pH36</th>
      <th>PaO20</th>
      <th>PaO212</th>
      <th>PaO224</th>
      <th>PaO236</th>
      <th>PaCO20</th>
      <th>PaCO212</th>
      <th>PaCO224</th>
      <th>PaCO236</th>
      <th>HCT0</th>
      <th>HCT12</th>
      <th>HCT24</th>
      <th>HCT36</th>
      <th>K0</th>
      <th>K12</th>
      <th>K24</th>
      <th>K36</th>
      <th>Creatinine0</th>
      <th>Creatinine12</th>
      <th>Creatinine24</th>
      <th>Creatinine36</th>
      <th>Platelets0</th>
      <th>Platelets12</th>
      <th>Platelets24</th>
      <th>Platelets36</th>
      <th>BUN0</th>
      <th>BUN12</th>
      <th>BUN24</th>
      <th>BUN36</th>
      <th>HCO30</th>
      <th>HCO312</th>
      <th>HCO324</th>
      <th>HCO336</th>
      <th>Mg0</th>
      <th>Mg12</th>
      <th>Mg24</th>
      <th>Mg36</th>
      <th>Na0</th>
      <th>Na12</th>
      <th>Na24</th>
      <th>Na36</th>
      <th>Glucose0</th>
      <th>Glucose12</th>
      <th>Glucose24</th>
      <th>Glucose36</th>
      <th>WBC0</th>
      <th>WBC12</th>
      <th>WBC24</th>
      <th>WBC36</th>
      <th>SaO20</th>
      <th>SaO212</th>
      <th>SaO224</th>
      <th>SaO236</th>
      <th>Lactate0</th>
      <th>Lactate12</th>
      <th>Lactate24</th>
      <th>Lactate36</th>
      <th>Coronary Care Unit</th>
      <th>Cardiac Surgery Recovery Unit</th>
      <th>Medical ICU</th>
      <th>Surgical ICU</th>
    </tr>
    <tr>
      <th>recordid</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>132539</td>
      <td>54</td>
      <td>0</td>
      <td>170.094476</td>
      <td>81.422068</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>65.454545</td>
      <td>66.090909</td>
      <td>78.833333</td>
      <td>78.571429</td>
      <td>80.711313</td>
      <td>79.358527</td>
      <td>80.922914</td>
      <td>81.500853</td>
      <td>115.156032</td>
      <td>118.234776</td>
      <td>120.376778</td>
      <td>121.372441</td>
      <td>58.554776</td>
      <td>58.822952</td>
      <td>59.476010</td>
      <td>59.876609</td>
      <td>92.000000</td>
      <td>86.250000</td>
      <td>173.333333</td>
      <td>308.333333</td>
      <td>82.648197</td>
      <td>83.383737</td>
      <td>83.651991</td>
      <td>83.759171</td>
      <td>111.700000</td>
      <td>104.125000</td>
      <td>111.333333</td>
      <td>120.875000</td>
      <td>50.600000</td>
      <td>45.125000</td>
      <td>44.333333</td>
      <td>56.125000</td>
      <td>70.967000</td>
      <td>64.792500</td>
      <td>66.666667</td>
      <td>77.708750</td>
      <td>37.833333</td>
      <td>37.100000</td>
      <td>38.066667</td>
      <td>37.766667</td>
      <td>15.000000</td>
      <td>15.0</td>
      <td>14.666667</td>
      <td>15.00</td>
      <td>16.545455</td>
      <td>15.545455</td>
      <td>17.857143</td>
      <td>19.636364</td>
      <td>0.575987</td>
      <td>0.509855</td>
      <td>0.506165</td>
      <td>0.500966</td>
      <td>7.422229</td>
      <td>7.390886</td>
      <td>7.565855</td>
      <td>7.394966</td>
      <td>167.119732</td>
      <td>128.397906</td>
      <td>121.516945</td>
      <td>119.794255</td>
      <td>40.839626</td>
      <td>39.716565</td>
      <td>39.386531</td>
      <td>39.795741</td>
      <td>33.600</td>
      <td>31.031831</td>
      <td>30.30</td>
      <td>30.158595</td>
      <td>4.400000</td>
      <td>4.168752</td>
      <td>4.000000</td>
      <td>4.114329</td>
      <td>0.8</td>
      <td>1.507457</td>
      <td>0.70000</td>
      <td>1.479688</td>
      <td>221.000000</td>
      <td>201.597978</td>
      <td>185.000000</td>
      <td>180.177778</td>
      <td>13.0</td>
      <td>26.8623</td>
      <td>8.000000</td>
      <td>27.386189</td>
      <td>26.0</td>
      <td>23.545873</td>
      <td>28.000000</td>
      <td>24.082561</td>
      <td>1.5</td>
      <td>2.038096</td>
      <td>1.900000</td>
      <td>2.070901</td>
      <td>137.000000</td>
      <td>139.053437</td>
      <td>136.000000</td>
      <td>138.672271</td>
      <td>205.000000</td>
      <td>131.459169</td>
      <td>115.000000</td>
      <td>133.509428</td>
      <td>11.2</td>
      <td>12.730003</td>
      <td>9.400000</td>
      <td>12.236603</td>
      <td>97.184859</td>
      <td>96.87319</td>
      <td>96.396396</td>
      <td>96.604748</td>
      <td>2.575443</td>
      <td>2.395522</td>
      <td>2.447271</td>
      <td>2.711155</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <td>132540</td>
      <td>76</td>
      <td>1</td>
      <td>175.300000</td>
      <td>76.000000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>87.076923</td>
      <td>80.055556</td>
      <td>75.307692</td>
      <td>73.636364</td>
      <td>75.740741</td>
      <td>76.111111</td>
      <td>78.384615</td>
      <td>80.111111</td>
      <td>106.714286</td>
      <td>114.222222</td>
      <td>119.923077</td>
      <td>123.222222</td>
      <td>60.892857</td>
      <td>56.888889</td>
      <td>57.846154</td>
      <td>58.222222</td>
      <td>158.933333</td>
      <td>103.181818</td>
      <td>170.714286</td>
      <td>187.500000</td>
      <td>82.648197</td>
      <td>80.600000</td>
      <td>80.600000</td>
      <td>81.236364</td>
      <td>117.173844</td>
      <td>111.750000</td>
      <td>115.142857</td>
      <td>107.333333</td>
      <td>57.986820</td>
      <td>57.500000</td>
      <td>61.000000</td>
      <td>45.666667</td>
      <td>76.655203</td>
      <td>75.580000</td>
      <td>79.047143</td>
      <td>66.223333</td>
      <td>36.688462</td>
      <td>37.500000</td>
      <td>36.850000</td>
      <td>36.800000</td>
      <td>10.800000</td>
      <td>15.0</td>
      <td>14.666667</td>
      <td>14.25</td>
      <td>19.613425</td>
      <td>19.551701</td>
      <td>19.646032</td>
      <td>19.636608</td>
      <td>0.560000</td>
      <td>0.509855</td>
      <td>0.506165</td>
      <td>0.500966</td>
      <td>7.385000</td>
      <td>7.400000</td>
      <td>7.565855</td>
      <td>7.385000</td>
      <td>226.250000</td>
      <td>128.397906</td>
      <td>121.516945</td>
      <td>111.000000</td>
      <td>37.000000</td>
      <td>39.716565</td>
      <td>39.386531</td>
      <td>45.000000</td>
      <td>27.625</td>
      <td>28.900000</td>
      <td>30.70</td>
      <td>29.450000</td>
      <td>4.154586</td>
      <td>4.300000</td>
      <td>4.057783</td>
      <td>3.500000</td>
      <td>0.8</td>
      <td>1.200000</td>
      <td>1.50415</td>
      <td>1.300000</td>
      <td>190.333333</td>
      <td>187.000000</td>
      <td>191.404915</td>
      <td>135.000000</td>
      <td>16.0</td>
      <td>18.0000</td>
      <td>28.141693</td>
      <td>21.000000</td>
      <td>21.0</td>
      <td>22.000000</td>
      <td>23.815756</td>
      <td>24.000000</td>
      <td>3.1</td>
      <td>1.900000</td>
      <td>2.061273</td>
      <td>2.100000</td>
      <td>139.341443</td>
      <td>139.000000</td>
      <td>139.309433</td>
      <td>135.000000</td>
      <td>150.083243</td>
      <td>105.000000</td>
      <td>132.354891</td>
      <td>146.000000</td>
      <td>7.4</td>
      <td>13.100000</td>
      <td>12.469809</td>
      <td>13.300000</td>
      <td>98.000000</td>
      <td>97.00000</td>
      <td>96.396396</td>
      <td>95.000000</td>
      <td>2.575443</td>
      <td>2.395522</td>
      <td>2.447271</td>
      <td>2.711155</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>132541</td>
      <td>44</td>
      <td>0</td>
      <td>170.094476</td>
      <td>56.700000</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>90.000000</td>
      <td>83.250000</td>
      <td>87.666667</td>
      <td>71.166667</td>
      <td>80.711313</td>
      <td>79.358527</td>
      <td>100.000000</td>
      <td>87.250000</td>
      <td>115.156032</td>
      <td>118.234776</td>
      <td>137.500000</td>
      <td>121.750000</td>
      <td>58.554776</td>
      <td>58.822952</td>
      <td>75.500000</td>
      <td>64.333333</td>
      <td>111.818182</td>
      <td>185.500000</td>
      <td>150.000000</td>
      <td>58.800000</td>
      <td>56.700000</td>
      <td>56.700000</td>
      <td>56.700000</td>
      <td>56.700000</td>
      <td>132.812500</td>
      <td>136.750000</td>
      <td>124.500000</td>
      <td>119.505340</td>
      <td>78.750000</td>
      <td>80.583333</td>
      <td>76.000000</td>
      <td>57.339935</td>
      <td>96.767500</td>
      <td>99.305000</td>
      <td>92.163750</td>
      <td>76.900645</td>
      <td>37.825000</td>
      <td>37.233333</td>
      <td>38.300000</td>
      <td>37.833333</td>
      <td>7.333333</td>
      <td>6.0</td>
      <td>5.000000</td>
      <td>5.00</td>
      <td>19.613425</td>
      <td>19.551701</td>
      <td>19.646032</td>
      <td>19.636608</td>
      <td>0.750000</td>
      <td>0.500000</td>
      <td>0.460000</td>
      <td>0.400000</td>
      <td>7.422229</td>
      <td>7.510000</td>
      <td>7.490000</td>
      <td>7.394966</td>
      <td>167.119732</td>
      <td>65.000000</td>
      <td>157.666667</td>
      <td>119.794255</td>
      <td>40.839626</td>
      <td>37.000000</td>
      <td>35.000000</td>
      <td>39.795741</td>
      <td>28.500</td>
      <td>26.700000</td>
      <td>28.85</td>
      <td>29.400000</td>
      <td>3.300000</td>
      <td>8.600000</td>
      <td>2.850000</td>
      <td>3.700000</td>
      <td>0.4</td>
      <td>0.300000</td>
      <td>1.50415</td>
      <td>0.300000</td>
      <td>72.000000</td>
      <td>84.000000</td>
      <td>191.404915</td>
      <td>113.000000</td>
      <td>8.0</td>
      <td>3.0000</td>
      <td>28.141693</td>
      <td>3.000000</td>
      <td>24.0</td>
      <td>26.000000</td>
      <td>23.815756</td>
      <td>25.000000</td>
      <td>1.9</td>
      <td>1.300000</td>
      <td>1.850000</td>
      <td>1.700000</td>
      <td>137.000000</td>
      <td>140.000000</td>
      <td>139.309433</td>
      <td>138.000000</td>
      <td>141.000000</td>
      <td>119.000000</td>
      <td>132.354891</td>
      <td>143.000000</td>
      <td>4.2</td>
      <td>3.700000</td>
      <td>12.469809</td>
      <td>6.200000</td>
      <td>97.184859</td>
      <td>95.00000</td>
      <td>96.396396</td>
      <td>96.604748</td>
      <td>1.300000</td>
      <td>1.900000</td>
      <td>0.900000</td>
      <td>2.711155</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>132543</td>
      <td>68</td>
      <td>1</td>
      <td>180.300000</td>
      <td>84.600000</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>72.238095</td>
      <td>72.500000</td>
      <td>63.769231</td>
      <td>74.000000</td>
      <td>80.711313</td>
      <td>79.358527</td>
      <td>80.922914</td>
      <td>81.500853</td>
      <td>115.156032</td>
      <td>118.234776</td>
      <td>120.376778</td>
      <td>121.372441</td>
      <td>58.554776</td>
      <td>58.822952</td>
      <td>59.476010</td>
      <td>59.876609</td>
      <td>600.000000</td>
      <td>400.000000</td>
      <td>675.000000</td>
      <td>600.000000</td>
      <td>84.600000</td>
      <td>84.600000</td>
      <td>84.600000</td>
      <td>84.600000</td>
      <td>122.571429</td>
      <td>122.090909</td>
      <td>117.750000</td>
      <td>121.545455</td>
      <td>68.285714</td>
      <td>61.636364</td>
      <td>61.500000</td>
      <td>66.181818</td>
      <td>86.381429</td>
      <td>81.788182</td>
      <td>80.250000</td>
      <td>84.637273</td>
      <td>35.966667</td>
      <td>36.433333</td>
      <td>36.133333</td>
      <td>36.333333</td>
      <td>14.500000</td>
      <td>15.0</td>
      <td>15.000000</td>
      <td>15.00</td>
      <td>16.100000</td>
      <td>14.916667</td>
      <td>13.384615</td>
      <td>16.750000</td>
      <td>0.575987</td>
      <td>0.509855</td>
      <td>0.506165</td>
      <td>0.500966</td>
      <td>7.422229</td>
      <td>7.390886</td>
      <td>7.565855</td>
      <td>7.394966</td>
      <td>167.119732</td>
      <td>128.397906</td>
      <td>121.516945</td>
      <td>119.794255</td>
      <td>40.839626</td>
      <td>39.716565</td>
      <td>39.386531</td>
      <td>39.795741</td>
      <td>37.300</td>
      <td>36.850000</td>
      <td>36.20</td>
      <td>36.300000</td>
      <td>4.200000</td>
      <td>4.168752</td>
      <td>3.800000</td>
      <td>4.114329</td>
      <td>0.7</td>
      <td>1.507457</td>
      <td>0.70000</td>
      <td>1.479688</td>
      <td>315.000000</td>
      <td>201.597978</td>
      <td>284.000000</td>
      <td>180.177778</td>
      <td>20.0</td>
      <td>26.8623</td>
      <td>10.000000</td>
      <td>27.386189</td>
      <td>27.0</td>
      <td>23.545873</td>
      <td>28.000000</td>
      <td>24.082561</td>
      <td>2.1</td>
      <td>2.038096</td>
      <td>1.900000</td>
      <td>2.070901</td>
      <td>141.000000</td>
      <td>139.053437</td>
      <td>137.000000</td>
      <td>138.672271</td>
      <td>106.000000</td>
      <td>131.459169</td>
      <td>117.000000</td>
      <td>133.509428</td>
      <td>8.8</td>
      <td>12.730003</td>
      <td>7.900000</td>
      <td>12.236603</td>
      <td>97.184859</td>
      <td>96.87319</td>
      <td>96.396396</td>
      <td>96.604748</td>
      <td>2.575443</td>
      <td>2.395522</td>
      <td>2.447271</td>
      <td>2.711155</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>132545</td>
      <td>88</td>
      <td>0</td>
      <td>170.094476</td>
      <td>81.422068</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>84.428571</td>
      <td>72.833333</td>
      <td>69.727273</td>
      <td>70.454545</td>
      <td>80.711313</td>
      <td>79.358527</td>
      <td>80.922914</td>
      <td>81.500853</td>
      <td>115.156032</td>
      <td>118.234776</td>
      <td>120.376778</td>
      <td>121.372441</td>
      <td>58.554776</td>
      <td>58.822952</td>
      <td>59.476010</td>
      <td>59.876609</td>
      <td>65.454545</td>
      <td>50.600000</td>
      <td>73.333333</td>
      <td>59.375000</td>
      <td>82.648197</td>
      <td>83.383737</td>
      <td>83.651991</td>
      <td>83.759171</td>
      <td>137.642857</td>
      <td>135.250000</td>
      <td>132.000000</td>
      <td>127.909091</td>
      <td>46.571429</td>
      <td>55.500000</td>
      <td>46.200000</td>
      <td>37.090909</td>
      <td>76.932143</td>
      <td>82.082500</td>
      <td>74.799000</td>
      <td>67.363636</td>
      <td>36.950000</td>
      <td>37.000000</td>
      <td>36.633333</td>
      <td>36.700000</td>
      <td>15.000000</td>
      <td>15.0</td>
      <td>15.000000</td>
      <td>15.00</td>
      <td>20.714286</td>
      <td>18.666667</td>
      <td>17.818182</td>
      <td>19.090909</td>
      <td>0.575987</td>
      <td>0.509855</td>
      <td>0.506165</td>
      <td>0.500966</td>
      <td>7.422229</td>
      <td>7.390886</td>
      <td>7.565855</td>
      <td>7.394966</td>
      <td>167.119732</td>
      <td>128.397906</td>
      <td>121.516945</td>
      <td>119.794255</td>
      <td>40.839626</td>
      <td>39.716565</td>
      <td>39.386531</td>
      <td>39.795741</td>
      <td>22.600</td>
      <td>30.466667</td>
      <td>32.40</td>
      <td>30.900000</td>
      <td>4.900000</td>
      <td>3.850000</td>
      <td>4.100000</td>
      <td>4.114329</td>
      <td>1.0</td>
      <td>1.507457</td>
      <td>1.00000</td>
      <td>1.479688</td>
      <td>109.000000</td>
      <td>201.597978</td>
      <td>97.000000</td>
      <td>180.177778</td>
      <td>45.0</td>
      <td>26.8623</td>
      <td>25.000000</td>
      <td>27.386189</td>
      <td>18.0</td>
      <td>23.545873</td>
      <td>20.000000</td>
      <td>24.082561</td>
      <td>1.5</td>
      <td>2.038096</td>
      <td>1.600000</td>
      <td>2.070901</td>
      <td>140.000000</td>
      <td>139.053437</td>
      <td>139.000000</td>
      <td>138.672271</td>
      <td>113.000000</td>
      <td>131.459169</td>
      <td>92.000000</td>
      <td>133.509428</td>
      <td>3.8</td>
      <td>12.730003</td>
      <td>4.800000</td>
      <td>12.236603</td>
      <td>97.184859</td>
      <td>96.87319</td>
      <td>96.396396</td>
      <td>96.604748</td>
      <td>2.575443</td>
      <td>2.395522</td>
      <td>2.447271</td>
      <td>2.711155</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Gender</th>
      <th>Height</th>
      <th>Weight</th>
      <th>Bilirubin</th>
      <th>AST</th>
      <th>ALT</th>
      <th>ALP</th>
      <th>Albumin</th>
      <th>TroponinT</th>
      <th>TroponinI</th>
      <th>Cholesterol</th>
      <th>MechVent</th>
      <th>HR0</th>
      <th>HR12</th>
      <th>HR24</th>
      <th>HR36</th>
      <th>MAP0</th>
      <th>MAP12</th>
      <th>MAP24</th>
      <th>MAP36</th>
      <th>SysABP0</th>
      <th>SysABP12</th>
      <th>SysABP24</th>
      <th>SysABP36</th>
      <th>DiasABP0</th>
      <th>DiasABP12</th>
      <th>DiasABP24</th>
      <th>DiasABP36</th>
      <th>Urine0</th>
      <th>Urine12</th>
      <th>Urine24</th>
      <th>Urine36</th>
      <th>Weight0</th>
      <th>Weight12</th>
      <th>Weight24</th>
      <th>Weight36</th>
      <th>NISysABP0</th>
      <th>NISysABP12</th>
      <th>NISysABP24</th>
      <th>NISysABP36</th>
      <th>NIDiasABP0</th>
      <th>NIDiasABP12</th>
      <th>NIDiasABP24</th>
      <th>NIDiasABP36</th>
      <th>NIMAP0</th>
      <th>NIMAP12</th>
      <th>NIMAP24</th>
      <th>NIMAP36</th>
      <th>Temp0</th>
      <th>Temp12</th>
      <th>Temp24</th>
      <th>Temp36</th>
      <th>GCS0</th>
      <th>GCS12</th>
      <th>GCS24</th>
      <th>GCS36</th>
      <th>RespRate0</th>
      <th>RespRate12</th>
      <th>RespRate24</th>
      <th>RespRate36</th>
      <th>FiO20</th>
      <th>FiO212</th>
      <th>FiO224</th>
      <th>FiO236</th>
      <th>pH0</th>
      <th>pH12</th>
      <th>pH24</th>
      <th>pH36</th>
      <th>PaO20</th>
      <th>PaO212</th>
      <th>PaO224</th>
      <th>PaO236</th>
      <th>PaCO20</th>
      <th>PaCO212</th>
      <th>PaCO224</th>
      <th>PaCO236</th>
      <th>HCT0</th>
      <th>HCT12</th>
      <th>HCT24</th>
      <th>HCT36</th>
      <th>K0</th>
      <th>K12</th>
      <th>K24</th>
      <th>K36</th>
      <th>Creatinine0</th>
      <th>Creatinine12</th>
      <th>Creatinine24</th>
      <th>Creatinine36</th>
      <th>Platelets0</th>
      <th>Platelets12</th>
      <th>Platelets24</th>
      <th>Platelets36</th>
      <th>BUN0</th>
      <th>BUN12</th>
      <th>BUN24</th>
      <th>BUN36</th>
      <th>HCO30</th>
      <th>HCO312</th>
      <th>HCO324</th>
      <th>HCO336</th>
      <th>Mg0</th>
      <th>Mg12</th>
      <th>Mg24</th>
      <th>Mg36</th>
      <th>Na0</th>
      <th>Na12</th>
      <th>Na24</th>
      <th>Na36</th>
      <th>Glucose0</th>
      <th>Glucose12</th>
      <th>Glucose24</th>
      <th>Glucose36</th>
      <th>WBC0</th>
      <th>WBC12</th>
      <th>WBC24</th>
      <th>WBC36</th>
      <th>SaO20</th>
      <th>SaO212</th>
      <th>SaO224</th>
      <th>SaO236</th>
      <th>Lactate0</th>
      <th>Lactate12</th>
      <th>Lactate24</th>
      <th>Lactate36</th>
      <th>Coronary Care Unit</th>
      <th>Cardiac Surgery Recovery Unit</th>
      <th>Medical ICU</th>
      <th>Surgical ICU</th>
    </tr>
    <tr>
      <th>recordid</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>135076</td>
      <td>56</td>
      <td>1</td>
      <td>177.80000</td>
      <td>110.0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>104.416667</td>
      <td>110.777778</td>
      <td>104.454545</td>
      <td>81.250000</td>
      <td>104.777778</td>
      <td>116.714286</td>
      <td>148.500000</td>
      <td>112.750000</td>
      <td>147.000000</td>
      <td>161.571429</td>
      <td>156.000000</td>
      <td>133.142857</td>
      <td>81.333333</td>
      <td>90.142857</td>
      <td>111.000000</td>
      <td>84.285714</td>
      <td>161.428571</td>
      <td>125.714286</td>
      <td>131.666667</td>
      <td>126.000000</td>
      <td>81.345255</td>
      <td>108.1</td>
      <td>108.1</td>
      <td>108.1</td>
      <td>120.500000</td>
      <td>158.875000</td>
      <td>148.272727</td>
      <td>146.000000</td>
      <td>66.500000</td>
      <td>70.375000</td>
      <td>77.818182</td>
      <td>76.090909</td>
      <td>84.510000</td>
      <td>99.862500</td>
      <td>101.308182</td>
      <td>99.390909</td>
      <td>38.066667</td>
      <td>37.600000</td>
      <td>37.70000</td>
      <td>37.850000</td>
      <td>7.000000</td>
      <td>11.75</td>
      <td>10.166667</td>
      <td>8.2</td>
      <td>19.424873</td>
      <td>19.622286</td>
      <td>19.55034</td>
      <td>19.782182</td>
      <td>0.600000</td>
      <td>0.507617</td>
      <td>0.515098</td>
      <td>0.50479</td>
      <td>7.315000</td>
      <td>7.430</td>
      <td>7.394394</td>
      <td>7.430000</td>
      <td>219.000000</td>
      <td>103.000000</td>
      <td>116.867607</td>
      <td>74.000000</td>
      <td>46.000000</td>
      <td>36.000000</td>
      <td>39.657571</td>
      <td>39.000000</td>
      <td>40.100000</td>
      <td>37.700000</td>
      <td>30.961398</td>
      <td>37.800000</td>
      <td>4.30000</td>
      <td>3.600000</td>
      <td>4.053152</td>
      <td>3.950000</td>
      <td>0.600000</td>
      <td>0.8</td>
      <td>1.449714</td>
      <td>0.700000</td>
      <td>228.000000</td>
      <td>188.00000</td>
      <td>194.81982</td>
      <td>231.00000</td>
      <td>12.00000</td>
      <td>13.0</td>
      <td>26.487741</td>
      <td>15.500000</td>
      <td>23.000000</td>
      <td>25.000000</td>
      <td>23.775188</td>
      <td>25.500000</td>
      <td>1.7</td>
      <td>1.700000</td>
      <td>2.051234</td>
      <td>2.000000</td>
      <td>140.000000</td>
      <td>139.000000</td>
      <td>138.878042</td>
      <td>143.000000</td>
      <td>114.000000</td>
      <td>141.000000</td>
      <td>130.070914</td>
      <td>119.000000</td>
      <td>14.40000</td>
      <td>15.900000</td>
      <td>12.230936</td>
      <td>16.000000</td>
      <td>97.027244</td>
      <td>96.526403</td>
      <td>96.56986</td>
      <td>96.444055</td>
      <td>2.695739</td>
      <td>2.383319</td>
      <td>2.131766</td>
      <td>2.090862</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <td>135077</td>
      <td>72</td>
      <td>1</td>
      <td>169.30233</td>
      <td>220.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>70.428571</td>
      <td>87.714286</td>
      <td>78.250000</td>
      <td>98.727273</td>
      <td>64.285714</td>
      <td>75.714286</td>
      <td>94.250000</td>
      <td>97.272727</td>
      <td>101.000000</td>
      <td>122.071429</td>
      <td>142.000000</td>
      <td>153.000000</td>
      <td>48.428571</td>
      <td>53.714286</td>
      <td>69.166667</td>
      <td>65.636364</td>
      <td>83.500000</td>
      <td>1.363636</td>
      <td>4.133333</td>
      <td>5.230769</td>
      <td>117.142857</td>
      <td>100.0</td>
      <td>100.0</td>
      <td>100.0</td>
      <td>117.325730</td>
      <td>115.694079</td>
      <td>160.000000</td>
      <td>119.579156</td>
      <td>58.746771</td>
      <td>57.478308</td>
      <td>70.000000</td>
      <td>57.771990</td>
      <td>76.931481</td>
      <td>75.786899</td>
      <td>100.000000</td>
      <td>77.191414</td>
      <td>37.766667</td>
      <td>38.280000</td>
      <td>38.24000</td>
      <td>37.700000</td>
      <td>9.000000</td>
      <td>13.80</td>
      <td>15.000000</td>
      <td>15.0</td>
      <td>19.424873</td>
      <td>19.622286</td>
      <td>19.55034</td>
      <td>19.782182</td>
      <td>0.420000</td>
      <td>0.400000</td>
      <td>0.550000</td>
      <td>0.66000</td>
      <td>7.260000</td>
      <td>7.316</td>
      <td>7.320000</td>
      <td>7.258000</td>
      <td>129.500000</td>
      <td>117.800000</td>
      <td>104.000000</td>
      <td>128.000000</td>
      <td>43.500000</td>
      <td>40.000000</td>
      <td>38.000000</td>
      <td>45.600000</td>
      <td>33.300000</td>
      <td>31.147414</td>
      <td>32.100000</td>
      <td>30.054173</td>
      <td>5.70000</td>
      <td>5.000000</td>
      <td>4.900000</td>
      <td>4.800000</td>
      <td>2.200000</td>
      <td>3.9</td>
      <td>4.600000</td>
      <td>5.000000</td>
      <td>167.000000</td>
      <td>199.19211</td>
      <td>150.00000</td>
      <td>176.60216</td>
      <td>24.00000</td>
      <td>31.0</td>
      <td>36.000000</td>
      <td>39.000000</td>
      <td>20.000000</td>
      <td>17.000000</td>
      <td>17.000000</td>
      <td>19.500000</td>
      <td>1.9</td>
      <td>2.052855</td>
      <td>2.100000</td>
      <td>2.200000</td>
      <td>138.000000</td>
      <td>139.000000</td>
      <td>140.000000</td>
      <td>140.500000</td>
      <td>100.000000</td>
      <td>102.000000</td>
      <td>119.000000</td>
      <td>98.000000</td>
      <td>10.10000</td>
      <td>12.437016</td>
      <td>13.200000</td>
      <td>12.394275</td>
      <td>97.027244</td>
      <td>96.526403</td>
      <td>96.56986</td>
      <td>96.444055</td>
      <td>2.695739</td>
      <td>2.383319</td>
      <td>2.131766</td>
      <td>2.090862</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>135079</td>
      <td>68</td>
      <td>1</td>
      <td>169.30233</td>
      <td>100.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>72.739130</td>
      <td>88.076923</td>
      <td>79.666667</td>
      <td>78.909091</td>
      <td>76.608696</td>
      <td>80.384615</td>
      <td>84.750000</td>
      <td>82.254622</td>
      <td>116.652174</td>
      <td>123.846154</td>
      <td>129.916667</td>
      <td>122.600009</td>
      <td>54.391304</td>
      <td>57.230769</td>
      <td>60.000000</td>
      <td>60.724172</td>
      <td>161.785714</td>
      <td>115.833333</td>
      <td>96.666667</td>
      <td>99.090909</td>
      <td>103.800000</td>
      <td>103.8</td>
      <td>104.2</td>
      <td>104.9</td>
      <td>108.666667</td>
      <td>115.694079</td>
      <td>118.492021</td>
      <td>128.000000</td>
      <td>57.666667</td>
      <td>57.478308</td>
      <td>58.259040</td>
      <td>63.636364</td>
      <td>70.000000</td>
      <td>75.786899</td>
      <td>77.178522</td>
      <td>79.545455</td>
      <td>36.485714</td>
      <td>37.275000</td>
      <td>36.45000</td>
      <td>36.500000</td>
      <td>7.250000</td>
      <td>14.00</td>
      <td>15.000000</td>
      <td>15.0</td>
      <td>19.424873</td>
      <td>19.622286</td>
      <td>19.55034</td>
      <td>19.782182</td>
      <td>0.433333</td>
      <td>0.507617</td>
      <td>0.515098</td>
      <td>0.50479</td>
      <td>7.325000</td>
      <td>7.340</td>
      <td>7.394394</td>
      <td>7.396834</td>
      <td>125.000000</td>
      <td>113.000000</td>
      <td>116.867607</td>
      <td>117.303447</td>
      <td>45.000000</td>
      <td>32.000000</td>
      <td>39.657571</td>
      <td>39.969184</td>
      <td>26.400000</td>
      <td>28.750000</td>
      <td>28.700000</td>
      <td>31.200000</td>
      <td>5.50000</td>
      <td>4.900000</td>
      <td>4.900000</td>
      <td>4.067108</td>
      <td>5.400000</td>
      <td>4.8</td>
      <td>3.700000</td>
      <td>1.503316</td>
      <td>133.000000</td>
      <td>126.00000</td>
      <td>108.00000</td>
      <td>176.60216</td>
      <td>54.00000</td>
      <td>53.0</td>
      <td>49.000000</td>
      <td>26.151218</td>
      <td>21.000000</td>
      <td>22.000000</td>
      <td>22.000000</td>
      <td>24.094853</td>
      <td>1.6</td>
      <td>1.600000</td>
      <td>1.800000</td>
      <td>2.039408</td>
      <td>137.000000</td>
      <td>139.000000</td>
      <td>138.000000</td>
      <td>138.872497</td>
      <td>263.000000</td>
      <td>180.000000</td>
      <td>146.000000</td>
      <td>128.065366</td>
      <td>9.40000</td>
      <td>9.500000</td>
      <td>6.950000</td>
      <td>12.394275</td>
      <td>97.027244</td>
      <td>96.526403</td>
      <td>96.56986</td>
      <td>96.444055</td>
      <td>1.700000</td>
      <td>2.383319</td>
      <td>2.131766</td>
      <td>2.090862</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <td>135080</td>
      <td>77</td>
      <td>1</td>
      <td>170.20000</td>
      <td>77.5</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>70.730769</td>
      <td>67.473684</td>
      <td>71.312500</td>
      <td>68.166667</td>
      <td>82.576923</td>
      <td>75.421053</td>
      <td>85.571429</td>
      <td>82.254622</td>
      <td>117.000000</td>
      <td>114.157895</td>
      <td>97.000000</td>
      <td>122.600009</td>
      <td>63.538462</td>
      <td>57.105263</td>
      <td>78.428571</td>
      <td>60.724172</td>
      <td>264.545455</td>
      <td>49.083333</td>
      <td>46.416667</td>
      <td>178.636364</td>
      <td>81.345255</td>
      <td>87.6</td>
      <td>87.6</td>
      <td>87.6</td>
      <td>117.325730</td>
      <td>115.694079</td>
      <td>116.533333</td>
      <td>122.833333</td>
      <td>58.746771</td>
      <td>57.478308</td>
      <td>53.466667</td>
      <td>55.083333</td>
      <td>76.931481</td>
      <td>75.786899</td>
      <td>74.489333</td>
      <td>77.665833</td>
      <td>36.584000</td>
      <td>37.684211</td>
      <td>37.50625</td>
      <td>37.080000</td>
      <td>9.500000</td>
      <td>11.50</td>
      <td>15.000000</td>
      <td>15.0</td>
      <td>19.424873</td>
      <td>19.622286</td>
      <td>19.55034</td>
      <td>19.782182</td>
      <td>0.653846</td>
      <td>0.500000</td>
      <td>0.515098</td>
      <td>0.50479</td>
      <td>7.385714</td>
      <td>7.375</td>
      <td>7.365000</td>
      <td>7.370000</td>
      <td>145.571429</td>
      <td>137.500000</td>
      <td>116.867607</td>
      <td>91.000000</td>
      <td>37.571429</td>
      <td>40.500000</td>
      <td>39.657571</td>
      <td>44.000000</td>
      <td>28.066667</td>
      <td>25.700000</td>
      <td>27.450000</td>
      <td>25.400000</td>
      <td>4.13294</td>
      <td>4.196807</td>
      <td>4.053152</td>
      <td>4.067108</td>
      <td>0.700000</td>
      <td>1.0</td>
      <td>1.449714</td>
      <td>1.100000</td>
      <td>156.666667</td>
      <td>276.00000</td>
      <td>194.81982</td>
      <td>165.00000</td>
      <td>10.00000</td>
      <td>11.0</td>
      <td>26.487741</td>
      <td>13.000000</td>
      <td>23.000849</td>
      <td>23.000000</td>
      <td>23.775188</td>
      <td>24.000000</td>
      <td>2.1</td>
      <td>2.100000</td>
      <td>1.900000</td>
      <td>1.900000</td>
      <td>138.744966</td>
      <td>138.918773</td>
      <td>138.878042</td>
      <td>138.872497</td>
      <td>152.224449</td>
      <td>131.516058</td>
      <td>130.070914</td>
      <td>128.065366</td>
      <td>6.15500</td>
      <td>9.600000</td>
      <td>12.230936</td>
      <td>8.600000</td>
      <td>97.027244</td>
      <td>98.000000</td>
      <td>96.56986</td>
      <td>96.444055</td>
      <td>2.695739</td>
      <td>2.800000</td>
      <td>2.131766</td>
      <td>2.090862</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>135081</td>
      <td>66</td>
      <td>1</td>
      <td>162.60000</td>
      <td>65.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>83.933333</td>
      <td>86.882353</td>
      <td>109.333333</td>
      <td>97.250000</td>
      <td>73.566667</td>
      <td>72.272727</td>
      <td>80.528667</td>
      <td>82.254622</td>
      <td>108.966667</td>
      <td>109.000000</td>
      <td>120.196298</td>
      <td>122.600009</td>
      <td>53.500000</td>
      <td>50.727273</td>
      <td>59.802690</td>
      <td>60.724172</td>
      <td>472.727273</td>
      <td>50.090909</td>
      <td>72.916667</td>
      <td>84.000000</td>
      <td>81.345255</td>
      <td>73.4</td>
      <td>73.4</td>
      <td>73.4</td>
      <td>117.325730</td>
      <td>114.428571</td>
      <td>100.142857</td>
      <td>99.500000</td>
      <td>58.746771</td>
      <td>57.000000</td>
      <td>53.047619</td>
      <td>49.500000</td>
      <td>76.931481</td>
      <td>76.142857</td>
      <td>68.746190</td>
      <td>66.166667</td>
      <td>36.640000</td>
      <td>37.618182</td>
      <td>36.35000</td>
      <td>36.033333</td>
      <td>4.833333</td>
      <td>15.00</td>
      <td>15.000000</td>
      <td>15.0</td>
      <td>19.424873</td>
      <td>19.622286</td>
      <td>19.55034</td>
      <td>19.782182</td>
      <td>0.584297</td>
      <td>0.507617</td>
      <td>0.515098</td>
      <td>0.50479</td>
      <td>7.380000</td>
      <td>7.395</td>
      <td>7.394394</td>
      <td>7.396834</td>
      <td>300.500000</td>
      <td>120.620393</td>
      <td>116.867607</td>
      <td>117.303447</td>
      <td>45.166667</td>
      <td>39.840366</td>
      <td>39.657571</td>
      <td>39.969184</td>
      <td>21.200000</td>
      <td>31.147414</td>
      <td>23.000000</td>
      <td>30.054173</td>
      <td>4.13294</td>
      <td>4.196807</td>
      <td>4.300000</td>
      <td>4.067108</td>
      <td>1.391063</td>
      <td>0.8</td>
      <td>1.449714</td>
      <td>1.503316</td>
      <td>116.000000</td>
      <td>199.19211</td>
      <td>194.81982</td>
      <td>176.60216</td>
      <td>25.69763</td>
      <td>12.0</td>
      <td>26.487741</td>
      <td>26.151218</td>
      <td>23.000849</td>
      <td>23.039752</td>
      <td>23.775188</td>
      <td>24.094853</td>
      <td>2.0</td>
      <td>2.052855</td>
      <td>1.600000</td>
      <td>1.900000</td>
      <td>138.744966</td>
      <td>138.918773</td>
      <td>138.878042</td>
      <td>138.872497</td>
      <td>152.224449</td>
      <td>131.516058</td>
      <td>221.000000</td>
      <td>128.065366</td>
      <td>13.00745</td>
      <td>12.437016</td>
      <td>12.230936</td>
      <td>12.394275</td>
      <td>98.750000</td>
      <td>96.526403</td>
      <td>96.56986</td>
      <td>96.444055</td>
      <td>2.695739</td>
      <td>2.383319</td>
      <td>2.131766</td>
      <td>2.090862</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Gender</th>
      <th>Height</th>
      <th>Weight</th>
      <th>Bilirubin</th>
      <th>AST</th>
      <th>ALT</th>
      <th>ALP</th>
      <th>Albumin</th>
      <th>TroponinT</th>
      <th>TroponinI</th>
      <th>Cholesterol</th>
      <th>MechVent</th>
      <th>HR0</th>
      <th>HR12</th>
      <th>HR24</th>
      <th>HR36</th>
      <th>MAP0</th>
      <th>MAP12</th>
      <th>MAP24</th>
      <th>MAP36</th>
      <th>SysABP0</th>
      <th>SysABP12</th>
      <th>SysABP24</th>
      <th>SysABP36</th>
      <th>DiasABP0</th>
      <th>DiasABP12</th>
      <th>DiasABP24</th>
      <th>DiasABP36</th>
      <th>Urine0</th>
      <th>Urine12</th>
      <th>Urine24</th>
      <th>Urine36</th>
      <th>Weight0</th>
      <th>Weight12</th>
      <th>Weight24</th>
      <th>Weight36</th>
      <th>NISysABP0</th>
      <th>NISysABP12</th>
      <th>NISysABP24</th>
      <th>NISysABP36</th>
      <th>NIDiasABP0</th>
      <th>NIDiasABP12</th>
      <th>NIDiasABP24</th>
      <th>NIDiasABP36</th>
      <th>NIMAP0</th>
      <th>NIMAP12</th>
      <th>NIMAP24</th>
      <th>NIMAP36</th>
      <th>Temp0</th>
      <th>Temp12</th>
      <th>Temp24</th>
      <th>Temp36</th>
      <th>GCS0</th>
      <th>GCS12</th>
      <th>GCS24</th>
      <th>GCS36</th>
      <th>RespRate0</th>
      <th>RespRate12</th>
      <th>RespRate24</th>
      <th>RespRate36</th>
      <th>FiO20</th>
      <th>FiO212</th>
      <th>FiO224</th>
      <th>FiO236</th>
      <th>pH0</th>
      <th>pH12</th>
      <th>pH24</th>
      <th>pH36</th>
      <th>PaO20</th>
      <th>PaO212</th>
      <th>PaO224</th>
      <th>PaO236</th>
      <th>PaCO20</th>
      <th>PaCO212</th>
      <th>PaCO224</th>
      <th>PaCO236</th>
      <th>HCT0</th>
      <th>HCT12</th>
      <th>HCT24</th>
      <th>HCT36</th>
      <th>K0</th>
      <th>K12</th>
      <th>K24</th>
      <th>K36</th>
      <th>Creatinine0</th>
      <th>Creatinine12</th>
      <th>Creatinine24</th>
      <th>Creatinine36</th>
      <th>Platelets0</th>
      <th>Platelets12</th>
      <th>Platelets24</th>
      <th>Platelets36</th>
      <th>BUN0</th>
      <th>BUN12</th>
      <th>BUN24</th>
      <th>BUN36</th>
      <th>HCO30</th>
      <th>HCO312</th>
      <th>HCO324</th>
      <th>HCO336</th>
      <th>Mg0</th>
      <th>Mg12</th>
      <th>Mg24</th>
      <th>Mg36</th>
      <th>Na0</th>
      <th>Na12</th>
      <th>Na24</th>
      <th>Na36</th>
      <th>Glucose0</th>
      <th>Glucose12</th>
      <th>Glucose24</th>
      <th>Glucose36</th>
      <th>WBC0</th>
      <th>WBC12</th>
      <th>WBC24</th>
      <th>WBC36</th>
      <th>SaO20</th>
      <th>SaO212</th>
      <th>SaO224</th>
      <th>SaO236</th>
      <th>Lactate0</th>
      <th>Lactate12</th>
      <th>Lactate24</th>
      <th>Lactate36</th>
      <th>Coronary Care Unit</th>
      <th>Cardiac Surgery Recovery Unit</th>
      <th>Medical ICU</th>
      <th>Surgical ICU</th>
    </tr>
    <tr>
      <th>recordid</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>137593</td>
      <td>57</td>
      <td>1</td>
      <td>157.500000</td>
      <td>88.60</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>98.931034</td>
      <td>90.428571</td>
      <td>90.000000</td>
      <td>87.181818</td>
      <td>71.310345</td>
      <td>72.285714</td>
      <td>67.235294</td>
      <td>73.230769</td>
      <td>106.482759</td>
      <td>112.357143</td>
      <td>105.000000</td>
      <td>115.538462</td>
      <td>56.068966</td>
      <td>53.571429</td>
      <td>50.176471</td>
      <td>55.769231</td>
      <td>257.916667</td>
      <td>55.833333</td>
      <td>130.454545</td>
      <td>62.500000</td>
      <td>82.704345</td>
      <td>89.0</td>
      <td>89.0</td>
      <td>88.200000</td>
      <td>116.815217</td>
      <td>116.953677</td>
      <td>118.307963</td>
      <td>120.750541</td>
      <td>58.166331</td>
      <td>57.120246</td>
      <td>57.762168</td>
      <td>57.936709</td>
      <td>76.704493</td>
      <td>76.099618</td>
      <td>76.978834</td>
      <td>77.800938</td>
      <td>38.373077</td>
      <td>38.214286</td>
      <td>37.821429</td>
      <td>37.266667</td>
      <td>10.800000</td>
      <td>15.000000</td>
      <td>15.000000</td>
      <td>14.500000</td>
      <td>19.736448</td>
      <td>19.336067</td>
      <td>19.339250</td>
      <td>19.523698</td>
      <td>0.600000</td>
      <td>0.400000</td>
      <td>0.850000</td>
      <td>0.700000</td>
      <td>7.391250</td>
      <td>7.370000</td>
      <td>7.380000</td>
      <td>7.420000</td>
      <td>168.250000</td>
      <td>125.811658</td>
      <td>85.250000</td>
      <td>88.500000</td>
      <td>45.125000</td>
      <td>39.343704</td>
      <td>46.000000</td>
      <td>50.00000</td>
      <td>31.200000</td>
      <td>34.300000</td>
      <td>30.446855</td>
      <td>30.300000</td>
      <td>4.500000</td>
      <td>4.600000</td>
      <td>4.048392</td>
      <td>3.700000</td>
      <td>0.600000</td>
      <td>0.600000</td>
      <td>1.407454</td>
      <td>0.700000</td>
      <td>214.000000</td>
      <td>229.000000</td>
      <td>194.35375</td>
      <td>171.500000</td>
      <td>11.000000</td>
      <td>9.000000</td>
      <td>27.093275</td>
      <td>16.000000</td>
      <td>27.000000</td>
      <td>27.00000</td>
      <td>23.755556</td>
      <td>30.00</td>
      <td>2.300000</td>
      <td>2.000000</td>
      <td>1.90000</td>
      <td>2.300000</td>
      <td>139.093356</td>
      <td>137.000000</td>
      <td>139.313275</td>
      <td>137.500000</td>
      <td>170.000000</td>
      <td>186.000000</td>
      <td>131.240753</td>
      <td>131.223025</td>
      <td>14.600000</td>
      <td>17.400000</td>
      <td>12.216938</td>
      <td>16.400000</td>
      <td>97.800000</td>
      <td>96.471630</td>
      <td>95.333333</td>
      <td>96.002072</td>
      <td>2.479119</td>
      <td>2.372085</td>
      <td>2.039958</td>
      <td>2.086082</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>137594</td>
      <td>87</td>
      <td>1</td>
      <td>170.425096</td>
      <td>72.50</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>65.800000</td>
      <td>68.222222</td>
      <td>66.500000</td>
      <td>65.428571</td>
      <td>84.000000</td>
      <td>79.843976</td>
      <td>80.927647</td>
      <td>82.065406</td>
      <td>151.000000</td>
      <td>118.389228</td>
      <td>120.700257</td>
      <td>123.362393</td>
      <td>55.000000</td>
      <td>59.625315</td>
      <td>60.037273</td>
      <td>60.544665</td>
      <td>75.142857</td>
      <td>97.777778</td>
      <td>244.000000</td>
      <td>152.500000</td>
      <td>72.500000</td>
      <td>72.5</td>
      <td>72.5</td>
      <td>72.500000</td>
      <td>135.000000</td>
      <td>142.777778</td>
      <td>136.833333</td>
      <td>134.142857</td>
      <td>56.684211</td>
      <td>49.555556</td>
      <td>51.333333</td>
      <td>42.285714</td>
      <td>82.792105</td>
      <td>80.630000</td>
      <td>79.833333</td>
      <td>72.905714</td>
      <td>36.625000</td>
      <td>37.050000</td>
      <td>36.900000</td>
      <td>36.300000</td>
      <td>13.142857</td>
      <td>12.777778</td>
      <td>11.142857</td>
      <td>12.142857</td>
      <td>14.421053</td>
      <td>15.444444</td>
      <td>14.333333</td>
      <td>15.000000</td>
      <td>0.591333</td>
      <td>0.516785</td>
      <td>0.520443</td>
      <td>0.506305</td>
      <td>7.366538</td>
      <td>7.388928</td>
      <td>7.397845</td>
      <td>7.399595</td>
      <td>160.449835</td>
      <td>125.811658</td>
      <td>117.976143</td>
      <td>113.725108</td>
      <td>40.434075</td>
      <td>39.343704</td>
      <td>39.350542</td>
      <td>40.04247</td>
      <td>31.900000</td>
      <td>34.700000</td>
      <td>36.100000</td>
      <td>36.300000</td>
      <td>4.122413</td>
      <td>3.500000</td>
      <td>3.900000</td>
      <td>3.800000</td>
      <td>1.316559</td>
      <td>0.800000</td>
      <td>0.900000</td>
      <td>0.800000</td>
      <td>210.566486</td>
      <td>251.000000</td>
      <td>241.00000</td>
      <td>248.000000</td>
      <td>26.151555</td>
      <td>18.000000</td>
      <td>18.000000</td>
      <td>16.000000</td>
      <td>22.950528</td>
      <td>26.00000</td>
      <td>27.000000</td>
      <td>27.00</td>
      <td>1.980247</td>
      <td>1.800000</td>
      <td>1.90000</td>
      <td>1.800000</td>
      <td>139.093356</td>
      <td>138.000000</td>
      <td>142.000000</td>
      <td>140.000000</td>
      <td>148.218042</td>
      <td>101.000000</td>
      <td>115.000000</td>
      <td>92.000000</td>
      <td>13.251714</td>
      <td>11.600000</td>
      <td>10.500000</td>
      <td>11.100000</td>
      <td>96.384891</td>
      <td>96.471630</td>
      <td>96.133857</td>
      <td>96.002072</td>
      <td>2.479119</td>
      <td>2.372085</td>
      <td>2.039958</td>
      <td>2.086082</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>137595</td>
      <td>73</td>
      <td>0</td>
      <td>165.100000</td>
      <td>77.27</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>83.275862</td>
      <td>89.600000</td>
      <td>92.166667</td>
      <td>87.909091</td>
      <td>77.250000</td>
      <td>83.600000</td>
      <td>87.250000</td>
      <td>80.454545</td>
      <td>105.413793</td>
      <td>125.066667</td>
      <td>136.166667</td>
      <td>119.727273</td>
      <td>60.482759</td>
      <td>65.333333</td>
      <td>66.166667</td>
      <td>59.363636</td>
      <td>261.071429</td>
      <td>225.000000</td>
      <td>164.583333</td>
      <td>218.636364</td>
      <td>82.704345</td>
      <td>92.7</td>
      <td>92.7</td>
      <td>91.609091</td>
      <td>88.000000</td>
      <td>116.953677</td>
      <td>118.307963</td>
      <td>120.750541</td>
      <td>48.000000</td>
      <td>57.120246</td>
      <td>57.762168</td>
      <td>57.936709</td>
      <td>61.330000</td>
      <td>76.099618</td>
      <td>76.978834</td>
      <td>77.800938</td>
      <td>35.655172</td>
      <td>37.775000</td>
      <td>38.358333</td>
      <td>37.763636</td>
      <td>3.000000</td>
      <td>7.666667</td>
      <td>10.000000</td>
      <td>11.000000</td>
      <td>19.736448</td>
      <td>19.336067</td>
      <td>19.339250</td>
      <td>19.523698</td>
      <td>0.780000</td>
      <td>0.650000</td>
      <td>0.516667</td>
      <td>0.487500</td>
      <td>7.340909</td>
      <td>7.391667</td>
      <td>7.416667</td>
      <td>7.435000</td>
      <td>200.545455</td>
      <td>95.833333</td>
      <td>91.000000</td>
      <td>91.500000</td>
      <td>38.727273</td>
      <td>34.333333</td>
      <td>35.000000</td>
      <td>34.50000</td>
      <td>27.275000</td>
      <td>36.050000</td>
      <td>30.446855</td>
      <td>32.400000</td>
      <td>4.122413</td>
      <td>5.100000</td>
      <td>4.048392</td>
      <td>4.300000</td>
      <td>0.550000</td>
      <td>0.600000</td>
      <td>1.407454</td>
      <td>0.800000</td>
      <td>118.000000</td>
      <td>94.000000</td>
      <td>194.35375</td>
      <td>67.000000</td>
      <td>9.000000</td>
      <td>8.000000</td>
      <td>27.093275</td>
      <td>12.000000</td>
      <td>19.500000</td>
      <td>20.00000</td>
      <td>23.755556</td>
      <td>23.00</td>
      <td>1.500000</td>
      <td>2.000000</td>
      <td>2.04915</td>
      <td>1.900000</td>
      <td>139.093356</td>
      <td>140.000000</td>
      <td>139.313275</td>
      <td>137.000000</td>
      <td>148.218042</td>
      <td>123.000000</td>
      <td>131.240753</td>
      <td>162.000000</td>
      <td>9.833333</td>
      <td>7.000000</td>
      <td>12.216938</td>
      <td>9.200000</td>
      <td>97.666667</td>
      <td>97.333333</td>
      <td>97.333333</td>
      <td>97.500000</td>
      <td>4.066667</td>
      <td>1.850000</td>
      <td>2.039958</td>
      <td>2.086082</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>137598</td>
      <td>72</td>
      <td>0</td>
      <td>170.425096</td>
      <td>131.80</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>85.200000</td>
      <td>105.312500</td>
      <td>95.000000</td>
      <td>100.727273</td>
      <td>80.628753</td>
      <td>79.843976</td>
      <td>80.927647</td>
      <td>82.065406</td>
      <td>116.470913</td>
      <td>118.389228</td>
      <td>120.700257</td>
      <td>123.362393</td>
      <td>59.560763</td>
      <td>59.625315</td>
      <td>60.037273</td>
      <td>60.544665</td>
      <td>165.860895</td>
      <td>127.358947</td>
      <td>133.163638</td>
      <td>142.398650</td>
      <td>131.800000</td>
      <td>131.8</td>
      <td>131.8</td>
      <td>131.800000</td>
      <td>89.368421</td>
      <td>95.000000</td>
      <td>113.083333</td>
      <td>102.090909</td>
      <td>36.473684</td>
      <td>37.062500</td>
      <td>39.250000</td>
      <td>29.818182</td>
      <td>48.444444</td>
      <td>54.375000</td>
      <td>56.416667</td>
      <td>47.363636</td>
      <td>36.666667</td>
      <td>37.233333</td>
      <td>36.866667</td>
      <td>37.075000</td>
      <td>9.000000</td>
      <td>9.000000</td>
      <td>9.666667</td>
      <td>11.000000</td>
      <td>19.736448</td>
      <td>19.336067</td>
      <td>19.339250</td>
      <td>19.523698</td>
      <td>0.460000</td>
      <td>0.400000</td>
      <td>0.400000</td>
      <td>0.400000</td>
      <td>7.290000</td>
      <td>7.388928</td>
      <td>7.397845</td>
      <td>7.399595</td>
      <td>98.000000</td>
      <td>125.811658</td>
      <td>117.976143</td>
      <td>113.725108</td>
      <td>42.000000</td>
      <td>39.343704</td>
      <td>39.350542</td>
      <td>40.04247</td>
      <td>24.550000</td>
      <td>30.832459</td>
      <td>23.200000</td>
      <td>29.934552</td>
      <td>4.200000</td>
      <td>4.178664</td>
      <td>4.400000</td>
      <td>4.087976</td>
      <td>3.900000</td>
      <td>1.378978</td>
      <td>4.300000</td>
      <td>1.360591</td>
      <td>629.500000</td>
      <td>191.870901</td>
      <td>625.00000</td>
      <td>170.852684</td>
      <td>49.500000</td>
      <td>25.842534</td>
      <td>57.000000</td>
      <td>25.798932</td>
      <td>16.000000</td>
      <td>23.19059</td>
      <td>17.000000</td>
      <td>24.27</td>
      <td>2.300000</td>
      <td>2.057708</td>
      <td>2.20000</td>
      <td>2.063583</td>
      <td>127.000000</td>
      <td>139.052282</td>
      <td>130.000000</td>
      <td>138.436415</td>
      <td>87.000000</td>
      <td>133.889028</td>
      <td>207.000000</td>
      <td>131.223025</td>
      <td>26.150000</td>
      <td>12.555877</td>
      <td>26.000000</td>
      <td>12.180153</td>
      <td>96.384891</td>
      <td>96.471630</td>
      <td>96.133857</td>
      <td>96.002072</td>
      <td>2.600000</td>
      <td>2.372085</td>
      <td>2.039958</td>
      <td>2.086082</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>137600</td>
      <td>76</td>
      <td>1</td>
      <td>172.700000</td>
      <td>86.10</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>88.000000</td>
      <td>88.000000</td>
      <td>88.000000</td>
      <td>87.928571</td>
      <td>77.833333</td>
      <td>67.272727</td>
      <td>67.285714</td>
      <td>71.000000</td>
      <td>113.000000</td>
      <td>101.545455</td>
      <td>109.857143</td>
      <td>113.214286</td>
      <td>58.666667</td>
      <td>49.272727</td>
      <td>47.214286</td>
      <td>49.714286</td>
      <td>192.000000</td>
      <td>94.166667</td>
      <td>105.000000</td>
      <td>101.000000</td>
      <td>82.704345</td>
      <td>92.6</td>
      <td>92.6</td>
      <td>93.035714</td>
      <td>116.815217</td>
      <td>116.953677</td>
      <td>118.307963</td>
      <td>120.750541</td>
      <td>58.166331</td>
      <td>57.120246</td>
      <td>57.762168</td>
      <td>57.936709</td>
      <td>76.704493</td>
      <td>76.099618</td>
      <td>76.978834</td>
      <td>77.800938</td>
      <td>36.116667</td>
      <td>37.281818</td>
      <td>38.014286</td>
      <td>37.307143</td>
      <td>10.590699</td>
      <td>15.000000</td>
      <td>15.000000</td>
      <td>14.000000</td>
      <td>19.736448</td>
      <td>19.336067</td>
      <td>19.339250</td>
      <td>19.523698</td>
      <td>0.666667</td>
      <td>0.516785</td>
      <td>0.520443</td>
      <td>0.506305</td>
      <td>7.363333</td>
      <td>7.322500</td>
      <td>7.397845</td>
      <td>7.380000</td>
      <td>301.666667</td>
      <td>137.000000</td>
      <td>117.976143</td>
      <td>96.000000</td>
      <td>39.666667</td>
      <td>37.500000</td>
      <td>39.350542</td>
      <td>40.00000</td>
      <td>31.639291</td>
      <td>27.200000</td>
      <td>24.600000</td>
      <td>25.100000</td>
      <td>4.122413</td>
      <td>5.600000</td>
      <td>5.300000</td>
      <td>4.200000</td>
      <td>1.316559</td>
      <td>1.400000</td>
      <td>1.407454</td>
      <td>1.400000</td>
      <td>271.000000</td>
      <td>276.000000</td>
      <td>194.35375</td>
      <td>184.000000</td>
      <td>26.151555</td>
      <td>24.000000</td>
      <td>27.093275</td>
      <td>23.000000</td>
      <td>22.950528</td>
      <td>20.00000</td>
      <td>23.755556</td>
      <td>23.00</td>
      <td>1.980247</td>
      <td>2.057708</td>
      <td>1.70000</td>
      <td>1.700000</td>
      <td>139.093356</td>
      <td>137.000000</td>
      <td>139.313275</td>
      <td>136.000000</td>
      <td>148.218042</td>
      <td>169.000000</td>
      <td>132.000000</td>
      <td>143.000000</td>
      <td>13.251714</td>
      <td>16.000000</td>
      <td>12.216938</td>
      <td>10.800000</td>
      <td>96.384891</td>
      <td>98.000000</td>
      <td>96.133857</td>
      <td>97.000000</td>
      <td>2.479119</td>
      <td>2.372085</td>
      <td>2.039958</td>
      <td>2.086082</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Gender</th>
      <th>Height</th>
      <th>Weight</th>
      <th>Bilirubin</th>
      <th>AST</th>
      <th>ALT</th>
      <th>ALP</th>
      <th>Albumin</th>
      <th>TroponinT</th>
      <th>TroponinI</th>
      <th>Cholesterol</th>
      <th>MechVent</th>
      <th>HR0</th>
      <th>HR12</th>
      <th>HR24</th>
      <th>HR36</th>
      <th>MAP0</th>
      <th>MAP12</th>
      <th>MAP24</th>
      <th>MAP36</th>
      <th>SysABP0</th>
      <th>SysABP12</th>
      <th>SysABP24</th>
      <th>SysABP36</th>
      <th>DiasABP0</th>
      <th>DiasABP12</th>
      <th>DiasABP24</th>
      <th>DiasABP36</th>
      <th>Urine0</th>
      <th>Urine12</th>
      <th>Urine24</th>
      <th>Urine36</th>
      <th>Weight0</th>
      <th>Weight12</th>
      <th>Weight24</th>
      <th>Weight36</th>
      <th>NISysABP0</th>
      <th>NISysABP12</th>
      <th>NISysABP24</th>
      <th>NISysABP36</th>
      <th>NIDiasABP0</th>
      <th>NIDiasABP12</th>
      <th>NIDiasABP24</th>
      <th>NIDiasABP36</th>
      <th>NIMAP0</th>
      <th>NIMAP12</th>
      <th>NIMAP24</th>
      <th>NIMAP36</th>
      <th>Temp0</th>
      <th>Temp12</th>
      <th>Temp24</th>
      <th>Temp36</th>
      <th>GCS0</th>
      <th>GCS12</th>
      <th>GCS24</th>
      <th>GCS36</th>
      <th>RespRate0</th>
      <th>RespRate12</th>
      <th>RespRate24</th>
      <th>RespRate36</th>
      <th>FiO20</th>
      <th>FiO212</th>
      <th>FiO224</th>
      <th>FiO236</th>
      <th>pH0</th>
      <th>pH12</th>
      <th>pH24</th>
      <th>pH36</th>
      <th>PaO20</th>
      <th>PaO212</th>
      <th>PaO224</th>
      <th>PaO236</th>
      <th>PaCO20</th>
      <th>PaCO212</th>
      <th>PaCO224</th>
      <th>PaCO236</th>
      <th>HCT0</th>
      <th>HCT12</th>
      <th>HCT24</th>
      <th>HCT36</th>
      <th>K0</th>
      <th>K12</th>
      <th>K24</th>
      <th>K36</th>
      <th>Creatinine0</th>
      <th>Creatinine12</th>
      <th>Creatinine24</th>
      <th>Creatinine36</th>
      <th>Platelets0</th>
      <th>Platelets12</th>
      <th>Platelets24</th>
      <th>Platelets36</th>
      <th>BUN0</th>
      <th>BUN12</th>
      <th>BUN24</th>
      <th>BUN36</th>
      <th>HCO30</th>
      <th>HCO312</th>
      <th>HCO324</th>
      <th>HCO336</th>
      <th>Mg0</th>
      <th>Mg12</th>
      <th>Mg24</th>
      <th>Mg36</th>
      <th>Na0</th>
      <th>Na12</th>
      <th>Na24</th>
      <th>Na36</th>
      <th>Glucose0</th>
      <th>Glucose12</th>
      <th>Glucose24</th>
      <th>Glucose36</th>
      <th>WBC0</th>
      <th>WBC12</th>
      <th>WBC24</th>
      <th>WBC36</th>
      <th>SaO20</th>
      <th>SaO212</th>
      <th>SaO224</th>
      <th>SaO236</th>
      <th>Lactate0</th>
      <th>Lactate12</th>
      <th>Lactate24</th>
      <th>Lactate36</th>
      <th>Coronary Care Unit</th>
      <th>Cardiac Surgery Recovery Unit</th>
      <th>Medical ICU</th>
      <th>Surgical ICU</th>
    </tr>
    <tr>
      <th>recordid</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>140101</td>
      <td>39</td>
      <td>0</td>
      <td>170.200000</td>
      <td>253.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>110.142857</td>
      <td>108.833333</td>
      <td>97.250000</td>
      <td>97.636364</td>
      <td>80.355020</td>
      <td>80.313226</td>
      <td>80.628562</td>
      <td>81.518378</td>
      <td>115.676644</td>
      <td>119.538003</td>
      <td>120.858617</td>
      <td>122.740704</td>
      <td>59.269006</td>
      <td>59.575281</td>
      <td>59.575021</td>
      <td>59.960344</td>
      <td>60.000000</td>
      <td>53.636364</td>
      <td>50.000000</td>
      <td>72.222222</td>
      <td>253.000000</td>
      <td>253.000000</td>
      <td>253.000000</td>
      <td>253.000000</td>
      <td>99.571429</td>
      <td>108.333333</td>
      <td>111.666667</td>
      <td>103.818182</td>
      <td>42.857143</td>
      <td>50.916667</td>
      <td>52.250000</td>
      <td>45.000000</td>
      <td>61.761429</td>
      <td>70.055000</td>
      <td>72.055000</td>
      <td>64.605455</td>
      <td>38.000000</td>
      <td>37.733333</td>
      <td>37.533333</td>
      <td>37.766667</td>
      <td>10.000000</td>
      <td>10.000000</td>
      <td>10.000000</td>
      <td>10.000000</td>
      <td>19.920446</td>
      <td>19.896906</td>
      <td>19.816753</td>
      <td>20.011274</td>
      <td>0.540000</td>
      <td>0.400000</td>
      <td>0.40000</td>
      <td>0.400000</td>
      <td>7.385000</td>
      <td>8.406452</td>
      <td>7.360000</td>
      <td>7.398353</td>
      <td>109.500000</td>
      <td>123.874998</td>
      <td>145.000000</td>
      <td>115.412815</td>
      <td>66.500000</td>
      <td>39.931142</td>
      <td>66.000000</td>
      <td>40.44737</td>
      <td>32.90</td>
      <td>30.964831</td>
      <td>32.9</td>
      <td>29.951503</td>
      <td>4.200000</td>
      <td>4.173645</td>
      <td>3.9</td>
      <td>4.071882</td>
      <td>0.60</td>
      <td>1.453197</td>
      <td>0.5</td>
      <td>1.487078</td>
      <td>149.000000</td>
      <td>197.180099</td>
      <td>179.0</td>
      <td>176.738114</td>
      <td>14.0</td>
      <td>25.731611</td>
      <td>13.0</td>
      <td>25.921424</td>
      <td>34.0</td>
      <td>23.411007</td>
      <td>33.0</td>
      <td>24.214374</td>
      <td>1.500000</td>
      <td>2.055607</td>
      <td>1.9</td>
      <td>2.051946</td>
      <td>140.000000</td>
      <td>139.053094</td>
      <td>142.0</td>
      <td>138.633454</td>
      <td>103.000000</td>
      <td>131.87701</td>
      <td>86.0</td>
      <td>128.530985</td>
      <td>15.800000</td>
      <td>12.416399</td>
      <td>10.7</td>
      <td>12.242523</td>
      <td>96.793083</td>
      <td>96.8269</td>
      <td>96.412421</td>
      <td>96.634002</td>
      <td>1.300000</td>
      <td>2.182108</td>
      <td>2.039218</td>
      <td>1.832424</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>140102</td>
      <td>70</td>
      <td>0</td>
      <td>169.337684</td>
      <td>123.5</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>76.444444</td>
      <td>76.769231</td>
      <td>78.636364</td>
      <td>84.000000</td>
      <td>80.355020</td>
      <td>83.600000</td>
      <td>87.545455</td>
      <td>95.272727</td>
      <td>115.676644</td>
      <td>109.200000</td>
      <td>119.181818</td>
      <td>132.500000</td>
      <td>59.269006</td>
      <td>61.000000</td>
      <td>69.272727</td>
      <td>73.916667</td>
      <td>39.285714</td>
      <td>128.125000</td>
      <td>27.818182</td>
      <td>107.625000</td>
      <td>123.500000</td>
      <td>123.500000</td>
      <td>123.500000</td>
      <td>123.500000</td>
      <td>102.777778</td>
      <td>99.142857</td>
      <td>117.967196</td>
      <td>119.822643</td>
      <td>46.666667</td>
      <td>47.142857</td>
      <td>56.889398</td>
      <td>57.930466</td>
      <td>65.368889</td>
      <td>64.475714</td>
      <td>76.214687</td>
      <td>77.475260</td>
      <td>36.350000</td>
      <td>36.633333</td>
      <td>36.766667</td>
      <td>36.933333</td>
      <td>10.500000</td>
      <td>11.666667</td>
      <td>11.000000</td>
      <td>10.666667</td>
      <td>19.920446</td>
      <td>19.896906</td>
      <td>19.816753</td>
      <td>20.011274</td>
      <td>0.500000</td>
      <td>0.500000</td>
      <td>0.50000</td>
      <td>0.560000</td>
      <td>7.390000</td>
      <td>7.430000</td>
      <td>7.561206</td>
      <td>7.398353</td>
      <td>85.000000</td>
      <td>88.000000</td>
      <td>118.137949</td>
      <td>115.412815</td>
      <td>58.000000</td>
      <td>53.000000</td>
      <td>39.565064</td>
      <td>40.44737</td>
      <td>28.90</td>
      <td>30.964831</td>
      <td>29.4</td>
      <td>29.951503</td>
      <td>4.500000</td>
      <td>4.173645</td>
      <td>4.1</td>
      <td>4.071882</td>
      <td>0.60</td>
      <td>1.453197</td>
      <td>0.5</td>
      <td>1.487078</td>
      <td>162.000000</td>
      <td>197.180099</td>
      <td>185.0</td>
      <td>176.738114</td>
      <td>25.0</td>
      <td>25.731611</td>
      <td>26.0</td>
      <td>25.921424</td>
      <td>34.0</td>
      <td>23.411007</td>
      <td>30.0</td>
      <td>24.214374</td>
      <td>2.200000</td>
      <td>2.055607</td>
      <td>2.2</td>
      <td>2.051946</td>
      <td>141.000000</td>
      <td>139.053094</td>
      <td>142.0</td>
      <td>138.633454</td>
      <td>134.000000</td>
      <td>131.87701</td>
      <td>144.0</td>
      <td>128.530985</td>
      <td>13.600000</td>
      <td>12.416399</td>
      <td>11.8</td>
      <td>12.242523</td>
      <td>96.793083</td>
      <td>96.8269</td>
      <td>96.412421</td>
      <td>96.634002</td>
      <td>2.700000</td>
      <td>2.182108</td>
      <td>2.039218</td>
      <td>1.832424</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>140104</td>
      <td>61</td>
      <td>1</td>
      <td>188.000000</td>
      <td>80.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>100.906250</td>
      <td>88.357143</td>
      <td>82.333333</td>
      <td>93.125000</td>
      <td>78.750000</td>
      <td>65.428571</td>
      <td>74.000000</td>
      <td>81.518378</td>
      <td>101.812500</td>
      <td>89.714286</td>
      <td>95.727273</td>
      <td>122.740704</td>
      <td>58.750000</td>
      <td>49.428571</td>
      <td>58.909091</td>
      <td>59.960344</td>
      <td>143.461538</td>
      <td>114.363636</td>
      <td>76.666667</td>
      <td>158.333333</td>
      <td>84.205934</td>
      <td>85.353485</td>
      <td>85.883694</td>
      <td>86.183251</td>
      <td>116.137862</td>
      <td>97.833333</td>
      <td>96.250000</td>
      <td>99.125000</td>
      <td>57.564944</td>
      <td>43.333333</td>
      <td>45.583333</td>
      <td>43.875000</td>
      <td>75.933999</td>
      <td>61.500000</td>
      <td>62.471667</td>
      <td>62.292500</td>
      <td>36.771429</td>
      <td>37.200000</td>
      <td>37.250000</td>
      <td>37.082040</td>
      <td>8.500000</td>
      <td>15.000000</td>
      <td>15.000000</td>
      <td>15.000000</td>
      <td>19.920446</td>
      <td>19.896906</td>
      <td>19.816753</td>
      <td>20.011274</td>
      <td>0.442857</td>
      <td>0.508116</td>
      <td>0.50586</td>
      <td>0.507407</td>
      <td>7.331429</td>
      <td>7.390000</td>
      <td>7.400000</td>
      <td>7.398353</td>
      <td>156.428571</td>
      <td>153.000000</td>
      <td>118.137949</td>
      <td>115.412815</td>
      <td>44.857143</td>
      <td>42.500000</td>
      <td>39.565064</td>
      <td>40.44737</td>
      <td>28.15</td>
      <td>30.964831</td>
      <td>28.8</td>
      <td>29.951503</td>
      <td>5.100000</td>
      <td>4.173645</td>
      <td>4.1</td>
      <td>4.071882</td>
      <td>0.75</td>
      <td>1.453197</td>
      <td>0.9</td>
      <td>1.487078</td>
      <td>223.500000</td>
      <td>197.180099</td>
      <td>221.0</td>
      <td>176.738114</td>
      <td>11.0</td>
      <td>25.731611</td>
      <td>18.0</td>
      <td>25.921424</td>
      <td>22.5</td>
      <td>23.411007</td>
      <td>30.0</td>
      <td>24.214374</td>
      <td>2.000000</td>
      <td>2.055607</td>
      <td>2.0</td>
      <td>2.051946</td>
      <td>138.000000</td>
      <td>139.053094</td>
      <td>138.0</td>
      <td>138.633454</td>
      <td>161.000000</td>
      <td>131.87701</td>
      <td>99.0</td>
      <td>128.530985</td>
      <td>17.100000</td>
      <td>12.416399</td>
      <td>17.3</td>
      <td>12.242523</td>
      <td>97.857143</td>
      <td>98.0000</td>
      <td>96.412421</td>
      <td>96.634002</td>
      <td>2.581794</td>
      <td>2.182108</td>
      <td>2.039218</td>
      <td>1.832424</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>140106</td>
      <td>64</td>
      <td>1</td>
      <td>162.600000</td>
      <td>80.0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>72.269231</td>
      <td>73.642857</td>
      <td>97.200000</td>
      <td>103.181818</td>
      <td>69.517241</td>
      <td>61.850000</td>
      <td>65.611111</td>
      <td>69.230769</td>
      <td>101.275862</td>
      <td>95.950000</td>
      <td>101.944444</td>
      <td>107.615385</td>
      <td>52.103448</td>
      <td>46.000000</td>
      <td>49.944444</td>
      <td>53.615385</td>
      <td>190.384615</td>
      <td>55.416667</td>
      <td>85.000000</td>
      <td>120.909091</td>
      <td>84.205934</td>
      <td>85.353485</td>
      <td>85.883694</td>
      <td>95.600000</td>
      <td>106.000000</td>
      <td>92.875000</td>
      <td>104.000000</td>
      <td>119.822643</td>
      <td>48.000000</td>
      <td>36.750000</td>
      <td>35.000000</td>
      <td>57.930466</td>
      <td>68.000000</td>
      <td>56.000000</td>
      <td>59.000000</td>
      <td>77.475260</td>
      <td>36.950000</td>
      <td>38.454545</td>
      <td>37.133333</td>
      <td>36.933333</td>
      <td>8.000000</td>
      <td>13.750000</td>
      <td>15.000000</td>
      <td>15.000000</td>
      <td>19.920446</td>
      <td>19.896906</td>
      <td>19.816753</td>
      <td>20.011274</td>
      <td>0.740000</td>
      <td>0.500000</td>
      <td>0.50586</td>
      <td>0.507407</td>
      <td>7.350000</td>
      <td>7.427500</td>
      <td>7.443333</td>
      <td>7.470000</td>
      <td>215.875000</td>
      <td>93.000000</td>
      <td>69.000000</td>
      <td>74.000000</td>
      <td>40.375000</td>
      <td>32.750000</td>
      <td>31.500000</td>
      <td>28.00000</td>
      <td>29.60</td>
      <td>29.600000</td>
      <td>26.3</td>
      <td>29.951503</td>
      <td>4.109867</td>
      <td>4.173645</td>
      <td>4.5</td>
      <td>4.071882</td>
      <td>1.00</td>
      <td>1.200000</td>
      <td>1.0</td>
      <td>1.487078</td>
      <td>86.666667</td>
      <td>140.000000</td>
      <td>108.0</td>
      <td>176.738114</td>
      <td>19.0</td>
      <td>18.000000</td>
      <td>14.0</td>
      <td>25.921424</td>
      <td>22.0</td>
      <td>21.000000</td>
      <td>22.0</td>
      <td>24.214374</td>
      <td>1.952595</td>
      <td>2.000000</td>
      <td>2.4</td>
      <td>2.051946</td>
      <td>138.840093</td>
      <td>136.000000</td>
      <td>135.0</td>
      <td>138.633454</td>
      <td>149.939522</td>
      <td>131.87701</td>
      <td>157.0</td>
      <td>128.530985</td>
      <td>6.366667</td>
      <td>8.900000</td>
      <td>8.4</td>
      <td>12.242523</td>
      <td>95.666667</td>
      <td>96.5000</td>
      <td>92.666667</td>
      <td>94.000000</td>
      <td>0.900000</td>
      <td>2.733333</td>
      <td>2.039218</td>
      <td>1.832424</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>140107</td>
      <td>45</td>
      <td>1</td>
      <td>169.337684</td>
      <td>105.5</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>97.181818</td>
      <td>84.466667</td>
      <td>86.350000</td>
      <td>84.363636</td>
      <td>80.355020</td>
      <td>80.313226</td>
      <td>96.272727</td>
      <td>81.000000</td>
      <td>115.676644</td>
      <td>119.538003</td>
      <td>135.818182</td>
      <td>122.818182</td>
      <td>59.269006</td>
      <td>59.575281</td>
      <td>78.000000</td>
      <td>64.363636</td>
      <td>86.500000</td>
      <td>56.875000</td>
      <td>71.250000</td>
      <td>59.375000</td>
      <td>105.500000</td>
      <td>105.500000</td>
      <td>105.500000</td>
      <td>105.500000</td>
      <td>123.090909</td>
      <td>122.923077</td>
      <td>121.636364</td>
      <td>119.822643</td>
      <td>68.181818</td>
      <td>66.538462</td>
      <td>69.000000</td>
      <td>57.930466</td>
      <td>80.500000</td>
      <td>79.461538</td>
      <td>79.600000</td>
      <td>77.475260</td>
      <td>39.600000</td>
      <td>37.700000</td>
      <td>38.300000</td>
      <td>38.300000</td>
      <td>8.666667</td>
      <td>9.666667</td>
      <td>9.333333</td>
      <td>10.000000</td>
      <td>19.920446</td>
      <td>19.896906</td>
      <td>19.816753</td>
      <td>20.011274</td>
      <td>0.575000</td>
      <td>0.500000</td>
      <td>0.62000</td>
      <td>0.450000</td>
      <td>7.430000</td>
      <td>8.406452</td>
      <td>7.460000</td>
      <td>7.495000</td>
      <td>104.000000</td>
      <td>123.874998</td>
      <td>116.000000</td>
      <td>95.500000</td>
      <td>50.500000</td>
      <td>39.931142</td>
      <td>39.500000</td>
      <td>41.00000</td>
      <td>30.90</td>
      <td>29.100000</td>
      <td>27.6</td>
      <td>29.951503</td>
      <td>3.900000</td>
      <td>3.900000</td>
      <td>3.3</td>
      <td>4.071882</td>
      <td>0.90</td>
      <td>1.000000</td>
      <td>0.6</td>
      <td>1.487078</td>
      <td>384.000000</td>
      <td>374.000000</td>
      <td>411.0</td>
      <td>176.738114</td>
      <td>21.0</td>
      <td>24.000000</td>
      <td>22.0</td>
      <td>25.921424</td>
      <td>31.0</td>
      <td>32.000000</td>
      <td>28.0</td>
      <td>24.214374</td>
      <td>2.600000</td>
      <td>2.700000</td>
      <td>2.5</td>
      <td>2.051946</td>
      <td>145.000000</td>
      <td>146.000000</td>
      <td>140.0</td>
      <td>138.633454</td>
      <td>125.000000</td>
      <td>135.00000</td>
      <td>139.0</td>
      <td>128.530985</td>
      <td>12.100000</td>
      <td>12.900000</td>
      <td>10.8</td>
      <td>12.242523</td>
      <td>97.000000</td>
      <td>96.8269</td>
      <td>96.412421</td>
      <td>96.634002</td>
      <td>1.200000</td>
      <td>2.182108</td>
      <td>0.850000</td>
      <td>1.000000</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[23]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#For MODEL 2</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">OneHotEncoder</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="k">import</span> <span class="n">sqrt</span>

<span class="n">result</span> <span class="o">=</span> <span class="p">{}</span> <span class="c1">#storing results of each fold</span>
<span class="n">all_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Testing on Fold&quot;</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">x_train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">y_train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    
    <span class="c1"># Getting train data set up</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">all_list</span> <span class="k">if</span> <span class="n">x</span> <span class="o">!=</span> <span class="n">i</span><span class="p">]:</span> 
        <span class="n">x_train_df</span> <span class="o">=</span> <span class="n">x_train_df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_fold</span><span class="p">[</span><span class="n">j</span><span class="p">])</span> <span class="c1">#Inserting X for train data</span>
        <span class="n">y_train_df</span> <span class="o">=</span> <span class="n">y_train_df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_fold</span><span class="p">[</span><span class="n">j</span><span class="p">])</span> <span class="c1">#Inserting Y for train data</span>
    <span class="n">y_train_df</span> <span class="o">=</span> <span class="n">y_train_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;Length_of_stay&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y_train_df</span> <span class="o">=</span> <span class="n">y_train_df</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1">#use -1 to indicate missing values</span>
    
    <span class="n">train_df</span> <span class="o">=</span> <span class="n">x_train_df</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">y_train_df</span><span class="p">,</span> <span class="n">left_on</span><span class="o">=</span><span class="s2">&quot;recordid&quot;</span><span class="p">,</span> <span class="n">right_on</span><span class="o">=</span><span class="s2">&quot;RecordID&quot;</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s1">&#39;outer&#39;</span><span class="p">)</span>
    <span class="n">train_df</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">&quot;RecordID&quot;</span><span class="p">)</span>
    <span class="n">train_df</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">train_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">!=</span> <span class="s1">&#39;In-hospital_death&#39;</span><span class="p">]</span>
    <span class="n">Y_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;In-hospital_death&#39;</span><span class="p">]</span>
    
        
    <span class="n">x_test_df</span> <span class="o">=</span> <span class="n">x_train_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">y_test_df</span> <span class="o">=</span> <span class="n">y_train_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># Getting test data set up</span>
    <span class="n">x_test_df</span> <span class="o">=</span> <span class="n">x_test_df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_fold</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">y_test_df</span> <span class="o">=</span> <span class="n">y_test_df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_fold</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">y_test_df</span> <span class="o">=</span> <span class="n">y_test_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;Length_of_stay&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
     <span class="c1"># Replace -1 with NaN</span>
<span class="c1">#     x_test_df = x_test_df.replace(-1, np.nan)</span>
    <span class="c1"># Replace not known length of stay to 2</span>
    <span class="n">y_test_df</span> <span class="o">=</span> <span class="n">y_test_df</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    
    <span class="n">test_df</span> <span class="o">=</span> <span class="n">x_test_df</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">y_test_df</span><span class="p">,</span> <span class="n">left_on</span><span class="o">=</span><span class="s2">&quot;recordid&quot;</span><span class="p">,</span> <span class="n">right_on</span><span class="o">=</span><span class="s2">&quot;RecordID&quot;</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s1">&#39;outer&#39;</span><span class="p">)</span>
    <span class="n">test_df</span> <span class="o">=</span> <span class="n">test_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">&quot;RecordID&quot;</span><span class="p">)</span>
    <span class="n">test_df</span> <span class="o">=</span> <span class="n">test_df</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">test_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">!=</span> <span class="s1">&#39;In-hospital_death&#39;</span><span class="p">]</span>
    <span class="n">Y_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[</span><span class="s1">&#39;In-hospital_death&#39;</span><span class="p">]</span>
    
<span class="c1">#     print(X_train.head())</span>
<span class="c1">#     best = 0</span>
    
    <span class="c1"># Logistic Regression</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;dim_reducer__n_components&#39;</span><span class="p">:[</span><span class="mi">60</span><span class="p">,</span><span class="mi">70</span><span class="p">],</span>
              <span class="s1">&#39;f_selecter__k&#39;</span><span class="p">:[</span><span class="mi">70</span><span class="p">,</span><span class="mi">80</span><span class="p">,</span><span class="mi">90</span><span class="p">]</span>
             <span class="p">}</span>
    
    <span class="n">model</span> <span class="o">=</span> <span class="n">imPipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[</span>
                              <span class="p">(</span><span class="s1">&#39;smote&#39;</span><span class="p">,</span> <span class="n">SMOTE</span><span class="p">()),</span>
                             <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
                            <span class="p">(</span><span class="s1">&#39;f_selecter&#39;</span><span class="p">,</span> <span class="n">SelectKBest</span><span class="p">()),</span>
                            <span class="p">(</span><span class="s1">&#39;dim_reducer&#39;</span><span class="p">,</span> <span class="n">PCA</span><span class="p">()),</span>
                           <span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">LogisticRegression</span><span class="p">())])</span>
    <span class="n">est</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scoring</span> <span class="o">=</span> <span class="s1">&#39;roc_auc&#39;</span><span class="p">)</span>
    <span class="n">est</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
    <span class="n">best_est</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">best_estimator_</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">best_est</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Logistic&quot;</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>            
        
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">result</span><span class="p">[</span><span class="s1">&#39;logistic&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">score</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span><span class="p">[</span><span class="s1">&#39;logistic&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;logistic&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">score</span>         
                
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">result</span><span class="p">:</span>
    <span class="n">result</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">/</span><span class="mi">4</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Testing on Fold 1
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\Asus\Anaconda3\lib\site-packages\pandas\core\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version
of pandas will change to not sort by default.

To accept the future behavior, pass &#39;sort=False&#39;.

To retain the current behavior and silence the warning, pass &#39;sort=True&#39;.

  sort=sort,
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;dim_reducer__n_components&#39;: 70, &#39;f_selecter__k&#39;: 80}
Logistic 0.710239651416122
Testing on Fold 2
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\Asus\Anaconda3\lib\site-packages\pandas\core\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version
of pandas will change to not sort by default.

To accept the future behavior, pass &#39;sort=False&#39;.

To retain the current behavior and silence the warning, pass &#39;sort=True&#39;.

  sort=sort,
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\pandas\core\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version
of pandas will change to not sort by default.

To accept the future behavior, pass &#39;sort=False&#39;.

To retain the current behavior and silence the warning, pass &#39;sort=True&#39;.

  sort=sort,
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;dim_reducer__n_components&#39;: 60, &#39;f_selecter__k&#39;: 90}
Logistic 0.7549803324451212
Testing on Fold 3
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\pandas\core\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version
of pandas will change to not sort by default.

To accept the future behavior, pass &#39;sort=False&#39;.

To retain the current behavior and silence the warning, pass &#39;sort=True&#39;.

  sort=sort,
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;dim_reducer__n_components&#39;: 60, &#39;f_selecter__k&#39;: 80}
Logistic 0.7167668012738435
Testing on Fold 4
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\Asus\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;dim_reducer__n_components&#39;: 60, &#39;f_selecter__k&#39;: 70}
Logistic 0.7338732798165137
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4>Result for M2 Classifier</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[24]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">result</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[24]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>{&#39;logistic&#39;: 0.7289650162379002}</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1>4. Deployment Workflow</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="data:image/png;base64,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" alt="Deployment%20Workflow%20Updated.png"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The hospital needs to input the first 48 hours of the patient data. This data then will be converted into Design Matrix 1 to predict the Length of Stay and Design Matrix 2 to predict the In-hospital Mortality. Design Matrix 1 will contain the statistics of each measurement in the 48 hours data for each patient <i>(full details above in Design Matrix 1)</i>. Design Matrix 2 transformed the input patient data in such a way that every parameters is grouped into a 12-hour bin (total of 4 bins for the first 48 hours). If a parameter is measured multiple times within a 12-hour bin, a mean of those measurement will be recorded to represent the measurement for that 12-hour bin instead <i>(full details above in Design Matrix 2)</i>.</p>
<p>The Design Matrix 1 data will be used by M2R which uses neural networks with Tensorflow Keras. The Design Matrix 2 will be used by M2C where it will be modeled with Logistic Regression classifier technique <i>(full details above in M2 Model Building)</i>.</p>
<p>For mortality prediction, the true positives is that a patient is predicted to die and he/she actually dies and the true negatives is that a patient is predicted to live and he/she lives. In an actual scenario which concerns life and death, this boils down to the morality for the prediction. Some patients prefer to know the 'truth', in this case, they want the actual result to be very close to the predicted one or in fact exactly the same as what was predicted and so sensitivity is very important to them. As the higher the sensitivity, the higher the chance the predicted result becomes true. They would rather be told that they are going to die and they actually die than being told that they will live, giving them false hope, but in the end die. A common myth of admission into ICU is that patients hardly have the chance of exiting alive. Thus, when one admits into the ICU, he/she is more concerned about whether he/she will die. To achieve the highest possible tpr (sensitivity) on the AUC curve, the point to choose on the curve will be as far to the right as possible.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    </div>
  </div>
</body>

 


</html>
