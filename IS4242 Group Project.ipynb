{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>IS4242 Group Project</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Import necessary libraries</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, confusion_matrix, accuracy_score, mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from imblearn.pipeline import Pipeline as imPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.pipeline import Pipeline as imPipeline\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Running the code</h5>\n",
    "Ensure that you are in the root folder of all the fold folders and target files\n",
    "read_text(fold_name):\n",
    "    fold_name: this is the name of the fold you want to read ALL patient files of. It will be read into a 2 dimensional\n",
    "    list. If you would like to retrieve just the first patient instead, you will need to change the line \n",
    "    \"txt_all.extend(txt[1:])\" to \"txt_all.append(txt[1:])\" and you will be to use \"read_text(fold1.txt)[0]\" to retrieve\n",
    "    the relevant patient's data\n",
    "read_ans(file_name):\n",
    "    file_name: this is the name of the file you want to read ALL targets of. It will be read into a 2 dimensional\n",
    "    list. To retrieve the first patient's target: read_ans(ans.csv)[0]\n",
    "put_single_into_dataframe(txt): This functions takes in 2 dimensional list ie the output of read_text(fold1.txt) \n",
    "put_multiple_into_dataframe(txt): Multiple is for using it with the output of read_text after you wanted to change it to append\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(fold_name):\n",
    "    txt_all = list()\n",
    "    for f in os.listdir(fold_name): # for each file in the directory\n",
    "        if f.endswith(\".txt\"):\n",
    "            with open(os.path.join(fold_name, f), 'r') as fp: # open each file\n",
    "                txt = fp.readlines() # read inside the file\n",
    "                recordid = txt[1].rstrip('\\n').split(',')[-1] # get recordid\n",
    "                txt = [[int(recordid)] + t.rstrip('\\n').split(',') for t in txt] # preface each row with the recordid as all patients are 1 file\n",
    "                txt_all.extend(txt[1:]) # skip the parameter list\n",
    "    return txt_all\n",
    "\n",
    "def read_one_text(fold_name):\n",
    "    txt_all = list()\n",
    "    for f in os.listdir(fold_name): # for each file in the directory\n",
    "        if f.endswith(\".txt\"):\n",
    "            with open(os.path.join(fold_name, f), 'r') as fp: # open each file\n",
    "                txt = fp.readlines() # read inside the file\n",
    "            recordid = txt[1].rstrip('\\n').split(',')[-1] # get recordid\n",
    "            txt = [[int(recordid)] + t.rstrip('\\n').split(',') for t in txt] # preface each row with the recordid as all patients are 1 file\n",
    "            txt_all.append(txt[1:]) # skip the parameter list\n",
    "    return txt_all\n",
    "\n",
    "def read_ans(file_name):\n",
    "    txt_all = list()\n",
    "    with open(file_name, 'r') as fp: # opens the csv file\n",
    "        txt = fp.readlines() \n",
    "    for i in range(1, len(txt)): # similar to above read_text\n",
    "        record_id, length_of_stay, hospital_death = txt[i].rstrip('\\n').split(',')\n",
    "        txt_all.append([record_id, length_of_stay, hospital_death])\n",
    "    return txt_all\n",
    "\n",
    "def put_multiple_into_dataframe(txt_all):\n",
    "    df = pd.DataFrame()\n",
    "    for i in txt_all:\n",
    "        df2 = pd.DataFrame(i, columns=['recordid', 'time', 'parameter', 'value'])\n",
    "        df = df.append(df2, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def put_single_into_dataframe(txt_all):\n",
    "    df = pd.DataFrame(txt_all, columns=['recordid', 'time', 'parameter', 'value'])\n",
    "    return df\n",
    "\n",
    "def get_X_add_ready(X_add, stat):\n",
    "    X_add = X_add.reset_index()\n",
    "    X_add = X_add.pivot(index='recordid', columns='parameter', values='value')\n",
    "    X_add = X_add.drop(stat_feat, axis = 1) \n",
    "#     X_add = X_add.drop(['RecordID'], axis = 1) \n",
    "    X_add.columns = [x+stat for x in X_add.columns]\n",
    "    X_add = X_add.reset_index()\n",
    "    return X_add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1. Data Exploration</h1>\n",
    "\n",
    "Firstly, read_one_text function is used to read in the folder (eg. Fold1) containing the patient's individual record file. The index 0 in this case in the first line: p1 = read_one_text('../Project_Data/Fold1')[0] is to retrieve the first patient's record in the folder entered inside the function read_one_text. For example, to retrieve the last patient record in Fold3 folder, one has to type: p1 = read_one_text('../Project_Data/Fold3')[999]\n",
    "\n",
    "Format to get individual record:  \n",
    "Line 1: p1 = read_one_text('../Project_Data/<b>x</b>')[<b>y</b>]  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>x</b> = folder of the patient to retrieve  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>y</b> = index row of the patient inside the folder - 1\n",
    "\n",
    "Next, this patient record will then undergo preprocessing steps as follows to remove unnecessary information and transform the data into a more meaningful one. This patient record is transformed into dataframe with the columns 'recordid', 'time', 'parameter', 'value'. The recordid of this patient is stored in a variable called 'recordId' before dropping both the column 'recordid' and the first row in the dataframe which contains the recordid of this patient as they are now redundant. \n",
    "\n",
    "Subsequently, data in the column for 'value' is converted to numeric so as to facilitate the plotting of graph later. A new column called 'time_value' is created to store a tuple of 'time' and 'value'. The dataframe is then grouped by the 'parameter' column and then the 'time_value' tuple is then stored in a list based on the parameter. \n",
    "\n",
    "Finally, the groupby object is then plotted into a graph with each unique parameter as a different line in the graph with the x-axis in hours and y-axis as the values which the parameter is measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x2160 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "p1 = read_one_text(\"../Project_Data/Fold1\")[0]\n",
    "p1df = pd.DataFrame(p1, columns=['recordid', 'time', 'parameter', 'value'])\n",
    "recordId = p1df.iloc[0]['value']\n",
    "df_1 = p1df.drop('recordid', axis=1)\n",
    "df_1.drop(df_1.index[:1], inplace=True)\n",
    "# df_1\n",
    "\n",
    "df_1['value'] = pd.to_numeric(df_1['value'])\n",
    "df_1.time = df_1.time.astype('str')\n",
    "df_1['time'] = df_1['time'].str.replace(':', '.')\n",
    "df_1['time'] = pd.to_numeric(df_1['time'])\n",
    "df_1['value'][df_1['value'] < 0] = 0\n",
    "df_1['time_value'] = list(zip(df_1.time, df_1.value))\n",
    "df_2 = df_1.groupby('parameter').time_value.apply(lambda x: x.unique().tolist())\n",
    "\n",
    "major_ticks = np.arange(0, 51, 10)\n",
    "minor_ticks = np.arange(0, 49, 2)\n",
    "fig = plt.figure(figsize=(18, 30))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "for i in range(len(df_2)):\n",
    "    testList2 = [(elem1, elem2) for elem1, elem2 in df_2[i]]\n",
    "    zip(*testList2)\n",
    "    plt.plot(*zip(*testList2), marker='o', label=df_2.index[i])\n",
    "plt.xlabel('Time (hours)')\n",
    "plt.ylabel('Value')\n",
    "plt.legend(loc='upper right')\n",
    "title = 'Temporal Data for the first 48 hours for patient of recordid: ' + recordId\n",
    "fig.suptitle(title, fontsize=20, y=0.9)\n",
    "# plt.xlim(xmin=0.0)\n",
    "# plt.ylim(ymin=0.0)\n",
    "ax.minorticks_on()\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "\n",
    "ax.grid(which='minor', alpha=0.2)\n",
    "ax.grid(which='major', alpha=0.5)\n",
    "# ax.tick_params(which = 'both', direction = 'out')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recordid</th>\n",
       "      <th>time</th>\n",
       "      <th>parameter</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132539</td>\n",
       "      <td>00:00</td>\n",
       "      <td>RecordID</td>\n",
       "      <td>132539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132539</td>\n",
       "      <td>00:00</td>\n",
       "      <td>Age</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>132539</td>\n",
       "      <td>00:00</td>\n",
       "      <td>Gender</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132539</td>\n",
       "      <td>00:00</td>\n",
       "      <td>Height</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>132539</td>\n",
       "      <td>00:00</td>\n",
       "      <td>ICUType</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   recordid   time parameter   value\n",
       "0    132539  00:00  RecordID  132539\n",
       "1    132539  00:00       Age      54\n",
       "2    132539  00:00    Gender       0\n",
       "3    132539  00:00    Height      -1\n",
       "4    132539  00:00   ICUType       4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat = pd.DataFrame()\n",
    "numberOfFolds = 0\n",
    "for i, name in enumerate([\"Fold1\"]): # what folds do you want to use?\n",
    "    string = \"../Project_Data/\"\n",
    "    string += name\n",
    "    df_feat = df_feat.append(put_single_into_dataframe(read_text(string)))\n",
    "    numberOfFolds = (i+1)\n",
    "numberOfRows = numberOfFolds*1000\n",
    "df_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recordid</th>\n",
       "      <th>days_in_hospital</th>\n",
       "      <th>mortality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132539</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132540</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>132541</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132543</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>132545</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  recordid days_in_hospital mortality\n",
       "0   132539                5         0\n",
       "1   132540                8         0\n",
       "2   132541               19         0\n",
       "3   132543                9         0\n",
       "4   132545                4         0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading Target\n",
    "df_target = pd.DataFrame(read_ans('../Project_Data/Fold1_Outcomes.csv'), columns=['recordid', 'days_in_hospital', 'mortality'])\n",
    "df_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of record ids: 1000\n",
      "HR             57.027\n",
      "MAP            36.092\n",
      "SysABP         35.979\n",
      "DiasABP        35.955\n",
      "Urine          34.208\n",
      "Weight         33.679\n",
      "NISysABP       24.457\n",
      "NIDiasABP      24.424\n",
      "NIMAP          24.088\n",
      "Temp           21.204\n",
      "GCS            15.214\n",
      "RespRate       13.775\n",
      "FiO2            7.815\n",
      "MechVent        7.596\n",
      "pH              5.770\n",
      "PaO2            5.496\n",
      "PaCO2           5.490\n",
      "HCT             4.626\n",
      "K               3.708\n",
      "Creatinine      3.573\n",
      "Platelets       3.566\n",
      "BUN             3.547\n",
      "HCO3            3.479\n",
      "Mg              3.468\n",
      "Na              3.462\n",
      "Glucose         3.338\n",
      "WBC             3.286\n",
      "SaO2            1.985\n",
      "Lactate         1.924\n",
      "Height          1.000\n",
      "RecordID        1.000\n",
      "ICUType         1.000\n",
      "Gender          1.000\n",
      "Age             1.000\n",
      "Bilirubin       0.858\n",
      "AST             0.857\n",
      "ALT             0.857\n",
      "ALP             0.833\n",
      "Albumin         0.617\n",
      "TroponinT       0.566\n",
      "TroponinI       0.130\n",
      "Cholesterol     0.077\n",
      "Name: parameter, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "bin_feat = ['MechVent']\n",
    "num_feat = ['Albumin', 'ALP', 'ALT', 'AST', 'Bilirubin', 'BUN', 'Cholesterol',\n",
    "           'Creatinine', 'DiasABP', 'FiO2', 'GCS', 'Glucose', 'HCO3', 'HCT',\n",
    "           'HR', 'K', 'Lactate', 'Mg', 'MAP', 'NA', 'NIDiasABP', 'NIMAP',\n",
    "           'NISysABP', 'PaCO2', 'PaO2', 'pH', 'Platelets', 'RespRate', 'SaO2',\n",
    "           'SysABP', 'Temp', 'Tropl', 'TropT', 'Urine', 'WBC', 'Weight']\n",
    "\n",
    "print(\"Number of record ids:\", len(df_feat['recordid'].unique()))\n",
    "unique_count = df_feat['parameter'].value_counts()/numberOfRows\n",
    "print(unique_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Analysis of Features</h2>\n",
    "<p>The data above shows the average number of times a variable observed per patient. Based on the data above and the feature description we classify the features into these categories:\n",
    "<ul>\n",
    "    <li>General Descriptors (static data) that are collected when the patient is admitted to the ICU. Weight is not included as weight are measured multiple times as a time series data. Each of the descriptors will be included as a feature into the model.</li>\n",
    "    <li>Rare features: measured on average less than one time per patient (less than 1.0). We use the <u>existence</u> of these measurements for each patient as a feature.</li>\n",
    "    <li>Features that measured often or more that one time per patient (more than 1.0). Calculate the hourly average of each measurements and put them into 48 columns. <i>Example, average HR on the first hour to HR_1, average HR on the second hour to HR_2, and so on.</i></li>\n",
    "</ul>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rare features ['Bilirubin', 'AST', 'ALT', 'ALP', 'Albumin', 'TroponinT', 'TroponinI', 'Cholesterol', 'MechVent']\n",
      "Normal features ['HR', 'MAP', 'SysABP', 'DiasABP', 'Urine', 'Weight', 'NISysABP', 'NIDiasABP', 'NIMAP', 'Temp', 'GCS', 'RespRate', 'FiO2', 'MechVent', 'pH', 'PaO2', 'PaCO2', 'HCT', 'K', 'Creatinine', 'Platelets', 'BUN', 'HCO3', 'Mg', 'Na', 'Glucose', 'WBC', 'SaO2', 'Lactate']\n"
     ]
    }
   ],
   "source": [
    "stat_feat = ['Age', 'Gender', 'Height', 'ICUType', 'RecordID'] #General Descriptors\n",
    "rare_feat = []\n",
    "nor_feat = []\n",
    "for index, value in unique_count.items():\n",
    "    if value < 1.0:\n",
    "        rare_feat.append(index)\n",
    "    elif index not in stat_feat:\n",
    "        nor_feat.append(index)\n",
    "rare_feat.append(\"MechVent\")\n",
    "print(\"Rare features\", rare_feat)\n",
    "print(\"Normal features\", nor_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code to produce test and train data\n",
    "df = df_feat.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Creation of Data Matrices</h2>\n",
    "<p>We create 3 different matrices to convert temporal data into a matrix that is a single feature vector per patient.</p>\n",
    "<h3>First Design Matrix</h3>\n",
    "<ul> \n",
    "    <li>In the cell below, we create a matrix that generalises a patient's attributes across the whole 48 hours, such as his max BUN measurement over the 48 hours. This is so as to create a much denser feature matrix as well as provide a much clearer signal to the learner, allowing it to generalise to data outside of the train set.</li>\n",
    "    <li>We will be utilising the min, max and mean of the measurements</li>\n",
    "    <li>We have converted sparse features like ALS into a binary variable where 1 represents \"It was recored\" and 0 representing \"It was not recorded at all\".</li>\n",
    "    <li>ICUType was one-hot encoded</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_x_for_design_matrix_1(df_feat):\n",
    "    non_bin_feat.clear()\n",
    "    # your code to produce test and train data\n",
    "    df = df_feat.copy()\n",
    "\n",
    "    df['value'] = pd.to_numeric(df['value'])\n",
    "    temp_df = df.loc[df['time'] == '00:00', :].copy() # get all the variables at time 0\n",
    "    temp_df = temp_df.loc[temp_df['parameter'].isin(stat_feat)] # prune the dataframe to only those static variables\n",
    "    temp_df = temp_df.pivot(index='recordid', columns='parameter', values='value') \n",
    "    temp_df = temp_df.reset_index()\n",
    "    for i in temp_df: # for loop to change all the -1 values for static variables into np.nan\n",
    "        idx = temp_df.index[temp_df[i] == -1].tolist()\n",
    "        for j in idx:\n",
    "            temp_df.loc[j, i] = np.nan\n",
    "    final_df = temp_df.copy()\n",
    "\n",
    "#     Dealing with rare_feat\n",
    "    d = df_feat.groupby(['recordid', 'parameter'])[['value']].count()\n",
    "    def specialFeature(special):\n",
    "        id = []\n",
    "        for index, row in d.iterrows():\n",
    "            if index[1] == special:\n",
    "                id.append(index[0])\n",
    "        return id\n",
    "    for x in rare_feat:\n",
    "        id = specialFeature(x)\n",
    "        final_df[x] = 0\n",
    "        for i in id:\n",
    "            for row in final_df.index:\n",
    "                if row == i:\n",
    "                    final_df.loc[row, x] = 1\n",
    "    final_df = final_df.drop([\"RecordID\"],axis=1)\n",
    "\n",
    "    # Getting the different attributes\n",
    "    pd.set_option('display.max_columns', 500)\n",
    "    temp_df = df.drop(df.index[df['parameter'].isin(rare_feat)].tolist())\n",
    "    temp_df = temp_df.groupby(['recordid', 'parameter'])[['value']]\n",
    "    for i in ['min', 'max', 'mean']: # the different parameters we will use\n",
    "        if (i=='min'):\n",
    "            X_add = temp_df.min() # get the min of each parameter\n",
    "            X_add = get_X_add_ready(X_add, '_min')\n",
    "            final_df = final_df.merge(X_add, left_on='recordid', right_on='recordid') # merge the min of the parameters to the final dataframe\n",
    "        elif (i=='max'):\n",
    "            X_add = temp_df.max() # get the max of each parameter\n",
    "            X_add = get_X_add_ready(X_add, '_max')\n",
    "            final_df = final_df.merge(X_add, left_on='recordid', right_on='recordid') # merge the min of the parameters to the final dataframe\n",
    "        elif (i=='mean'):\n",
    "            X_add = temp_df.mean() # get the mean of each parameter\n",
    "            X_add = get_X_add_ready(X_add, '_mean')\n",
    "            final_df = final_df.merge(X_add, left_on='recordid', right_on='recordid') # merge the min of the parameters to the final dataframe\n",
    "\n",
    "    # dealing with ICUType categorical\n",
    "    one_hot = pd.get_dummies(final_df['ICUType'])\n",
    "    meaning_of_icu_types = {1:'Coronary Care Unit', 2: 'Cardiac Surgery Recovery Unit', 3: 'Medical ICU', 4: 'Surgical ICU'}\n",
    "    one_hot.columns = [meaning_of_icu_types[x] for x in one_hot.columns]\n",
    "    final_df = final_df.merge(one_hot, left_index=True, right_index=True)\n",
    "    final_df = final_df.drop('ICUType', axis=1)\n",
    "    \n",
    "    # Extreme height values is set to np.nan\n",
    "    for index, row in final_df.iterrows():\n",
    "        if row[\"Height\"] < 40 or row[\"Height\"] > 210:\n",
    "            row[\"Height\"] = np.nan\n",
    "\n",
    "\n",
    "    # Drop recordID column\n",
    "    final_df = final_df.drop(\"recordid\", axis=1)\n",
    "            \n",
    "    # Creating non binary column list and filling na values with mean\n",
    "    for i in final_df:\n",
    "        if i in rare_feat or i in bin_feat:\n",
    "            continue\n",
    "        final_df = final_df.fillna(final_df.mean())\n",
    "        non_bin_feat.append(i)\n",
    "        \n",
    "#     display(final_df.head())\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Second Design Matrix</h3>\n",
    "<ul> \n",
    "    <li>The second design matrix is created in such a way that every parameters is grouped into a 12-hour bin (eg. 0-12, 12-24, 24-36, 36-48). If a parameter is measured multiple times within a 12-hour bin, a mean of those measurement will be recorded to represent the measurement for that 12-hour bin instead. The assumption for using the mean for 12-hour is that once patients are sent to the ICU, they must be closely monitored and furthermore, the working shift of nurses in hospital is usually by 12-hour and so this ensures lesser human errors when the nurses change shift and patient is being measured.</li>\n",
    "        <li>We have converted sparse features like ALS into a binary variable where 1 represents \"It was recored\" and 0 representing \"It was not recorded at all\".</li>\n",
    "    <li>ICUType was one-hot encoded</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Matrix\n",
    "def preprocess_x_for_design_matrix_2(df):\n",
    "    non_bin_feat.clear()\n",
    "    \n",
    "    # retrieving static data of the patients at time 0 and transformed into dataframe with static feature as the column \n",
    "    # headers and each row in the dataframe representing each patient record, which is indexed by their record id\n",
    "    df_static = df.loc[df['time'] == '00:00', :].copy()\n",
    "    static_vars = ['RecordID', 'Age', 'Gender', 'Height', 'ICUType', 'Weight']\n",
    "    df_static.drop('time', axis=1, inplace=True)\n",
    "    df_static = df_static.loc[df_static['parameter'].isin(static_vars)]\n",
    "    df_static = df_static.groupby(['recordid', 'parameter'])[['value']].last()\n",
    "    df_static.reset_index(inplace=True)\n",
    "    df_static = df_static.pivot(index='recordid', columns='parameter', values='value')\n",
    "\n",
    "    for c in df_static.columns:\n",
    "        df_static[c] = pd.to_numeric(df_static[c])\n",
    "\n",
    "    # replacing height and weight which does not exist (represented with '-1') with NaN\n",
    "    for c in df_static.columns:\n",
    "        x = df_static[c]\n",
    "        if c == 'Height':\n",
    "            idx = x < 0\n",
    "            df_static.loc[idx, c] = np.nan\n",
    "        elif c == 'Weight':\n",
    "            idx = x < 0\n",
    "            df_static.loc[idx, c] = np.nan\n",
    "    d = df.groupby(['recordid', 'parameter'])[['value']].count()\n",
    "    def specialFeature(special):\n",
    "        id = []\n",
    "        for index, row in d.iterrows():\n",
    "            if index[1] == special:\n",
    "                id.append(index[0])\n",
    "        return id\n",
    "\n",
    "    # RecordID column is dropped as every row is already indexed by the recordid\n",
    "    df2 = df_static.copy()\n",
    "    df2.drop('RecordID', axis=1, inplace=True)\n",
    "\n",
    "    # rare features which are rarely measured such as Cholesterol are converted to binary features which indicates whether \n",
    "    # any measurement for these variables were recorded within the first 48 hours. Binary feature was also created for \n",
    "    # MechVent to indicate whether the patient was placed on mechanical ventilation\n",
    "    for x in rare_feat:\n",
    "        id = specialFeature(x)\n",
    "        df2[x] = 0\n",
    "        for i in id:\n",
    "            for row in df2.index:\n",
    "                if row == i:\n",
    "                    df2.loc[row, x] = 1\n",
    "    df2.head()\n",
    "    normal_feat = nor_feat.copy()\n",
    "    normal_feat.remove('MechVent')\n",
    "    \n",
    "    # grouped the rest of the parameters into 12-hour bins and the mean of the parameter in a given 12-hour interval\n",
    "    # is taken if it was measured more than once during that 12-hour interval\n",
    "    idx = df['parameter'].isin(normal_feat)\n",
    "    df3 = df.loc[idx, :].copy()\n",
    "    df3[['hour','min']] = df3.time.str.split(':', expand=True)\n",
    "    df3[\"hour\"] = pd.to_numeric(df3[\"hour\"])\n",
    "    df3[\"value\"] = pd.to_numeric(df3[\"value\"])\n",
    "    bins = [0, 12, 24, 36, 48]\n",
    "    labels = ['0', '12', '24', '36']\n",
    "    \n",
    "    # grouped by recordid first, then by each 12-hour interval then by the parameters\n",
    "    df3 = df3.groupby(['recordid', pd.cut(df3.hour, bins=bins, labels=labels), 'parameter'])[['value']].mean()\n",
    "    df3\n",
    "\n",
    "    # filled in the dataframe with NaN first\n",
    "    for n in normal_feat:    \n",
    "        df2[n +'0'] = np.nan\n",
    "        df2[n +'12'] = np.nan\n",
    "        df2[n +'24'] = np.nan\n",
    "        df2[n +'36'] = np.nan\n",
    "    df2.head()\n",
    "\n",
    "    # go through every row in groupby table and insert these values into the dataframe with NaN, replacing them\n",
    "    for index, row in df3.iterrows():\n",
    "        recordId = index[0]\n",
    "        hour = index[1]\n",
    "        parameter = index[2]\n",
    "        df2.loc[recordId, parameter+hour] = row[\"value\"]\n",
    "\n",
    "    one_hot = pd.get_dummies(df2['ICUType'])\n",
    "    meaning_of_icu_types = {1:'Coronary Care Unit', 2: 'Cardiac Surgery Recovery Unit', 3: 'Medical ICU', 4: 'Surgical ICU'}\n",
    "    one_hot.columns = [meaning_of_icu_types[x] for x in one_hot.columns]\n",
    "    df2 = df2.merge(one_hot, left_index=True, right_index=True)\n",
    "    df2 = df2.drop('ICUType', axis=1)\n",
    "    \n",
    "    # Extreme height values is set to np.nan\n",
    "    for index, row in df2.iterrows():\n",
    "        if row[\"Height\"] < 40 or row[\"Height\"] > 210:\n",
    "            row[\"Height\"] = np.nan\n",
    "            \n",
    "    # Creating non binary column list and filling na values with mean\n",
    "    for i in df2:\n",
    "        if i in rare_feat or i in bin_feat:\n",
    "            continue\n",
    "        df2 = df2.fillna(df2.mean())\n",
    "        non_bin_feat.append(i)\n",
    "\n",
    "    display(df2.head())\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Third Design Matrix</h3>\n",
    "<p>This design matrix splits all numerical time series data into hourly bins. The average value of the measurements on each hour will be taken and put into the columns. The idea of this is to get the condition of the patient on each hour from coming in to the hospital until the 48 hour mark. All rare features will be taken as a binary feature of whether the measurement appear within 48 hours. All general descriptors taken as it is except weight.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_x_for_design_matrix_3(df_feat):\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    tot_values = {} # the sum of the values\n",
    "    count = {} # num of occurences of a measurement\n",
    "    for index,row in df_feat.iterrows():\n",
    "        if row['parameter'] == 'RecordID' and index != 0:\n",
    "            # count the average of the previous record\n",
    "            for key,value in tot_values.items():\n",
    "                tot_values[key] = float(tot_values[key])/count[key]\n",
    "\n",
    "            df = df.append(tot_values, ignore_index=True)\n",
    "            tot_values.clear()\n",
    "            count.clear()\n",
    "            for feat in rare_feat:\n",
    "                tot_values[feat] = 0;\n",
    "                count[feat] = 1;\n",
    "\n",
    "        if row['parameter'] in stat_feat:\n",
    "            tot_values[row['parameter']] = row['value']\n",
    "            count[row['parameter']] = 1\n",
    "        elif row['parameter'] in rare_feat:\n",
    "            tot_values[row['parameter']] = 1\n",
    "            count[row['parameter']] = 1\n",
    "        elif row['parameter'] in nor_feat and row['parameter'] != 'MechVent':\n",
    "            hour = int(row['time'][0:2]) + 1\n",
    "            if hour == 49: hour-=1\n",
    "            col = row['parameter'] + '_' + str(hour)\n",
    "            tot_values[col] = row['value']\n",
    "            if col in count:\n",
    "                count[col] = count[col] + 1\n",
    "            else:\n",
    "                count[col] = 1\n",
    "\n",
    "    # count the average of the previous record\n",
    "    for key,value in tot_values.items():\n",
    "        tot_values[key] = float(tot_values[key])/count[key]\n",
    "\n",
    "    df = df.append(tot_values, ignore_index=True)\n",
    "    \n",
    "    df = df.astype({'RecordID': 'int32', 'ICUType':'int32'})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model Building for M1</h1>\n",
    "<p>We develop models for our design matrix.</p>\n",
    "\n",
    "<p>We have several common steps for all 3 model building which we will list. If there are any particularities, it will be further described at the respective model. Here are the steps.\n",
    "    <ol>\n",
    "        <li><b>Imputation</b>\n",
    "            <ul>\n",
    "                <li>Categorical: we impute empty data with the most frequent appearing value of that feature. Mean or median is not used for categorical because the value of a categorical feature doesn't depend on how big or small the value is but the value represents a category of that feature.</li>\n",
    "                <li>Numerical: we impute the data with the mean value of the feature.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><b>One-Hot Encoder</b> <i>(only for Categorical features)</i>\n",
    "            <br>One-Hot encoding is used to replace the categories inside a categorical feature into multiple binary features. The purpose of this is to make sure that the model is not biased into thinking bigger values mean better categories. Example: A categorical feature has values 1,2,3 represents animals such as Dog, Cat, and Fish respectively. Say the model is counting the average of those numbers, then it can means the average of 1 and 3 is 2. Which equals to Dog and Fish on average is Cat which is obviously not true. Some models may have implemented this step in the creation of the design matrix already.</br>\n",
    "            </li>\n",
    "        <li><b>Scaler</b>\n",
    "            <br>The data in this case is on different scale meaning some numbers are bigger than the others. The model might weigh the bigger values higher. To reduce this problem, we scale the data.\n",
    "            </br>\n",
    "        </li>\n",
    "        <li><b>SelectKBest</b>\n",
    "            <br>SelectKBest is used to extract features which is actually important for our prediction. There are a lot of features for some matrices (1400+) and some features might not be as influencing for our model. SelectKBest selects K best features that is useful for the model.</br>\n",
    "        </li>\n",
    "        <li><b>Variance Threshold</b>\n",
    "            <br>Similar to how SelectKBest is used to extract features, where we work off the idea that when a feature doesn’t vary much within itself, it generally has very little predictive power.</br>\n",
    "        </li>\n",
    "        <li><b>Dimensionality Reduction</b>\n",
    "            <br>We understand that some features might be overlapping each other. We used PCA to extract the principle features to reduce redundant features.</br>\n",
    "        </li>\n",
    "        <li><b>Classifier Model</b>\n",
    "            <br>We select our classifier model empirically based on which model has the best results on average across all folds. There are multiple learners we use that will be listed below.</br>\n",
    "        </li>\n",
    "    </ol>\n",
    "    <p>We run <b>GridSearchCV</b> on our training data to optimize our hyperparameters. Our hyperparameter optimization is based on the ROC AUC or Mean Squared Error score. Hyperparameter optimization is important because each set of steps in our pipeline has different values such as the learning rate, K value for SelectKBest, or number of components at PCA, and each values will give different results and some are better than the others. We use this method mainly as way to find the best learner algorithm to use for M2.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess y dataframe for use in linear regression\n",
    "def preprocess_y_linear(temp_df):\n",
    "    temp_df = temp_df.drop(['recordid','mortality'], axis=1)\n",
    "    temp_df['days_in_hospital'] = pd.to_numeric(temp_df['days_in_hospital'])\n",
    "    for index, row in temp_df.iterrows():\n",
    "        if row['days_in_hospital'] == -1:\n",
    "            row['days_in_hospital'] = 2   \n",
    "    return temp_df\n",
    "\n",
    "# preprocess y dataframe for use in classification\n",
    "def preprocess_y_class(temp_df):\n",
    "    temp_df = temp_df.drop(['recordid','days_in_hospital'], axis=1)\n",
    "    temp_df['mortality'] = pd.to_numeric(temp_df['mortality'])  \n",
    "    return temp_df\n",
    "\n",
    "bin_feat = [\"Gender\", \"Coronary Care Unit\", \"Cardiac Surgery Recovery Unit\", \"Medical ICU\", \"Surgical ICU\"]\n",
    "non_bin_feat = []\n",
    "scaler = ColumnTransformer(\n",
    "    remainder = 'passthrough',\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), non_bin_feat)])\n",
    "all_list = [1,2,3,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>First Model</h3>\n",
    "<ul> \n",
    "    <li>First, in the cell below, we make use of the first design matrix above to pass in as input to the models. Training is done on 3 folds and a test I.E. validation on the remaining fold. Included is 3 regresssion models (Linear, DecisionTree, MLP) and 3 classification models (AdaboostedDecisionTrees, MLP, GaussianNB). </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an array containing the preprocessed folds\n",
    "folds_x = {}\n",
    "folds_y_linear = {}\n",
    "folds_y_class = {}\n",
    "for i in all_list:\n",
    "    x_string = \"../Project_Data/Fold\"+str(i)\n",
    "    y_string = \"../Project_Data/Fold\"+str(i)+\"_Outcomes.csv\"\n",
    "    temp_df_y = pd.DataFrame(read_ans(y_string), columns=['recordid', 'days_in_hospital', 'mortality'])\n",
    "    temp_df_x = put_single_into_dataframe(read_text(x_string))\n",
    "    folds_x[i] = preprocess_x_for_design_matrix_1(temp_df_x)\n",
    "    folds_y_linear[i] = preprocess_y_linear(temp_df_y)\n",
    "    folds_y_class[i] = preprocess_y_class(temp_df_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Regressor Task</h4>\n",
    "\n",
    "<p>Steps:</p>\n",
    "<ol>\n",
    "    <li><b>Standard Scaler</b></li>\n",
    "    <li><b>Dimensionality Reducer with PCA</b></li>\n",
    "    <li><b>Classifier</b>\n",
    "        <br>We compare different regression models based on the mean squared error. The three classifiers are Linear Regression, DecisionTreeRegressor and MLPRegressor.</br>\n",
    "    </li>\n",
    "</ol>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on Fold 1\n",
      "# Getting train data set up\n",
      "# Getting test data set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on Fold 2\n",
      "# Getting train data set up\n",
      "# Getting test data set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (700) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on Fold 3\n",
      "# Getting train data set up\n",
      "# Getting test data set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (700) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on Fold 4\n",
      "# Getting train data set up\n",
      "# Getting test data set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (700) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Running CV\n",
    "\n",
    "#Create a matrix to store the results where row represnets the model and column represents the result tested on fold i\n",
    "w, h = 4, 5\n",
    "matrix = [[0 for x in range(w)] for y in range(h)] \n",
    "matrix_class = [[0 for x in range(w)] for y in range(h)] \n",
    "\n",
    "param = \"\"\n",
    "\n",
    "all_list = [1,2,3,4]\n",
    "for i in range(1,5):\n",
    "    print(\"Testing on Fold\", i)\n",
    "    x_train_df = pd.DataFrame()\n",
    "    y_train_linear_df = pd.DataFrame()\n",
    "    y_train_class_df = pd.DataFrame()\n",
    "    x_test_df = pd.DataFrame()\n",
    "    y_test_linear_df = pd.DataFrame()\n",
    "    y_test_class_df = pd.DataFrame()\n",
    "\n",
    "    # Getting train data set up\n",
    "    print(\"# Getting train data set up\")\n",
    "    for j in [x for x in all_list if x != i]:\n",
    "        x_train_df = x_train_df.append(folds_x[j])\n",
    "        y_train_linear_df = y_train_linear_df.append(folds_y_linear[j])\n",
    "        y_train_class_df = y_train_class_df.append(folds_y_class[j])\n",
    "\n",
    "    # Getting test data set up \n",
    "    print(\"# Getting test data set up\")\n",
    "    x_test_df = folds_x[i]\n",
    "    y_test_linear_df= folds_y_linear[i]\n",
    "    y_test_class_df = folds_y_class[i]\n",
    "\n",
    "    # Linear\n",
    "    best = sys.maxsize\n",
    "    parameters = {\n",
    "                  'dim_reducer__n_components':[60,70]}\n",
    "    est = Pipeline(steps=[\n",
    "            ('scaler', scaler),\n",
    "            ('dim_reducer', PCA()),\n",
    "            ('classifier', LinearRegression())])\n",
    "    estimator = GridSearchCV(est, parameters, cv=3, scoring='neg_mean_squared_error')\n",
    "    estimator.fit(x_train_df, y_train_linear_df)# gridsearch obejct has scanned thorugh all the best parameters to set it\n",
    "    prediction = estimator.predict(x_test_df)\n",
    "    matrix[i-1][0] = (mean_squared_error(y_test_linear_df, prediction))\n",
    "    \n",
    "    \n",
    "    # DecisionTreeRegressor\n",
    "    best = sys.maxsize\n",
    "    parameters = {\n",
    "                  'dim_reducer__n_components':[60,70],\n",
    "                  'classifier__min_samples_leaf':[1,3]\n",
    "                 }\n",
    "    est = Pipeline(steps=[\n",
    "            ('scaler', scaler),\n",
    "            ('dim_reducer', PCA()),\n",
    "            ('classifier', DecisionTreeRegressor())])\n",
    "    estimator = GridSearchCV(est, parameters, cv=3, scoring='neg_mean_squared_error')\n",
    "    estimator.fit(x_train_df, y_train_linear_df)# gridsearch obejct has scanned thorugh all the best parameters to set it\n",
    "    prediction = estimator.predict(x_test_df)\n",
    "    matrix[i-1][1] = (mean_squared_error(y_test_linear_df, prediction))\n",
    "    \n",
    "    \n",
    "    # MLPRegressor\n",
    "    best = sys.maxsize\n",
    "    parameters = { \n",
    "                  'dim_reducer__n_components':[60,70],\n",
    "                  'classifier__hidden_layer_sizes':[(130, 110, 90, 70, 50, 30, 10, 5), (130, 100, 50)],\n",
    "                  'classifier__learning_rate_init':[0.9, 0.105]\n",
    "                 }\n",
    "    est = Pipeline(steps=[\n",
    "            ('scaler', scaler),\n",
    "            ('dim_reducer', PCA()),\n",
    "            ('classifier', MLPRegressor(max_iter=700))])\n",
    "    estimator = GridSearchCV(est, parameters, cv=3, scoring='neg_mean_squared_error')\n",
    "    estimator.fit(x_train_df, y_train_linear_df)# gridsearch obejct has scanned thorugh all the best parameters to set it\n",
    "    prediction = estimator.predict(x_test_df)\n",
    "    matrix[i-1][2] = (mean_squared_error(y_test_linear_df, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Classifier Task</h4>\n",
    "\n",
    "<p>Steps:</p>\n",
    "<ol>\n",
    "    <li><b>Standard Scaler</b></li>\n",
    "    <li><b>SMOTE</b></li>\n",
    "    <li><b>Dimensionality Reducer with Variance Threshold</b></li>\n",
    "    <li><b>Dimensionality Reducer with PCA</b></li>\n",
    "    <li><b>Classifier</b>\n",
    "        <br>We compare four different classifier models based on the mean roc_auc_score error. The three classifiers are KNeighbours, Logistic Regression and RandomForestClassifier.</br>\n",
    "    </li>\n",
    "</ol>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "all_list = [1,2,3,4]\n",
    "for i in range(1,5):\n",
    "    # KNeighbours\n",
    "    best = sys.maxsize\n",
    "    parameters = {'f_selecter__threshold':[0.5], \n",
    "                  'dim_reducer__n_components':[60],\n",
    "                  'classifier__n_neighbors':[3,2]\n",
    "                 }\n",
    "    est = imPipeline(steps=[\n",
    "            ('smote', SMOTE()),\n",
    "            ('scaler', scaler),\n",
    "            ('f_selecter', VarianceThreshold()),\n",
    "            ('dim_reducer', PCA()),\n",
    "            ('classifier', KNeighborsClassifier())])\n",
    "    estimator = GridSearchCV(est, parameters, cv=3, scoring='roc_auc')\n",
    "    estimator.fit(x_train_df, y_train_class_df)# gridsearch obejct has scanned thorugh all the best parameters to set it\n",
    "    prediction = estimator.predict(x_test_df)\n",
    "    matrix_class[i-1][0] = (roc_auc_score(y_test_class_df, prediction))\n",
    "    \n",
    "    # Logistic Regression\n",
    "    best = sys.maxsize\n",
    "    parameters = {'f_selecter__threshold':[0.5], \n",
    "                  'dim_reducer__n_components':[60]\n",
    "                 }\n",
    "    est = imPipeline(steps=[\n",
    "            ('smote', SMOTE()),\n",
    "            ('scaler', scaler),\n",
    "            ('f_selecter', VarianceThreshold()),\n",
    "            ('dim_reducer', PCA()),\n",
    "            ('classifier', LogisticRegression())])\n",
    "    estimator = GridSearchCV(est, parameters, cv=3)\n",
    "    estimator.fit(x_train_df, y_train_class_df)# gridsearch obejct has scanned thorugh all the best parameters to set it\n",
    "    prediction = estimator.predict(x_test_df)\n",
    "    matrix_class[i-1][1] = (roc_auc_score(y_test_class_df, prediction))\n",
    "    \n",
    "    # RandomForestClassifier\n",
    "    best = sys.maxsize\n",
    "    parameters = {'f_selecter__threshold':[0.5], \n",
    "                  'dim_reducer__n_components':[60],\n",
    "                  'classifier__n_estimators':[10,15]\n",
    "                 }\n",
    "    est = imPipeline(steps=[\n",
    "            ('smote', SMOTE()),\n",
    "            ('scaler', scaler),\n",
    "            ('f_selecter', VarianceThreshold()),\n",
    "            ('dim_reducer', PCA()),\n",
    "            ('classifier', RandomForestClassifier())])\n",
    "    estimator = GridSearchCV(est, parameters, cv=3)\n",
    "    estimator.fit(x_train_df, y_train_class_df)# gridsearch obejct has scanned thorugh all the best parameters to set it\n",
    "    prediction = estimator.predict(x_test_df)\n",
    "    matrix_class[i-1][2] = (roc_auc_score(y_test_class_df, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Results</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Linear</th>\n",
       "      <th>DecisionTree</th>\n",
       "      <th>MLPRegressor</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>181.931888</td>\n",
       "      <td>319.447198</td>\n",
       "      <td>200.483845</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126.571049</td>\n",
       "      <td>255.426125</td>\n",
       "      <td>137.452468</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>128.414533</td>\n",
       "      <td>218.447538</td>\n",
       "      <td>139.760740</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>237.443435</td>\n",
       "      <td>251.121457</td>\n",
       "      <td>119.134381</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.984230</td>\n",
       "      <td>16.158916</td>\n",
       "      <td>12.215067</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Linear  DecisionTree  MLPRegressor   \n",
       "0  181.931888    319.447198    200.483845  0\n",
       "1  126.571049    255.426125    137.452468  0\n",
       "2  128.414533    218.447538    139.760740  0\n",
       "3  237.443435    251.121457    119.134381  0\n",
       "4   12.984230     16.158916     12.215067  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KNeighbour</th>\n",
       "      <th>Logistic</th>\n",
       "      <th>RandomForests</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.557483</td>\n",
       "      <td>0.695993</td>\n",
       "      <td>0.544187</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.553577</td>\n",
       "      <td>0.687142</td>\n",
       "      <td>0.540281</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.553003</td>\n",
       "      <td>0.685314</td>\n",
       "      <td>0.549814</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.541284</td>\n",
       "      <td>0.691621</td>\n",
       "      <td>0.566693</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.551337</td>\n",
       "      <td>0.690018</td>\n",
       "      <td>0.550244</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   KNeighbour  Logistic  RandomForests   \n",
       "0    0.557483  0.695993       0.544187  0\n",
       "1    0.553577  0.687142       0.540281  0\n",
       "2    0.553003  0.685314       0.549814  0\n",
       "3    0.541284  0.691621       0.566693  0\n",
       "4    0.551337  0.690018       0.550244  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "number_of_learners = 3\n",
    "score_df = pd.DataFrame(matrix, columns=[\"Linear\", \"DecisionTree\", \"MLPRegressor\",\"\"])\n",
    "score_class_df = pd.DataFrame(matrix_class, columns=[\"KNeighbour\", \"Logistic\", \"RandomForests\",\"\"])\n",
    "for i in range(number_of_learners):\n",
    "    score_df.iloc[4,i] = np.sqrt(score_df.iloc[0:4,i].mean())\n",
    "    score_class_df.iloc[4,i] = score_class_df.iloc[0:4,i].mean()\n",
    "display(score_df)\n",
    "display(score_class_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Second Model</h3>\n",
    "<ul> \n",
    "    <li>We make use of the second Design matrix described above on the same parameters that we tried in the Model building 1.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Bilirubin</th>\n",
       "      <th>AST</th>\n",
       "      <th>ALT</th>\n",
       "      <th>ALP</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>TroponinT</th>\n",
       "      <th>TroponinI</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>MechVent</th>\n",
       "      <th>HR0</th>\n",
       "      <th>HR12</th>\n",
       "      <th>HR24</th>\n",
       "      <th>HR36</th>\n",
       "      <th>MAP0</th>\n",
       "      <th>MAP12</th>\n",
       "      <th>MAP24</th>\n",
       "      <th>MAP36</th>\n",
       "      <th>SysABP0</th>\n",
       "      <th>SysABP12</th>\n",
       "      <th>SysABP24</th>\n",
       "      <th>SysABP36</th>\n",
       "      <th>DiasABP0</th>\n",
       "      <th>DiasABP12</th>\n",
       "      <th>DiasABP24</th>\n",
       "      <th>DiasABP36</th>\n",
       "      <th>Urine0</th>\n",
       "      <th>Urine12</th>\n",
       "      <th>Urine24</th>\n",
       "      <th>Urine36</th>\n",
       "      <th>Weight0</th>\n",
       "      <th>Weight12</th>\n",
       "      <th>Weight24</th>\n",
       "      <th>Weight36</th>\n",
       "      <th>NISysABP0</th>\n",
       "      <th>NISysABP12</th>\n",
       "      <th>NISysABP24</th>\n",
       "      <th>NISysABP36</th>\n",
       "      <th>NIDiasABP0</th>\n",
       "      <th>NIDiasABP12</th>\n",
       "      <th>NIDiasABP24</th>\n",
       "      <th>NIDiasABP36</th>\n",
       "      <th>NIMAP0</th>\n",
       "      <th>NIMAP12</th>\n",
       "      <th>NIMAP24</th>\n",
       "      <th>NIMAP36</th>\n",
       "      <th>Temp0</th>\n",
       "      <th>Temp12</th>\n",
       "      <th>Temp24</th>\n",
       "      <th>Temp36</th>\n",
       "      <th>GCS0</th>\n",
       "      <th>GCS12</th>\n",
       "      <th>GCS24</th>\n",
       "      <th>GCS36</th>\n",
       "      <th>RespRate0</th>\n",
       "      <th>RespRate12</th>\n",
       "      <th>RespRate24</th>\n",
       "      <th>RespRate36</th>\n",
       "      <th>FiO20</th>\n",
       "      <th>FiO212</th>\n",
       "      <th>FiO224</th>\n",
       "      <th>FiO236</th>\n",
       "      <th>pH0</th>\n",
       "      <th>pH12</th>\n",
       "      <th>pH24</th>\n",
       "      <th>pH36</th>\n",
       "      <th>PaO20</th>\n",
       "      <th>PaO212</th>\n",
       "      <th>PaO224</th>\n",
       "      <th>PaO236</th>\n",
       "      <th>PaCO20</th>\n",
       "      <th>PaCO212</th>\n",
       "      <th>PaCO224</th>\n",
       "      <th>PaCO236</th>\n",
       "      <th>HCT0</th>\n",
       "      <th>HCT12</th>\n",
       "      <th>HCT24</th>\n",
       "      <th>HCT36</th>\n",
       "      <th>K0</th>\n",
       "      <th>K12</th>\n",
       "      <th>K24</th>\n",
       "      <th>K36</th>\n",
       "      <th>Creatinine0</th>\n",
       "      <th>Creatinine12</th>\n",
       "      <th>Creatinine24</th>\n",
       "      <th>Creatinine36</th>\n",
       "      <th>Platelets0</th>\n",
       "      <th>Platelets12</th>\n",
       "      <th>Platelets24</th>\n",
       "      <th>Platelets36</th>\n",
       "      <th>BUN0</th>\n",
       "      <th>BUN12</th>\n",
       "      <th>BUN24</th>\n",
       "      <th>BUN36</th>\n",
       "      <th>HCO30</th>\n",
       "      <th>HCO312</th>\n",
       "      <th>HCO324</th>\n",
       "      <th>HCO336</th>\n",
       "      <th>Mg0</th>\n",
       "      <th>Mg12</th>\n",
       "      <th>Mg24</th>\n",
       "      <th>Mg36</th>\n",
       "      <th>Na0</th>\n",
       "      <th>Na12</th>\n",
       "      <th>Na24</th>\n",
       "      <th>Na36</th>\n",
       "      <th>Glucose0</th>\n",
       "      <th>Glucose12</th>\n",
       "      <th>Glucose24</th>\n",
       "      <th>Glucose36</th>\n",
       "      <th>WBC0</th>\n",
       "      <th>WBC12</th>\n",
       "      <th>WBC24</th>\n",
       "      <th>WBC36</th>\n",
       "      <th>SaO20</th>\n",
       "      <th>SaO212</th>\n",
       "      <th>SaO224</th>\n",
       "      <th>SaO236</th>\n",
       "      <th>Lactate0</th>\n",
       "      <th>Lactate12</th>\n",
       "      <th>Lactate24</th>\n",
       "      <th>Lactate36</th>\n",
       "      <th>Coronary Care Unit</th>\n",
       "      <th>Cardiac Surgery Recovery Unit</th>\n",
       "      <th>Medical ICU</th>\n",
       "      <th>Surgical ICU</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recordid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>132539</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>170.094476</td>\n",
       "      <td>81.422068</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65.454545</td>\n",
       "      <td>66.090909</td>\n",
       "      <td>78.833333</td>\n",
       "      <td>78.571429</td>\n",
       "      <td>80.711313</td>\n",
       "      <td>79.358527</td>\n",
       "      <td>80.922914</td>\n",
       "      <td>81.500853</td>\n",
       "      <td>115.156032</td>\n",
       "      <td>118.234776</td>\n",
       "      <td>120.376778</td>\n",
       "      <td>121.372441</td>\n",
       "      <td>58.554776</td>\n",
       "      <td>58.822952</td>\n",
       "      <td>59.476010</td>\n",
       "      <td>59.876609</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>86.250000</td>\n",
       "      <td>173.333333</td>\n",
       "      <td>308.333333</td>\n",
       "      <td>82.648197</td>\n",
       "      <td>83.383737</td>\n",
       "      <td>83.651991</td>\n",
       "      <td>83.759171</td>\n",
       "      <td>111.700000</td>\n",
       "      <td>104.125000</td>\n",
       "      <td>111.333333</td>\n",
       "      <td>120.875000</td>\n",
       "      <td>50.600000</td>\n",
       "      <td>45.125000</td>\n",
       "      <td>44.333333</td>\n",
       "      <td>56.125000</td>\n",
       "      <td>70.967000</td>\n",
       "      <td>64.792500</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>77.708750</td>\n",
       "      <td>37.833333</td>\n",
       "      <td>37.100000</td>\n",
       "      <td>38.066667</td>\n",
       "      <td>37.766667</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.666667</td>\n",
       "      <td>15.00</td>\n",
       "      <td>16.545455</td>\n",
       "      <td>15.545455</td>\n",
       "      <td>17.857143</td>\n",
       "      <td>19.636364</td>\n",
       "      <td>0.575987</td>\n",
       "      <td>0.509855</td>\n",
       "      <td>0.506165</td>\n",
       "      <td>0.500966</td>\n",
       "      <td>7.422229</td>\n",
       "      <td>7.390886</td>\n",
       "      <td>7.565855</td>\n",
       "      <td>7.394966</td>\n",
       "      <td>167.119732</td>\n",
       "      <td>128.397906</td>\n",
       "      <td>121.516945</td>\n",
       "      <td>119.794255</td>\n",
       "      <td>40.839626</td>\n",
       "      <td>39.716565</td>\n",
       "      <td>39.386531</td>\n",
       "      <td>39.795741</td>\n",
       "      <td>33.600</td>\n",
       "      <td>31.031831</td>\n",
       "      <td>30.30</td>\n",
       "      <td>30.158595</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>4.168752</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.114329</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.507457</td>\n",
       "      <td>0.70000</td>\n",
       "      <td>1.479688</td>\n",
       "      <td>221.000000</td>\n",
       "      <td>201.597978</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>180.177778</td>\n",
       "      <td>13.0</td>\n",
       "      <td>26.8623</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>27.386189</td>\n",
       "      <td>26.0</td>\n",
       "      <td>23.545873</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>24.082561</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.038096</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>2.070901</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>139.053437</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>138.672271</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>131.459169</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>133.509428</td>\n",
       "      <td>11.2</td>\n",
       "      <td>12.730003</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>12.236603</td>\n",
       "      <td>97.184859</td>\n",
       "      <td>96.87319</td>\n",
       "      <td>96.396396</td>\n",
       "      <td>96.604748</td>\n",
       "      <td>2.575443</td>\n",
       "      <td>2.395522</td>\n",
       "      <td>2.447271</td>\n",
       "      <td>2.711155</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132540</th>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>175.300000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>87.076923</td>\n",
       "      <td>80.055556</td>\n",
       "      <td>75.307692</td>\n",
       "      <td>73.636364</td>\n",
       "      <td>75.740741</td>\n",
       "      <td>76.111111</td>\n",
       "      <td>78.384615</td>\n",
       "      <td>80.111111</td>\n",
       "      <td>106.714286</td>\n",
       "      <td>114.222222</td>\n",
       "      <td>119.923077</td>\n",
       "      <td>123.222222</td>\n",
       "      <td>60.892857</td>\n",
       "      <td>56.888889</td>\n",
       "      <td>57.846154</td>\n",
       "      <td>58.222222</td>\n",
       "      <td>158.933333</td>\n",
       "      <td>103.181818</td>\n",
       "      <td>170.714286</td>\n",
       "      <td>187.500000</td>\n",
       "      <td>82.648197</td>\n",
       "      <td>80.600000</td>\n",
       "      <td>80.600000</td>\n",
       "      <td>81.236364</td>\n",
       "      <td>117.173844</td>\n",
       "      <td>111.750000</td>\n",
       "      <td>115.142857</td>\n",
       "      <td>107.333333</td>\n",
       "      <td>57.986820</td>\n",
       "      <td>57.500000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>45.666667</td>\n",
       "      <td>76.655203</td>\n",
       "      <td>75.580000</td>\n",
       "      <td>79.047143</td>\n",
       "      <td>66.223333</td>\n",
       "      <td>36.688462</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>36.850000</td>\n",
       "      <td>36.800000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.666667</td>\n",
       "      <td>14.25</td>\n",
       "      <td>19.613425</td>\n",
       "      <td>19.551701</td>\n",
       "      <td>19.646032</td>\n",
       "      <td>19.636608</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.509855</td>\n",
       "      <td>0.506165</td>\n",
       "      <td>0.500966</td>\n",
       "      <td>7.385000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>7.565855</td>\n",
       "      <td>7.385000</td>\n",
       "      <td>226.250000</td>\n",
       "      <td>128.397906</td>\n",
       "      <td>121.516945</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>39.716565</td>\n",
       "      <td>39.386531</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>27.625</td>\n",
       "      <td>28.900000</td>\n",
       "      <td>30.70</td>\n",
       "      <td>29.450000</td>\n",
       "      <td>4.154586</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>4.057783</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.50415</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>190.333333</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>191.404915</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>28.141693</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>23.815756</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>2.061273</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>139.341443</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>139.309433</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>150.083243</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>132.354891</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>7.4</td>\n",
       "      <td>13.100000</td>\n",
       "      <td>12.469809</td>\n",
       "      <td>13.300000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>97.00000</td>\n",
       "      <td>96.396396</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>2.575443</td>\n",
       "      <td>2.395522</td>\n",
       "      <td>2.447271</td>\n",
       "      <td>2.711155</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132541</th>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>170.094476</td>\n",
       "      <td>56.700000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>83.250000</td>\n",
       "      <td>87.666667</td>\n",
       "      <td>71.166667</td>\n",
       "      <td>80.711313</td>\n",
       "      <td>79.358527</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>87.250000</td>\n",
       "      <td>115.156032</td>\n",
       "      <td>118.234776</td>\n",
       "      <td>137.500000</td>\n",
       "      <td>121.750000</td>\n",
       "      <td>58.554776</td>\n",
       "      <td>58.822952</td>\n",
       "      <td>75.500000</td>\n",
       "      <td>64.333333</td>\n",
       "      <td>111.818182</td>\n",
       "      <td>185.500000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>58.800000</td>\n",
       "      <td>56.700000</td>\n",
       "      <td>56.700000</td>\n",
       "      <td>56.700000</td>\n",
       "      <td>56.700000</td>\n",
       "      <td>132.812500</td>\n",
       "      <td>136.750000</td>\n",
       "      <td>124.500000</td>\n",
       "      <td>119.505340</td>\n",
       "      <td>78.750000</td>\n",
       "      <td>80.583333</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>57.339935</td>\n",
       "      <td>96.767500</td>\n",
       "      <td>99.305000</td>\n",
       "      <td>92.163750</td>\n",
       "      <td>76.900645</td>\n",
       "      <td>37.825000</td>\n",
       "      <td>37.233333</td>\n",
       "      <td>38.300000</td>\n",
       "      <td>37.833333</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.00</td>\n",
       "      <td>19.613425</td>\n",
       "      <td>19.551701</td>\n",
       "      <td>19.646032</td>\n",
       "      <td>19.636608</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>7.422229</td>\n",
       "      <td>7.510000</td>\n",
       "      <td>7.490000</td>\n",
       "      <td>7.394966</td>\n",
       "      <td>167.119732</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>157.666667</td>\n",
       "      <td>119.794255</td>\n",
       "      <td>40.839626</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>39.795741</td>\n",
       "      <td>28.500</td>\n",
       "      <td>26.700000</td>\n",
       "      <td>28.85</td>\n",
       "      <td>29.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>2.850000</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.50415</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>191.404915</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>28.141693</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>23.815756</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.850000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>139.309433</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>132.354891</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>12.469809</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>97.184859</td>\n",
       "      <td>95.00000</td>\n",
       "      <td>96.396396</td>\n",
       "      <td>96.604748</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>2.711155</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132543</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>180.300000</td>\n",
       "      <td>84.600000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72.238095</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>63.769231</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>80.711313</td>\n",
       "      <td>79.358527</td>\n",
       "      <td>80.922914</td>\n",
       "      <td>81.500853</td>\n",
       "      <td>115.156032</td>\n",
       "      <td>118.234776</td>\n",
       "      <td>120.376778</td>\n",
       "      <td>121.372441</td>\n",
       "      <td>58.554776</td>\n",
       "      <td>58.822952</td>\n",
       "      <td>59.476010</td>\n",
       "      <td>59.876609</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>675.000000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>84.600000</td>\n",
       "      <td>84.600000</td>\n",
       "      <td>84.600000</td>\n",
       "      <td>84.600000</td>\n",
       "      <td>122.571429</td>\n",
       "      <td>122.090909</td>\n",
       "      <td>117.750000</td>\n",
       "      <td>121.545455</td>\n",
       "      <td>68.285714</td>\n",
       "      <td>61.636364</td>\n",
       "      <td>61.500000</td>\n",
       "      <td>66.181818</td>\n",
       "      <td>86.381429</td>\n",
       "      <td>81.788182</td>\n",
       "      <td>80.250000</td>\n",
       "      <td>84.637273</td>\n",
       "      <td>35.966667</td>\n",
       "      <td>36.433333</td>\n",
       "      <td>36.133333</td>\n",
       "      <td>36.333333</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.00</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>14.916667</td>\n",
       "      <td>13.384615</td>\n",
       "      <td>16.750000</td>\n",
       "      <td>0.575987</td>\n",
       "      <td>0.509855</td>\n",
       "      <td>0.506165</td>\n",
       "      <td>0.500966</td>\n",
       "      <td>7.422229</td>\n",
       "      <td>7.390886</td>\n",
       "      <td>7.565855</td>\n",
       "      <td>7.394966</td>\n",
       "      <td>167.119732</td>\n",
       "      <td>128.397906</td>\n",
       "      <td>121.516945</td>\n",
       "      <td>119.794255</td>\n",
       "      <td>40.839626</td>\n",
       "      <td>39.716565</td>\n",
       "      <td>39.386531</td>\n",
       "      <td>39.795741</td>\n",
       "      <td>37.300</td>\n",
       "      <td>36.850000</td>\n",
       "      <td>36.20</td>\n",
       "      <td>36.300000</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>4.168752</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>4.114329</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.507457</td>\n",
       "      <td>0.70000</td>\n",
       "      <td>1.479688</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>201.597978</td>\n",
       "      <td>284.000000</td>\n",
       "      <td>180.177778</td>\n",
       "      <td>20.0</td>\n",
       "      <td>26.8623</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>27.386189</td>\n",
       "      <td>27.0</td>\n",
       "      <td>23.545873</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>24.082561</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.038096</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>2.070901</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>139.053437</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>138.672271</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>131.459169</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>133.509428</td>\n",
       "      <td>8.8</td>\n",
       "      <td>12.730003</td>\n",
       "      <td>7.900000</td>\n",
       "      <td>12.236603</td>\n",
       "      <td>97.184859</td>\n",
       "      <td>96.87319</td>\n",
       "      <td>96.396396</td>\n",
       "      <td>96.604748</td>\n",
       "      <td>2.575443</td>\n",
       "      <td>2.395522</td>\n",
       "      <td>2.447271</td>\n",
       "      <td>2.711155</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132545</th>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>170.094476</td>\n",
       "      <td>81.422068</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84.428571</td>\n",
       "      <td>72.833333</td>\n",
       "      <td>69.727273</td>\n",
       "      <td>70.454545</td>\n",
       "      <td>80.711313</td>\n",
       "      <td>79.358527</td>\n",
       "      <td>80.922914</td>\n",
       "      <td>81.500853</td>\n",
       "      <td>115.156032</td>\n",
       "      <td>118.234776</td>\n",
       "      <td>120.376778</td>\n",
       "      <td>121.372441</td>\n",
       "      <td>58.554776</td>\n",
       "      <td>58.822952</td>\n",
       "      <td>59.476010</td>\n",
       "      <td>59.876609</td>\n",
       "      <td>65.454545</td>\n",
       "      <td>50.600000</td>\n",
       "      <td>73.333333</td>\n",
       "      <td>59.375000</td>\n",
       "      <td>82.648197</td>\n",
       "      <td>83.383737</td>\n",
       "      <td>83.651991</td>\n",
       "      <td>83.759171</td>\n",
       "      <td>137.642857</td>\n",
       "      <td>135.250000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>127.909091</td>\n",
       "      <td>46.571429</td>\n",
       "      <td>55.500000</td>\n",
       "      <td>46.200000</td>\n",
       "      <td>37.090909</td>\n",
       "      <td>76.932143</td>\n",
       "      <td>82.082500</td>\n",
       "      <td>74.799000</td>\n",
       "      <td>67.363636</td>\n",
       "      <td>36.950000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>36.633333</td>\n",
       "      <td>36.700000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.00</td>\n",
       "      <td>20.714286</td>\n",
       "      <td>18.666667</td>\n",
       "      <td>17.818182</td>\n",
       "      <td>19.090909</td>\n",
       "      <td>0.575987</td>\n",
       "      <td>0.509855</td>\n",
       "      <td>0.506165</td>\n",
       "      <td>0.500966</td>\n",
       "      <td>7.422229</td>\n",
       "      <td>7.390886</td>\n",
       "      <td>7.565855</td>\n",
       "      <td>7.394966</td>\n",
       "      <td>167.119732</td>\n",
       "      <td>128.397906</td>\n",
       "      <td>121.516945</td>\n",
       "      <td>119.794255</td>\n",
       "      <td>40.839626</td>\n",
       "      <td>39.716565</td>\n",
       "      <td>39.386531</td>\n",
       "      <td>39.795741</td>\n",
       "      <td>22.600</td>\n",
       "      <td>30.466667</td>\n",
       "      <td>32.40</td>\n",
       "      <td>30.900000</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>3.850000</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>4.114329</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.507457</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.479688</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>201.597978</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>180.177778</td>\n",
       "      <td>45.0</td>\n",
       "      <td>26.8623</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>27.386189</td>\n",
       "      <td>18.0</td>\n",
       "      <td>23.545873</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>24.082561</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.038096</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>2.070901</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>139.053437</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>138.672271</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>131.459169</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>133.509428</td>\n",
       "      <td>3.8</td>\n",
       "      <td>12.730003</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>12.236603</td>\n",
       "      <td>97.184859</td>\n",
       "      <td>96.87319</td>\n",
       "      <td>96.396396</td>\n",
       "      <td>96.604748</td>\n",
       "      <td>2.575443</td>\n",
       "      <td>2.395522</td>\n",
       "      <td>2.447271</td>\n",
       "      <td>2.711155</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Age  Gender      Height     Weight  Bilirubin  AST  ALT  ALP  \\\n",
       "recordid                                                                 \n",
       "132539     54       0  170.094476  81.422068          0    0    0    0   \n",
       "132540     76       1  175.300000  76.000000          0    0    0    0   \n",
       "132541     44       0  170.094476  56.700000          1    1    1    1   \n",
       "132543     68       1  180.300000  84.600000          1    1    1    1   \n",
       "132545     88       0  170.094476  81.422068          0    0    0    0   \n",
       "\n",
       "          Albumin  TroponinT  TroponinI  Cholesterol  MechVent        HR0  \\\n",
       "recordid                                                                    \n",
       "132539          0          0          0            0         0  65.454545   \n",
       "132540          0          0          0            0         1  87.076923   \n",
       "132541          1          0          0            0         1  90.000000   \n",
       "132543          1          0          0            0         0  72.238095   \n",
       "132545          1          0          0            0         0  84.428571   \n",
       "\n",
       "               HR12       HR24       HR36       MAP0      MAP12       MAP24  \\\n",
       "recordid                                                                      \n",
       "132539    66.090909  78.833333  78.571429  80.711313  79.358527   80.922914   \n",
       "132540    80.055556  75.307692  73.636364  75.740741  76.111111   78.384615   \n",
       "132541    83.250000  87.666667  71.166667  80.711313  79.358527  100.000000   \n",
       "132543    72.500000  63.769231  74.000000  80.711313  79.358527   80.922914   \n",
       "132545    72.833333  69.727273  70.454545  80.711313  79.358527   80.922914   \n",
       "\n",
       "              MAP36     SysABP0    SysABP12    SysABP24    SysABP36  \\\n",
       "recordid                                                              \n",
       "132539    81.500853  115.156032  118.234776  120.376778  121.372441   \n",
       "132540    80.111111  106.714286  114.222222  119.923077  123.222222   \n",
       "132541    87.250000  115.156032  118.234776  137.500000  121.750000   \n",
       "132543    81.500853  115.156032  118.234776  120.376778  121.372441   \n",
       "132545    81.500853  115.156032  118.234776  120.376778  121.372441   \n",
       "\n",
       "           DiasABP0  DiasABP12  DiasABP24  DiasABP36      Urine0     Urine12  \\\n",
       "recordid                                                                       \n",
       "132539    58.554776  58.822952  59.476010  59.876609   92.000000   86.250000   \n",
       "132540    60.892857  56.888889  57.846154  58.222222  158.933333  103.181818   \n",
       "132541    58.554776  58.822952  75.500000  64.333333  111.818182  185.500000   \n",
       "132543    58.554776  58.822952  59.476010  59.876609  600.000000  400.000000   \n",
       "132545    58.554776  58.822952  59.476010  59.876609   65.454545   50.600000   \n",
       "\n",
       "             Urine24     Urine36    Weight0   Weight12   Weight24   Weight36  \\\n",
       "recordid                                                                       \n",
       "132539    173.333333  308.333333  82.648197  83.383737  83.651991  83.759171   \n",
       "132540    170.714286  187.500000  82.648197  80.600000  80.600000  81.236364   \n",
       "132541    150.000000   58.800000  56.700000  56.700000  56.700000  56.700000   \n",
       "132543    675.000000  600.000000  84.600000  84.600000  84.600000  84.600000   \n",
       "132545     73.333333   59.375000  82.648197  83.383737  83.651991  83.759171   \n",
       "\n",
       "           NISysABP0  NISysABP12  NISysABP24  NISysABP36  NIDiasABP0  \\\n",
       "recordid                                                               \n",
       "132539    111.700000  104.125000  111.333333  120.875000   50.600000   \n",
       "132540    117.173844  111.750000  115.142857  107.333333   57.986820   \n",
       "132541    132.812500  136.750000  124.500000  119.505340   78.750000   \n",
       "132543    122.571429  122.090909  117.750000  121.545455   68.285714   \n",
       "132545    137.642857  135.250000  132.000000  127.909091   46.571429   \n",
       "\n",
       "          NIDiasABP12  NIDiasABP24  NIDiasABP36     NIMAP0    NIMAP12  \\\n",
       "recordid                                                                \n",
       "132539      45.125000    44.333333    56.125000  70.967000  64.792500   \n",
       "132540      57.500000    61.000000    45.666667  76.655203  75.580000   \n",
       "132541      80.583333    76.000000    57.339935  96.767500  99.305000   \n",
       "132543      61.636364    61.500000    66.181818  86.381429  81.788182   \n",
       "132545      55.500000    46.200000    37.090909  76.932143  82.082500   \n",
       "\n",
       "            NIMAP24    NIMAP36      Temp0     Temp12     Temp24     Temp36  \\\n",
       "recordid                                                                     \n",
       "132539    66.666667  77.708750  37.833333  37.100000  38.066667  37.766667   \n",
       "132540    79.047143  66.223333  36.688462  37.500000  36.850000  36.800000   \n",
       "132541    92.163750  76.900645  37.825000  37.233333  38.300000  37.833333   \n",
       "132543    80.250000  84.637273  35.966667  36.433333  36.133333  36.333333   \n",
       "132545    74.799000  67.363636  36.950000  37.000000  36.633333  36.700000   \n",
       "\n",
       "               GCS0  GCS12      GCS24  GCS36  RespRate0  RespRate12  \\\n",
       "recordid                                                              \n",
       "132539    15.000000   15.0  14.666667  15.00  16.545455   15.545455   \n",
       "132540    10.800000   15.0  14.666667  14.25  19.613425   19.551701   \n",
       "132541     7.333333    6.0   5.000000   5.00  19.613425   19.551701   \n",
       "132543    14.500000   15.0  15.000000  15.00  16.100000   14.916667   \n",
       "132545    15.000000   15.0  15.000000  15.00  20.714286   18.666667   \n",
       "\n",
       "          RespRate24  RespRate36     FiO20    FiO212    FiO224    FiO236  \\\n",
       "recordid                                                                   \n",
       "132539     17.857143   19.636364  0.575987  0.509855  0.506165  0.500966   \n",
       "132540     19.646032   19.636608  0.560000  0.509855  0.506165  0.500966   \n",
       "132541     19.646032   19.636608  0.750000  0.500000  0.460000  0.400000   \n",
       "132543     13.384615   16.750000  0.575987  0.509855  0.506165  0.500966   \n",
       "132545     17.818182   19.090909  0.575987  0.509855  0.506165  0.500966   \n",
       "\n",
       "               pH0      pH12      pH24      pH36       PaO20      PaO212  \\\n",
       "recordid                                                                   \n",
       "132539    7.422229  7.390886  7.565855  7.394966  167.119732  128.397906   \n",
       "132540    7.385000  7.400000  7.565855  7.385000  226.250000  128.397906   \n",
       "132541    7.422229  7.510000  7.490000  7.394966  167.119732   65.000000   \n",
       "132543    7.422229  7.390886  7.565855  7.394966  167.119732  128.397906   \n",
       "132545    7.422229  7.390886  7.565855  7.394966  167.119732  128.397906   \n",
       "\n",
       "              PaO224      PaO236     PaCO20    PaCO212    PaCO224    PaCO236  \\\n",
       "recordid                                                                       \n",
       "132539    121.516945  119.794255  40.839626  39.716565  39.386531  39.795741   \n",
       "132540    121.516945  111.000000  37.000000  39.716565  39.386531  45.000000   \n",
       "132541    157.666667  119.794255  40.839626  37.000000  35.000000  39.795741   \n",
       "132543    121.516945  119.794255  40.839626  39.716565  39.386531  39.795741   \n",
       "132545    121.516945  119.794255  40.839626  39.716565  39.386531  39.795741   \n",
       "\n",
       "            HCT0      HCT12  HCT24      HCT36        K0       K12       K24  \\\n",
       "recordid                                                                      \n",
       "132539    33.600  31.031831  30.30  30.158595  4.400000  4.168752  4.000000   \n",
       "132540    27.625  28.900000  30.70  29.450000  4.154586  4.300000  4.057783   \n",
       "132541    28.500  26.700000  28.85  29.400000  3.300000  8.600000  2.850000   \n",
       "132543    37.300  36.850000  36.20  36.300000  4.200000  4.168752  3.800000   \n",
       "132545    22.600  30.466667  32.40  30.900000  4.900000  3.850000  4.100000   \n",
       "\n",
       "               K36  Creatinine0  Creatinine12  Creatinine24  Creatinine36  \\\n",
       "recordid                                                                    \n",
       "132539    4.114329          0.8      1.507457       0.70000      1.479688   \n",
       "132540    3.500000          0.8      1.200000       1.50415      1.300000   \n",
       "132541    3.700000          0.4      0.300000       1.50415      0.300000   \n",
       "132543    4.114329          0.7      1.507457       0.70000      1.479688   \n",
       "132545    4.114329          1.0      1.507457       1.00000      1.479688   \n",
       "\n",
       "          Platelets0  Platelets12  Platelets24  Platelets36  BUN0    BUN12  \\\n",
       "recordid                                                                     \n",
       "132539    221.000000   201.597978   185.000000   180.177778  13.0  26.8623   \n",
       "132540    190.333333   187.000000   191.404915   135.000000  16.0  18.0000   \n",
       "132541     72.000000    84.000000   191.404915   113.000000   8.0   3.0000   \n",
       "132543    315.000000   201.597978   284.000000   180.177778  20.0  26.8623   \n",
       "132545    109.000000   201.597978    97.000000   180.177778  45.0  26.8623   \n",
       "\n",
       "              BUN24      BUN36  HCO30     HCO312     HCO324     HCO336  Mg0  \\\n",
       "recordid                                                                      \n",
       "132539     8.000000  27.386189   26.0  23.545873  28.000000  24.082561  1.5   \n",
       "132540    28.141693  21.000000   21.0  22.000000  23.815756  24.000000  3.1   \n",
       "132541    28.141693   3.000000   24.0  26.000000  23.815756  25.000000  1.9   \n",
       "132543    10.000000  27.386189   27.0  23.545873  28.000000  24.082561  2.1   \n",
       "132545    25.000000  27.386189   18.0  23.545873  20.000000  24.082561  1.5   \n",
       "\n",
       "              Mg12      Mg24      Mg36         Na0        Na12        Na24  \\\n",
       "recordid                                                                     \n",
       "132539    2.038096  1.900000  2.070901  137.000000  139.053437  136.000000   \n",
       "132540    1.900000  2.061273  2.100000  139.341443  139.000000  139.309433   \n",
       "132541    1.300000  1.850000  1.700000  137.000000  140.000000  139.309433   \n",
       "132543    2.038096  1.900000  2.070901  141.000000  139.053437  137.000000   \n",
       "132545    2.038096  1.600000  2.070901  140.000000  139.053437  139.000000   \n",
       "\n",
       "                Na36    Glucose0   Glucose12   Glucose24   Glucose36  WBC0  \\\n",
       "recordid                                                                     \n",
       "132539    138.672271  205.000000  131.459169  115.000000  133.509428  11.2   \n",
       "132540    135.000000  150.083243  105.000000  132.354891  146.000000   7.4   \n",
       "132541    138.000000  141.000000  119.000000  132.354891  143.000000   4.2   \n",
       "132543    138.672271  106.000000  131.459169  117.000000  133.509428   8.8   \n",
       "132545    138.672271  113.000000  131.459169   92.000000  133.509428   3.8   \n",
       "\n",
       "              WBC12      WBC24      WBC36      SaO20    SaO212     SaO224  \\\n",
       "recordid                                                                    \n",
       "132539    12.730003   9.400000  12.236603  97.184859  96.87319  96.396396   \n",
       "132540    13.100000  12.469809  13.300000  98.000000  97.00000  96.396396   \n",
       "132541     3.700000  12.469809   6.200000  97.184859  95.00000  96.396396   \n",
       "132543    12.730003   7.900000  12.236603  97.184859  96.87319  96.396396   \n",
       "132545    12.730003   4.800000  12.236603  97.184859  96.87319  96.396396   \n",
       "\n",
       "             SaO236  Lactate0  Lactate12  Lactate24  Lactate36  \\\n",
       "recordid                                                         \n",
       "132539    96.604748  2.575443   2.395522   2.447271   2.711155   \n",
       "132540    95.000000  2.575443   2.395522   2.447271   2.711155   \n",
       "132541    96.604748  1.300000   1.900000   0.900000   2.711155   \n",
       "132543    96.604748  2.575443   2.395522   2.447271   2.711155   \n",
       "132545    96.604748  2.575443   2.395522   2.447271   2.711155   \n",
       "\n",
       "          Coronary Care Unit  Cardiac Surgery Recovery Unit  Medical ICU  \\\n",
       "recordid                                                                   \n",
       "132539                     0                              0            0   \n",
       "132540                     0                              1            0   \n",
       "132541                     0                              0            1   \n",
       "132543                     0                              0            1   \n",
       "132545                     0                              0            1   \n",
       "\n",
       "          Surgical ICU  \n",
       "recordid                \n",
       "132539               1  \n",
       "132540               0  \n",
       "132541               0  \n",
       "132543               0  \n",
       "132545               0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Bilirubin</th>\n",
       "      <th>AST</th>\n",
       "      <th>ALT</th>\n",
       "      <th>ALP</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>TroponinT</th>\n",
       "      <th>TroponinI</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>MechVent</th>\n",
       "      <th>HR0</th>\n",
       "      <th>HR12</th>\n",
       "      <th>HR24</th>\n",
       "      <th>HR36</th>\n",
       "      <th>MAP0</th>\n",
       "      <th>MAP12</th>\n",
       "      <th>MAP24</th>\n",
       "      <th>MAP36</th>\n",
       "      <th>SysABP0</th>\n",
       "      <th>SysABP12</th>\n",
       "      <th>SysABP24</th>\n",
       "      <th>SysABP36</th>\n",
       "      <th>DiasABP0</th>\n",
       "      <th>DiasABP12</th>\n",
       "      <th>DiasABP24</th>\n",
       "      <th>DiasABP36</th>\n",
       "      <th>Urine0</th>\n",
       "      <th>Urine12</th>\n",
       "      <th>Urine24</th>\n",
       "      <th>Urine36</th>\n",
       "      <th>Weight0</th>\n",
       "      <th>Weight12</th>\n",
       "      <th>Weight24</th>\n",
       "      <th>Weight36</th>\n",
       "      <th>NISysABP0</th>\n",
       "      <th>NISysABP12</th>\n",
       "      <th>NISysABP24</th>\n",
       "      <th>NISysABP36</th>\n",
       "      <th>NIDiasABP0</th>\n",
       "      <th>NIDiasABP12</th>\n",
       "      <th>NIDiasABP24</th>\n",
       "      <th>NIDiasABP36</th>\n",
       "      <th>NIMAP0</th>\n",
       "      <th>NIMAP12</th>\n",
       "      <th>NIMAP24</th>\n",
       "      <th>NIMAP36</th>\n",
       "      <th>Temp0</th>\n",
       "      <th>Temp12</th>\n",
       "      <th>Temp24</th>\n",
       "      <th>Temp36</th>\n",
       "      <th>GCS0</th>\n",
       "      <th>GCS12</th>\n",
       "      <th>GCS24</th>\n",
       "      <th>GCS36</th>\n",
       "      <th>RespRate0</th>\n",
       "      <th>RespRate12</th>\n",
       "      <th>RespRate24</th>\n",
       "      <th>RespRate36</th>\n",
       "      <th>FiO20</th>\n",
       "      <th>FiO212</th>\n",
       "      <th>FiO224</th>\n",
       "      <th>FiO236</th>\n",
       "      <th>pH0</th>\n",
       "      <th>pH12</th>\n",
       "      <th>pH24</th>\n",
       "      <th>pH36</th>\n",
       "      <th>PaO20</th>\n",
       "      <th>PaO212</th>\n",
       "      <th>PaO224</th>\n",
       "      <th>PaO236</th>\n",
       "      <th>PaCO20</th>\n",
       "      <th>PaCO212</th>\n",
       "      <th>PaCO224</th>\n",
       "      <th>PaCO236</th>\n",
       "      <th>HCT0</th>\n",
       "      <th>HCT12</th>\n",
       "      <th>HCT24</th>\n",
       "      <th>HCT36</th>\n",
       "      <th>K0</th>\n",
       "      <th>K12</th>\n",
       "      <th>K24</th>\n",
       "      <th>K36</th>\n",
       "      <th>Creatinine0</th>\n",
       "      <th>Creatinine12</th>\n",
       "      <th>Creatinine24</th>\n",
       "      <th>Creatinine36</th>\n",
       "      <th>Platelets0</th>\n",
       "      <th>Platelets12</th>\n",
       "      <th>Platelets24</th>\n",
       "      <th>Platelets36</th>\n",
       "      <th>BUN0</th>\n",
       "      <th>BUN12</th>\n",
       "      <th>BUN24</th>\n",
       "      <th>BUN36</th>\n",
       "      <th>HCO30</th>\n",
       "      <th>HCO312</th>\n",
       "      <th>HCO324</th>\n",
       "      <th>HCO336</th>\n",
       "      <th>Mg0</th>\n",
       "      <th>Mg12</th>\n",
       "      <th>Mg24</th>\n",
       "      <th>Mg36</th>\n",
       "      <th>Na0</th>\n",
       "      <th>Na12</th>\n",
       "      <th>Na24</th>\n",
       "      <th>Na36</th>\n",
       "      <th>Glucose0</th>\n",
       "      <th>Glucose12</th>\n",
       "      <th>Glucose24</th>\n",
       "      <th>Glucose36</th>\n",
       "      <th>WBC0</th>\n",
       "      <th>WBC12</th>\n",
       "      <th>WBC24</th>\n",
       "      <th>WBC36</th>\n",
       "      <th>SaO20</th>\n",
       "      <th>SaO212</th>\n",
       "      <th>SaO224</th>\n",
       "      <th>SaO236</th>\n",
       "      <th>Lactate0</th>\n",
       "      <th>Lactate12</th>\n",
       "      <th>Lactate24</th>\n",
       "      <th>Lactate36</th>\n",
       "      <th>Coronary Care Unit</th>\n",
       "      <th>Cardiac Surgery Recovery Unit</th>\n",
       "      <th>Medical ICU</th>\n",
       "      <th>Surgical ICU</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recordid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135076</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>177.80000</td>\n",
       "      <td>110.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>104.416667</td>\n",
       "      <td>110.777778</td>\n",
       "      <td>104.454545</td>\n",
       "      <td>81.250000</td>\n",
       "      <td>104.777778</td>\n",
       "      <td>116.714286</td>\n",
       "      <td>148.500000</td>\n",
       "      <td>112.750000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>161.571429</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>133.142857</td>\n",
       "      <td>81.333333</td>\n",
       "      <td>90.142857</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>84.285714</td>\n",
       "      <td>161.428571</td>\n",
       "      <td>125.714286</td>\n",
       "      <td>131.666667</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>81.345255</td>\n",
       "      <td>108.1</td>\n",
       "      <td>108.1</td>\n",
       "      <td>108.1</td>\n",
       "      <td>120.500000</td>\n",
       "      <td>158.875000</td>\n",
       "      <td>148.272727</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>66.500000</td>\n",
       "      <td>70.375000</td>\n",
       "      <td>77.818182</td>\n",
       "      <td>76.090909</td>\n",
       "      <td>84.510000</td>\n",
       "      <td>99.862500</td>\n",
       "      <td>101.308182</td>\n",
       "      <td>99.390909</td>\n",
       "      <td>38.066667</td>\n",
       "      <td>37.600000</td>\n",
       "      <td>37.70000</td>\n",
       "      <td>37.850000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>11.75</td>\n",
       "      <td>10.166667</td>\n",
       "      <td>8.2</td>\n",
       "      <td>19.424873</td>\n",
       "      <td>19.622286</td>\n",
       "      <td>19.55034</td>\n",
       "      <td>19.782182</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.507617</td>\n",
       "      <td>0.515098</td>\n",
       "      <td>0.50479</td>\n",
       "      <td>7.315000</td>\n",
       "      <td>7.430</td>\n",
       "      <td>7.394394</td>\n",
       "      <td>7.430000</td>\n",
       "      <td>219.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>116.867607</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>39.657571</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>40.100000</td>\n",
       "      <td>37.700000</td>\n",
       "      <td>30.961398</td>\n",
       "      <td>37.800000</td>\n",
       "      <td>4.30000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>4.053152</td>\n",
       "      <td>3.950000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.449714</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>188.00000</td>\n",
       "      <td>194.81982</td>\n",
       "      <td>231.00000</td>\n",
       "      <td>12.00000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>26.487741</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>23.775188</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>2.051234</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>138.878042</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>130.070914</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>14.40000</td>\n",
       "      <td>15.900000</td>\n",
       "      <td>12.230936</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>97.027244</td>\n",
       "      <td>96.526403</td>\n",
       "      <td>96.56986</td>\n",
       "      <td>96.444055</td>\n",
       "      <td>2.695739</td>\n",
       "      <td>2.383319</td>\n",
       "      <td>2.131766</td>\n",
       "      <td>2.090862</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135077</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>169.30233</td>\n",
       "      <td>220.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>70.428571</td>\n",
       "      <td>87.714286</td>\n",
       "      <td>78.250000</td>\n",
       "      <td>98.727273</td>\n",
       "      <td>64.285714</td>\n",
       "      <td>75.714286</td>\n",
       "      <td>94.250000</td>\n",
       "      <td>97.272727</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>122.071429</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>48.428571</td>\n",
       "      <td>53.714286</td>\n",
       "      <td>69.166667</td>\n",
       "      <td>65.636364</td>\n",
       "      <td>83.500000</td>\n",
       "      <td>1.363636</td>\n",
       "      <td>4.133333</td>\n",
       "      <td>5.230769</td>\n",
       "      <td>117.142857</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>117.325730</td>\n",
       "      <td>115.694079</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>119.579156</td>\n",
       "      <td>58.746771</td>\n",
       "      <td>57.478308</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>57.771990</td>\n",
       "      <td>76.931481</td>\n",
       "      <td>75.786899</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>77.191414</td>\n",
       "      <td>37.766667</td>\n",
       "      <td>38.280000</td>\n",
       "      <td>38.24000</td>\n",
       "      <td>37.700000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>13.80</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>19.424873</td>\n",
       "      <td>19.622286</td>\n",
       "      <td>19.55034</td>\n",
       "      <td>19.782182</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.66000</td>\n",
       "      <td>7.260000</td>\n",
       "      <td>7.316</td>\n",
       "      <td>7.320000</td>\n",
       "      <td>7.258000</td>\n",
       "      <td>129.500000</td>\n",
       "      <td>117.800000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>45.600000</td>\n",
       "      <td>33.300000</td>\n",
       "      <td>31.147414</td>\n",
       "      <td>32.100000</td>\n",
       "      <td>30.054173</td>\n",
       "      <td>5.70000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>3.9</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>199.19211</td>\n",
       "      <td>150.00000</td>\n",
       "      <td>176.60216</td>\n",
       "      <td>24.00000</td>\n",
       "      <td>31.0</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.052855</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>140.500000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>10.10000</td>\n",
       "      <td>12.437016</td>\n",
       "      <td>13.200000</td>\n",
       "      <td>12.394275</td>\n",
       "      <td>97.027244</td>\n",
       "      <td>96.526403</td>\n",
       "      <td>96.56986</td>\n",
       "      <td>96.444055</td>\n",
       "      <td>2.695739</td>\n",
       "      <td>2.383319</td>\n",
       "      <td>2.131766</td>\n",
       "      <td>2.090862</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135079</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>169.30233</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>72.739130</td>\n",
       "      <td>88.076923</td>\n",
       "      <td>79.666667</td>\n",
       "      <td>78.909091</td>\n",
       "      <td>76.608696</td>\n",
       "      <td>80.384615</td>\n",
       "      <td>84.750000</td>\n",
       "      <td>82.254622</td>\n",
       "      <td>116.652174</td>\n",
       "      <td>123.846154</td>\n",
       "      <td>129.916667</td>\n",
       "      <td>122.600009</td>\n",
       "      <td>54.391304</td>\n",
       "      <td>57.230769</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>60.724172</td>\n",
       "      <td>161.785714</td>\n",
       "      <td>115.833333</td>\n",
       "      <td>96.666667</td>\n",
       "      <td>99.090909</td>\n",
       "      <td>103.800000</td>\n",
       "      <td>103.8</td>\n",
       "      <td>104.2</td>\n",
       "      <td>104.9</td>\n",
       "      <td>108.666667</td>\n",
       "      <td>115.694079</td>\n",
       "      <td>118.492021</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>57.666667</td>\n",
       "      <td>57.478308</td>\n",
       "      <td>58.259040</td>\n",
       "      <td>63.636364</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>75.786899</td>\n",
       "      <td>77.178522</td>\n",
       "      <td>79.545455</td>\n",
       "      <td>36.485714</td>\n",
       "      <td>37.275000</td>\n",
       "      <td>36.45000</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>7.250000</td>\n",
       "      <td>14.00</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>19.424873</td>\n",
       "      <td>19.622286</td>\n",
       "      <td>19.55034</td>\n",
       "      <td>19.782182</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.507617</td>\n",
       "      <td>0.515098</td>\n",
       "      <td>0.50479</td>\n",
       "      <td>7.325000</td>\n",
       "      <td>7.340</td>\n",
       "      <td>7.394394</td>\n",
       "      <td>7.396834</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>116.867607</td>\n",
       "      <td>117.303447</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>39.657571</td>\n",
       "      <td>39.969184</td>\n",
       "      <td>26.400000</td>\n",
       "      <td>28.750000</td>\n",
       "      <td>28.700000</td>\n",
       "      <td>31.200000</td>\n",
       "      <td>5.50000</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>4.067108</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1.503316</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>126.00000</td>\n",
       "      <td>108.00000</td>\n",
       "      <td>176.60216</td>\n",
       "      <td>54.00000</td>\n",
       "      <td>53.0</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>26.151218</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>24.094853</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.039408</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>138.872497</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>128.065366</td>\n",
       "      <td>9.40000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>12.394275</td>\n",
       "      <td>97.027244</td>\n",
       "      <td>96.526403</td>\n",
       "      <td>96.56986</td>\n",
       "      <td>96.444055</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>2.383319</td>\n",
       "      <td>2.131766</td>\n",
       "      <td>2.090862</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135080</th>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>170.20000</td>\n",
       "      <td>77.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>70.730769</td>\n",
       "      <td>67.473684</td>\n",
       "      <td>71.312500</td>\n",
       "      <td>68.166667</td>\n",
       "      <td>82.576923</td>\n",
       "      <td>75.421053</td>\n",
       "      <td>85.571429</td>\n",
       "      <td>82.254622</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>114.157895</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>122.600009</td>\n",
       "      <td>63.538462</td>\n",
       "      <td>57.105263</td>\n",
       "      <td>78.428571</td>\n",
       "      <td>60.724172</td>\n",
       "      <td>264.545455</td>\n",
       "      <td>49.083333</td>\n",
       "      <td>46.416667</td>\n",
       "      <td>178.636364</td>\n",
       "      <td>81.345255</td>\n",
       "      <td>87.6</td>\n",
       "      <td>87.6</td>\n",
       "      <td>87.6</td>\n",
       "      <td>117.325730</td>\n",
       "      <td>115.694079</td>\n",
       "      <td>116.533333</td>\n",
       "      <td>122.833333</td>\n",
       "      <td>58.746771</td>\n",
       "      <td>57.478308</td>\n",
       "      <td>53.466667</td>\n",
       "      <td>55.083333</td>\n",
       "      <td>76.931481</td>\n",
       "      <td>75.786899</td>\n",
       "      <td>74.489333</td>\n",
       "      <td>77.665833</td>\n",
       "      <td>36.584000</td>\n",
       "      <td>37.684211</td>\n",
       "      <td>37.50625</td>\n",
       "      <td>37.080000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>11.50</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>19.424873</td>\n",
       "      <td>19.622286</td>\n",
       "      <td>19.55034</td>\n",
       "      <td>19.782182</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.515098</td>\n",
       "      <td>0.50479</td>\n",
       "      <td>7.385714</td>\n",
       "      <td>7.375</td>\n",
       "      <td>7.365000</td>\n",
       "      <td>7.370000</td>\n",
       "      <td>145.571429</td>\n",
       "      <td>137.500000</td>\n",
       "      <td>116.867607</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>37.571429</td>\n",
       "      <td>40.500000</td>\n",
       "      <td>39.657571</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>28.066667</td>\n",
       "      <td>25.700000</td>\n",
       "      <td>27.450000</td>\n",
       "      <td>25.400000</td>\n",
       "      <td>4.13294</td>\n",
       "      <td>4.196807</td>\n",
       "      <td>4.053152</td>\n",
       "      <td>4.067108</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.449714</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>156.666667</td>\n",
       "      <td>276.00000</td>\n",
       "      <td>194.81982</td>\n",
       "      <td>165.00000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>26.487741</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>23.000849</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>23.775188</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>138.744966</td>\n",
       "      <td>138.918773</td>\n",
       "      <td>138.878042</td>\n",
       "      <td>138.872497</td>\n",
       "      <td>152.224449</td>\n",
       "      <td>131.516058</td>\n",
       "      <td>130.070914</td>\n",
       "      <td>128.065366</td>\n",
       "      <td>6.15500</td>\n",
       "      <td>9.600000</td>\n",
       "      <td>12.230936</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>97.027244</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>96.56986</td>\n",
       "      <td>96.444055</td>\n",
       "      <td>2.695739</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>2.131766</td>\n",
       "      <td>2.090862</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135081</th>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>162.60000</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>83.933333</td>\n",
       "      <td>86.882353</td>\n",
       "      <td>109.333333</td>\n",
       "      <td>97.250000</td>\n",
       "      <td>73.566667</td>\n",
       "      <td>72.272727</td>\n",
       "      <td>80.528667</td>\n",
       "      <td>82.254622</td>\n",
       "      <td>108.966667</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>120.196298</td>\n",
       "      <td>122.600009</td>\n",
       "      <td>53.500000</td>\n",
       "      <td>50.727273</td>\n",
       "      <td>59.802690</td>\n",
       "      <td>60.724172</td>\n",
       "      <td>472.727273</td>\n",
       "      <td>50.090909</td>\n",
       "      <td>72.916667</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>81.345255</td>\n",
       "      <td>73.4</td>\n",
       "      <td>73.4</td>\n",
       "      <td>73.4</td>\n",
       "      <td>117.325730</td>\n",
       "      <td>114.428571</td>\n",
       "      <td>100.142857</td>\n",
       "      <td>99.500000</td>\n",
       "      <td>58.746771</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>53.047619</td>\n",
       "      <td>49.500000</td>\n",
       "      <td>76.931481</td>\n",
       "      <td>76.142857</td>\n",
       "      <td>68.746190</td>\n",
       "      <td>66.166667</td>\n",
       "      <td>36.640000</td>\n",
       "      <td>37.618182</td>\n",
       "      <td>36.35000</td>\n",
       "      <td>36.033333</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>19.424873</td>\n",
       "      <td>19.622286</td>\n",
       "      <td>19.55034</td>\n",
       "      <td>19.782182</td>\n",
       "      <td>0.584297</td>\n",
       "      <td>0.507617</td>\n",
       "      <td>0.515098</td>\n",
       "      <td>0.50479</td>\n",
       "      <td>7.380000</td>\n",
       "      <td>7.395</td>\n",
       "      <td>7.394394</td>\n",
       "      <td>7.396834</td>\n",
       "      <td>300.500000</td>\n",
       "      <td>120.620393</td>\n",
       "      <td>116.867607</td>\n",
       "      <td>117.303447</td>\n",
       "      <td>45.166667</td>\n",
       "      <td>39.840366</td>\n",
       "      <td>39.657571</td>\n",
       "      <td>39.969184</td>\n",
       "      <td>21.200000</td>\n",
       "      <td>31.147414</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.054173</td>\n",
       "      <td>4.13294</td>\n",
       "      <td>4.196807</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>4.067108</td>\n",
       "      <td>1.391063</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.449714</td>\n",
       "      <td>1.503316</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>199.19211</td>\n",
       "      <td>194.81982</td>\n",
       "      <td>176.60216</td>\n",
       "      <td>25.69763</td>\n",
       "      <td>12.0</td>\n",
       "      <td>26.487741</td>\n",
       "      <td>26.151218</td>\n",
       "      <td>23.000849</td>\n",
       "      <td>23.039752</td>\n",
       "      <td>23.775188</td>\n",
       "      <td>24.094853</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.052855</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>138.744966</td>\n",
       "      <td>138.918773</td>\n",
       "      <td>138.878042</td>\n",
       "      <td>138.872497</td>\n",
       "      <td>152.224449</td>\n",
       "      <td>131.516058</td>\n",
       "      <td>221.000000</td>\n",
       "      <td>128.065366</td>\n",
       "      <td>13.00745</td>\n",
       "      <td>12.437016</td>\n",
       "      <td>12.230936</td>\n",
       "      <td>12.394275</td>\n",
       "      <td>98.750000</td>\n",
       "      <td>96.526403</td>\n",
       "      <td>96.56986</td>\n",
       "      <td>96.444055</td>\n",
       "      <td>2.695739</td>\n",
       "      <td>2.383319</td>\n",
       "      <td>2.131766</td>\n",
       "      <td>2.090862</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Age  Gender     Height  Weight  Bilirubin  AST  ALT  ALP  Albumin  \\\n",
       "recordid                                                                      \n",
       "135076     56       1  177.80000   110.0          1    1    1    1        0   \n",
       "135077     72       1  169.30233   220.0          0    0    0    0        1   \n",
       "135079     68       1  169.30233   100.0          0    0    0    0        0   \n",
       "135080     77       1  170.20000    77.5          0    0    0    0        0   \n",
       "135081     66       1  162.60000    65.0          0    0    0    0        0   \n",
       "\n",
       "          TroponinT  TroponinI  Cholesterol  MechVent         HR0        HR12  \\\n",
       "recordid                                                                        \n",
       "135076            0          0            0         1  104.416667  110.777778   \n",
       "135077            0          0            0         1   70.428571   87.714286   \n",
       "135079            1          0            0         1   72.739130   88.076923   \n",
       "135080            0          0            0         1   70.730769   67.473684   \n",
       "135081            0          0            0         1   83.933333   86.882353   \n",
       "\n",
       "                HR24       HR36        MAP0       MAP12       MAP24  \\\n",
       "recordid                                                              \n",
       "135076    104.454545  81.250000  104.777778  116.714286  148.500000   \n",
       "135077     78.250000  98.727273   64.285714   75.714286   94.250000   \n",
       "135079     79.666667  78.909091   76.608696   80.384615   84.750000   \n",
       "135080     71.312500  68.166667   82.576923   75.421053   85.571429   \n",
       "135081    109.333333  97.250000   73.566667   72.272727   80.528667   \n",
       "\n",
       "               MAP36     SysABP0    SysABP12    SysABP24    SysABP36  \\\n",
       "recordid                                                               \n",
       "135076    112.750000  147.000000  161.571429  156.000000  133.142857   \n",
       "135077     97.272727  101.000000  122.071429  142.000000  153.000000   \n",
       "135079     82.254622  116.652174  123.846154  129.916667  122.600009   \n",
       "135080     82.254622  117.000000  114.157895   97.000000  122.600009   \n",
       "135081     82.254622  108.966667  109.000000  120.196298  122.600009   \n",
       "\n",
       "           DiasABP0  DiasABP12   DiasABP24  DiasABP36      Urine0     Urine12  \\\n",
       "recordid                                                                        \n",
       "135076    81.333333  90.142857  111.000000  84.285714  161.428571  125.714286   \n",
       "135077    48.428571  53.714286   69.166667  65.636364   83.500000    1.363636   \n",
       "135079    54.391304  57.230769   60.000000  60.724172  161.785714  115.833333   \n",
       "135080    63.538462  57.105263   78.428571  60.724172  264.545455   49.083333   \n",
       "135081    53.500000  50.727273   59.802690  60.724172  472.727273   50.090909   \n",
       "\n",
       "             Urine24     Urine36     Weight0  Weight12  Weight24  Weight36  \\\n",
       "recordid                                                                     \n",
       "135076    131.666667  126.000000   81.345255     108.1     108.1     108.1   \n",
       "135077      4.133333    5.230769  117.142857     100.0     100.0     100.0   \n",
       "135079     96.666667   99.090909  103.800000     103.8     104.2     104.9   \n",
       "135080     46.416667  178.636364   81.345255      87.6      87.6      87.6   \n",
       "135081     72.916667   84.000000   81.345255      73.4      73.4      73.4   \n",
       "\n",
       "           NISysABP0  NISysABP12  NISysABP24  NISysABP36  NIDiasABP0  \\\n",
       "recordid                                                               \n",
       "135076    120.500000  158.875000  148.272727  146.000000   66.500000   \n",
       "135077    117.325730  115.694079  160.000000  119.579156   58.746771   \n",
       "135079    108.666667  115.694079  118.492021  128.000000   57.666667   \n",
       "135080    117.325730  115.694079  116.533333  122.833333   58.746771   \n",
       "135081    117.325730  114.428571  100.142857   99.500000   58.746771   \n",
       "\n",
       "          NIDiasABP12  NIDiasABP24  NIDiasABP36     NIMAP0    NIMAP12  \\\n",
       "recordid                                                                \n",
       "135076      70.375000    77.818182    76.090909  84.510000  99.862500   \n",
       "135077      57.478308    70.000000    57.771990  76.931481  75.786899   \n",
       "135079      57.478308    58.259040    63.636364  70.000000  75.786899   \n",
       "135080      57.478308    53.466667    55.083333  76.931481  75.786899   \n",
       "135081      57.000000    53.047619    49.500000  76.931481  76.142857   \n",
       "\n",
       "             NIMAP24    NIMAP36      Temp0     Temp12    Temp24     Temp36  \\\n",
       "recordid                                                                     \n",
       "135076    101.308182  99.390909  38.066667  37.600000  37.70000  37.850000   \n",
       "135077    100.000000  77.191414  37.766667  38.280000  38.24000  37.700000   \n",
       "135079     77.178522  79.545455  36.485714  37.275000  36.45000  36.500000   \n",
       "135080     74.489333  77.665833  36.584000  37.684211  37.50625  37.080000   \n",
       "135081     68.746190  66.166667  36.640000  37.618182  36.35000  36.033333   \n",
       "\n",
       "              GCS0  GCS12      GCS24  GCS36  RespRate0  RespRate12  \\\n",
       "recordid                                                             \n",
       "135076    7.000000  11.75  10.166667    8.2  19.424873   19.622286   \n",
       "135077    9.000000  13.80  15.000000   15.0  19.424873   19.622286   \n",
       "135079    7.250000  14.00  15.000000   15.0  19.424873   19.622286   \n",
       "135080    9.500000  11.50  15.000000   15.0  19.424873   19.622286   \n",
       "135081    4.833333  15.00  15.000000   15.0  19.424873   19.622286   \n",
       "\n",
       "          RespRate24  RespRate36     FiO20    FiO212    FiO224   FiO236  \\\n",
       "recordid                                                                  \n",
       "135076      19.55034   19.782182  0.600000  0.507617  0.515098  0.50479   \n",
       "135077      19.55034   19.782182  0.420000  0.400000  0.550000  0.66000   \n",
       "135079      19.55034   19.782182  0.433333  0.507617  0.515098  0.50479   \n",
       "135080      19.55034   19.782182  0.653846  0.500000  0.515098  0.50479   \n",
       "135081      19.55034   19.782182  0.584297  0.507617  0.515098  0.50479   \n",
       "\n",
       "               pH0   pH12      pH24      pH36       PaO20      PaO212  \\\n",
       "recordid                                                                \n",
       "135076    7.315000  7.430  7.394394  7.430000  219.000000  103.000000   \n",
       "135077    7.260000  7.316  7.320000  7.258000  129.500000  117.800000   \n",
       "135079    7.325000  7.340  7.394394  7.396834  125.000000  113.000000   \n",
       "135080    7.385714  7.375  7.365000  7.370000  145.571429  137.500000   \n",
       "135081    7.380000  7.395  7.394394  7.396834  300.500000  120.620393   \n",
       "\n",
       "              PaO224      PaO236     PaCO20    PaCO212    PaCO224    PaCO236  \\\n",
       "recordid                                                                       \n",
       "135076    116.867607   74.000000  46.000000  36.000000  39.657571  39.000000   \n",
       "135077    104.000000  128.000000  43.500000  40.000000  38.000000  45.600000   \n",
       "135079    116.867607  117.303447  45.000000  32.000000  39.657571  39.969184   \n",
       "135080    116.867607   91.000000  37.571429  40.500000  39.657571  44.000000   \n",
       "135081    116.867607  117.303447  45.166667  39.840366  39.657571  39.969184   \n",
       "\n",
       "               HCT0      HCT12      HCT24      HCT36       K0       K12  \\\n",
       "recordid                                                                  \n",
       "135076    40.100000  37.700000  30.961398  37.800000  4.30000  3.600000   \n",
       "135077    33.300000  31.147414  32.100000  30.054173  5.70000  5.000000   \n",
       "135079    26.400000  28.750000  28.700000  31.200000  5.50000  4.900000   \n",
       "135080    28.066667  25.700000  27.450000  25.400000  4.13294  4.196807   \n",
       "135081    21.200000  31.147414  23.000000  30.054173  4.13294  4.196807   \n",
       "\n",
       "               K24       K36  Creatinine0  Creatinine12  Creatinine24  \\\n",
       "recordid                                                                \n",
       "135076    4.053152  3.950000     0.600000           0.8      1.449714   \n",
       "135077    4.900000  4.800000     2.200000           3.9      4.600000   \n",
       "135079    4.900000  4.067108     5.400000           4.8      3.700000   \n",
       "135080    4.053152  4.067108     0.700000           1.0      1.449714   \n",
       "135081    4.300000  4.067108     1.391063           0.8      1.449714   \n",
       "\n",
       "          Creatinine36  Platelets0  Platelets12  Platelets24  Platelets36  \\\n",
       "recordid                                                                    \n",
       "135076        0.700000  228.000000    188.00000    194.81982    231.00000   \n",
       "135077        5.000000  167.000000    199.19211    150.00000    176.60216   \n",
       "135079        1.503316  133.000000    126.00000    108.00000    176.60216   \n",
       "135080        1.100000  156.666667    276.00000    194.81982    165.00000   \n",
       "135081        1.503316  116.000000    199.19211    194.81982    176.60216   \n",
       "\n",
       "              BUN0  BUN12      BUN24      BUN36      HCO30     HCO312  \\\n",
       "recordid                                                                \n",
       "135076    12.00000   13.0  26.487741  15.500000  23.000000  25.000000   \n",
       "135077    24.00000   31.0  36.000000  39.000000  20.000000  17.000000   \n",
       "135079    54.00000   53.0  49.000000  26.151218  21.000000  22.000000   \n",
       "135080    10.00000   11.0  26.487741  13.000000  23.000849  23.000000   \n",
       "135081    25.69763   12.0  26.487741  26.151218  23.000849  23.039752   \n",
       "\n",
       "             HCO324     HCO336  Mg0      Mg12      Mg24      Mg36         Na0  \\\n",
       "recordid                                                                        \n",
       "135076    23.775188  25.500000  1.7  1.700000  2.051234  2.000000  140.000000   \n",
       "135077    17.000000  19.500000  1.9  2.052855  2.100000  2.200000  138.000000   \n",
       "135079    22.000000  24.094853  1.6  1.600000  1.800000  2.039408  137.000000   \n",
       "135080    23.775188  24.000000  2.1  2.100000  1.900000  1.900000  138.744966   \n",
       "135081    23.775188  24.094853  2.0  2.052855  1.600000  1.900000  138.744966   \n",
       "\n",
       "                Na12        Na24        Na36    Glucose0   Glucose12  \\\n",
       "recordid                                                               \n",
       "135076    139.000000  138.878042  143.000000  114.000000  141.000000   \n",
       "135077    139.000000  140.000000  140.500000  100.000000  102.000000   \n",
       "135079    139.000000  138.000000  138.872497  263.000000  180.000000   \n",
       "135080    138.918773  138.878042  138.872497  152.224449  131.516058   \n",
       "135081    138.918773  138.878042  138.872497  152.224449  131.516058   \n",
       "\n",
       "           Glucose24   Glucose36      WBC0      WBC12      WBC24      WBC36  \\\n",
       "recordid                                                                      \n",
       "135076    130.070914  119.000000  14.40000  15.900000  12.230936  16.000000   \n",
       "135077    119.000000   98.000000  10.10000  12.437016  13.200000  12.394275   \n",
       "135079    146.000000  128.065366   9.40000   9.500000   6.950000  12.394275   \n",
       "135080    130.070914  128.065366   6.15500   9.600000  12.230936   8.600000   \n",
       "135081    221.000000  128.065366  13.00745  12.437016  12.230936  12.394275   \n",
       "\n",
       "              SaO20     SaO212    SaO224     SaO236  Lactate0  Lactate12  \\\n",
       "recordid                                                                   \n",
       "135076    97.027244  96.526403  96.56986  96.444055  2.695739   2.383319   \n",
       "135077    97.027244  96.526403  96.56986  96.444055  2.695739   2.383319   \n",
       "135079    97.027244  96.526403  96.56986  96.444055  1.700000   2.383319   \n",
       "135080    97.027244  98.000000  96.56986  96.444055  2.695739   2.800000   \n",
       "135081    98.750000  96.526403  96.56986  96.444055  2.695739   2.383319   \n",
       "\n",
       "          Lactate24  Lactate36  Coronary Care Unit  \\\n",
       "recordid                                             \n",
       "135076     2.131766   2.090862                   0   \n",
       "135077     2.131766   2.090862                   0   \n",
       "135079     2.131766   2.090862                   0   \n",
       "135080     2.131766   2.090862                   0   \n",
       "135081     2.131766   2.090862                   0   \n",
       "\n",
       "          Cardiac Surgery Recovery Unit  Medical ICU  Surgical ICU  \n",
       "recordid                                                            \n",
       "135076                                0            0             1  \n",
       "135077                                0            1             0  \n",
       "135079                                0            0             1  \n",
       "135080                                1            0             0  \n",
       "135081                                1            0             0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Bilirubin</th>\n",
       "      <th>AST</th>\n",
       "      <th>ALT</th>\n",
       "      <th>ALP</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>TroponinT</th>\n",
       "      <th>TroponinI</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>MechVent</th>\n",
       "      <th>HR0</th>\n",
       "      <th>HR12</th>\n",
       "      <th>HR24</th>\n",
       "      <th>HR36</th>\n",
       "      <th>MAP0</th>\n",
       "      <th>MAP12</th>\n",
       "      <th>MAP24</th>\n",
       "      <th>MAP36</th>\n",
       "      <th>SysABP0</th>\n",
       "      <th>SysABP12</th>\n",
       "      <th>SysABP24</th>\n",
       "      <th>SysABP36</th>\n",
       "      <th>DiasABP0</th>\n",
       "      <th>DiasABP12</th>\n",
       "      <th>DiasABP24</th>\n",
       "      <th>DiasABP36</th>\n",
       "      <th>Urine0</th>\n",
       "      <th>Urine12</th>\n",
       "      <th>Urine24</th>\n",
       "      <th>Urine36</th>\n",
       "      <th>Weight0</th>\n",
       "      <th>Weight12</th>\n",
       "      <th>Weight24</th>\n",
       "      <th>Weight36</th>\n",
       "      <th>NISysABP0</th>\n",
       "      <th>NISysABP12</th>\n",
       "      <th>NISysABP24</th>\n",
       "      <th>NISysABP36</th>\n",
       "      <th>NIDiasABP0</th>\n",
       "      <th>NIDiasABP12</th>\n",
       "      <th>NIDiasABP24</th>\n",
       "      <th>NIDiasABP36</th>\n",
       "      <th>NIMAP0</th>\n",
       "      <th>NIMAP12</th>\n",
       "      <th>NIMAP24</th>\n",
       "      <th>NIMAP36</th>\n",
       "      <th>Temp0</th>\n",
       "      <th>Temp12</th>\n",
       "      <th>Temp24</th>\n",
       "      <th>Temp36</th>\n",
       "      <th>GCS0</th>\n",
       "      <th>GCS12</th>\n",
       "      <th>GCS24</th>\n",
       "      <th>GCS36</th>\n",
       "      <th>RespRate0</th>\n",
       "      <th>RespRate12</th>\n",
       "      <th>RespRate24</th>\n",
       "      <th>RespRate36</th>\n",
       "      <th>FiO20</th>\n",
       "      <th>FiO212</th>\n",
       "      <th>FiO224</th>\n",
       "      <th>FiO236</th>\n",
       "      <th>pH0</th>\n",
       "      <th>pH12</th>\n",
       "      <th>pH24</th>\n",
       "      <th>pH36</th>\n",
       "      <th>PaO20</th>\n",
       "      <th>PaO212</th>\n",
       "      <th>PaO224</th>\n",
       "      <th>PaO236</th>\n",
       "      <th>PaCO20</th>\n",
       "      <th>PaCO212</th>\n",
       "      <th>PaCO224</th>\n",
       "      <th>PaCO236</th>\n",
       "      <th>HCT0</th>\n",
       "      <th>HCT12</th>\n",
       "      <th>HCT24</th>\n",
       "      <th>HCT36</th>\n",
       "      <th>K0</th>\n",
       "      <th>K12</th>\n",
       "      <th>K24</th>\n",
       "      <th>K36</th>\n",
       "      <th>Creatinine0</th>\n",
       "      <th>Creatinine12</th>\n",
       "      <th>Creatinine24</th>\n",
       "      <th>Creatinine36</th>\n",
       "      <th>Platelets0</th>\n",
       "      <th>Platelets12</th>\n",
       "      <th>Platelets24</th>\n",
       "      <th>Platelets36</th>\n",
       "      <th>BUN0</th>\n",
       "      <th>BUN12</th>\n",
       "      <th>BUN24</th>\n",
       "      <th>BUN36</th>\n",
       "      <th>HCO30</th>\n",
       "      <th>HCO312</th>\n",
       "      <th>HCO324</th>\n",
       "      <th>HCO336</th>\n",
       "      <th>Mg0</th>\n",
       "      <th>Mg12</th>\n",
       "      <th>Mg24</th>\n",
       "      <th>Mg36</th>\n",
       "      <th>Na0</th>\n",
       "      <th>Na12</th>\n",
       "      <th>Na24</th>\n",
       "      <th>Na36</th>\n",
       "      <th>Glucose0</th>\n",
       "      <th>Glucose12</th>\n",
       "      <th>Glucose24</th>\n",
       "      <th>Glucose36</th>\n",
       "      <th>WBC0</th>\n",
       "      <th>WBC12</th>\n",
       "      <th>WBC24</th>\n",
       "      <th>WBC36</th>\n",
       "      <th>SaO20</th>\n",
       "      <th>SaO212</th>\n",
       "      <th>SaO224</th>\n",
       "      <th>SaO236</th>\n",
       "      <th>Lactate0</th>\n",
       "      <th>Lactate12</th>\n",
       "      <th>Lactate24</th>\n",
       "      <th>Lactate36</th>\n",
       "      <th>Coronary Care Unit</th>\n",
       "      <th>Cardiac Surgery Recovery Unit</th>\n",
       "      <th>Medical ICU</th>\n",
       "      <th>Surgical ICU</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recordid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137593</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>157.500000</td>\n",
       "      <td>88.60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>98.931034</td>\n",
       "      <td>90.428571</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>87.181818</td>\n",
       "      <td>71.310345</td>\n",
       "      <td>72.285714</td>\n",
       "      <td>67.235294</td>\n",
       "      <td>73.230769</td>\n",
       "      <td>106.482759</td>\n",
       "      <td>112.357143</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>115.538462</td>\n",
       "      <td>56.068966</td>\n",
       "      <td>53.571429</td>\n",
       "      <td>50.176471</td>\n",
       "      <td>55.769231</td>\n",
       "      <td>257.916667</td>\n",
       "      <td>55.833333</td>\n",
       "      <td>130.454545</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>82.704345</td>\n",
       "      <td>89.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>88.200000</td>\n",
       "      <td>116.815217</td>\n",
       "      <td>116.953677</td>\n",
       "      <td>118.307963</td>\n",
       "      <td>120.750541</td>\n",
       "      <td>58.166331</td>\n",
       "      <td>57.120246</td>\n",
       "      <td>57.762168</td>\n",
       "      <td>57.936709</td>\n",
       "      <td>76.704493</td>\n",
       "      <td>76.099618</td>\n",
       "      <td>76.978834</td>\n",
       "      <td>77.800938</td>\n",
       "      <td>38.373077</td>\n",
       "      <td>38.214286</td>\n",
       "      <td>37.821429</td>\n",
       "      <td>37.266667</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>19.736448</td>\n",
       "      <td>19.336067</td>\n",
       "      <td>19.339250</td>\n",
       "      <td>19.523698</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>7.391250</td>\n",
       "      <td>7.370000</td>\n",
       "      <td>7.380000</td>\n",
       "      <td>7.420000</td>\n",
       "      <td>168.250000</td>\n",
       "      <td>125.811658</td>\n",
       "      <td>85.250000</td>\n",
       "      <td>88.500000</td>\n",
       "      <td>45.125000</td>\n",
       "      <td>39.343704</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>31.200000</td>\n",
       "      <td>34.300000</td>\n",
       "      <td>30.446855</td>\n",
       "      <td>30.300000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>4.048392</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.407454</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>229.000000</td>\n",
       "      <td>194.35375</td>\n",
       "      <td>171.500000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>27.093275</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.00000</td>\n",
       "      <td>23.755556</td>\n",
       "      <td>30.00</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.90000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>139.093356</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>139.313275</td>\n",
       "      <td>137.500000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>186.000000</td>\n",
       "      <td>131.240753</td>\n",
       "      <td>131.223025</td>\n",
       "      <td>14.600000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>12.216938</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>97.800000</td>\n",
       "      <td>96.471630</td>\n",
       "      <td>95.333333</td>\n",
       "      <td>96.002072</td>\n",
       "      <td>2.479119</td>\n",
       "      <td>2.372085</td>\n",
       "      <td>2.039958</td>\n",
       "      <td>2.086082</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137594</th>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>170.425096</td>\n",
       "      <td>72.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65.800000</td>\n",
       "      <td>68.222222</td>\n",
       "      <td>66.500000</td>\n",
       "      <td>65.428571</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>79.843976</td>\n",
       "      <td>80.927647</td>\n",
       "      <td>82.065406</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>118.389228</td>\n",
       "      <td>120.700257</td>\n",
       "      <td>123.362393</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>59.625315</td>\n",
       "      <td>60.037273</td>\n",
       "      <td>60.544665</td>\n",
       "      <td>75.142857</td>\n",
       "      <td>97.777778</td>\n",
       "      <td>244.000000</td>\n",
       "      <td>152.500000</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>72.5</td>\n",
       "      <td>72.5</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>142.777778</td>\n",
       "      <td>136.833333</td>\n",
       "      <td>134.142857</td>\n",
       "      <td>56.684211</td>\n",
       "      <td>49.555556</td>\n",
       "      <td>51.333333</td>\n",
       "      <td>42.285714</td>\n",
       "      <td>82.792105</td>\n",
       "      <td>80.630000</td>\n",
       "      <td>79.833333</td>\n",
       "      <td>72.905714</td>\n",
       "      <td>36.625000</td>\n",
       "      <td>37.050000</td>\n",
       "      <td>36.900000</td>\n",
       "      <td>36.300000</td>\n",
       "      <td>13.142857</td>\n",
       "      <td>12.777778</td>\n",
       "      <td>11.142857</td>\n",
       "      <td>12.142857</td>\n",
       "      <td>14.421053</td>\n",
       "      <td>15.444444</td>\n",
       "      <td>14.333333</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.591333</td>\n",
       "      <td>0.516785</td>\n",
       "      <td>0.520443</td>\n",
       "      <td>0.506305</td>\n",
       "      <td>7.366538</td>\n",
       "      <td>7.388928</td>\n",
       "      <td>7.397845</td>\n",
       "      <td>7.399595</td>\n",
       "      <td>160.449835</td>\n",
       "      <td>125.811658</td>\n",
       "      <td>117.976143</td>\n",
       "      <td>113.725108</td>\n",
       "      <td>40.434075</td>\n",
       "      <td>39.343704</td>\n",
       "      <td>39.350542</td>\n",
       "      <td>40.04247</td>\n",
       "      <td>31.900000</td>\n",
       "      <td>34.700000</td>\n",
       "      <td>36.100000</td>\n",
       "      <td>36.300000</td>\n",
       "      <td>4.122413</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>1.316559</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>210.566486</td>\n",
       "      <td>251.000000</td>\n",
       "      <td>241.00000</td>\n",
       "      <td>248.000000</td>\n",
       "      <td>26.151555</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>22.950528</td>\n",
       "      <td>26.00000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.00</td>\n",
       "      <td>1.980247</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1.90000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>139.093356</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>148.218042</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>13.251714</td>\n",
       "      <td>11.600000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>96.384891</td>\n",
       "      <td>96.471630</td>\n",
       "      <td>96.133857</td>\n",
       "      <td>96.002072</td>\n",
       "      <td>2.479119</td>\n",
       "      <td>2.372085</td>\n",
       "      <td>2.039958</td>\n",
       "      <td>2.086082</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137595</th>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>165.100000</td>\n",
       "      <td>77.27</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>83.275862</td>\n",
       "      <td>89.600000</td>\n",
       "      <td>92.166667</td>\n",
       "      <td>87.909091</td>\n",
       "      <td>77.250000</td>\n",
       "      <td>83.600000</td>\n",
       "      <td>87.250000</td>\n",
       "      <td>80.454545</td>\n",
       "      <td>105.413793</td>\n",
       "      <td>125.066667</td>\n",
       "      <td>136.166667</td>\n",
       "      <td>119.727273</td>\n",
       "      <td>60.482759</td>\n",
       "      <td>65.333333</td>\n",
       "      <td>66.166667</td>\n",
       "      <td>59.363636</td>\n",
       "      <td>261.071429</td>\n",
       "      <td>225.000000</td>\n",
       "      <td>164.583333</td>\n",
       "      <td>218.636364</td>\n",
       "      <td>82.704345</td>\n",
       "      <td>92.7</td>\n",
       "      <td>92.7</td>\n",
       "      <td>91.609091</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>116.953677</td>\n",
       "      <td>118.307963</td>\n",
       "      <td>120.750541</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>57.120246</td>\n",
       "      <td>57.762168</td>\n",
       "      <td>57.936709</td>\n",
       "      <td>61.330000</td>\n",
       "      <td>76.099618</td>\n",
       "      <td>76.978834</td>\n",
       "      <td>77.800938</td>\n",
       "      <td>35.655172</td>\n",
       "      <td>37.775000</td>\n",
       "      <td>38.358333</td>\n",
       "      <td>37.763636</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>19.736448</td>\n",
       "      <td>19.336067</td>\n",
       "      <td>19.339250</td>\n",
       "      <td>19.523698</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.487500</td>\n",
       "      <td>7.340909</td>\n",
       "      <td>7.391667</td>\n",
       "      <td>7.416667</td>\n",
       "      <td>7.435000</td>\n",
       "      <td>200.545455</td>\n",
       "      <td>95.833333</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>91.500000</td>\n",
       "      <td>38.727273</td>\n",
       "      <td>34.333333</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>34.50000</td>\n",
       "      <td>27.275000</td>\n",
       "      <td>36.050000</td>\n",
       "      <td>30.446855</td>\n",
       "      <td>32.400000</td>\n",
       "      <td>4.122413</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>4.048392</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.407454</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>194.35375</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>27.093275</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>23.755556</td>\n",
       "      <td>23.00</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.04915</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>139.093356</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>139.313275</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>148.218042</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>131.240753</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>9.833333</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>12.216938</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>97.666667</td>\n",
       "      <td>97.333333</td>\n",
       "      <td>97.333333</td>\n",
       "      <td>97.500000</td>\n",
       "      <td>4.066667</td>\n",
       "      <td>1.850000</td>\n",
       "      <td>2.039958</td>\n",
       "      <td>2.086082</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137598</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>170.425096</td>\n",
       "      <td>131.80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>85.200000</td>\n",
       "      <td>105.312500</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>100.727273</td>\n",
       "      <td>80.628753</td>\n",
       "      <td>79.843976</td>\n",
       "      <td>80.927647</td>\n",
       "      <td>82.065406</td>\n",
       "      <td>116.470913</td>\n",
       "      <td>118.389228</td>\n",
       "      <td>120.700257</td>\n",
       "      <td>123.362393</td>\n",
       "      <td>59.560763</td>\n",
       "      <td>59.625315</td>\n",
       "      <td>60.037273</td>\n",
       "      <td>60.544665</td>\n",
       "      <td>165.860895</td>\n",
       "      <td>127.358947</td>\n",
       "      <td>133.163638</td>\n",
       "      <td>142.398650</td>\n",
       "      <td>131.800000</td>\n",
       "      <td>131.8</td>\n",
       "      <td>131.8</td>\n",
       "      <td>131.800000</td>\n",
       "      <td>89.368421</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>113.083333</td>\n",
       "      <td>102.090909</td>\n",
       "      <td>36.473684</td>\n",
       "      <td>37.062500</td>\n",
       "      <td>39.250000</td>\n",
       "      <td>29.818182</td>\n",
       "      <td>48.444444</td>\n",
       "      <td>54.375000</td>\n",
       "      <td>56.416667</td>\n",
       "      <td>47.363636</td>\n",
       "      <td>36.666667</td>\n",
       "      <td>37.233333</td>\n",
       "      <td>36.866667</td>\n",
       "      <td>37.075000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.666667</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>19.736448</td>\n",
       "      <td>19.336067</td>\n",
       "      <td>19.339250</td>\n",
       "      <td>19.523698</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>7.290000</td>\n",
       "      <td>7.388928</td>\n",
       "      <td>7.397845</td>\n",
       "      <td>7.399595</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>125.811658</td>\n",
       "      <td>117.976143</td>\n",
       "      <td>113.725108</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>39.343704</td>\n",
       "      <td>39.350542</td>\n",
       "      <td>40.04247</td>\n",
       "      <td>24.550000</td>\n",
       "      <td>30.832459</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>29.934552</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>4.178664</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>4.087976</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>1.378978</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>1.360591</td>\n",
       "      <td>629.500000</td>\n",
       "      <td>191.870901</td>\n",
       "      <td>625.00000</td>\n",
       "      <td>170.852684</td>\n",
       "      <td>49.500000</td>\n",
       "      <td>25.842534</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>25.798932</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>23.19059</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>24.27</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>2.057708</td>\n",
       "      <td>2.20000</td>\n",
       "      <td>2.063583</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>139.052282</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>138.436415</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>133.889028</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>131.223025</td>\n",
       "      <td>26.150000</td>\n",
       "      <td>12.555877</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>12.180153</td>\n",
       "      <td>96.384891</td>\n",
       "      <td>96.471630</td>\n",
       "      <td>96.133857</td>\n",
       "      <td>96.002072</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>2.372085</td>\n",
       "      <td>2.039958</td>\n",
       "      <td>2.086082</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137600</th>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>172.700000</td>\n",
       "      <td>86.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>87.928571</td>\n",
       "      <td>77.833333</td>\n",
       "      <td>67.272727</td>\n",
       "      <td>67.285714</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>101.545455</td>\n",
       "      <td>109.857143</td>\n",
       "      <td>113.214286</td>\n",
       "      <td>58.666667</td>\n",
       "      <td>49.272727</td>\n",
       "      <td>47.214286</td>\n",
       "      <td>49.714286</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>94.166667</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>82.704345</td>\n",
       "      <td>92.6</td>\n",
       "      <td>92.6</td>\n",
       "      <td>93.035714</td>\n",
       "      <td>116.815217</td>\n",
       "      <td>116.953677</td>\n",
       "      <td>118.307963</td>\n",
       "      <td>120.750541</td>\n",
       "      <td>58.166331</td>\n",
       "      <td>57.120246</td>\n",
       "      <td>57.762168</td>\n",
       "      <td>57.936709</td>\n",
       "      <td>76.704493</td>\n",
       "      <td>76.099618</td>\n",
       "      <td>76.978834</td>\n",
       "      <td>77.800938</td>\n",
       "      <td>36.116667</td>\n",
       "      <td>37.281818</td>\n",
       "      <td>38.014286</td>\n",
       "      <td>37.307143</td>\n",
       "      <td>10.590699</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>19.736448</td>\n",
       "      <td>19.336067</td>\n",
       "      <td>19.339250</td>\n",
       "      <td>19.523698</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.516785</td>\n",
       "      <td>0.520443</td>\n",
       "      <td>0.506305</td>\n",
       "      <td>7.363333</td>\n",
       "      <td>7.322500</td>\n",
       "      <td>7.397845</td>\n",
       "      <td>7.380000</td>\n",
       "      <td>301.666667</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>117.976143</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>39.666667</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>39.350542</td>\n",
       "      <td>40.00000</td>\n",
       "      <td>31.639291</td>\n",
       "      <td>27.200000</td>\n",
       "      <td>24.600000</td>\n",
       "      <td>25.100000</td>\n",
       "      <td>4.122413</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>1.316559</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1.407454</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>271.000000</td>\n",
       "      <td>276.000000</td>\n",
       "      <td>194.35375</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>26.151555</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>27.093275</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>22.950528</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>23.755556</td>\n",
       "      <td>23.00</td>\n",
       "      <td>1.980247</td>\n",
       "      <td>2.057708</td>\n",
       "      <td>1.70000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>139.093356</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>139.313275</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>148.218042</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>13.251714</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>12.216938</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>96.384891</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>96.133857</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>2.479119</td>\n",
       "      <td>2.372085</td>\n",
       "      <td>2.039958</td>\n",
       "      <td>2.086082</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Age  Gender      Height  Weight  Bilirubin  AST  ALT  ALP  Albumin  \\\n",
       "recordid                                                                       \n",
       "137593     57       1  157.500000   88.60          0    0    0    0        0   \n",
       "137594     87       1  170.425096   72.50          0    0    0    0        0   \n",
       "137595     73       0  165.100000   77.27          1    1    1    1        0   \n",
       "137598     72       0  170.425096  131.80          0    0    0    0        0   \n",
       "137600     76       1  172.700000   86.10          0    0    0    0        0   \n",
       "\n",
       "          TroponinT  TroponinI  Cholesterol  MechVent        HR0        HR12  \\\n",
       "recordid                                                                       \n",
       "137593            0          0            0         1  98.931034   90.428571   \n",
       "137594            0          0            0         0  65.800000   68.222222   \n",
       "137595            0          0            0         1  83.275862   89.600000   \n",
       "137598            0          0            0         1  85.200000  105.312500   \n",
       "137600            0          0            0         1  88.000000   88.000000   \n",
       "\n",
       "               HR24        HR36       MAP0      MAP12      MAP24      MAP36  \\\n",
       "recordid                                                                      \n",
       "137593    90.000000   87.181818  71.310345  72.285714  67.235294  73.230769   \n",
       "137594    66.500000   65.428571  84.000000  79.843976  80.927647  82.065406   \n",
       "137595    92.166667   87.909091  77.250000  83.600000  87.250000  80.454545   \n",
       "137598    95.000000  100.727273  80.628753  79.843976  80.927647  82.065406   \n",
       "137600    88.000000   87.928571  77.833333  67.272727  67.285714  71.000000   \n",
       "\n",
       "             SysABP0    SysABP12    SysABP24    SysABP36   DiasABP0  \\\n",
       "recordid                                                              \n",
       "137593    106.482759  112.357143  105.000000  115.538462  56.068966   \n",
       "137594    151.000000  118.389228  120.700257  123.362393  55.000000   \n",
       "137595    105.413793  125.066667  136.166667  119.727273  60.482759   \n",
       "137598    116.470913  118.389228  120.700257  123.362393  59.560763   \n",
       "137600    113.000000  101.545455  109.857143  113.214286  58.666667   \n",
       "\n",
       "          DiasABP12  DiasABP24  DiasABP36      Urine0     Urine12     Urine24  \\\n",
       "recordid                                                                        \n",
       "137593    53.571429  50.176471  55.769231  257.916667   55.833333  130.454545   \n",
       "137594    59.625315  60.037273  60.544665   75.142857   97.777778  244.000000   \n",
       "137595    65.333333  66.166667  59.363636  261.071429  225.000000  164.583333   \n",
       "137598    59.625315  60.037273  60.544665  165.860895  127.358947  133.163638   \n",
       "137600    49.272727  47.214286  49.714286  192.000000   94.166667  105.000000   \n",
       "\n",
       "             Urine36     Weight0  Weight12  Weight24    Weight36   NISysABP0  \\\n",
       "recordid                                                                       \n",
       "137593     62.500000   82.704345      89.0      89.0   88.200000  116.815217   \n",
       "137594    152.500000   72.500000      72.5      72.5   72.500000  135.000000   \n",
       "137595    218.636364   82.704345      92.7      92.7   91.609091   88.000000   \n",
       "137598    142.398650  131.800000     131.8     131.8  131.800000   89.368421   \n",
       "137600    101.000000   82.704345      92.6      92.6   93.035714  116.815217   \n",
       "\n",
       "          NISysABP12  NISysABP24  NISysABP36  NIDiasABP0  NIDiasABP12  \\\n",
       "recordid                                                                \n",
       "137593    116.953677  118.307963  120.750541   58.166331    57.120246   \n",
       "137594    142.777778  136.833333  134.142857   56.684211    49.555556   \n",
       "137595    116.953677  118.307963  120.750541   48.000000    57.120246   \n",
       "137598     95.000000  113.083333  102.090909   36.473684    37.062500   \n",
       "137600    116.953677  118.307963  120.750541   58.166331    57.120246   \n",
       "\n",
       "          NIDiasABP24  NIDiasABP36     NIMAP0    NIMAP12    NIMAP24  \\\n",
       "recordid                                                              \n",
       "137593      57.762168    57.936709  76.704493  76.099618  76.978834   \n",
       "137594      51.333333    42.285714  82.792105  80.630000  79.833333   \n",
       "137595      57.762168    57.936709  61.330000  76.099618  76.978834   \n",
       "137598      39.250000    29.818182  48.444444  54.375000  56.416667   \n",
       "137600      57.762168    57.936709  76.704493  76.099618  76.978834   \n",
       "\n",
       "            NIMAP36      Temp0     Temp12     Temp24     Temp36       GCS0  \\\n",
       "recordid                                                                     \n",
       "137593    77.800938  38.373077  38.214286  37.821429  37.266667  10.800000   \n",
       "137594    72.905714  36.625000  37.050000  36.900000  36.300000  13.142857   \n",
       "137595    77.800938  35.655172  37.775000  38.358333  37.763636   3.000000   \n",
       "137598    47.363636  36.666667  37.233333  36.866667  37.075000   9.000000   \n",
       "137600    77.800938  36.116667  37.281818  38.014286  37.307143  10.590699   \n",
       "\n",
       "              GCS12      GCS24      GCS36  RespRate0  RespRate12  RespRate24  \\\n",
       "recordid                                                                       \n",
       "137593    15.000000  15.000000  14.500000  19.736448   19.336067   19.339250   \n",
       "137594    12.777778  11.142857  12.142857  14.421053   15.444444   14.333333   \n",
       "137595     7.666667  10.000000  11.000000  19.736448   19.336067   19.339250   \n",
       "137598     9.000000   9.666667  11.000000  19.736448   19.336067   19.339250   \n",
       "137600    15.000000  15.000000  14.000000  19.736448   19.336067   19.339250   \n",
       "\n",
       "          RespRate36     FiO20    FiO212    FiO224    FiO236       pH0  \\\n",
       "recordid                                                                 \n",
       "137593     19.523698  0.600000  0.400000  0.850000  0.700000  7.391250   \n",
       "137594     15.000000  0.591333  0.516785  0.520443  0.506305  7.366538   \n",
       "137595     19.523698  0.780000  0.650000  0.516667  0.487500  7.340909   \n",
       "137598     19.523698  0.460000  0.400000  0.400000  0.400000  7.290000   \n",
       "137600     19.523698  0.666667  0.516785  0.520443  0.506305  7.363333   \n",
       "\n",
       "              pH12      pH24      pH36       PaO20      PaO212      PaO224  \\\n",
       "recordid                                                                     \n",
       "137593    7.370000  7.380000  7.420000  168.250000  125.811658   85.250000   \n",
       "137594    7.388928  7.397845  7.399595  160.449835  125.811658  117.976143   \n",
       "137595    7.391667  7.416667  7.435000  200.545455   95.833333   91.000000   \n",
       "137598    7.388928  7.397845  7.399595   98.000000  125.811658  117.976143   \n",
       "137600    7.322500  7.397845  7.380000  301.666667  137.000000  117.976143   \n",
       "\n",
       "              PaO236     PaCO20    PaCO212    PaCO224   PaCO236       HCT0  \\\n",
       "recordid                                                                     \n",
       "137593     88.500000  45.125000  39.343704  46.000000  50.00000  31.200000   \n",
       "137594    113.725108  40.434075  39.343704  39.350542  40.04247  31.900000   \n",
       "137595     91.500000  38.727273  34.333333  35.000000  34.50000  27.275000   \n",
       "137598    113.725108  42.000000  39.343704  39.350542  40.04247  24.550000   \n",
       "137600     96.000000  39.666667  37.500000  39.350542  40.00000  31.639291   \n",
       "\n",
       "              HCT12      HCT24      HCT36        K0       K12       K24  \\\n",
       "recordid                                                                  \n",
       "137593    34.300000  30.446855  30.300000  4.500000  4.600000  4.048392   \n",
       "137594    34.700000  36.100000  36.300000  4.122413  3.500000  3.900000   \n",
       "137595    36.050000  30.446855  32.400000  4.122413  5.100000  4.048392   \n",
       "137598    30.832459  23.200000  29.934552  4.200000  4.178664  4.400000   \n",
       "137600    27.200000  24.600000  25.100000  4.122413  5.600000  5.300000   \n",
       "\n",
       "               K36  Creatinine0  Creatinine12  Creatinine24  Creatinine36  \\\n",
       "recordid                                                                    \n",
       "137593    3.700000     0.600000      0.600000      1.407454      0.700000   \n",
       "137594    3.800000     1.316559      0.800000      0.900000      0.800000   \n",
       "137595    4.300000     0.550000      0.600000      1.407454      0.800000   \n",
       "137598    4.087976     3.900000      1.378978      4.300000      1.360591   \n",
       "137600    4.200000     1.316559      1.400000      1.407454      1.400000   \n",
       "\n",
       "          Platelets0  Platelets12  Platelets24  Platelets36       BUN0  \\\n",
       "recordid                                                                 \n",
       "137593    214.000000   229.000000    194.35375   171.500000  11.000000   \n",
       "137594    210.566486   251.000000    241.00000   248.000000  26.151555   \n",
       "137595    118.000000    94.000000    194.35375    67.000000   9.000000   \n",
       "137598    629.500000   191.870901    625.00000   170.852684  49.500000   \n",
       "137600    271.000000   276.000000    194.35375   184.000000  26.151555   \n",
       "\n",
       "              BUN12      BUN24      BUN36      HCO30    HCO312     HCO324  \\\n",
       "recordid                                                                    \n",
       "137593     9.000000  27.093275  16.000000  27.000000  27.00000  23.755556   \n",
       "137594    18.000000  18.000000  16.000000  22.950528  26.00000  27.000000   \n",
       "137595     8.000000  27.093275  12.000000  19.500000  20.00000  23.755556   \n",
       "137598    25.842534  57.000000  25.798932  16.000000  23.19059  17.000000   \n",
       "137600    24.000000  27.093275  23.000000  22.950528  20.00000  23.755556   \n",
       "\n",
       "          HCO336       Mg0      Mg12     Mg24      Mg36         Na0  \\\n",
       "recordid                                                              \n",
       "137593     30.00  2.300000  2.000000  1.90000  2.300000  139.093356   \n",
       "137594     27.00  1.980247  1.800000  1.90000  1.800000  139.093356   \n",
       "137595     23.00  1.500000  2.000000  2.04915  1.900000  139.093356   \n",
       "137598     24.27  2.300000  2.057708  2.20000  2.063583  127.000000   \n",
       "137600     23.00  1.980247  2.057708  1.70000  1.700000  139.093356   \n",
       "\n",
       "                Na12        Na24        Na36    Glucose0   Glucose12  \\\n",
       "recordid                                                               \n",
       "137593    137.000000  139.313275  137.500000  170.000000  186.000000   \n",
       "137594    138.000000  142.000000  140.000000  148.218042  101.000000   \n",
       "137595    140.000000  139.313275  137.000000  148.218042  123.000000   \n",
       "137598    139.052282  130.000000  138.436415   87.000000  133.889028   \n",
       "137600    137.000000  139.313275  136.000000  148.218042  169.000000   \n",
       "\n",
       "           Glucose24   Glucose36       WBC0      WBC12      WBC24      WBC36  \\\n",
       "recordid                                                                       \n",
       "137593    131.240753  131.223025  14.600000  17.400000  12.216938  16.400000   \n",
       "137594    115.000000   92.000000  13.251714  11.600000  10.500000  11.100000   \n",
       "137595    131.240753  162.000000   9.833333   7.000000  12.216938   9.200000   \n",
       "137598    207.000000  131.223025  26.150000  12.555877  26.000000  12.180153   \n",
       "137600    132.000000  143.000000  13.251714  16.000000  12.216938  10.800000   \n",
       "\n",
       "              SaO20     SaO212     SaO224     SaO236  Lactate0  Lactate12  \\\n",
       "recordid                                                                    \n",
       "137593    97.800000  96.471630  95.333333  96.002072  2.479119   2.372085   \n",
       "137594    96.384891  96.471630  96.133857  96.002072  2.479119   2.372085   \n",
       "137595    97.666667  97.333333  97.333333  97.500000  4.066667   1.850000   \n",
       "137598    96.384891  96.471630  96.133857  96.002072  2.600000   2.372085   \n",
       "137600    96.384891  98.000000  96.133857  97.000000  2.479119   2.372085   \n",
       "\n",
       "          Lactate24  Lactate36  Coronary Care Unit  \\\n",
       "recordid                                             \n",
       "137593     2.039958   2.086082                   0   \n",
       "137594     2.039958   2.086082                   0   \n",
       "137595     2.039958   2.086082                   0   \n",
       "137598     2.039958   2.086082                   0   \n",
       "137600     2.039958   2.086082                   0   \n",
       "\n",
       "          Cardiac Surgery Recovery Unit  Medical ICU  Surgical ICU  \n",
       "recordid                                                            \n",
       "137593                                1            0             0  \n",
       "137594                                0            1             0  \n",
       "137595                                1            0             0  \n",
       "137598                                0            1             0  \n",
       "137600                                1            0             0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Bilirubin</th>\n",
       "      <th>AST</th>\n",
       "      <th>ALT</th>\n",
       "      <th>ALP</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>TroponinT</th>\n",
       "      <th>TroponinI</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>MechVent</th>\n",
       "      <th>HR0</th>\n",
       "      <th>HR12</th>\n",
       "      <th>HR24</th>\n",
       "      <th>HR36</th>\n",
       "      <th>MAP0</th>\n",
       "      <th>MAP12</th>\n",
       "      <th>MAP24</th>\n",
       "      <th>MAP36</th>\n",
       "      <th>SysABP0</th>\n",
       "      <th>SysABP12</th>\n",
       "      <th>SysABP24</th>\n",
       "      <th>SysABP36</th>\n",
       "      <th>DiasABP0</th>\n",
       "      <th>DiasABP12</th>\n",
       "      <th>DiasABP24</th>\n",
       "      <th>DiasABP36</th>\n",
       "      <th>Urine0</th>\n",
       "      <th>Urine12</th>\n",
       "      <th>Urine24</th>\n",
       "      <th>Urine36</th>\n",
       "      <th>Weight0</th>\n",
       "      <th>Weight12</th>\n",
       "      <th>Weight24</th>\n",
       "      <th>Weight36</th>\n",
       "      <th>NISysABP0</th>\n",
       "      <th>NISysABP12</th>\n",
       "      <th>NISysABP24</th>\n",
       "      <th>NISysABP36</th>\n",
       "      <th>NIDiasABP0</th>\n",
       "      <th>NIDiasABP12</th>\n",
       "      <th>NIDiasABP24</th>\n",
       "      <th>NIDiasABP36</th>\n",
       "      <th>NIMAP0</th>\n",
       "      <th>NIMAP12</th>\n",
       "      <th>NIMAP24</th>\n",
       "      <th>NIMAP36</th>\n",
       "      <th>Temp0</th>\n",
       "      <th>Temp12</th>\n",
       "      <th>Temp24</th>\n",
       "      <th>Temp36</th>\n",
       "      <th>GCS0</th>\n",
       "      <th>GCS12</th>\n",
       "      <th>GCS24</th>\n",
       "      <th>GCS36</th>\n",
       "      <th>RespRate0</th>\n",
       "      <th>RespRate12</th>\n",
       "      <th>RespRate24</th>\n",
       "      <th>RespRate36</th>\n",
       "      <th>FiO20</th>\n",
       "      <th>FiO212</th>\n",
       "      <th>FiO224</th>\n",
       "      <th>FiO236</th>\n",
       "      <th>pH0</th>\n",
       "      <th>pH12</th>\n",
       "      <th>pH24</th>\n",
       "      <th>pH36</th>\n",
       "      <th>PaO20</th>\n",
       "      <th>PaO212</th>\n",
       "      <th>PaO224</th>\n",
       "      <th>PaO236</th>\n",
       "      <th>PaCO20</th>\n",
       "      <th>PaCO212</th>\n",
       "      <th>PaCO224</th>\n",
       "      <th>PaCO236</th>\n",
       "      <th>HCT0</th>\n",
       "      <th>HCT12</th>\n",
       "      <th>HCT24</th>\n",
       "      <th>HCT36</th>\n",
       "      <th>K0</th>\n",
       "      <th>K12</th>\n",
       "      <th>K24</th>\n",
       "      <th>K36</th>\n",
       "      <th>Creatinine0</th>\n",
       "      <th>Creatinine12</th>\n",
       "      <th>Creatinine24</th>\n",
       "      <th>Creatinine36</th>\n",
       "      <th>Platelets0</th>\n",
       "      <th>Platelets12</th>\n",
       "      <th>Platelets24</th>\n",
       "      <th>Platelets36</th>\n",
       "      <th>BUN0</th>\n",
       "      <th>BUN12</th>\n",
       "      <th>BUN24</th>\n",
       "      <th>BUN36</th>\n",
       "      <th>HCO30</th>\n",
       "      <th>HCO312</th>\n",
       "      <th>HCO324</th>\n",
       "      <th>HCO336</th>\n",
       "      <th>Mg0</th>\n",
       "      <th>Mg12</th>\n",
       "      <th>Mg24</th>\n",
       "      <th>Mg36</th>\n",
       "      <th>Na0</th>\n",
       "      <th>Na12</th>\n",
       "      <th>Na24</th>\n",
       "      <th>Na36</th>\n",
       "      <th>Glucose0</th>\n",
       "      <th>Glucose12</th>\n",
       "      <th>Glucose24</th>\n",
       "      <th>Glucose36</th>\n",
       "      <th>WBC0</th>\n",
       "      <th>WBC12</th>\n",
       "      <th>WBC24</th>\n",
       "      <th>WBC36</th>\n",
       "      <th>SaO20</th>\n",
       "      <th>SaO212</th>\n",
       "      <th>SaO224</th>\n",
       "      <th>SaO236</th>\n",
       "      <th>Lactate0</th>\n",
       "      <th>Lactate12</th>\n",
       "      <th>Lactate24</th>\n",
       "      <th>Lactate36</th>\n",
       "      <th>Coronary Care Unit</th>\n",
       "      <th>Cardiac Surgery Recovery Unit</th>\n",
       "      <th>Medical ICU</th>\n",
       "      <th>Surgical ICU</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recordid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>140101</th>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>170.200000</td>\n",
       "      <td>253.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110.142857</td>\n",
       "      <td>108.833333</td>\n",
       "      <td>97.250000</td>\n",
       "      <td>97.636364</td>\n",
       "      <td>80.355020</td>\n",
       "      <td>80.313226</td>\n",
       "      <td>80.628562</td>\n",
       "      <td>81.518378</td>\n",
       "      <td>115.676644</td>\n",
       "      <td>119.538003</td>\n",
       "      <td>120.858617</td>\n",
       "      <td>122.740704</td>\n",
       "      <td>59.269006</td>\n",
       "      <td>59.575281</td>\n",
       "      <td>59.575021</td>\n",
       "      <td>59.960344</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>53.636364</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>72.222222</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>99.571429</td>\n",
       "      <td>108.333333</td>\n",
       "      <td>111.666667</td>\n",
       "      <td>103.818182</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>50.916667</td>\n",
       "      <td>52.250000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>61.761429</td>\n",
       "      <td>70.055000</td>\n",
       "      <td>72.055000</td>\n",
       "      <td>64.605455</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>37.733333</td>\n",
       "      <td>37.533333</td>\n",
       "      <td>37.766667</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>19.920446</td>\n",
       "      <td>19.896906</td>\n",
       "      <td>19.816753</td>\n",
       "      <td>20.011274</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.40000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>7.385000</td>\n",
       "      <td>8.406452</td>\n",
       "      <td>7.360000</td>\n",
       "      <td>7.398353</td>\n",
       "      <td>109.500000</td>\n",
       "      <td>123.874998</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>115.412815</td>\n",
       "      <td>66.500000</td>\n",
       "      <td>39.931142</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>40.44737</td>\n",
       "      <td>32.90</td>\n",
       "      <td>30.964831</td>\n",
       "      <td>32.9</td>\n",
       "      <td>29.951503</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>4.173645</td>\n",
       "      <td>3.9</td>\n",
       "      <td>4.071882</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.453197</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.487078</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>197.180099</td>\n",
       "      <td>179.0</td>\n",
       "      <td>176.738114</td>\n",
       "      <td>14.0</td>\n",
       "      <td>25.731611</td>\n",
       "      <td>13.0</td>\n",
       "      <td>25.921424</td>\n",
       "      <td>34.0</td>\n",
       "      <td>23.411007</td>\n",
       "      <td>33.0</td>\n",
       "      <td>24.214374</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.055607</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.051946</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>139.053094</td>\n",
       "      <td>142.0</td>\n",
       "      <td>138.633454</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>131.87701</td>\n",
       "      <td>86.0</td>\n",
       "      <td>128.530985</td>\n",
       "      <td>15.800000</td>\n",
       "      <td>12.416399</td>\n",
       "      <td>10.7</td>\n",
       "      <td>12.242523</td>\n",
       "      <td>96.793083</td>\n",
       "      <td>96.8269</td>\n",
       "      <td>96.412421</td>\n",
       "      <td>96.634002</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>2.182108</td>\n",
       "      <td>2.039218</td>\n",
       "      <td>1.832424</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140102</th>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>169.337684</td>\n",
       "      <td>123.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>76.444444</td>\n",
       "      <td>76.769231</td>\n",
       "      <td>78.636364</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>80.355020</td>\n",
       "      <td>83.600000</td>\n",
       "      <td>87.545455</td>\n",
       "      <td>95.272727</td>\n",
       "      <td>115.676644</td>\n",
       "      <td>109.200000</td>\n",
       "      <td>119.181818</td>\n",
       "      <td>132.500000</td>\n",
       "      <td>59.269006</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>69.272727</td>\n",
       "      <td>73.916667</td>\n",
       "      <td>39.285714</td>\n",
       "      <td>128.125000</td>\n",
       "      <td>27.818182</td>\n",
       "      <td>107.625000</td>\n",
       "      <td>123.500000</td>\n",
       "      <td>123.500000</td>\n",
       "      <td>123.500000</td>\n",
       "      <td>123.500000</td>\n",
       "      <td>102.777778</td>\n",
       "      <td>99.142857</td>\n",
       "      <td>117.967196</td>\n",
       "      <td>119.822643</td>\n",
       "      <td>46.666667</td>\n",
       "      <td>47.142857</td>\n",
       "      <td>56.889398</td>\n",
       "      <td>57.930466</td>\n",
       "      <td>65.368889</td>\n",
       "      <td>64.475714</td>\n",
       "      <td>76.214687</td>\n",
       "      <td>77.475260</td>\n",
       "      <td>36.350000</td>\n",
       "      <td>36.633333</td>\n",
       "      <td>36.766667</td>\n",
       "      <td>36.933333</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>11.666667</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>19.920446</td>\n",
       "      <td>19.896906</td>\n",
       "      <td>19.816753</td>\n",
       "      <td>20.011274</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>7.390000</td>\n",
       "      <td>7.430000</td>\n",
       "      <td>7.561206</td>\n",
       "      <td>7.398353</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>118.137949</td>\n",
       "      <td>115.412815</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>39.565064</td>\n",
       "      <td>40.44737</td>\n",
       "      <td>28.90</td>\n",
       "      <td>30.964831</td>\n",
       "      <td>29.4</td>\n",
       "      <td>29.951503</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.173645</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.071882</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.453197</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.487078</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>197.180099</td>\n",
       "      <td>185.0</td>\n",
       "      <td>176.738114</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.731611</td>\n",
       "      <td>26.0</td>\n",
       "      <td>25.921424</td>\n",
       "      <td>34.0</td>\n",
       "      <td>23.411007</td>\n",
       "      <td>30.0</td>\n",
       "      <td>24.214374</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>2.055607</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.051946</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>139.053094</td>\n",
       "      <td>142.0</td>\n",
       "      <td>138.633454</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>131.87701</td>\n",
       "      <td>144.0</td>\n",
       "      <td>128.530985</td>\n",
       "      <td>13.600000</td>\n",
       "      <td>12.416399</td>\n",
       "      <td>11.8</td>\n",
       "      <td>12.242523</td>\n",
       "      <td>96.793083</td>\n",
       "      <td>96.8269</td>\n",
       "      <td>96.412421</td>\n",
       "      <td>96.634002</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>2.182108</td>\n",
       "      <td>2.039218</td>\n",
       "      <td>1.832424</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140104</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100.906250</td>\n",
       "      <td>88.357143</td>\n",
       "      <td>82.333333</td>\n",
       "      <td>93.125000</td>\n",
       "      <td>78.750000</td>\n",
       "      <td>65.428571</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>81.518378</td>\n",
       "      <td>101.812500</td>\n",
       "      <td>89.714286</td>\n",
       "      <td>95.727273</td>\n",
       "      <td>122.740704</td>\n",
       "      <td>58.750000</td>\n",
       "      <td>49.428571</td>\n",
       "      <td>58.909091</td>\n",
       "      <td>59.960344</td>\n",
       "      <td>143.461538</td>\n",
       "      <td>114.363636</td>\n",
       "      <td>76.666667</td>\n",
       "      <td>158.333333</td>\n",
       "      <td>84.205934</td>\n",
       "      <td>85.353485</td>\n",
       "      <td>85.883694</td>\n",
       "      <td>86.183251</td>\n",
       "      <td>116.137862</td>\n",
       "      <td>97.833333</td>\n",
       "      <td>96.250000</td>\n",
       "      <td>99.125000</td>\n",
       "      <td>57.564944</td>\n",
       "      <td>43.333333</td>\n",
       "      <td>45.583333</td>\n",
       "      <td>43.875000</td>\n",
       "      <td>75.933999</td>\n",
       "      <td>61.500000</td>\n",
       "      <td>62.471667</td>\n",
       "      <td>62.292500</td>\n",
       "      <td>36.771429</td>\n",
       "      <td>37.200000</td>\n",
       "      <td>37.250000</td>\n",
       "      <td>37.082040</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>19.920446</td>\n",
       "      <td>19.896906</td>\n",
       "      <td>19.816753</td>\n",
       "      <td>20.011274</td>\n",
       "      <td>0.442857</td>\n",
       "      <td>0.508116</td>\n",
       "      <td>0.50586</td>\n",
       "      <td>0.507407</td>\n",
       "      <td>7.331429</td>\n",
       "      <td>7.390000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>7.398353</td>\n",
       "      <td>156.428571</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>118.137949</td>\n",
       "      <td>115.412815</td>\n",
       "      <td>44.857143</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>39.565064</td>\n",
       "      <td>40.44737</td>\n",
       "      <td>28.15</td>\n",
       "      <td>30.964831</td>\n",
       "      <td>28.8</td>\n",
       "      <td>29.951503</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>4.173645</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.071882</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.453197</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.487078</td>\n",
       "      <td>223.500000</td>\n",
       "      <td>197.180099</td>\n",
       "      <td>221.0</td>\n",
       "      <td>176.738114</td>\n",
       "      <td>11.0</td>\n",
       "      <td>25.731611</td>\n",
       "      <td>18.0</td>\n",
       "      <td>25.921424</td>\n",
       "      <td>22.5</td>\n",
       "      <td>23.411007</td>\n",
       "      <td>30.0</td>\n",
       "      <td>24.214374</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.055607</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.051946</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>139.053094</td>\n",
       "      <td>138.0</td>\n",
       "      <td>138.633454</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>131.87701</td>\n",
       "      <td>99.0</td>\n",
       "      <td>128.530985</td>\n",
       "      <td>17.100000</td>\n",
       "      <td>12.416399</td>\n",
       "      <td>17.3</td>\n",
       "      <td>12.242523</td>\n",
       "      <td>97.857143</td>\n",
       "      <td>98.0000</td>\n",
       "      <td>96.412421</td>\n",
       "      <td>96.634002</td>\n",
       "      <td>2.581794</td>\n",
       "      <td>2.182108</td>\n",
       "      <td>2.039218</td>\n",
       "      <td>1.832424</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140106</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>162.600000</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>72.269231</td>\n",
       "      <td>73.642857</td>\n",
       "      <td>97.200000</td>\n",
       "      <td>103.181818</td>\n",
       "      <td>69.517241</td>\n",
       "      <td>61.850000</td>\n",
       "      <td>65.611111</td>\n",
       "      <td>69.230769</td>\n",
       "      <td>101.275862</td>\n",
       "      <td>95.950000</td>\n",
       "      <td>101.944444</td>\n",
       "      <td>107.615385</td>\n",
       "      <td>52.103448</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>49.944444</td>\n",
       "      <td>53.615385</td>\n",
       "      <td>190.384615</td>\n",
       "      <td>55.416667</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>120.909091</td>\n",
       "      <td>84.205934</td>\n",
       "      <td>85.353485</td>\n",
       "      <td>85.883694</td>\n",
       "      <td>95.600000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>92.875000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>119.822643</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>36.750000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>57.930466</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>77.475260</td>\n",
       "      <td>36.950000</td>\n",
       "      <td>38.454545</td>\n",
       "      <td>37.133333</td>\n",
       "      <td>36.933333</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>13.750000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>19.920446</td>\n",
       "      <td>19.896906</td>\n",
       "      <td>19.816753</td>\n",
       "      <td>20.011274</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.50586</td>\n",
       "      <td>0.507407</td>\n",
       "      <td>7.350000</td>\n",
       "      <td>7.427500</td>\n",
       "      <td>7.443333</td>\n",
       "      <td>7.470000</td>\n",
       "      <td>215.875000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>40.375000</td>\n",
       "      <td>32.750000</td>\n",
       "      <td>31.500000</td>\n",
       "      <td>28.00000</td>\n",
       "      <td>29.60</td>\n",
       "      <td>29.600000</td>\n",
       "      <td>26.3</td>\n",
       "      <td>29.951503</td>\n",
       "      <td>4.109867</td>\n",
       "      <td>4.173645</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.071882</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.487078</td>\n",
       "      <td>86.666667</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>108.0</td>\n",
       "      <td>176.738114</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>25.921424</td>\n",
       "      <td>22.0</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>24.214374</td>\n",
       "      <td>1.952595</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.051946</td>\n",
       "      <td>138.840093</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>135.0</td>\n",
       "      <td>138.633454</td>\n",
       "      <td>149.939522</td>\n",
       "      <td>131.87701</td>\n",
       "      <td>157.0</td>\n",
       "      <td>128.530985</td>\n",
       "      <td>6.366667</td>\n",
       "      <td>8.900000</td>\n",
       "      <td>8.4</td>\n",
       "      <td>12.242523</td>\n",
       "      <td>95.666667</td>\n",
       "      <td>96.5000</td>\n",
       "      <td>92.666667</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>2.733333</td>\n",
       "      <td>2.039218</td>\n",
       "      <td>1.832424</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140107</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>169.337684</td>\n",
       "      <td>105.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>97.181818</td>\n",
       "      <td>84.466667</td>\n",
       "      <td>86.350000</td>\n",
       "      <td>84.363636</td>\n",
       "      <td>80.355020</td>\n",
       "      <td>80.313226</td>\n",
       "      <td>96.272727</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>115.676644</td>\n",
       "      <td>119.538003</td>\n",
       "      <td>135.818182</td>\n",
       "      <td>122.818182</td>\n",
       "      <td>59.269006</td>\n",
       "      <td>59.575281</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>64.363636</td>\n",
       "      <td>86.500000</td>\n",
       "      <td>56.875000</td>\n",
       "      <td>71.250000</td>\n",
       "      <td>59.375000</td>\n",
       "      <td>105.500000</td>\n",
       "      <td>105.500000</td>\n",
       "      <td>105.500000</td>\n",
       "      <td>105.500000</td>\n",
       "      <td>123.090909</td>\n",
       "      <td>122.923077</td>\n",
       "      <td>121.636364</td>\n",
       "      <td>119.822643</td>\n",
       "      <td>68.181818</td>\n",
       "      <td>66.538462</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>57.930466</td>\n",
       "      <td>80.500000</td>\n",
       "      <td>79.461538</td>\n",
       "      <td>79.600000</td>\n",
       "      <td>77.475260</td>\n",
       "      <td>39.600000</td>\n",
       "      <td>37.700000</td>\n",
       "      <td>38.300000</td>\n",
       "      <td>38.300000</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>9.666667</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>19.920446</td>\n",
       "      <td>19.896906</td>\n",
       "      <td>19.816753</td>\n",
       "      <td>20.011274</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.62000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>7.430000</td>\n",
       "      <td>8.406452</td>\n",
       "      <td>7.460000</td>\n",
       "      <td>7.495000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>123.874998</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>95.500000</td>\n",
       "      <td>50.500000</td>\n",
       "      <td>39.931142</td>\n",
       "      <td>39.500000</td>\n",
       "      <td>41.00000</td>\n",
       "      <td>30.90</td>\n",
       "      <td>29.100000</td>\n",
       "      <td>27.6</td>\n",
       "      <td>29.951503</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4.071882</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.487078</td>\n",
       "      <td>384.000000</td>\n",
       "      <td>374.000000</td>\n",
       "      <td>411.0</td>\n",
       "      <td>176.738114</td>\n",
       "      <td>21.0</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>25.921424</td>\n",
       "      <td>31.0</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>28.0</td>\n",
       "      <td>24.214374</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.051946</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>140.0</td>\n",
       "      <td>138.633454</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>135.00000</td>\n",
       "      <td>139.0</td>\n",
       "      <td>128.530985</td>\n",
       "      <td>12.100000</td>\n",
       "      <td>12.900000</td>\n",
       "      <td>10.8</td>\n",
       "      <td>12.242523</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>96.8269</td>\n",
       "      <td>96.412421</td>\n",
       "      <td>96.634002</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>2.182108</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Age  Gender      Height  Weight  Bilirubin  AST  ALT  ALP  Albumin  \\\n",
       "recordid                                                                       \n",
       "140101     39       0  170.200000   253.0          0    0    0    0        0   \n",
       "140102     70       0  169.337684   123.5          0    0    0    0        1   \n",
       "140104     61       1  188.000000    80.0          0    0    0    0        0   \n",
       "140106     64       1  162.600000    80.0          1    1    1    1        1   \n",
       "140107     45       1  169.337684   105.5          1    1    1    1        0   \n",
       "\n",
       "          TroponinT  TroponinI  Cholesterol  MechVent         HR0        HR12  \\\n",
       "recordid                                                                        \n",
       "140101            0          0            0         1  110.142857  108.833333   \n",
       "140102            0          0            0         1   76.444444   76.769231   \n",
       "140104            0          0            0         1  100.906250   88.357143   \n",
       "140106            0          0            0         1   72.269231   73.642857   \n",
       "140107            0          0            0         1   97.181818   84.466667   \n",
       "\n",
       "               HR24        HR36       MAP0      MAP12      MAP24      MAP36  \\\n",
       "recordid                                                                      \n",
       "140101    97.250000   97.636364  80.355020  80.313226  80.628562  81.518378   \n",
       "140102    78.636364   84.000000  80.355020  83.600000  87.545455  95.272727   \n",
       "140104    82.333333   93.125000  78.750000  65.428571  74.000000  81.518378   \n",
       "140106    97.200000  103.181818  69.517241  61.850000  65.611111  69.230769   \n",
       "140107    86.350000   84.363636  80.355020  80.313226  96.272727  81.000000   \n",
       "\n",
       "             SysABP0    SysABP12    SysABP24    SysABP36   DiasABP0  \\\n",
       "recordid                                                              \n",
       "140101    115.676644  119.538003  120.858617  122.740704  59.269006   \n",
       "140102    115.676644  109.200000  119.181818  132.500000  59.269006   \n",
       "140104    101.812500   89.714286   95.727273  122.740704  58.750000   \n",
       "140106    101.275862   95.950000  101.944444  107.615385  52.103448   \n",
       "140107    115.676644  119.538003  135.818182  122.818182  59.269006   \n",
       "\n",
       "          DiasABP12  DiasABP24  DiasABP36      Urine0     Urine12    Urine24  \\\n",
       "recordid                                                                       \n",
       "140101    59.575281  59.575021  59.960344   60.000000   53.636364  50.000000   \n",
       "140102    61.000000  69.272727  73.916667   39.285714  128.125000  27.818182   \n",
       "140104    49.428571  58.909091  59.960344  143.461538  114.363636  76.666667   \n",
       "140106    46.000000  49.944444  53.615385  190.384615   55.416667  85.000000   \n",
       "140107    59.575281  78.000000  64.363636   86.500000   56.875000  71.250000   \n",
       "\n",
       "             Urine36     Weight0    Weight12    Weight24    Weight36  \\\n",
       "recordid                                                               \n",
       "140101     72.222222  253.000000  253.000000  253.000000  253.000000   \n",
       "140102    107.625000  123.500000  123.500000  123.500000  123.500000   \n",
       "140104    158.333333   84.205934   85.353485   85.883694   86.183251   \n",
       "140106    120.909091   84.205934   85.353485   85.883694   95.600000   \n",
       "140107     59.375000  105.500000  105.500000  105.500000  105.500000   \n",
       "\n",
       "           NISysABP0  NISysABP12  NISysABP24  NISysABP36  NIDiasABP0  \\\n",
       "recordid                                                               \n",
       "140101     99.571429  108.333333  111.666667  103.818182   42.857143   \n",
       "140102    102.777778   99.142857  117.967196  119.822643   46.666667   \n",
       "140104    116.137862   97.833333   96.250000   99.125000   57.564944   \n",
       "140106    106.000000   92.875000  104.000000  119.822643   48.000000   \n",
       "140107    123.090909  122.923077  121.636364  119.822643   68.181818   \n",
       "\n",
       "          NIDiasABP12  NIDiasABP24  NIDiasABP36     NIMAP0    NIMAP12  \\\n",
       "recordid                                                                \n",
       "140101      50.916667    52.250000    45.000000  61.761429  70.055000   \n",
       "140102      47.142857    56.889398    57.930466  65.368889  64.475714   \n",
       "140104      43.333333    45.583333    43.875000  75.933999  61.500000   \n",
       "140106      36.750000    35.000000    57.930466  68.000000  56.000000   \n",
       "140107      66.538462    69.000000    57.930466  80.500000  79.461538   \n",
       "\n",
       "            NIMAP24    NIMAP36      Temp0     Temp12     Temp24     Temp36  \\\n",
       "recordid                                                                     \n",
       "140101    72.055000  64.605455  38.000000  37.733333  37.533333  37.766667   \n",
       "140102    76.214687  77.475260  36.350000  36.633333  36.766667  36.933333   \n",
       "140104    62.471667  62.292500  36.771429  37.200000  37.250000  37.082040   \n",
       "140106    59.000000  77.475260  36.950000  38.454545  37.133333  36.933333   \n",
       "140107    79.600000  77.475260  39.600000  37.700000  38.300000  38.300000   \n",
       "\n",
       "               GCS0      GCS12      GCS24      GCS36  RespRate0  RespRate12  \\\n",
       "recordid                                                                      \n",
       "140101    10.000000  10.000000  10.000000  10.000000  19.920446   19.896906   \n",
       "140102    10.500000  11.666667  11.000000  10.666667  19.920446   19.896906   \n",
       "140104     8.500000  15.000000  15.000000  15.000000  19.920446   19.896906   \n",
       "140106     8.000000  13.750000  15.000000  15.000000  19.920446   19.896906   \n",
       "140107     8.666667   9.666667   9.333333  10.000000  19.920446   19.896906   \n",
       "\n",
       "          RespRate24  RespRate36     FiO20    FiO212   FiO224    FiO236  \\\n",
       "recordid                                                                  \n",
       "140101     19.816753   20.011274  0.540000  0.400000  0.40000  0.400000   \n",
       "140102     19.816753   20.011274  0.500000  0.500000  0.50000  0.560000   \n",
       "140104     19.816753   20.011274  0.442857  0.508116  0.50586  0.507407   \n",
       "140106     19.816753   20.011274  0.740000  0.500000  0.50586  0.507407   \n",
       "140107     19.816753   20.011274  0.575000  0.500000  0.62000  0.450000   \n",
       "\n",
       "               pH0      pH12      pH24      pH36       PaO20      PaO212  \\\n",
       "recordid                                                                   \n",
       "140101    7.385000  8.406452  7.360000  7.398353  109.500000  123.874998   \n",
       "140102    7.390000  7.430000  7.561206  7.398353   85.000000   88.000000   \n",
       "140104    7.331429  7.390000  7.400000  7.398353  156.428571  153.000000   \n",
       "140106    7.350000  7.427500  7.443333  7.470000  215.875000   93.000000   \n",
       "140107    7.430000  8.406452  7.460000  7.495000  104.000000  123.874998   \n",
       "\n",
       "              PaO224      PaO236     PaCO20    PaCO212    PaCO224   PaCO236  \\\n",
       "recordid                                                                      \n",
       "140101    145.000000  115.412815  66.500000  39.931142  66.000000  40.44737   \n",
       "140102    118.137949  115.412815  58.000000  53.000000  39.565064  40.44737   \n",
       "140104    118.137949  115.412815  44.857143  42.500000  39.565064  40.44737   \n",
       "140106     69.000000   74.000000  40.375000  32.750000  31.500000  28.00000   \n",
       "140107    116.000000   95.500000  50.500000  39.931142  39.500000  41.00000   \n",
       "\n",
       "           HCT0      HCT12  HCT24      HCT36        K0       K12  K24  \\\n",
       "recordid                                                                \n",
       "140101    32.90  30.964831   32.9  29.951503  4.200000  4.173645  3.9   \n",
       "140102    28.90  30.964831   29.4  29.951503  4.500000  4.173645  4.1   \n",
       "140104    28.15  30.964831   28.8  29.951503  5.100000  4.173645  4.1   \n",
       "140106    29.60  29.600000   26.3  29.951503  4.109867  4.173645  4.5   \n",
       "140107    30.90  29.100000   27.6  29.951503  3.900000  3.900000  3.3   \n",
       "\n",
       "               K36  Creatinine0  Creatinine12  Creatinine24  Creatinine36  \\\n",
       "recordid                                                                    \n",
       "140101    4.071882         0.60      1.453197           0.5      1.487078   \n",
       "140102    4.071882         0.60      1.453197           0.5      1.487078   \n",
       "140104    4.071882         0.75      1.453197           0.9      1.487078   \n",
       "140106    4.071882         1.00      1.200000           1.0      1.487078   \n",
       "140107    4.071882         0.90      1.000000           0.6      1.487078   \n",
       "\n",
       "          Platelets0  Platelets12  Platelets24  Platelets36  BUN0      BUN12  \\\n",
       "recordid                                                                       \n",
       "140101    149.000000   197.180099        179.0   176.738114  14.0  25.731611   \n",
       "140102    162.000000   197.180099        185.0   176.738114  25.0  25.731611   \n",
       "140104    223.500000   197.180099        221.0   176.738114  11.0  25.731611   \n",
       "140106     86.666667   140.000000        108.0   176.738114  19.0  18.000000   \n",
       "140107    384.000000   374.000000        411.0   176.738114  21.0  24.000000   \n",
       "\n",
       "          BUN24      BUN36  HCO30     HCO312  HCO324     HCO336       Mg0  \\\n",
       "recordid                                                                    \n",
       "140101     13.0  25.921424   34.0  23.411007    33.0  24.214374  1.500000   \n",
       "140102     26.0  25.921424   34.0  23.411007    30.0  24.214374  2.200000   \n",
       "140104     18.0  25.921424   22.5  23.411007    30.0  24.214374  2.000000   \n",
       "140106     14.0  25.921424   22.0  21.000000    22.0  24.214374  1.952595   \n",
       "140107     22.0  25.921424   31.0  32.000000    28.0  24.214374  2.600000   \n",
       "\n",
       "              Mg12  Mg24      Mg36         Na0        Na12   Na24        Na36  \\\n",
       "recordid                                                                        \n",
       "140101    2.055607   1.9  2.051946  140.000000  139.053094  142.0  138.633454   \n",
       "140102    2.055607   2.2  2.051946  141.000000  139.053094  142.0  138.633454   \n",
       "140104    2.055607   2.0  2.051946  138.000000  139.053094  138.0  138.633454   \n",
       "140106    2.000000   2.4  2.051946  138.840093  136.000000  135.0  138.633454   \n",
       "140107    2.700000   2.5  2.051946  145.000000  146.000000  140.0  138.633454   \n",
       "\n",
       "            Glucose0  Glucose12  Glucose24   Glucose36       WBC0      WBC12  \\\n",
       "recordid                                                                       \n",
       "140101    103.000000  131.87701       86.0  128.530985  15.800000  12.416399   \n",
       "140102    134.000000  131.87701      144.0  128.530985  13.600000  12.416399   \n",
       "140104    161.000000  131.87701       99.0  128.530985  17.100000  12.416399   \n",
       "140106    149.939522  131.87701      157.0  128.530985   6.366667   8.900000   \n",
       "140107    125.000000  135.00000      139.0  128.530985  12.100000  12.900000   \n",
       "\n",
       "          WBC24      WBC36      SaO20   SaO212     SaO224     SaO236  \\\n",
       "recordid                                                               \n",
       "140101     10.7  12.242523  96.793083  96.8269  96.412421  96.634002   \n",
       "140102     11.8  12.242523  96.793083  96.8269  96.412421  96.634002   \n",
       "140104     17.3  12.242523  97.857143  98.0000  96.412421  96.634002   \n",
       "140106      8.4  12.242523  95.666667  96.5000  92.666667  94.000000   \n",
       "140107     10.8  12.242523  97.000000  96.8269  96.412421  96.634002   \n",
       "\n",
       "          Lactate0  Lactate12  Lactate24  Lactate36  Coronary Care Unit  \\\n",
       "recordid                                                                  \n",
       "140101    1.300000   2.182108   2.039218   1.832424                   0   \n",
       "140102    2.700000   2.182108   2.039218   1.832424                   0   \n",
       "140104    2.581794   2.182108   2.039218   1.832424                   0   \n",
       "140106    0.900000   2.733333   2.039218   1.832424                   0   \n",
       "140107    1.200000   2.182108   0.850000   1.000000                   0   \n",
       "\n",
       "          Cardiac Surgery Recovery Unit  Medical ICU  Surgical ICU  \n",
       "recordid                                                            \n",
       "140101                                0            1             0  \n",
       "140102                                0            1             0  \n",
       "140104                                1            0             0  \n",
       "140106                                1            0             0  \n",
       "140107                                0            1             0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating an array containing the preprocessed folds\n",
    "folds_x = {}\n",
    "folds_y_linear = {}\n",
    "folds_y_class = {}\n",
    "for i in all_list:\n",
    "    x_string = \"../Project_Data/Fold\"+str(i)\n",
    "    y_string = \"../Project_Data/Fold\"+str(i)+\"_Outcomes.csv\"\n",
    "    temp_df_y = pd.DataFrame(read_ans(y_string), columns=['recordid', 'days_in_hospital', 'mortality'])\n",
    "    temp_df_x = put_single_into_dataframe(read_text(x_string))\n",
    "    folds_x[i] = preprocess_x_for_design_matrix_2(temp_df_x)\n",
    "    folds_y_linear[i] = preprocess_y_linear(temp_df_y)\n",
    "    folds_y_class[i] = preprocess_y_class(temp_df_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Regressor Task</h4>\n",
    "\n",
    "<p>Steps:</p>\n",
    "<ol>\n",
    "    <li><b>Standard Scaler</b></li>\n",
    "    <li><b>Dimensionality Reducer with Variance Threshold</b></li>\n",
    "    <li><b>Dimensionality Reducer with PCA</b></li>\n",
    "    <li><b>Classifier</b>\n",
    "        <br>We compare four different classifier models based on the mean squared error. The three classifiers are Linear Regression, DecisionTreeRegressor and MLPRegressor.</br>\n",
    "    </li>\n",
    "</ol>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on Fold 1\n",
      "# Getting train data set up\n",
      "# Getting test data set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on Fold 2\n",
      "# Getting train data set up\n",
      "# Getting test data set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on Fold 3\n",
      "# Getting train data set up\n",
      "# Getting test data set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on Fold 4\n",
      "# Getting train data set up\n",
      "# Getting test data set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1321: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Running CV\n",
    "\n",
    "#Create a matrix to store the results where row represnets the model and column represents the result tested on fold i\n",
    "w, h = 4, 5\n",
    "matrix = [[0 for x in range(w)] for y in range(h)] \n",
    "\n",
    "# keep track of the best score when trying out different hyperparameters in a fold\n",
    "best = sys.maxsize\n",
    "param = \"\"\n",
    "\n",
    "all_list = [1,2,3,4]\n",
    "for i in range(1,5):\n",
    "    print(\"Testing on Fold\", i)\n",
    "    x_train_df = pd.DataFrame()\n",
    "    y_train_linear_df = pd.DataFrame()\n",
    "    y_train_class_df = pd.DataFrame()\n",
    "    x_test_df = pd.DataFrame()\n",
    "    y_test_linear_df = pd.DataFrame()\n",
    "    y_test_class_df = pd.DataFrame()\n",
    "\n",
    "    # Getting train data set up\n",
    "    print(\"# Getting train data set up\")\n",
    "    for j in [x for x in all_list if x != i]:\n",
    "        x_train_df = x_train_df.append(folds_x[j])\n",
    "        y_train_linear_df = y_train_linear_df.append(folds_y_linear[i])\n",
    "        y_train_class_df = y_train_class_df.append(folds_y_class[i])\n",
    "\n",
    "    # Getting test data set up \n",
    "    print(\"# Getting test data set up\")\n",
    "    x_test_df = folds_x[i]\n",
    "    y_test_linear_df= folds_y_linear[i]\n",
    "    y_test_class_df = folds_y_class[i]\n",
    "\n",
    "    # Linear\n",
    "    best = sys.maxsize\n",
    "    parameters = {'f_selecter__threshold':[0.5], \n",
    "                  'dim_reducer__n_components':[60,70]}\n",
    "    est = Pipeline(steps=[\n",
    "            ('scaler', scaler),\n",
    "            ('f_selecter', VarianceThreshold()),\n",
    "            ('dim_reducer', PCA()),\n",
    "            ('classifier', LinearRegression())])\n",
    "    estimator = GridSearchCV(est, parameters, cv=3, scoring='neg_mean_squared_error')\n",
    "    estimator.fit(x_train_df, y_train_linear_df)# gridsearch obejct has scanned thorugh all the best parameters to set it\n",
    "    prediction = estimator.predict(x_test_df)\n",
    "    matrix[i-1][0] = (mean_squared_error(y_test_linear_df, prediction))\n",
    "    \n",
    "    \n",
    "    # DecisionTreeRegressor\n",
    "    best = sys.maxsize\n",
    "    parameters = {'f_selecter__threshold':[0.5], \n",
    "                  'dim_reducer__n_components':[60,70],\n",
    "                  'classifier__min_samples_leaf':[1,3]\n",
    "                 }\n",
    "    est = Pipeline(steps=[\n",
    "            ('scaler', scaler),\n",
    "            ('f_selecter', VarianceThreshold()),\n",
    "            ('dim_reducer', PCA()),\n",
    "            ('classifier', DecisionTreeRegressor())])\n",
    "    estimator = GridSearchCV(est, parameters, cv=3, scoring='neg_mean_squared_error')\n",
    "    estimator.fit(x_train_df, y_train_linear_df)# gridsearch obejct has scanned thorugh all the best parameters to set it\n",
    "    prediction = estimator.predict(x_test_df)\n",
    "    matrix[i-1][1] = (mean_squared_error(y_test_linear_df, prediction))\n",
    "    \n",
    "    \n",
    "    # MLPRegressor\n",
    "    best = sys.maxsize\n",
    "    parameters = {'f_selecter__threshold':[0.5], \n",
    "                  'dim_reducer__n_components':[60,70],\n",
    "                  'classifier__hidden_layer_sizes':[(130, 110, 90, 70, 50, 30, 10, 5), (130,100)],\n",
    "                  'classifier__learning_rate_init':[0.9,0.105]\n",
    "                 }\n",
    "    est = Pipeline(steps=[\n",
    "            ('scaler', scaler),\n",
    "            ('f_selecter', VarianceThreshold()),\n",
    "            ('dim_reducer', PCA()),\n",
    "            ('classifier', MLPRegressor())])\n",
    "    estimator = GridSearchCV(est, parameters, cv=3, scoring='neg_mean_squared_error')\n",
    "    estimator.fit(x_train_df, y_train_linear_df)# gridsearch obejct has scanned thorugh all the best parameters to set it\n",
    "    prediction = estimator.predict(x_test_df)\n",
    "    matrix[i-1][2] = (mean_squared_error(y_test_linear_df, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Result</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Linear</th>\n",
       "      <th>DecisionTree</th>\n",
       "      <th>MLPRegressor</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>204.702878</td>\n",
       "      <td>340.127821</td>\n",
       "      <td>199.545426</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>140.603207</td>\n",
       "      <td>267.058755</td>\n",
       "      <td>137.440724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>142.208207</td>\n",
       "      <td>248.519249</td>\n",
       "      <td>138.764072</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1338.477124</td>\n",
       "      <td>417.637773</td>\n",
       "      <td>118.310005</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.365810</td>\n",
       "      <td>17.841970</td>\n",
       "      <td>12.186675</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Linear  DecisionTree  MLPRegressor   \n",
       "0   204.702878    340.127821    199.545426  0\n",
       "1   140.603207    267.058755    137.440724  0\n",
       "2   142.208207    248.519249    138.764072  0\n",
       "3  1338.477124    417.637773    118.310005  0\n",
       "4    21.365810     17.841970     12.186675  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resultnumber_of_learners = 3\n",
    "score_df = pd.DataFrame(matrix, columns=[\"Linear\", \"DecisionTree\", \"MLPRegressor\",\"\"])\n",
    "for i in range(number_of_learners):\n",
    "    score_df.iloc[4,i] = np.sqrt(score_df.iloc[0:4,i].mean())\n",
    "display(score_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Classifier Task</h4>\n",
    "<p>Steps:</p>\n",
    "<ol>\n",
    "    <li><b>SMOTE</b></li>\n",
    "    <li><b>Standard Scaler</b></li>\n",
    "    <li><b>Dimensionality Reducer with PCA</b></li>\n",
    "    <li><b>Classifier</b>\n",
    "        <br>We compare four different classifier models based on the ROC AUC score. The four classifiers are Logistic Regression, Random Forest, K Nearest Neighbors, and MLP.</br>\n",
    "    </li>\n",
    "</ol>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import all folds\n",
    "# Modify into Design Matrix 2\n",
    "x_fold = {}\n",
    "y_fold = {}\n",
    "non_bin_feat = []\n",
    "for i in range (1,5):\n",
    "    string = \"../Project_Data/Fold\"+str(i)\n",
    "    y_file = \"../Project_Data/Fold\"+str(i)+\"_Outcomes.csv\"\n",
    "    x_fold[i] = preprocess_x_for_design_matrix_2(put_single_into_dataframe(read_text(string)))\n",
    "    y_fold[i] = pd.read_csv(y_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim_reducer__n_components': 60, 'f_selecter__k': 70}\n",
      "Logistic 0.7155160675381262\n",
      "{'dim_reducer__n_components': 60, 'f_selecter__k': 80, 'rf__max_depth': 4, 'rf__n_estimators': 150}\n",
      "RF 0.7026824618736384\n",
      "{'classifier__n_neighbors': 5, 'dim_reducer__n_components': 70, 'f_selecter__k': 90}\n",
      "KNeighbors 0.7191244553376906\n",
      "{'MLP__hidden_layer_sizes': (100,), 'MLP__learning_rate_init': 0.01, 'dim_reducer__n_components': 70, 'f_selecter__k': 80}\n",
      "MLP 0.6681304466230937\n",
      "Testing on Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim_reducer__n_components': 60, 'f_selecter__k': 90}\n",
      "Logistic 0.7536480142113944\n",
      "{'dim_reducer__n_components': 70, 'f_selecter__k': 70, 'rf__max_depth': 5, 'rf__n_estimators': 100}\n",
      "RF 0.7057797233853572\n",
      "{'classifier__n_neighbors': 5, 'dim_reducer__n_components': 60, 'f_selecter__k': 70}\n",
      "KNeighbors 0.7197690648394873\n",
      "{'MLP__hidden_layer_sizes': (130, 100), 'MLP__learning_rate_init': 0.01, 'dim_reducer__n_components': 70, 'f_selecter__k': 70}\n",
      "MLP 0.6005107219895952\n",
      "Testing on Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim_reducer__n_components': 60, 'f_selecter__k': 70}\n",
      "Logistic 0.7285449292491546\n",
      "{'dim_reducer__n_components': 60, 'f_selecter__k': 90, 'rf__max_depth': 5, 'rf__n_estimators': 150}\n",
      "RF 0.6920450441577203\n",
      "{'classifier__n_neighbors': 5, 'dim_reducer__n_components': 70, 'f_selecter__k': 80}\n",
      "KNeighbors 0.6349354870481632\n",
      "{'MLP__hidden_layer_sizes': (100,), 'MLP__learning_rate_init': 0.01, 'dim_reducer__n_components': 70, 'f_selecter__k': 90}\n",
      "MLP 0.6012672773236154\n",
      "Testing on Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim_reducer__n_components': 60, 'f_selecter__k': 70}\n",
      "Logistic 0.7213661123853211\n",
      "{'dim_reducer__n_components': 60, 'f_selecter__k': 90, 'rf__max_depth': 4, 'rf__n_estimators': 100}\n",
      "RF 0.6829845183486238\n",
      "{'classifier__n_neighbors': 5, 'dim_reducer__n_components': 60, 'f_selecter__k': 90}\n",
      "KNeighbors 0.6539205848623854\n",
      "{'MLP__hidden_layer_sizes': (100,), 'MLP__learning_rate_init': 0.01, 'dim_reducer__n_components': 70, 'f_selecter__k': 70}\n",
      "MLP 0.5767990252293578\n"
     ]
    }
   ],
   "source": [
    "#For MODEL 2\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from math import sqrt\n",
    "\n",
    "result = {} #storing results of each fold\n",
    "all_list = [1,2,3,4]\n",
    "for i in range(1,5):\n",
    "    print(\"Testing on Fold\", i)\n",
    "    x_train_df = pd.DataFrame()\n",
    "    y_train_df = pd.DataFrame()\n",
    "    \n",
    "    # Getting train data set up\n",
    "    for j in [x for x in all_list if x != i]: \n",
    "        x_train_df = x_train_df.append(x_fold[j]) #Inserting X for train data\n",
    "        y_train_df = y_train_df.append(y_fold[j]) #Inserting Y for train data\n",
    "    y_train_df = y_train_df.drop(['Length_of_stay'], axis=1)\n",
    "    y_train_df = y_train_df.replace(-1, 2) #use -1 to indicate missing values\n",
    "    \n",
    "    train_df = x_train_df.merge(y_train_df, left_on=\"recordid\", right_on=\"RecordID\", how='outer')\n",
    "    train_df = train_df.set_index(\"RecordID\")\n",
    "    train_df = train_df.replace(np.nan, -1)\n",
    "    X_train = train_df.loc[:, train_df.columns != 'In-hospital_death']\n",
    "    Y_train = train_df['In-hospital_death']\n",
    "    \n",
    "        \n",
    "    x_test_df = x_train_df.iloc[0:0]\n",
    "    y_test_df = y_train_df.iloc[0:0]\n",
    "    # Getting test data set up\n",
    "    x_test_df = x_test_df.append(x_fold[i])\n",
    "    y_test_df = y_test_df.append(y_fold[i])\n",
    "    y_test_df = y_test_df.drop(['Length_of_stay'], axis=1)\n",
    "     # Replace -1 with NaN\n",
    "#     x_test_df = x_test_df.replace(-1, np.nan)\n",
    "    # Replace not known length of stay to 2\n",
    "    y_test_df = y_test_df.replace(-1, 2)\n",
    "    \n",
    "    test_df = x_test_df.merge(y_test_df, left_on=\"recordid\", right_on=\"RecordID\", how='outer')\n",
    "    test_df = test_df.set_index(\"RecordID\")\n",
    "    test_df = test_df.replace(np.nan, -1)\n",
    "    X_test = test_df.loc[:, test_df.columns != 'In-hospital_death']\n",
    "    Y_test = test_df['In-hospital_death']\n",
    "    \n",
    "#     print(X_train.head())\n",
    "#     best = 0\n",
    "    \n",
    "    # Logistic Regression\n",
    "    parameters = {'dim_reducer__n_components':[60,70],\n",
    "              'f_selecter__k':[70,80,90]\n",
    "             }\n",
    "    \n",
    "    model = imPipeline(steps=[\n",
    "                              ('smote', SMOTE()),\n",
    "                             ('scaler', StandardScaler()),\n",
    "                            ('f_selecter', SelectKBest()),\n",
    "                            ('dim_reducer', PCA()),\n",
    "                           ('lr', LogisticRegression())])\n",
    "    est = GridSearchCV(model, parameters, cv=3, scoring = 'roc_auc')\n",
    "    est.fit(X_train, Y_train)\n",
    "    best_est = est.best_estimator_\n",
    "    print(est.best_params_)\n",
    "    y_pred = best_est.predict(X_test)\n",
    "    score = roc_auc_score(Y_test, y_pred)\n",
    "    print(\"Logistic\", score)            \n",
    "        \n",
    "    if i == 1:\n",
    "        result['logistic'] = score\n",
    "    else:\n",
    "        result['logistic'] = result['logistic'] + score\n",
    "    \n",
    "\n",
    "    # Random Forest\n",
    "    parameters = {'dim_reducer__n_components':[60,70],\n",
    "              'f_selecter__k':[70,80,90],\n",
    "              'rf__max_depth':[3,4,5],\n",
    "              'rf__n_estimators': [100,125,150]\n",
    "             }\n",
    "    \n",
    "    model = imPipeline(steps=[\n",
    "                                      ('smote', SMOTE()),\n",
    "                                      ('scaler', StandardScaler()),\n",
    "                                            ('f_selecter', SelectKBest()),\n",
    "                                            ('dim_reducer', PCA()),\n",
    "                                           ('rf', RandomForestClassifier())])\n",
    "    \n",
    "    est = GridSearchCV(model, parameters, cv=3, scoring = 'roc_auc')\n",
    "    est.fit(X_train, Y_train)\n",
    "    best_est = est.best_estimator_\n",
    "    print(est.best_params_)\n",
    "    y_pred = best_est.predict(X_test)\n",
    "    score = roc_auc_score(Y_test, y_pred)\n",
    "    print(\"RF\", score)            \n",
    "        \n",
    "    if i == 1:\n",
    "        result['random_forest'] = score\n",
    "    else:\n",
    "        result['random_forest'] = result['random_forest'] + score\n",
    "    \n",
    "    # K-Neighbors\n",
    "    parameters = {'dim_reducer__n_components':[60,70],\n",
    "              'f_selecter__k':[70,80,90],\n",
    "                  'classifier__n_neighbors':[3,4,5]\n",
    "             }\n",
    "    \n",
    "    model = imPipeline(steps=[\n",
    "                                      ('smote', SMOTE()),\n",
    "                                      ('scaler', StandardScaler()),\n",
    "                                            ('f_selecter', SelectKBest()),\n",
    "                                            ('dim_reducer', PCA()),\n",
    "                                           ('classifier', KNeighborsClassifier())])\n",
    "    \n",
    "    est = GridSearchCV(model, parameters, cv=3, scoring = 'roc_auc')\n",
    "    est.fit(X_train, Y_train)\n",
    "    best_est = est.best_estimator_\n",
    "    print(est.best_params_)\n",
    "    y_pred = best_est.predict(X_test)\n",
    "    score = roc_auc_score(Y_test, y_pred)\n",
    "    print(\"KNeighbors\", score)\n",
    "    \n",
    "    if i == 1:\n",
    "        result['k_neighbors'] = score\n",
    "    else:\n",
    "        result['k_neighbors'] = result['k_neighbors'] + score   \n",
    "    \n",
    "    # MLP Classifier\n",
    "    parameters = {'dim_reducer__n_components':[60,70],\n",
    "              'f_selecter__k':[70,80,90],\n",
    "                  'MLP__hidden_layer_sizes':[(130,100,), (100,)],\n",
    "                  'MLP__learning_rate_init': [0.01]\n",
    "             }\n",
    "    model = imPipeline(steps=[\n",
    "                              ('smote', SMOTE()),\n",
    "                              ('scaler', StandardScaler()),\n",
    "                              ('f_selecter', SelectKBest()),\n",
    "                              ('dim_reducer', PCA()),\n",
    "                              ('MLP', MLPClassifier())])\n",
    "    est = GridSearchCV(model, parameters, cv=3, scoring = 'roc_auc')\n",
    "    est.fit(X_train, Y_train)\n",
    "    best_est = est.best_estimator_\n",
    "    print(est.best_params_)\n",
    "    y_pred = best_est.predict(X_test)\n",
    "    score = roc_auc_score(Y_test, y_pred)\n",
    "    print(\"MLP\", score)            \n",
    "        \n",
    "                \n",
    "    if i == 1:\n",
    "        result['MLP'] = score\n",
    "    else:\n",
    "        result['MLP'] = result['MLP'] + score            \n",
    "                \n",
    "for x in result:\n",
    "    result[x] = result[x]/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Result</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logistic': 0.7297687808459992,\n",
       " 'random_forest': 0.6958729369413349,\n",
       " 'k_neighbors': 0.6819373980219317,\n",
       " 'MLP': 0.6116768677914155}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression yields better result than other methods. This might happen because Logistic Regression able to generalize the data better than other methods. At first we thought MLP would perform best, however we believe the MLP in this case overfit the training data. Hence, unable to perform well in the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Third Model</h3>\n",
    "<ul> \n",
    "    <li>We make use of the second Design matrix described above on the same parameters that we tried in the Model building 1.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all folds\n",
    "# Modify into Design Matrix 3\n",
    "x_fold = {}\n",
    "y_fold = {}\n",
    "for i in range (1,5):\n",
    "    string = \"../Project_Data/Fold\"+str(i)\n",
    "    y_file = \"../Project_Data/Fold\"+str(i)+\"_Outcomes.csv\"\n",
    "    x_fold[i] = preprocess_x_for_design_matrix_3(put_single_into_dataframe(read_text(string)))\n",
    "    y_fold[i] = pd.read_csv(y_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Classifier Task</h4>\n",
    "<p>We have several steps for classifier task of this design matrix.\n",
    "    <ol>\n",
    "        <li><b>SMOTE</b>\n",
    "            <br>SMOTE is used to balanced the data. From intuition, we believe that living patients should outnumber the rate of mortalities. And based on our observation of data it's true. Only around a tenth of the patients actually passed away in the hospital. SMOTE will oversample the mortality data to achieve balance.</br>\n",
    "        </li>\n",
    "        <li><b>Classifier Model</b>\n",
    "            <br>We select our classifier model empirically based on which model has the best results on average across all folds. There are three classifiers that we tested which are Logistic Regression, Random Forest, and MLP.</br>\n",
    "        </li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim_reducer__n_components': 70, 'f_selecter__k': 350}\n",
      "Logistic 0.6797385620915033\n",
      "{'dim_reducer__n_components': 70, 'f_selecter__k': 350, 'rf__max_depth': 5, 'rf__n_estimators': 125}\n",
      "RF 0.6773556644880174\n",
      "{'MLP__hidden_layer_sizes': (130, 110, 90, 70, 50, 30, 10, 5), 'MLP__learning_rate_init': 0.01, 'dim_reducer__n_components': 70, 'f_selecter__k': 400}\n",
      "MLP 0.616489651416122\n",
      "Testing on Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim_reducer__n_components': 70, 'f_selecter__k': 400}\n",
      "Logistic 0.6906166730110394\n",
      "{'dim_reducer__n_components': 70, 'f_selecter__k': 400, 'rf__max_depth': 5, 'rf__n_estimators': 150}\n",
      "RF 0.6552309351605127\n",
      "{'MLP__hidden_layer_sizes': (130, 100, 100, 100, 100, 100), 'MLP__learning_rate_init': 0.01, 'dim_reducer__n_components': 70, 'f_selecter__k': 350}\n",
      "MLP 0.6094880091358965\n",
      "Testing on Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim_reducer__n_components': 60, 'f_selecter__k': 350}\n",
      "Logistic 0.7172510588003547\n",
      "{'dim_reducer__n_components': 70, 'f_selecter__k': 350, 'rf__max_depth': 4, 'rf__n_estimators': 150}\n",
      "RF 0.6415098985521521\n",
      "{'MLP__hidden_layer_sizes': (130, 110, 90, 70, 50, 30, 10, 5), 'MLP__learning_rate_init': 0.01, 'dim_reducer__n_components': 60, 'f_selecter__k': 400}\n",
      "MLP 0.592452148790177\n",
      "Testing on Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim_reducer__n_components': 60, 'f_selecter__k': 400}\n",
      "Logistic 0.6945599197247706\n",
      "{'dim_reducer__n_components': 70, 'f_selecter__k': 400, 'rf__max_depth': 5, 'rf__n_estimators': 150}\n",
      "RF 0.6663919151376148\n",
      "{'MLP__hidden_layer_sizes': (130, 110, 90, 70, 50, 30, 10, 5), 'MLP__learning_rate_init': 0.01, 'dim_reducer__n_components': 150, 'f_selecter__k': 400}\n",
      "MLP 0.5927823967889908\n"
     ]
    }
   ],
   "source": [
    "#For MODEL 3\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from math import sqrt\n",
    "\n",
    "num_feat = []\n",
    "normal_feat = nor_feat.copy()\n",
    "normal_feat.remove(\"MechVent\")\n",
    "for feat in normal_feat:\n",
    "    for hour in range(1,49):\n",
    "        num_feat.append(feat + '_' +  str(hour))\n",
    "cat_feat = [\"ICUType\"]\n",
    "\n",
    "cat_transformer = Pipeline(steps=[('imputer', SimpleImputer(-1, strategy='most_frequent')),\n",
    "                                 ('OneHotEncoder', OneHotEncoder(handle_unknown='ignore', sparse=False))])\n",
    "\n",
    "stat_feat_cp = stat_feat.copy()\n",
    "stat_feat_cp.remove(\"RecordID\")\n",
    "stat_feat_cp.remove(\"ICUType\")\n",
    "prepro = ColumnTransformer(\n",
    "    remainder = 'passthrough',\n",
    "    transformers=[\n",
    "        ('cat', cat_transformer, cat_feat),\n",
    "    ('num', SimpleImputer(-1, strategy='mean'), num_feat),\n",
    "    ('stat', SimpleImputer(-1, strategy='mean'), stat_feat_cp)])\n",
    "\n",
    "result = {} #storing results of each fold\n",
    "all_list = [1,2,3,4]\n",
    "for i in range(1,5):\n",
    "    print(\"Testing on Fold\", i)\n",
    "    x_train_df = pd.DataFrame()\n",
    "    y_train_df = pd.DataFrame()\n",
    "    \n",
    "    # Getting train data set up\n",
    "    for j in [x for x in all_list if x != i]: \n",
    "        x_train_df = x_train_df.append(x_fold[j]) #Inserting X for train data\n",
    "        y_train_df = y_train_df.append(y_fold[j]) #Inserting Y for train data\n",
    "    y_train_df = y_train_df.drop(['Length_of_stay'], axis=1)\n",
    "    y_train_df = y_train_df.replace(-1, 2) #use -1 to indicate missing values\n",
    "    \n",
    "    train_df = x_train_df.merge(y_train_df, on=\"RecordID\", how='outer')\n",
    "    train_df = train_df.set_index(\"RecordID\")\n",
    "    train_df = train_df.replace(np.nan, -1)\n",
    "    X_train = train_df.loc[:, train_df.columns != 'In-hospital_death']\n",
    "    Y_train = train_df['In-hospital_death']\n",
    "    \n",
    "        \n",
    "    x_test_df = x_train_df.iloc[0:0]\n",
    "    y_test_df = y_train_df.iloc[0:0]\n",
    "    # Getting test data set up\n",
    "    x_test_df = x_test_df.append(x_fold[i])\n",
    "    y_test_df = y_test_df.append(y_fold[i])\n",
    "    y_test_df = y_test_df.drop(['Length_of_stay'], axis=1)\n",
    "     # Replace -1 with NaN\n",
    "#     x_test_df = x_test_df.replace(-1, np.nan)\n",
    "    # Replace not known length of stay to 2\n",
    "    y_test_df = y_test_df.replace(-1, 2)\n",
    "    \n",
    "    test_df = x_test_df.merge(y_test_df, on=\"RecordID\", how='outer')\n",
    "    test_df = test_df.set_index(\"RecordID\")\n",
    "    test_df = test_df.replace(np.nan, -1)\n",
    "    X_test = test_df.loc[:, test_df.columns != 'In-hospital_death']\n",
    "    Y_test = test_df['In-hospital_death']\n",
    "    \n",
    "#     print(X_train.head())\n",
    "    best = 0\n",
    "    \n",
    "    # Logistic Regression\n",
    "    parameters = {'dim_reducer__n_components':[60,70, 150, 200],\n",
    "              'f_selecter__k':[350,400]\n",
    "             }\n",
    "    \n",
    "    model = imPipeline(steps=[('imputer', prepro),\n",
    "                              ('smote', SMOTE()),\n",
    "                             ('scaler', StandardScaler()),\n",
    "                            ('f_selecter', SelectKBest()),\n",
    "                            ('dim_reducer', PCA()),\n",
    "                           ('lr', LogisticRegression())])\n",
    "    est = GridSearchCV(model, parameters, cv=3, scoring = 'roc_auc')\n",
    "    est.fit(X_train, Y_train)\n",
    "    best_est = est.best_estimator_\n",
    "    print(est.best_params_)\n",
    "    y_pred = best_est.predict(X_test)\n",
    "    score = roc_auc_score(Y_test, y_pred)\n",
    "    print(\"Logistic\", score)            \n",
    "        \n",
    "    if i == 1:\n",
    "        result['logistic'] = score\n",
    "    else:\n",
    "        result['logistic'] = result['logistic'] + score\n",
    "    \n",
    "\n",
    "    # Random Forest\n",
    "    parameters = {'dim_reducer__n_components':[60,70],\n",
    "              'f_selecter__k':[350,400],\n",
    "              'rf__max_depth':[3,4,5],\n",
    "              'rf__n_estimators': [100,125,150]\n",
    "             }\n",
    "    \n",
    "    model = imPipeline(steps=[('imputer', prepro),\n",
    "                                      ('smote', SMOTE()),\n",
    "                                      ('scaler', StandardScaler()),\n",
    "                                            ('f_selecter', SelectKBest()),\n",
    "                                            ('dim_reducer', PCA()),\n",
    "                                           ('rf', RandomForestClassifier())])\n",
    "    \n",
    "    est = GridSearchCV(model, parameters, cv=3, scoring = 'roc_auc')\n",
    "    est.fit(X_train, Y_train)\n",
    "    best_est = est.best_estimator_\n",
    "    print(est.best_params_)\n",
    "    y_pred = best_est.predict(X_test)\n",
    "    score = roc_auc_score(Y_test, y_pred)\n",
    "    print(\"RF\", score)            \n",
    "        \n",
    "    if i == 1:\n",
    "        result['random_forest'] = score\n",
    "    else:\n",
    "        result['random_forest'] = result['random_forest'] + score\n",
    "    \n",
    "    # MLP Classifier\n",
    "    parameters = {'dim_reducer__n_components':[60,70, 150, 200],\n",
    "              'f_selecter__k':[350,400],\n",
    "                  'MLP__hidden_layer_sizes':[(130, 110, 90, 70, 50, 30, 10, 5),(130, 100, 100, 100, 100, 100)],\n",
    "                  'MLP__learning_rate_init': [0.01]\n",
    "             }\n",
    "    model = imPipeline(steps=[('imputer', prepro),\n",
    "                              ('smote', SMOTE()),\n",
    "                              ('scaler', StandardScaler()),\n",
    "                              ('f_selecter', SelectKBest()),\n",
    "                              ('dim_reducer', PCA()),\n",
    "                              ('MLP', MLPClassifier())])\n",
    "    est = GridSearchCV(model, parameters, cv=3, scoring = 'roc_auc')\n",
    "    est.fit(X_train, Y_train)\n",
    "    best_est = est.best_estimator_\n",
    "    print(est.best_params_)\n",
    "    y_pred = best_est.predict(X_test)\n",
    "    score = roc_auc_score(Y_test, y_pred)\n",
    "    print(\"MLP\", score)            \n",
    "        \n",
    "                \n",
    "    if i == 1:\n",
    "        result['MLP'] = score\n",
    "    else:\n",
    "        result['MLP'] = result['MLP'] + score            \n",
    "                \n",
    "for x in result:\n",
    "    result[x] = result[x]/4\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Result</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logistic': 0.695541553406917,\n",
       " 'random_forest': 0.6601221033345742,\n",
       " 'MLP': 0.6028030515327966}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression yields better result than other methods. The data on this table is really sparsed before imputation and after the imputation the values might not represent the right condition of each patient. Logistic regression able to generalize the data more in this condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Regression Task</h4>\n",
    "<p>Steps:</p>\n",
    "<ol>\n",
    "    <li><b>Imputer</b></li>\n",
    "    <li><b>One Hot Encoder</b> <i>(only for categorical features)</i>\n",
    "    <li><b>Standard Scaler</b></li>\n",
    "    <li><b>Dimensionality Reducer with PCA</b></li>\n",
    "    <li><b>Classifier</b>\n",
    "        <br>We compare four different classifier models based on the ROC AUC score. The four classifiers are Logistic Regression, Random Forest, K Nearest Neighbors, and MLP.</br>\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim_reducer__n_components': 60, 'f_selecter__threshold': 0}\n",
      "Linear 13.79711067167092\n",
      "{'classifier__min_samples_leaf': 3, 'classifier__min_samples_split': 8, 'dim_reducer__n_components': 70, 'f_selecter__threshold': 0.25}\n",
      "Decision Tree 16.953649951500253\n",
      "{'dim_reducer__n_components': 70, 'f_selecter__threshold': 0.5}\n",
      "Lasso 13.816103812644338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__hidden_layer_sizes': (130, 110, 90, 70, 50, 30, 10, 5), 'dim_reducer__n_components': 50, 'f_selecter__k': 350}\n",
      "MLP 14.13199767158594\n",
      "Testing on Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim_reducer__n_components': 70, 'f_selecter__threshold': 0}\n",
      "Linear 11.41485528892256\n",
      "{'classifier__min_samples_leaf': 3, 'classifier__min_samples_split': 2, 'dim_reducer__n_components': 55, 'f_selecter__threshold': 0.25}\n",
      "Decision Tree 15.62640993638654\n",
      "{'dim_reducer__n_components': 60, 'f_selecter__threshold': 0.25}\n",
      "Lasso 11.416677627429614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__hidden_layer_sizes': (130, 110, 90, 70, 50, 30, 10, 5), 'dim_reducer__n_components': 50, 'f_selecter__k': 350}\n",
      "MLP 11.73174594932755\n",
      "Testing on Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim_reducer__n_components': 50, 'f_selecter__threshold': 0.5}\n",
      "Linear 11.542117343810261\n",
      "{'classifier__min_samples_leaf': 3, 'classifier__min_samples_split': 8, 'dim_reducer__n_components': 60, 'f_selecter__threshold': 0.25}\n",
      "Decision Tree 14.548822282677218\n",
      "{'dim_reducer__n_components': 50, 'f_selecter__threshold': 0}\n",
      "Lasso 11.537678741383548\n",
      "{'classifier__hidden_layer_sizes': (130, 100, 100, 100, 100, 100), 'dim_reducer__n_components': 70, 'f_selecter__k': 400}\n",
      "MLP 11.792209151253871\n",
      "Testing on Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim_reducer__n_components': 60, 'f_selecter__threshold': 0}\n",
      "Linear 10.853264272470186\n",
      "{'classifier__min_samples_leaf': 3, 'classifier__min_samples_split': 8, 'dim_reducer__n_components': 60, 'f_selecter__threshold': 0}\n",
      "Decision Tree 13.870632463096406\n",
      "{'dim_reducer__n_components': 60, 'f_selecter__threshold': 0.25}\n",
      "Lasso 10.860807620318155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__hidden_layer_sizes': (130, 100, 100, 100, 100, 100), 'dim_reducer__n_components': 60, 'f_selecter__k': 350}\n",
      "MLP 10.877115240369134\n"
     ]
    }
   ],
   "source": [
    "#For MODEL 3\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from math import sqrt\n",
    "num_feat = []\n",
    "normal_feat = nor_feat.copy()\n",
    "normal_feat.remove(\"MechVent\")\n",
    "for feat in normal_feat:\n",
    "    for hour in range(1,49):\n",
    "        num_feat.append(feat + '_' +  str(hour))\n",
    "cat_feat = [\"ICUType\"]\n",
    "\n",
    "cat_transformer = Pipeline(steps=[('imputer', SimpleImputer(-1, strategy='most_frequent')),\n",
    "                                 ('OneHotEncoder', OneHotEncoder(handle_unknown='ignore', sparse=False))])\n",
    "\n",
    "stat_feat_cp = stat_feat.copy()\n",
    "stat_feat_cp.remove(\"RecordID\")\n",
    "stat_feat_cp.remove(\"ICUType\")\n",
    "prepro = ColumnTransformer(\n",
    "    remainder = 'passthrough',\n",
    "    transformers=[\n",
    "        ('cat', cat_transformer, cat_feat),\n",
    "    ('num', SimpleImputer(-1, strategy='mean'), num_feat),\n",
    "    ('stat', SimpleImputer(-1, strategy='mean'), stat_feat_cp)])\n",
    "\n",
    "parameters = {'VarianceThreshold':[0, 0.25, 0.5], 'PCA':[50,55,60,70],\n",
    "              'SelectKBest':[350,400],\n",
    "              'RFDepth':[3,4,5],\n",
    "              'RFEst': [100,125,150],\n",
    "              'KNeighbours':[3,5,7],\n",
    "              'DT_min_samples_split':[2,4,8],\n",
    "              'MLPClassifier_hiddenLayer':[(45,45,), (100,), (30,30,30)],\n",
    "              'DecisionTreeRegressor_min_samples_leaf':[1,3],\n",
    "              'MLPRegressor_hiddenLayer':[(130, 110, 90, 70, 50, 30, 10, 5),(130, 100, 100, 100, 100, 100)]\n",
    "             }\n",
    "    \n",
    "result = {}    \n",
    "all_list = [1,2,3,4]\n",
    "for i in range(1,5):\n",
    "    print(\"Testing on Fold\", i)\n",
    "    x_train_df = pd.DataFrame()\n",
    "    y_train_df = pd.DataFrame()\n",
    "    \n",
    "    # Getting train data set up\n",
    "    for j in [x for x in all_list if x != i]: \n",
    "        x_train_df = x_train_df.append(x_fold[j])\n",
    "        y_train_df = y_train_df.append(y_fold[j])\n",
    "    y_train_df = y_train_df.drop(['In-hospital_death'], axis=1)\n",
    "    \n",
    "    # Replace not known length of stay to 2\n",
    "    y_train_df = y_train_df.replace(-1, 2)\n",
    "    \n",
    "    train_df = x_train_df.merge(y_train_df, on=\"RecordID\", how='outer')\n",
    "    train_df = train_df.set_index(\"RecordID\")\n",
    "    train_df = train_df.replace(np.nan, -1)\n",
    "    X_train = train_df.loc[:, train_df.columns != 'Length_of_stay']\n",
    "    Y_train = train_df['Length_of_stay']\n",
    "    \n",
    "        \n",
    "    x_test_df = x_train_df.iloc[0:0]\n",
    "    y_test_df = y_train_df.iloc[0:0]\n",
    "    # Getting test data set up\n",
    "    x_test_df = x_test_df.append(x_fold[i])\n",
    "    y_test_df = y_test_df.append(y_fold[i])\n",
    "    y_test_df = y_test_df.drop(['In-hospital_death'], axis=1)\n",
    "    # Replace not known length of stay to 2\n",
    "    y_test_df = y_test_df.replace(-1, 2)\n",
    "    \n",
    "    test_df = x_test_df.merge(y_test_df, on=\"RecordID\", how='outer')\n",
    "    test_df = test_df.set_index(\"RecordID\")\n",
    "    test_df = test_df.replace(np.nan, -1)\n",
    "    X_test = test_df.loc[:, test_df.columns != 'Length_of_stay']\n",
    "    Y_test = test_df['Length_of_stay']\n",
    "               \n",
    "#     Creating regression model and parameters to try out\n",
    "    #Linear Regression\n",
    "    \n",
    "    parameters = {'f_selecter__threshold':[0, 0.25, 0.5], \n",
    "                  'dim_reducer__n_components':[50,55,60,70]\n",
    "             }\n",
    "    \n",
    "    model = Pipeline(steps=[\n",
    "            ('prepro', prepro),\n",
    "                ('scaler', StandardScaler()),\n",
    "            ('f_selecter', VarianceThreshold()),\n",
    "            ('dim_reducer', PCA()),\n",
    "            ('classifier', LinearRegression())])\n",
    "            \n",
    "    est = GridSearchCV(model, parameters, cv=3, scoring = 'neg_mean_squared_error')\n",
    "    est.fit(X_train, Y_train)\n",
    "    best_est = est.best_estimator_\n",
    "    print(est.best_params_)\n",
    "    y_pred = best_est.predict(X_test)\n",
    "    score = sqrt(mean_squared_error(Y_test, y_pred))\n",
    "    print(\"Linear\", score)   \n",
    "    \n",
    "    if i == 1:\n",
    "        result['linear'] = score\n",
    "    else:\n",
    "        result['linear'] = result['linear'] + score  \n",
    "        \n",
    "    \n",
    "    #Decision Tree\n",
    "    parameters = {'f_selecter__threshold':[0, 0.25, 0.5], \n",
    "                  'dim_reducer__n_components':[50,55,60,70],\n",
    "              'classifier__min_samples_split':[2,4,8],\n",
    "              'classifier__min_samples_leaf':[1,3]\n",
    "             }\n",
    "    \n",
    "    model = Pipeline(steps=[('prepro', prepro),\n",
    "                          ('scaler', StandardScaler()),\n",
    "                          ('f_selecter', VarianceThreshold()),\n",
    "                          ('dim_reducer', PCA()),\n",
    "                          ('classifier', DecisionTreeRegressor())])\n",
    "    est = GridSearchCV(model, parameters, cv=3, scoring = 'neg_mean_squared_error')\n",
    "    est.fit(X_train, Y_train)\n",
    "    best_est = est.best_estimator_\n",
    "    print(est.best_params_)\n",
    "    y_pred = best_est.predict(X_test)\n",
    "    score = sqrt(mean_squared_error(Y_test, y_pred))\n",
    "    print(\"Decision Tree\", score)\n",
    "    \n",
    "    if i == 1:\n",
    "        result['decision_tree'] = score\n",
    "    else:\n",
    "        result['decision_tree'] = result['decision_tree'] + score\n",
    "    \n",
    "    #Lasso\n",
    "    parameters = {'f_selecter__threshold':[0, 0.25, 0.5], \n",
    "                  'dim_reducer__n_components':[50,55,60,70]\n",
    "             }\n",
    "    \n",
    "    model = Pipeline(steps=[('prepro', prepro),\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('f_selecter', VarianceThreshold()),\n",
    "                ('dim_reducer', PCA()),\n",
    "                ('classifier', Lasso())])\n",
    "        \n",
    "    est = GridSearchCV(model, parameters, cv=3, scoring = 'neg_mean_squared_error')\n",
    "    est.fit(X_train, Y_train)\n",
    "    best_est = est.best_estimator_\n",
    "    print(est.best_params_)\n",
    "    y_pred = best_est.predict(X_test)\n",
    "    score = sqrt(mean_squared_error(Y_test, y_pred))\n",
    "    print(\"Lasso\", score)\n",
    "    \n",
    "    if i == 1:\n",
    "        result['lasso'] = score\n",
    "    else:\n",
    "        result['lasso'] = result['lasso'] + score\n",
    "    \n",
    "    #MLP\n",
    "    parameters = { 'dim_reducer__n_components':[50,60,70],\n",
    "              'f_selecter__k':[350,400],\n",
    "              'classifier__hidden_layer_sizes':[(130, 110, 90, 70, 50, 30, 10, 5),(130, 100, 100, 100, 100, 100)]\n",
    "             }\n",
    "    \n",
    "    model = Pipeline(steps=[\n",
    "                ('prepro', prepro),\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('f_selecter', SelectKBest()),\n",
    "                ('dim_reducer', PCA()),\n",
    "                ('classifier', MLPRegressor(learning_rate_init = 0.105))])\n",
    "    est = GridSearchCV(model, parameters, cv=3, scoring = 'neg_mean_squared_error')\n",
    "    est.fit(X_train, Y_train)\n",
    "    best_est = est.best_estimator_\n",
    "    print(est.best_params_)\n",
    "    y_pred = best_est.predict(X_test)\n",
    "    score = sqrt(mean_squared_error(Y_test, y_pred))\n",
    "    print(\"MLP\", score)    \n",
    "    \n",
    "    if i == 1:\n",
    "        result['MLP'] = score\n",
    "    else:\n",
    "        result['MLP'] = result['MLP'] + score\n",
    "    \n",
    "for x in result:\n",
    "    result[x] = result[x]/4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Result</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'linear': 11.901836894218482,\n",
       " 'decision_tree': 15.249878658415104,\n",
       " 'lasso': 11.907816950443912,\n",
       " 'MLP': 12.133267003134124}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Model Building for M2</h2>  \n",
    "\n",
    "<b>Set up</b>\n",
    "<p>IMPT: Please ensure that you have 2 files called \"best_model_1.h5\" and \"best_model_2.h5\" in the same folder as this file</p>\n",
    "<p>IMPT: Please ensure you have Tensorflow installed.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# preprocess y dataframe for use in linear regression\n",
    "def preprocess_y_linear(temp_df):\n",
    "    temp_df = temp_df.drop(['recordid','mortality'], axis=1)\n",
    "    temp_df['days_in_hospital'] = pd.to_numeric(temp_df['days_in_hospital'])\n",
    "    for index, row in temp_df.iterrows():\n",
    "        if row['days_in_hospital'] == -1:\n",
    "            row['days_in_hospital'] = 2   \n",
    "    return temp_df\n",
    "\n",
    "# preprocess y dataframe for use in classification\n",
    "def preprocess_y_class(temp_df):\n",
    "    temp_df = temp_df.drop(['recordid','days_in_hospital'], axis=1)\n",
    "    temp_df['mortality'] = pd.to_numeric(temp_df['mortality'])  \n",
    "    return temp_df\n",
    "\n",
    "bin_feat = [\"Gender\", \"Coronary Care Unit\", \"Cardiac Surgery Recovery Unit\", \"Medical ICU\", \"Surgical ICU\"]\n",
    "non_bin_feat = []\n",
    "scaler = ColumnTransformer(\n",
    "    remainder = 'passthrough',\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), non_bin_feat)])\n",
    "all_list = [1,2,3,4]\n",
    "\n",
    "\n",
    "# Creating an array containing the preprocessed folds\n",
    "folds_x = {}\n",
    "folds_y_linear = {}\n",
    "folds_y_class = {}\n",
    "for i in all_list:\n",
    "    x_string = \"../Project_Data/Fold\"+str(i)\n",
    "    y_string = \"../Project_Data/Fold\"+str(i)+\"_Outcomes.csv\"\n",
    "    temp_df_y = pd.DataFrame(read_ans(y_string), columns=['recordid', 'days_in_hospital', 'mortality'])\n",
    "    temp_df_x = put_single_into_dataframe(read_text(x_string))\n",
    "    folds_x[i] = preprocess_x_for_design_matrix_1(temp_df_x)\n",
    "    folds_y_linear[i] = preprocess_y_linear(temp_df_y)\n",
    "    folds_y_class[i] = preprocess_y_class(temp_df_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Layer Selection</h3>  \n",
    "\n",
    "<p><b>Regressor Task</b>   \n",
    "For the regressor model, we decided to go with a fairly deep network as we have the benefit of working with the ReLU activation function for all layers, mitigating the Vanishing/Exploding gradidents problem. We have added dropout layers, as this a fairly complex netowrk. The dropout layers essentially block off certain neurons during the training process i.e. during the prediction and when backprop flows through the network, the weights of those neuron \"dropped\" are not used / affected. We have also included Early Stopping and Model Checkpointing as meta algorithms to prevent overfitting of the network.  </p>\n",
    "\n",
    "\n",
    "<p>If you do not have / want to have the h5 files, please comment out lines 42 . And in line 41 remove \"mc1\" </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on Fold 1\n",
      "# Getting train data set up\n",
      "# Getting test data set up\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 2400 samples, validate on 600 samples\n",
      "Epoch 1/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 164.8710 - mse: 164.8710\n",
      "Epoch 00001: val_loss improved from inf to 141.52421, saving model to best_model_1.h5\n",
      "2400/2400 [==============================] - 2s 644us/sample - loss: 162.3914 - mse: 162.3914 - val_loss: 141.5242 - val_mse: 141.5242\n",
      "Epoch 2/300\n",
      "2176/2400 [==========================>...] - ETA: 0s - loss: 143.8299 - mse: 143.8299\n",
      "Epoch 00002: val_loss did not improve from 141.52421\n",
      "2400/2400 [==============================] - 1s 266us/sample - loss: 143.6615 - mse: 143.6615 - val_loss: 171.6009 - val_mse: 171.6009\n",
      "Epoch 3/300\n",
      "2208/2400 [==========================>...] - ETA: 0s - loss: 139.0507 - mse: 139.0508\n",
      "Epoch 00003: val_loss improved from 141.52421 to 132.23532, saving model to best_model_1.h5\n",
      "2400/2400 [==============================] - 1s 298us/sample - loss: 142.2663 - mse: 142.2663 - val_loss: 132.2353 - val_mse: 132.2353\n",
      "Epoch 4/300\n",
      "2176/2400 [==========================>...] - ETA: 0s - loss: 142.0051 - mse: 142.0051\n",
      "Epoch 00004: val_loss improved from 132.23532 to 118.33066, saving model to best_model_1.h5\n",
      "2400/2400 [==============================] - 1s 300us/sample - loss: 140.7435 - mse: 140.7435 - val_loss: 118.3307 - val_mse: 118.3307\n",
      "Epoch 5/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 133.5620 - mse: 133.5620\n",
      "Epoch 00005: val_loss did not improve from 118.33066\n",
      "2400/2400 [==============================] - 1s 271us/sample - loss: 132.4610 - mse: 132.4610 - val_loss: 122.2728 - val_mse: 122.2728\n",
      "Epoch 6/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 130.6330 - mse: 130.6330\n",
      "Epoch 00006: val_loss did not improve from 118.33066\n",
      "2400/2400 [==============================] - 1s 280us/sample - loss: 132.3879 - mse: 132.3879 - val_loss: 119.4514 - val_mse: 119.4514\n",
      "Epoch 7/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 131.8630 - mse: 131.8629\n",
      "Epoch 00007: val_loss did not improve from 118.33066\n",
      "2400/2400 [==============================] - 1s 273us/sample - loss: 133.9364 - mse: 133.9364 - val_loss: 127.1325 - val_mse: 127.1325\n",
      "Epoch 8/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 132.4788 - mse: 132.4788\n",
      "Epoch 00008: val_loss did not improve from 118.33066\n",
      "2400/2400 [==============================] - 1s 273us/sample - loss: 132.2768 - mse: 132.2768 - val_loss: 119.5878 - val_mse: 119.5878\n",
      "Epoch 9/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 148.2541 - mse: 148.2541\n",
      "Epoch 00009: val_loss did not improve from 118.33066\n",
      "2400/2400 [==============================] - 1s 265us/sample - loss: 145.6373 - mse: 145.6373 - val_loss: 121.9616 - val_mse: 121.9616\n",
      "Epoch 10/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 139.5219 - mse: 139.5219\n",
      "Epoch 00010: val_loss did not improve from 118.33066\n",
      "2400/2400 [==============================] - 1s 297us/sample - loss: 140.5865 - mse: 140.5865 - val_loss: 121.9785 - val_mse: 121.9785\n",
      "Epoch 11/300\n",
      "2208/2400 [==========================>...] - ETA: 0s - loss: 132.8802 - mse: 132.8802\n",
      "Epoch 00011: val_loss did not improve from 118.33066\n",
      "2400/2400 [==============================] - 1s 291us/sample - loss: 134.6081 - mse: 134.6081 - val_loss: 122.4022 - val_mse: 122.4022\n",
      "Epoch 12/300\n",
      "2208/2400 [==========================>...] - ETA: 0s - loss: 127.8727 - mse: 127.8727\n",
      "Epoch 00012: val_loss improved from 118.33066 to 117.94716, saving model to best_model_1.h5\n",
      "2400/2400 [==============================] - 1s 343us/sample - loss: 137.0318 - mse: 137.0318 - val_loss: 117.9472 - val_mse: 117.9472\n",
      "Epoch 13/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 133.7355 - mse: 133.7355\n",
      "Epoch 00013: val_loss improved from 117.94716 to 114.84641, saving model to best_model_1.h5\n",
      "2400/2400 [==============================] - 1s 323us/sample - loss: 132.9344 - mse: 132.9344 - val_loss: 114.8464 - val_mse: 114.8464\n",
      "Epoch 14/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 130.3861 - mse: 130.3861\n",
      "Epoch 00014: val_loss did not improve from 114.84641\n",
      "2400/2400 [==============================] - 1s 274us/sample - loss: 129.4436 - mse: 129.4436 - val_loss: 126.0768 - val_mse: 126.0768\n",
      "Epoch 15/300\n",
      "2176/2400 [==========================>...] - ETA: 0s - loss: 124.7781 - mse: 124.7781\n",
      "Epoch 00015: val_loss did not improve from 114.84641\n",
      "2400/2400 [==============================] - 1s 263us/sample - loss: 125.4342 - mse: 125.4342 - val_loss: 120.7424 - val_mse: 120.7424\n",
      "Epoch 16/300\n",
      "2176/2400 [==========================>...] - ETA: 0s - loss: 128.0135 - mse: 128.0135\n",
      "Epoch 00016: val_loss did not improve from 114.84641\n",
      "2400/2400 [==============================] - 1s 270us/sample - loss: 129.8740 - mse: 129.8740 - val_loss: 116.0668 - val_mse: 116.0668\n",
      "Epoch 17/300\n",
      "2176/2400 [==========================>...] - ETA: 0s - loss: 122.4569 - mse: 122.4569\n",
      "Epoch 00017: val_loss did not improve from 114.84641\n",
      "2400/2400 [==============================] - 1s 264us/sample - loss: 126.7798 - mse: 126.7798 - val_loss: 122.8133 - val_mse: 122.8133\n",
      "Epoch 18/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 131.8050 - mse: 131.8051\n",
      "Epoch 00018: val_loss did not improve from 114.84641\n",
      "2400/2400 [==============================] - 1s 264us/sample - loss: 129.5982 - mse: 129.5982 - val_loss: 118.8595 - val_mse: 118.8595\n",
      "Epoch 19/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 130.3034 - mse: 130.3034\n",
      "Epoch 00019: val_loss did not improve from 114.84641\n",
      "2400/2400 [==============================] - 1s 276us/sample - loss: 129.8817 - mse: 129.8817 - val_loss: 118.2230 - val_mse: 118.2230\n",
      "Epoch 20/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 128.7275 - mse: 128.7275\n",
      "Epoch 00020: val_loss did not improve from 114.84641\n",
      "2400/2400 [==============================] - 1s 273us/sample - loss: 126.8990 - mse: 126.8990 - val_loss: 120.1250 - val_mse: 120.1251\n",
      "Epoch 21/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 128.5322 - mse: 128.5322\n",
      "Epoch 00021: val_loss did not improve from 114.84641\n",
      "2400/2400 [==============================] - 1s 278us/sample - loss: 128.6115 - mse: 128.6116 - val_loss: 124.3562 - val_mse: 124.3562\n",
      "Epoch 22/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 128.3039 - mse: 128.3039\n",
      "Epoch 00022: val_loss did not improve from 114.84641\n",
      "2400/2400 [==============================] - 1s 353us/sample - loss: 127.8874 - mse: 127.8874 - val_loss: 117.8866 - val_mse: 117.8866\n",
      "Epoch 23/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 124.9559 - mse: 124.9560- ETA: 0s - loss: 158.6196 - mse:\n",
      "Epoch 00023: val_loss did not improve from 114.84641\n",
      "2400/2400 [==============================] - 1s 299us/sample - loss: 125.2068 - mse: 125.2068 - val_loss: 123.7982 - val_mse: 123.7983\n",
      "Epoch 24/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 120.1965 - mse: 120.1965\n",
      "Epoch 00024: val_loss did not improve from 114.84641\n",
      "2400/2400 [==============================] - 1s 231us/sample - loss: 125.1374 - mse: 125.1374 - val_loss: 119.5456 - val_mse: 119.5456\n",
      "Epoch 25/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 133.6687 - mse: 133.6687\n",
      "Epoch 00025: val_loss did not improve from 114.84641\n",
      "2400/2400 [==============================] - 1s 230us/sample - loss: 130.4130 - mse: 130.4130 - val_loss: 118.1579 - val_mse: 118.1579\n",
      "Epoch 26/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 118.7687 - mse: 118.7687\n",
      "Epoch 00026: val_loss did not improve from 114.84641\n",
      "2400/2400 [==============================] - 1s 235us/sample - loss: 124.1631 - mse: 124.1631 - val_loss: 123.8415 - val_mse: 123.8415\n",
      "Epoch 27/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 121.5146 - mse: 121.5146\n",
      "Epoch 00027: val_loss did not improve from 114.84641\n",
      "2400/2400 [==============================] - 1s 251us/sample - loss: 122.5971 - mse: 122.5971 - val_loss: 120.5564 - val_mse: 120.5564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 130.5110 - mse: 130.5110\n",
      "Epoch 00028: val_loss did not improve from 114.84641\n",
      "2400/2400 [==============================] - 1s 253us/sample - loss: 130.3652 - mse: 130.3652 - val_loss: 120.8951 - val_mse: 120.8951\n",
      "Epoch 29/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 123.2409 - mse: 123.2409\n",
      "Epoch 00029: val_loss did not improve from 114.84641\n",
      "2400/2400 [==============================] - 1s 255us/sample - loss: 124.4319 - mse: 124.4319 - val_loss: 120.9092 - val_mse: 120.9092\n",
      "Epoch 30/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 129.0192 - mse: 129.0192\n",
      "Epoch 00030: val_loss did not improve from 114.84641\n",
      "2400/2400 [==============================] - 1s 256us/sample - loss: 126.1479 - mse: 126.1480 - val_loss: 115.4599 - val_mse: 115.4599\n",
      "Epoch 31/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 119.1918 - mse: 119.1918\n",
      "Epoch 00031: val_loss did not improve from 114.84641\n",
      "2400/2400 [==============================] - 1s 255us/sample - loss: 123.3002 - mse: 123.3002 - val_loss: 122.1458 - val_mse: 122.1458\n",
      "Epoch 32/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 128.3961 - mse: 128.3961\n",
      "Epoch 00032: val_loss did not improve from 114.84641\n",
      "2400/2400 [==============================] - 1s 255us/sample - loss: 128.7600 - mse: 128.7600 - val_loss: 122.0434 - val_mse: 122.0434\n",
      "Epoch 33/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 128.3043 - mse: 128.3043\n",
      "Epoch 00033: val_loss did not improve from 114.84641\n",
      "2400/2400 [==============================] - 1s 256us/sample - loss: 126.6241 - mse: 126.6241 - val_loss: 124.5964 - val_mse: 124.5964\n",
      "Epoch 34/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 166.7074 - mse: 166.7074\n",
      "Epoch 00034: val_loss did not improve from 114.84641\n",
      "2400/2400 [==============================] - 1s 279us/sample - loss: 165.6710 - mse: 165.6710 - val_loss: 121.8104 - val_mse: 121.8104\n",
      "Epoch 35/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 138.2363 - mse: 138.2363\n",
      "Epoch 00035: val_loss did not improve from 114.84641\n",
      "2400/2400 [==============================] - 1s 295us/sample - loss: 136.7408 - mse: 136.7408 - val_loss: 119.4955 - val_mse: 119.4955\n",
      "Epoch 36/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 130.7666 - mse: 130.7666\n",
      "Epoch 00036: val_loss did not improve from 114.84641\n",
      "2400/2400 [==============================] - 1s 286us/sample - loss: 131.4742 - mse: 131.4742 - val_loss: 118.8574 - val_mse: 118.8574\n",
      "Epoch 37/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 127.3315 - mse: 127.3315\n",
      "Epoch 00037: val_loss improved from 114.84641 to 113.45613, saving model to best_model_1.h5\n",
      "2400/2400 [==============================] - 1s 315us/sample - loss: 127.0131 - mse: 127.0131 - val_loss: 113.4561 - val_mse: 113.4561\n",
      "Epoch 38/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 127.9626 - mse: 127.9626\n",
      "Epoch 00038: val_loss did not improve from 113.45613\n",
      "2400/2400 [==============================] - 1s 265us/sample - loss: 126.6597 - mse: 126.6597 - val_loss: 117.7337 - val_mse: 117.7337\n",
      "Epoch 39/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 126.1965 - mse: 126.1965\n",
      "Epoch 00039: val_loss did not improve from 113.45613\n",
      "2400/2400 [==============================] - 1s 283us/sample - loss: 124.4978 - mse: 124.4978 - val_loss: 115.7045 - val_mse: 115.7045\n",
      "Epoch 40/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 126.6967 - mse: 126.6967\n",
      "Epoch 00040: val_loss did not improve from 113.45613\n",
      "2400/2400 [==============================] - 1s 263us/sample - loss: 124.8832 - mse: 124.8832 - val_loss: 116.5119 - val_mse: 116.5119\n",
      "Epoch 41/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 124.0411 - mse: 124.0411\n",
      "Epoch 00041: val_loss improved from 113.45613 to 112.79528, saving model to best_model_1.h5\n",
      "2400/2400 [==============================] - 1s 295us/sample - loss: 123.6786 - mse: 123.6786 - val_loss: 112.7953 - val_mse: 112.7953\n",
      "Epoch 42/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 120.7204 - mse: 120.7204\n",
      "Epoch 00042: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 262us/sample - loss: 120.2536 - mse: 120.2536 - val_loss: 123.1326 - val_mse: 123.1327\n",
      "Epoch 43/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 121.0830 - mse: 121.0830\n",
      "Epoch 00043: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 264us/sample - loss: 123.8378 - mse: 123.8378 - val_loss: 122.6851 - val_mse: 122.6851\n",
      "Epoch 44/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 125.3792 - mse: 125.3791\n",
      "Epoch 00044: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 261us/sample - loss: 127.6727 - mse: 127.6726 - val_loss: 115.3400 - val_mse: 115.3400\n",
      "Epoch 45/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 124.5467 - mse: 124.5468\n",
      "Epoch 00045: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 260us/sample - loss: 122.6618 - mse: 122.6618 - val_loss: 122.0379 - val_mse: 122.0379\n",
      "Epoch 46/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 122.5430 - mse: 122.5430\n",
      "Epoch 00046: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 260us/sample - loss: 123.7210 - mse: 123.7210 - val_loss: 119.0095 - val_mse: 119.0095\n",
      "Epoch 47/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 127.9625 - mse: 127.9625\n",
      "Epoch 00047: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 255us/sample - loss: 125.2684 - mse: 125.2684 - val_loss: 119.3714 - val_mse: 119.3714\n",
      "Epoch 48/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 122.8069 - mse: 122.8069\n",
      "Epoch 00048: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 254us/sample - loss: 120.3547 - mse: 120.3547 - val_loss: 126.6672 - val_mse: 126.6672\n",
      "Epoch 49/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 115.9164 - mse: 115.9164\n",
      "Epoch 00049: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 254us/sample - loss: 118.2329 - mse: 118.2328 - val_loss: 114.2809 - val_mse: 114.2809\n",
      "Epoch 50/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 122.8151 - mse: 122.8151\n",
      "Epoch 00050: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 256us/sample - loss: 121.4919 - mse: 121.4919 - val_loss: 125.5626 - val_mse: 125.5626\n",
      "Epoch 51/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 123.8852 - mse: 123.8852\n",
      "Epoch 00051: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 257us/sample - loss: 124.7160 - mse: 124.7160 - val_loss: 114.5009 - val_mse: 114.5009\n",
      "Epoch 52/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 125.7884 - mse: 125.7885\n",
      "Epoch 00052: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 265us/sample - loss: 126.9659 - mse: 126.9659 - val_loss: 116.4168 - val_mse: 116.4167\n",
      "Epoch 53/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 122.3039 - mse: 122.3039\n",
      "Epoch 00053: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 253us/sample - loss: 122.7692 - mse: 122.7692 - val_loss: 117.9092 - val_mse: 117.9092\n",
      "Epoch 54/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 123.9693 - mse: 123.9693\n",
      "Epoch 00054: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 259us/sample - loss: 121.7996 - mse: 121.7996 - val_loss: 119.5678 - val_mse: 119.5678\n",
      "Epoch 55/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 129.5857 - mse: 129.5857\n",
      "Epoch 00055: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 255us/sample - loss: 127.9660 - mse: 127.9660 - val_loss: 122.4610 - val_mse: 122.4610\n",
      "Epoch 56/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2272/2400 [===========================>..] - ETA: 0s - loss: 119.8815 - mse: 119.8815\n",
      "Epoch 00056: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 255us/sample - loss: 121.0262 - mse: 121.0262 - val_loss: 121.1369 - val_mse: 121.1368\n",
      "Epoch 57/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 121.5164 - mse: 121.5164\n",
      "Epoch 00057: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 254us/sample - loss: 125.1014 - mse: 125.1014 - val_loss: 119.5230 - val_mse: 119.5230\n",
      "Epoch 58/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 126.1010 - mse: 126.1010\n",
      "Epoch 00058: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 257us/sample - loss: 123.9144 - mse: 123.9144 - val_loss: 118.6722 - val_mse: 118.6722\n",
      "Epoch 59/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 120.8975 - mse: 120.8975\n",
      "Epoch 00059: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 288us/sample - loss: 121.9327 - mse: 121.9327 - val_loss: 118.8985 - val_mse: 118.8985\n",
      "Epoch 60/300\n",
      "2208/2400 [==========================>...] - ETA: 0s - loss: 118.4560 - mse: 118.4560\n",
      "Epoch 00060: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 293us/sample - loss: 115.5782 - mse: 115.5782 - val_loss: 126.5413 - val_mse: 126.5413\n",
      "Epoch 61/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 120.2253 - mse: 120.2253\n",
      "Epoch 00061: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 288us/sample - loss: 118.3552 - mse: 118.3552 - val_loss: 116.1227 - val_mse: 116.1227\n",
      "Epoch 62/300\n",
      "2208/2400 [==========================>...] - ETA: 0s - loss: 113.5129 - mse: 113.5129\n",
      "Epoch 00062: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 267us/sample - loss: 116.5973 - mse: 116.5973 - val_loss: 134.6209 - val_mse: 134.6208\n",
      "Epoch 63/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 113.4605 - mse: 113.4605\n",
      "Epoch 00063: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 254us/sample - loss: 115.6827 - mse: 115.6827 - val_loss: 130.7501 - val_mse: 130.7501\n",
      "Epoch 64/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 121.2945 - mse: 121.2945\n",
      "Epoch 00064: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 253us/sample - loss: 120.7546 - mse: 120.7546 - val_loss: 115.0489 - val_mse: 115.0489\n",
      "Epoch 65/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 122.5218 - mse: 122.5218\n",
      "Epoch 00065: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 254us/sample - loss: 120.5571 - mse: 120.5571 - val_loss: 117.2192 - val_mse: 117.2192\n",
      "Epoch 66/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 117.5954 - mse: 117.5954\n",
      "Epoch 00066: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 257us/sample - loss: 120.9928 - mse: 120.9928 - val_loss: 116.6677 - val_mse: 116.6677\n",
      "Epoch 67/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 122.8924 - mse: 122.8924\n",
      "Epoch 00067: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 255us/sample - loss: 123.0217 - mse: 123.0218 - val_loss: 114.9485 - val_mse: 114.9485\n",
      "Epoch 68/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 116.6488 - mse: 116.6488\n",
      "Epoch 00068: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 256us/sample - loss: 116.8505 - mse: 116.8505 - val_loss: 116.6214 - val_mse: 116.6214\n",
      "Epoch 69/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 119.7858 - mse: 119.7858\n",
      "Epoch 00069: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 255us/sample - loss: 117.7845 - mse: 117.7845 - val_loss: 118.0287 - val_mse: 118.0287\n",
      "Epoch 70/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 113.4125 - mse: 113.4125\n",
      "Epoch 00070: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 282us/sample - loss: 114.0462 - mse: 114.0462 - val_loss: 121.8457 - val_mse: 121.8457\n",
      "Epoch 71/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 120.2681 - mse: 120.2681\n",
      "Epoch 00071: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 263us/sample - loss: 118.0929 - mse: 118.0929 - val_loss: 118.1506 - val_mse: 118.1506\n",
      "Epoch 72/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 120.1612 - mse: 120.1612\n",
      "Epoch 00072: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 259us/sample - loss: 119.0806 - mse: 119.0806 - val_loss: 121.0191 - val_mse: 121.0191\n",
      "Epoch 73/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 116.2180 - mse: 116.2180\n",
      "Epoch 00073: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 266us/sample - loss: 114.0384 - mse: 114.0385 - val_loss: 119.3464 - val_mse: 119.3464\n",
      "Epoch 74/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 115.7181 - mse: 115.7181\n",
      "Epoch 00074: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 254us/sample - loss: 114.5286 - mse: 114.5286 - val_loss: 136.7914 - val_mse: 136.7914\n",
      "Epoch 75/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 121.1766 - mse: 121.1766\n",
      "Epoch 00075: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 253us/sample - loss: 121.0926 - mse: 121.0926 - val_loss: 115.2806 - val_mse: 115.2805\n",
      "Epoch 76/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 116.5562 - mse: 116.5562\n",
      "Epoch 00076: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 253us/sample - loss: 116.1293 - mse: 116.1293 - val_loss: 116.6159 - val_mse: 116.6159\n",
      "Epoch 77/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 118.8918 - mse: 118.8918\n",
      "Epoch 00077: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 263us/sample - loss: 117.7084 - mse: 117.7084 - val_loss: 120.2344 - val_mse: 120.2344\n",
      "Epoch 78/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 114.1048 - mse: 114.1047\n",
      "Epoch 00078: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 256us/sample - loss: 117.6752 - mse: 117.6752 - val_loss: 117.8624 - val_mse: 117.8624\n",
      "Epoch 79/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 120.5463 - mse: 120.5463\n",
      "Epoch 00079: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 258us/sample - loss: 119.8526 - mse: 119.8525 - val_loss: 121.5190 - val_mse: 121.5190\n",
      "Epoch 80/300\n",
      "2176/2400 [==========================>...] - ETA: 0s - loss: 128.0487 - mse: 128.0488\n",
      "Epoch 00080: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 266us/sample - loss: 127.4292 - mse: 127.4292 - val_loss: 114.8983 - val_mse: 114.8983\n",
      "Epoch 81/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 120.3131 - mse: 120.3131\n",
      "Epoch 00081: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 255us/sample - loss: 124.3989 - mse: 124.3988 - val_loss: 117.8965 - val_mse: 117.8965\n",
      "Epoch 82/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 115.5470 - mse: 115.5470\n",
      "Epoch 00082: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 258us/sample - loss: 118.2492 - mse: 118.2492 - val_loss: 123.0251 - val_mse: 123.0252\n",
      "Epoch 83/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 123.6242 - mse: 123.6242\n",
      "Epoch 00083: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 253us/sample - loss: 125.8704 - mse: 125.8704 - val_loss: 121.0274 - val_mse: 121.0274\n",
      "Epoch 84/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 122.9302 - mse: 122.9302\n",
      "Epoch 00084: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 270us/sample - loss: 122.2455 - mse: 122.2455 - val_loss: 123.3198 - val_mse: 123.3198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/300\n",
      "2208/2400 [==========================>...] - ETA: 0s - loss: 116.3975 - mse: 116.3975\n",
      "Epoch 00085: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 296us/sample - loss: 115.9414 - mse: 115.9414 - val_loss: 113.7267 - val_mse: 113.7267\n",
      "Epoch 86/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 114.4688 - mse: 114.4688\n",
      "Epoch 00086: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 287us/sample - loss: 114.6204 - mse: 114.6204 - val_loss: 121.6378 - val_mse: 121.6378\n",
      "Epoch 87/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 121.2106 - mse: 121.2106\n",
      "Epoch 00087: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 282us/sample - loss: 119.4120 - mse: 119.4120 - val_loss: 130.4273 - val_mse: 130.4273\n",
      "Epoch 88/300\n",
      "2208/2400 [==========================>...] - ETA: 0s - loss: 113.6389 - mse: 113.6390\n",
      "Epoch 00088: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 268us/sample - loss: 113.1784 - mse: 113.1784 - val_loss: 120.9204 - val_mse: 120.9204\n",
      "Epoch 89/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 119.9265 - mse: 119.9265\n",
      "Epoch 00089: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 261us/sample - loss: 117.2780 - mse: 117.2780 - val_loss: 117.9797 - val_mse: 117.9797\n",
      "Epoch 90/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 112.6109 - mse: 112.6109\n",
      "Epoch 00090: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 261us/sample - loss: 114.7137 - mse: 114.7137 - val_loss: 141.2917 - val_mse: 141.2917\n",
      "Epoch 91/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 118.0495 - mse: 118.0495\n",
      "Epoch 00091: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 257us/sample - loss: 116.7717 - mse: 116.7717 - val_loss: 118.6819 - val_mse: 118.6819\n",
      "Epoch 92/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 110.5201 - mse: 110.5201\n",
      "Epoch 00092: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 257us/sample - loss: 110.6833 - mse: 110.6833 - val_loss: 154.6758 - val_mse: 154.6758\n",
      "Epoch 93/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 122.3064 - mse: 122.3064\n",
      "Epoch 00093: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 257us/sample - loss: 121.5099 - mse: 121.5099 - val_loss: 134.5619 - val_mse: 134.5619\n",
      "Epoch 94/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 114.8619 - mse: 114.8619\n",
      "Epoch 00094: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 261us/sample - loss: 115.1866 - mse: 115.1867 - val_loss: 122.2507 - val_mse: 122.2507\n",
      "Epoch 95/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 111.9695 - mse: 111.9695\n",
      "Epoch 00095: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 258us/sample - loss: 113.1175 - mse: 113.1175 - val_loss: 117.3280 - val_mse: 117.3280\n",
      "Epoch 96/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 115.4691 - mse: 115.4691\n",
      "Epoch 00096: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 258us/sample - loss: 114.2552 - mse: 114.2552 - val_loss: 127.1972 - val_mse: 127.1972\n",
      "Epoch 97/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 120.1135 - mse: 120.1135\n",
      "Epoch 00097: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 256us/sample - loss: 118.5987 - mse: 118.5987 - val_loss: 120.7310 - val_mse: 120.7310\n",
      "Epoch 98/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 123.9872 - mse: 123.9872\n",
      "Epoch 00098: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 256us/sample - loss: 122.7277 - mse: 122.7277 - val_loss: 117.9232 - val_mse: 117.9232\n",
      "Epoch 99/300\n",
      "2208/2400 [==========================>...] - ETA: 0s - loss: 116.2656 - mse: 116.2655\n",
      "Epoch 00099: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 265us/sample - loss: 118.8402 - mse: 118.8402 - val_loss: 115.1516 - val_mse: 115.1516\n",
      "Epoch 100/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 112.8224 - mse: 112.8224\n",
      "Epoch 00100: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 257us/sample - loss: 117.4148 - mse: 117.4148 - val_loss: 122.1039 - val_mse: 122.1039\n",
      "Epoch 101/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 117.1969 - mse: 117.1970\n",
      "Epoch 00101: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 260us/sample - loss: 116.8373 - mse: 116.8373 - val_loss: 139.0960 - val_mse: 139.0960\n",
      "Epoch 102/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 113.0144 - mse: 113.0144\n",
      "Epoch 00102: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 256us/sample - loss: 113.4211 - mse: 113.4211 - val_loss: 132.5585 - val_mse: 132.5585\n",
      "Epoch 103/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 109.8102 - mse: 109.8102\n",
      "Epoch 00103: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 256us/sample - loss: 110.5784 - mse: 110.5784 - val_loss: 129.8334 - val_mse: 129.8334\n",
      "Epoch 104/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 119.0166 - mse: 119.0165\n",
      "Epoch 00104: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 259us/sample - loss: 118.9971 - mse: 118.9971 - val_loss: 116.5273 - val_mse: 116.5273\n",
      "Epoch 105/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 116.2415 - mse: 116.2415\n",
      "Epoch 00105: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 259us/sample - loss: 113.5553 - mse: 113.5553 - val_loss: 116.0129 - val_mse: 116.0128\n",
      "Epoch 106/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 117.2355 - mse: 117.2355\n",
      "Epoch 00106: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 255us/sample - loss: 115.0748 - mse: 115.0748 - val_loss: 116.3593 - val_mse: 116.3593\n",
      "Epoch 107/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 119.4657 - mse: 119.4657\n",
      "Epoch 00107: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 265us/sample - loss: 119.4913 - mse: 119.4913 - val_loss: 118.0379 - val_mse: 118.0380\n",
      "Epoch 108/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 123.0065 - mse: 123.0065\n",
      "Epoch 00108: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 260us/sample - loss: 120.5422 - mse: 120.5421 - val_loss: 115.7271 - val_mse: 115.7271\n",
      "Epoch 109/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 117.2541 - mse: 117.2541\n",
      "Epoch 00109: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 266us/sample - loss: 116.1874 - mse: 116.1874 - val_loss: 124.6098 - val_mse: 124.6097\n",
      "Epoch 110/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 117.2431 - mse: 117.2431\n",
      "Epoch 00110: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 296us/sample - loss: 120.8075 - mse: 120.8075 - val_loss: 135.8392 - val_mse: 135.8392\n",
      "Epoch 111/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 123.9666 - mse: 123.9666- ETA: 0s - loss: 127.3325 - mse:\n",
      "Epoch 00111: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 297us/sample - loss: 124.2528 - mse: 124.2527 - val_loss: 117.5120 - val_mse: 117.5120\n",
      "Epoch 112/300\n",
      "2208/2400 [==========================>...] - ETA: 0s - loss: 114.8347 - mse: 114.8347\n",
      "Epoch 00112: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 298us/sample - loss: 112.6422 - mse: 112.6422 - val_loss: 119.3087 - val_mse: 119.3087\n",
      "Epoch 113/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 108.5884 - mse: 108.5884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00113: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 268us/sample - loss: 109.1243 - mse: 109.1243 - val_loss: 125.8933 - val_mse: 125.8933\n",
      "Epoch 114/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 108.2185 - mse: 108.2185\n",
      "Epoch 00114: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 265us/sample - loss: 110.0576 - mse: 110.0576 - val_loss: 125.1322 - val_mse: 125.1322\n",
      "Epoch 115/300\n",
      "2208/2400 [==========================>...] - ETA: 0s - loss: 113.2103 - mse: 113.2103\n",
      "Epoch 00115: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 268us/sample - loss: 111.2375 - mse: 111.2375 - val_loss: 119.5266 - val_mse: 119.5266\n",
      "Epoch 116/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 110.4400 - mse: 110.4400\n",
      "Epoch 00116: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 276us/sample - loss: 109.6759 - mse: 109.6759 - val_loss: 119.3578 - val_mse: 119.3578\n",
      "Epoch 117/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 107.5278 - mse: 107.5278\n",
      "Epoch 00117: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 263us/sample - loss: 108.1306 - mse: 108.1306 - val_loss: 144.9584 - val_mse: 144.9584\n",
      "Epoch 118/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 109.0788 - mse: 109.0788\n",
      "Epoch 00118: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 263us/sample - loss: 107.8112 - mse: 107.8112 - val_loss: 125.4921 - val_mse: 125.4921\n",
      "Epoch 119/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 109.4369 - mse: 109.4368\n",
      "Epoch 00119: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 263us/sample - loss: 107.9002 - mse: 107.9002 - val_loss: 158.4112 - val_mse: 158.4112\n",
      "Epoch 120/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 105.6424 - mse: 105.6424\n",
      "Epoch 00120: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 264us/sample - loss: 104.9744 - mse: 104.9744 - val_loss: 133.7110 - val_mse: 133.7110\n",
      "Epoch 121/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 107.1830 - mse: 107.1830\n",
      "Epoch 00121: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 260us/sample - loss: 106.0692 - mse: 106.0692 - val_loss: 119.2843 - val_mse: 119.2843\n",
      "Epoch 122/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 104.7061 - mse: 104.7061\n",
      "Epoch 00122: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 259us/sample - loss: 107.0763 - mse: 107.0763 - val_loss: 127.7036 - val_mse: 127.7036\n",
      "Epoch 123/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 114.9302 - mse: 114.9303\n",
      "Epoch 00123: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 276us/sample - loss: 114.7806 - mse: 114.7806 - val_loss: 120.2735 - val_mse: 120.2735\n",
      "Epoch 124/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 117.2540 - mse: 117.2540\n",
      "Epoch 00124: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 260us/sample - loss: 115.7299 - mse: 115.7300 - val_loss: 120.8538 - val_mse: 120.8538\n",
      "Epoch 125/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 108.8379 - mse: 108.8379\n",
      "Epoch 00125: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 273us/sample - loss: 108.2928 - mse: 108.2928 - val_loss: 116.8169 - val_mse: 116.8169\n",
      "Epoch 126/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 109.0638 - mse: 109.0638\n",
      "Epoch 00126: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 258us/sample - loss: 110.7132 - mse: 110.7132 - val_loss: 129.2570 - val_mse: 129.2570\n",
      "Epoch 127/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 114.1130 - mse: 114.1130\n",
      "Epoch 00127: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 263us/sample - loss: 114.9050 - mse: 114.9050 - val_loss: 120.8689 - val_mse: 120.8689\n",
      "Epoch 128/300\n",
      "2208/2400 [==========================>...] - ETA: 0s - loss: 108.4050 - mse: 108.4051\n",
      "Epoch 00128: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 273us/sample - loss: 107.8723 - mse: 107.8723 - val_loss: 116.5260 - val_mse: 116.5260\n",
      "Epoch 129/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 107.3705 - mse: 107.3705\n",
      "Epoch 00129: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 265us/sample - loss: 112.3845 - mse: 112.3846 - val_loss: 135.4605 - val_mse: 135.4605\n",
      "Epoch 130/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 128.7333 - mse: 128.7333\n",
      "Epoch 00130: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 269us/sample - loss: 126.1998 - mse: 126.1998 - val_loss: 127.3549 - val_mse: 127.3549\n",
      "Epoch 131/300\n",
      "2208/2400 [==========================>...] - ETA: 0s - loss: 127.2152 - mse: 127.2152\n",
      "Epoch 00131: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 271us/sample - loss: 125.4479 - mse: 125.4479 - val_loss: 145.0553 - val_mse: 145.0553\n",
      "Epoch 132/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 120.5118 - mse: 120.5118\n",
      "Epoch 00132: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 258us/sample - loss: 125.7407 - mse: 125.7407 - val_loss: 129.0017 - val_mse: 129.0017\n",
      "Epoch 133/300\n",
      "2176/2400 [==========================>...] - ETA: 0s - loss: 113.3958 - mse: 113.3958\n",
      "Epoch 00133: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 300us/sample - loss: 120.3936 - mse: 120.3936 - val_loss: 164.1217 - val_mse: 164.1217\n",
      "Epoch 134/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 118.4704 - mse: 118.4704\n",
      "Epoch 00134: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 292us/sample - loss: 117.9292 - mse: 117.9292 - val_loss: 115.7531 - val_mse: 115.7531\n",
      "Epoch 135/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 122.5937 - mse: 122.5937\n",
      "Epoch 00135: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 327us/sample - loss: 122.0675 - mse: 122.0675 - val_loss: 118.7533 - val_mse: 118.7533\n",
      "Epoch 136/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 109.5747 - mse: 109.5747\n",
      "Epoch 00136: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 352us/sample - loss: 114.8494 - mse: 114.8494 - val_loss: 114.7719 - val_mse: 114.7719\n",
      "Epoch 137/300\n",
      "2144/2400 [=========================>....] - ETA: 0s - loss: 109.9515 - mse: 109.9515\n",
      "Epoch 00137: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 279us/sample - loss: 110.6702 - mse: 110.6702 - val_loss: 137.1744 - val_mse: 137.1743\n",
      "Epoch 138/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 110.3411 - mse: 110.3411\n",
      "Epoch 00138: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 284us/sample - loss: 110.3122 - mse: 110.3122 - val_loss: 152.0226 - val_mse: 152.0225\n",
      "Epoch 139/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 107.6313 - mse: 107.6313\n",
      "Epoch 00139: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 269us/sample - loss: 108.2868 - mse: 108.2868 - val_loss: 126.0123 - val_mse: 126.0123\n",
      "Epoch 140/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 114.0695 - mse: 114.0695\n",
      "Epoch 00140: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 255us/sample - loss: 114.0815 - mse: 114.0815 - val_loss: 132.8704 - val_mse: 132.8704\n",
      "Epoch 141/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 114.6026 - mse: 114.6026\n",
      "Epoch 00141: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 254us/sample - loss: 118.6051 - mse: 118.6051 - val_loss: 116.2750 - val_mse: 116.2750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 107.3035 - mse: 107.3035\n",
      "Epoch 00142: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 265us/sample - loss: 107.4144 - mse: 107.4143 - val_loss: 121.9852 - val_mse: 121.9852\n",
      "Epoch 143/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 115.5195 - mse: 115.5194\n",
      "Epoch 00143: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 270us/sample - loss: 115.9718 - mse: 115.9717 - val_loss: 119.0777 - val_mse: 119.0777\n",
      "Epoch 144/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 123.5784 - mse: 123.5784\n",
      "Epoch 00144: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 278us/sample - loss: 124.5000 - mse: 124.5000 - val_loss: 118.5302 - val_mse: 118.5302\n",
      "Epoch 145/300\n",
      "2208/2400 [==========================>...] - ETA: 0s - loss: 120.2563 - mse: 120.2563\n",
      "Epoch 00145: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 287us/sample - loss: 116.9244 - mse: 116.9244 - val_loss: 117.2148 - val_mse: 117.2148\n",
      "Epoch 146/300\n",
      "2208/2400 [==========================>...] - ETA: 0s - loss: 132.5114 - mse: 132.5114\n",
      "Epoch 00146: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 255us/sample - loss: 134.9833 - mse: 134.9832 - val_loss: 129.5646 - val_mse: 129.5646\n",
      "Epoch 147/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 137.5585 - mse: 137.5585\n",
      "Epoch 00147: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 266us/sample - loss: 134.8671 - mse: 134.8671 - val_loss: 121.3138 - val_mse: 121.3138\n",
      "Epoch 148/300\n",
      "2176/2400 [==========================>...] - ETA: 0s - loss: 130.0236 - mse: 130.0236\n",
      "Epoch 00148: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 267us/sample - loss: 129.0739 - mse: 129.0739 - val_loss: 116.5626 - val_mse: 116.5626\n",
      "Epoch 149/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 119.9418 - mse: 119.9418\n",
      "Epoch 00149: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 258us/sample - loss: 124.5878 - mse: 124.5878 - val_loss: 122.6518 - val_mse: 122.6518\n",
      "Epoch 150/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 125.8149 - mse: 125.8149\n",
      "Epoch 00150: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 254us/sample - loss: 127.4527 - mse: 127.4527 - val_loss: 119.9838 - val_mse: 119.9838\n",
      "Epoch 151/300\n",
      "2208/2400 [==========================>...] - ETA: 0s - loss: 123.6277 - mse: 123.6277\n",
      "Epoch 00151: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 263us/sample - loss: 121.9317 - mse: 121.9317 - val_loss: 121.3738 - val_mse: 121.3738\n",
      "Epoch 152/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 124.7144 - mse: 124.7145\n",
      "Epoch 00152: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 254us/sample - loss: 122.2221 - mse: 122.2221 - val_loss: 113.9750 - val_mse: 113.9750\n",
      "Epoch 153/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 121.2859 - mse: 121.2859\n",
      "Epoch 00153: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 257us/sample - loss: 119.9994 - mse: 119.9994 - val_loss: 114.7935 - val_mse: 114.7935\n",
      "Epoch 154/300\n",
      "2176/2400 [==========================>...] - ETA: 0s - loss: 117.5786 - mse: 117.5786\n",
      "Epoch 00154: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 273us/sample - loss: 117.9965 - mse: 117.9965 - val_loss: 117.8807 - val_mse: 117.8807\n",
      "Epoch 155/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 120.4625 - mse: 120.4625\n",
      "Epoch 00155: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 267us/sample - loss: 117.9548 - mse: 117.9548 - val_loss: 124.5226 - val_mse: 124.5226\n",
      "Epoch 156/300\n",
      "2208/2400 [==========================>...] - ETA: 0s - loss: 119.0826 - mse: 119.0826\n",
      "Epoch 00156: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 270us/sample - loss: 118.0692 - mse: 118.0692 - val_loss: 121.2026 - val_mse: 121.2026\n",
      "Epoch 157/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 115.1732 - mse: 115.1732\n",
      "Epoch 00157: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 277us/sample - loss: 116.3441 - mse: 116.3441 - val_loss: 122.6181 - val_mse: 122.6180\n",
      "Epoch 158/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 116.9512 - mse: 116.9512\n",
      "Epoch 00158: val_loss did not improve from 112.79528\n",
      "2400/2400 [==============================] - 1s 283us/sample - loss: 116.1816 - mse: 116.1816 - val_loss: 117.2767 - val_mse: 117.2767\n",
      "Epoch 159/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 117.9601 - mse: 117.9601\n",
      "Epoch 00159: val_loss improved from 112.79528 to 111.88478, saving model to best_model_1.h5\n",
      "2400/2400 [==============================] - 1s 349us/sample - loss: 116.0338 - mse: 116.0338 - val_loss: 111.8848 - val_mse: 111.8848\n",
      "Epoch 160/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 114.4618 - mse: 114.4618\n",
      "Epoch 00160: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 287us/sample - loss: 114.6031 - mse: 114.6031 - val_loss: 125.7753 - val_mse: 125.7753\n",
      "Epoch 161/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 113.8174 - mse: 113.8175\n",
      "Epoch 00161: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 284us/sample - loss: 113.0908 - mse: 113.0908 - val_loss: 119.4743 - val_mse: 119.4743\n",
      "Epoch 162/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 113.6449 - mse: 113.6449\n",
      "Epoch 00162: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 254us/sample - loss: 112.4984 - mse: 112.4984 - val_loss: 139.3179 - val_mse: 139.3179\n",
      "Epoch 163/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 113.0072 - mse: 113.0072\n",
      "Epoch 00163: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 259us/sample - loss: 112.2817 - mse: 112.2817 - val_loss: 145.6286 - val_mse: 145.6286\n",
      "Epoch 164/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 112.6184 - mse: 112.6184\n",
      "Epoch 00164: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 265us/sample - loss: 110.8758 - mse: 110.8758 - val_loss: 129.5801 - val_mse: 129.5801\n",
      "Epoch 165/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 121.1280 - mse: 121.1279\n",
      "Epoch 00165: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 268us/sample - loss: 120.7710 - mse: 120.7710 - val_loss: 120.5602 - val_mse: 120.5602\n",
      "Epoch 166/300\n",
      "2176/2400 [==========================>...] - ETA: 0s - loss: 120.1266 - mse: 120.1266\n",
      "Epoch 00166: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 272us/sample - loss: 126.7120 - mse: 126.7120 - val_loss: 117.1427 - val_mse: 117.1427\n",
      "Epoch 167/300\n",
      "2208/2400 [==========================>...] - ETA: 0s - loss: 107.9516 - mse: 107.9516\n",
      "Epoch 00167: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 271us/sample - loss: 119.5968 - mse: 119.5968 - val_loss: 132.5210 - val_mse: 132.5210\n",
      "Epoch 168/300\n",
      "2208/2400 [==========================>...] - ETA: 0s - loss: 106.9501 - mse: 106.9501\n",
      "Epoch 00168: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 274us/sample - loss: 117.6744 - mse: 117.6744 - val_loss: 125.9026 - val_mse: 125.9026\n",
      "Epoch 169/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 116.5261 - mse: 116.5261\n",
      "Epoch 00169: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 295us/sample - loss: 122.3464 - mse: 122.3465 - val_loss: 132.4685 - val_mse: 132.4685\n",
      "Epoch 170/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2240/2400 [===========================>..] - ETA: 0s - loss: 114.5922 - mse: 114.5922\n",
      "Epoch 00170: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 263us/sample - loss: 116.0027 - mse: 116.0027 - val_loss: 123.7892 - val_mse: 123.7892\n",
      "Epoch 171/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 118.2034 - mse: 118.2035\n",
      "Epoch 00171: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 264us/sample - loss: 114.8318 - mse: 114.8318 - val_loss: 136.6140 - val_mse: 136.6140\n",
      "Epoch 172/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 111.8500 - mse: 111.8500\n",
      "Epoch 00172: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 257us/sample - loss: 110.3950 - mse: 110.3950 - val_loss: 114.7723 - val_mse: 114.7723\n",
      "Epoch 173/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 114.7424 - mse: 114.7424\n",
      "Epoch 00173: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 267us/sample - loss: 113.4685 - mse: 113.4685 - val_loss: 133.8482 - val_mse: 133.8482\n",
      "Epoch 174/300\n",
      "2176/2400 [==========================>...] - ETA: 0s - loss: 111.7294 - mse: 111.7294\n",
      "Epoch 00174: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 271us/sample - loss: 109.4434 - mse: 109.4434 - val_loss: 114.5436 - val_mse: 114.5436\n",
      "Epoch 175/300\n",
      "2176/2400 [==========================>...] - ETA: 0s - loss: 101.4024 - mse: 101.4024\n",
      "Epoch 00175: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 273us/sample - loss: 108.6159 - mse: 108.6159 - val_loss: 114.0312 - val_mse: 114.0312\n",
      "Epoch 176/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 111.2202 - mse: 111.2202\n",
      "Epoch 00176: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 268us/sample - loss: 110.7411 - mse: 110.7411 - val_loss: 125.7672 - val_mse: 125.7672\n",
      "Epoch 177/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 115.5330 - mse: 115.5330\n",
      "Epoch 00177: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 263us/sample - loss: 114.2932 - mse: 114.2932 - val_loss: 115.2003 - val_mse: 115.2003\n",
      "Epoch 178/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 113.2173 - mse: 113.2173\n",
      "Epoch 00178: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 254us/sample - loss: 112.8251 - mse: 112.8251 - val_loss: 118.6475 - val_mse: 118.6475\n",
      "Epoch 179/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 114.0503 - mse: 114.0503\n",
      "Epoch 00179: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 257us/sample - loss: 113.4232 - mse: 113.4233 - val_loss: 115.5322 - val_mse: 115.5322\n",
      "Epoch 180/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 108.0628 - mse: 108.0628\n",
      "Epoch 00180: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 257us/sample - loss: 110.3087 - mse: 110.3087 - val_loss: 127.9147 - val_mse: 127.9147\n",
      "Epoch 181/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 108.7325 - mse: 108.7325\n",
      "Epoch 00181: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 256us/sample - loss: 109.4155 - mse: 109.4156 - val_loss: 121.0242 - val_mse: 121.0242\n",
      "Epoch 182/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 110.6160 - mse: 110.6160\n",
      "Epoch 00182: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 243us/sample - loss: 110.1487 - mse: 110.1487 - val_loss: 119.2664 - val_mse: 119.2664\n",
      "Epoch 183/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 115.8628 - mse: 115.8628\n",
      "Epoch 00183: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 287us/sample - loss: 115.6056 - mse: 115.6056 - val_loss: 125.1901 - val_mse: 125.1901\n",
      "Epoch 184/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 111.6781 - mse: 111.6781\n",
      "Epoch 00184: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 288us/sample - loss: 111.6269 - mse: 111.6269 - val_loss: 153.7047 - val_mse: 153.7047\n",
      "Epoch 185/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 117.8747 - mse: 117.8747\n",
      "Epoch 00185: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 292us/sample - loss: 119.4212 - mse: 119.4212 - val_loss: 169.0949 - val_mse: 169.0949\n",
      "Epoch 186/300\n",
      "2208/2400 [==========================>...] - ETA: 0s - loss: 118.0033 - mse: 118.0033\n",
      "Epoch 00186: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 270us/sample - loss: 116.2677 - mse: 116.2677 - val_loss: 126.2772 - val_mse: 126.2772\n",
      "Epoch 187/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 111.3057 - mse: 111.3056\n",
      "Epoch 00187: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 258us/sample - loss: 111.1769 - mse: 111.1769 - val_loss: 117.2089 - val_mse: 117.2089\n",
      "Epoch 188/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 100.0279 - mse: 100.0279\n",
      "Epoch 00188: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 256us/sample - loss: 104.4593 - mse: 104.4593 - val_loss: 303.3150 - val_mse: 303.3150\n",
      "Epoch 189/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 143.6635 - mse: 143.6634\n",
      "Epoch 00189: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 258us/sample - loss: 140.9885 - mse: 140.9884 - val_loss: 129.1629 - val_mse: 129.1629\n",
      "Epoch 190/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 131.4510 - mse: 131.4511\n",
      "Epoch 00190: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 261us/sample - loss: 133.9429 - mse: 133.9430 - val_loss: 122.1782 - val_mse: 122.1782\n",
      "Epoch 191/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 126.1677 - mse: 126.1677\n",
      "Epoch 00191: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 258us/sample - loss: 128.1254 - mse: 128.1254 - val_loss: 120.7430 - val_mse: 120.7430\n",
      "Epoch 192/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 137.4608 - mse: 137.4608\n",
      "Epoch 00192: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 274us/sample - loss: 136.2870 - mse: 136.2870 - val_loss: 122.6225 - val_mse: 122.6225\n",
      "Epoch 193/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 131.0422 - mse: 131.0422\n",
      "Epoch 00193: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 262us/sample - loss: 135.3982 - mse: 135.3982 - val_loss: 121.7305 - val_mse: 121.7305\n",
      "Epoch 194/300\n",
      "2208/2400 [==========================>...] - ETA: 0s - loss: 133.8941 - mse: 133.8941\n",
      "Epoch 00194: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 267us/sample - loss: 133.2698 - mse: 133.2698 - val_loss: 118.9627 - val_mse: 118.9627\n",
      "Epoch 195/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 132.6601 - mse: 132.6601\n",
      "Epoch 00195: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 257us/sample - loss: 131.2567 - mse: 131.2567 - val_loss: 125.8389 - val_mse: 125.8390\n",
      "Epoch 196/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 130.2357 - mse: 130.2357\n",
      "Epoch 00196: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 255us/sample - loss: 128.7412 - mse: 128.7412 - val_loss: 121.5368 - val_mse: 121.5368\n",
      "Epoch 197/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 135.5495 - mse: 135.5495\n",
      "Epoch 00197: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 257us/sample - loss: 134.5914 - mse: 134.5913 - val_loss: 124.2807 - val_mse: 124.2807\n",
      "Epoch 198/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 129.2341 - mse: 129.2341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00198: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 263us/sample - loss: 129.9308 - mse: 129.9308 - val_loss: 126.6245 - val_mse: 126.6245\n",
      "Epoch 199/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 126.6088 - mse: 126.6088\n",
      "Epoch 00199: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 255us/sample - loss: 139.8548 - mse: 139.8548 - val_loss: 121.7510 - val_mse: 121.7510\n",
      "Epoch 200/300\n",
      "2208/2400 [==========================>...] - ETA: 0s - loss: 126.6067 - mse: 126.6067\n",
      "Epoch 00200: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 266us/sample - loss: 136.6338 - mse: 136.6338 - val_loss: 123.0822 - val_mse: 123.0822\n",
      "Epoch 201/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 136.0159 - mse: 136.0160\n",
      "Epoch 00201: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 258us/sample - loss: 136.3415 - mse: 136.3415 - val_loss: 122.2615 - val_mse: 122.2615\n",
      "Epoch 202/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 138.1456 - mse: 138.1456\n",
      "Epoch 00202: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 260us/sample - loss: 135.1965 - mse: 135.1965 - val_loss: 122.4331 - val_mse: 122.4331\n",
      "Epoch 203/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 137.1257 - mse: 137.1257\n",
      "Epoch 00203: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 256us/sample - loss: 135.7133 - mse: 135.7133 - val_loss: 121.8794 - val_mse: 121.8794\n",
      "Epoch 204/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 139.0261 - mse: 139.0261\n",
      "Epoch 00204: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 254us/sample - loss: 135.3208 - mse: 135.3208 - val_loss: 121.9004 - val_mse: 121.9004\n",
      "Epoch 205/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 137.9321 - mse: 137.9321ETA: 0s - loss: 95.9407 - mse\n",
      "Epoch 00205: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 259us/sample - loss: 136.7546 - mse: 136.7546 - val_loss: 122.3572 - val_mse: 122.3572\n",
      "Epoch 206/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 133.9194 - mse: 133.9193\n",
      "Epoch 00206: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 261us/sample - loss: 134.8571 - mse: 134.8571 - val_loss: 122.3921 - val_mse: 122.3921\n",
      "Epoch 207/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 136.7494 - mse: 136.7495\n",
      "Epoch 00207: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 258us/sample - loss: 134.7608 - mse: 134.7608 - val_loss: 122.1613 - val_mse: 122.1613\n",
      "Epoch 208/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 137.0868 - mse: 137.0868\n",
      "Epoch 00208: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 281us/sample - loss: 135.2090 - mse: 135.2090 - val_loss: 122.2836 - val_mse: 122.2836\n",
      "Epoch 209/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 137.0488 - mse: 137.0488\n",
      "Epoch 00209: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 315us/sample - loss: 135.4605 - mse: 135.4605 - val_loss: 122.0651 - val_mse: 122.0651\n",
      "Epoch 210/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 137.6861 - mse: 137.6861\n",
      "Epoch 00210: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 288us/sample - loss: 136.1931 - mse: 136.1931 - val_loss: 122.4454 - val_mse: 122.4454\n",
      "Epoch 211/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 135.5934 - mse: 135.5933\n",
      "Epoch 00211: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 285us/sample - loss: 134.9502 - mse: 134.9501 - val_loss: 121.9749 - val_mse: 121.9749\n",
      "Epoch 212/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 135.4753 - mse: 135.4753\n",
      "Epoch 00212: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 269us/sample - loss: 134.5327 - mse: 134.5327 - val_loss: 121.9792 - val_mse: 121.9791\n",
      "Epoch 213/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 137.9402 - mse: 137.9402\n",
      "Epoch 00213: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 279us/sample - loss: 135.7340 - mse: 135.7340 - val_loss: 121.9122 - val_mse: 121.9122\n",
      "Epoch 214/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 134.0298 - mse: 134.0299\n",
      "Epoch 00214: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 264us/sample - loss: 134.9492 - mse: 134.9492 - val_loss: 122.0313 - val_mse: 122.0313\n",
      "Epoch 215/300\n",
      "2208/2400 [==========================>...] - ETA: 0s - loss: 136.5402 - mse: 136.5402\n",
      "Epoch 00215: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 268us/sample - loss: 135.9715 - mse: 135.9715 - val_loss: 121.7788 - val_mse: 121.7788\n",
      "Epoch 216/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 136.9594 - mse: 136.9594\n",
      "Epoch 00216: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 261us/sample - loss: 134.6380 - mse: 134.6380 - val_loss: 121.7310 - val_mse: 121.7310\n",
      "Epoch 217/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 134.9447 - mse: 134.9447\n",
      "Epoch 00217: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 262us/sample - loss: 134.5361 - mse: 134.5361 - val_loss: 121.9970 - val_mse: 121.9970\n",
      "Epoch 218/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 131.9028 - mse: 131.9028\n",
      "Epoch 00218: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 265us/sample - loss: 135.1731 - mse: 135.1731 - val_loss: 121.7328 - val_mse: 121.7328\n",
      "Epoch 219/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 130.9341 - mse: 130.9341\n",
      "Epoch 00219: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 265us/sample - loss: 135.2969 - mse: 135.2969 - val_loss: 122.8347 - val_mse: 122.8347\n",
      "Epoch 220/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 132.1751 - mse: 132.1752\n",
      "Epoch 00220: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 258us/sample - loss: 135.2548 - mse: 135.2549 - val_loss: 118.2911 - val_mse: 118.2911\n",
      "Epoch 221/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 133.1440 - mse: 133.1440\n",
      "Epoch 00221: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 261us/sample - loss: 131.9049 - mse: 131.9049 - val_loss: 120.1260 - val_mse: 120.1260\n",
      "Epoch 222/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 130.4689 - mse: 130.4689\n",
      "Epoch 00222: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 263us/sample - loss: 129.0863 - mse: 129.0863 - val_loss: 121.7603 - val_mse: 121.7603\n",
      "Epoch 223/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 118.3976 - mse: 118.3976\n",
      "Epoch 00223: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 261us/sample - loss: 132.5269 - mse: 132.5269 - val_loss: 114.8539 - val_mse: 114.8539\n",
      "Epoch 224/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 123.6608 - mse: 123.6608\n",
      "Epoch 00224: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 263us/sample - loss: 126.4851 - mse: 126.4851 - val_loss: 116.8047 - val_mse: 116.8046\n",
      "Epoch 225/300\n",
      "2176/2400 [==========================>...] - ETA: 0s - loss: 126.8939 - mse: 126.8939\n",
      "Epoch 00225: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 275us/sample - loss: 127.4059 - mse: 127.4059 - val_loss: 116.4890 - val_mse: 116.4890\n",
      "Epoch 226/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 128.1336 - mse: 128.133 - ETA: 0s - loss: 125.8610 - mse: 125.8610\n",
      "Epoch 00226: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 267us/sample - loss: 125.0385 - mse: 125.0385 - val_loss: 118.8981 - val_mse: 118.8981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 128.2156 - mse: 128.2156\n",
      "Epoch 00227: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 260us/sample - loss: 127.2940 - mse: 127.2941 - val_loss: 123.6489 - val_mse: 123.6489\n",
      "Epoch 228/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 125.2867 - mse: 125.2867\n",
      "Epoch 00228: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 261us/sample - loss: 125.7307 - mse: 125.7307 - val_loss: 115.2157 - val_mse: 115.2157\n",
      "Epoch 229/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 129.0650 - mse: 129.0650\n",
      "Epoch 00229: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 281us/sample - loss: 128.9945 - mse: 128.9945 - val_loss: 114.6669 - val_mse: 114.6669\n",
      "Epoch 230/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 125.9864 - mse: 125.9865\n",
      "Epoch 00230: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 294us/sample - loss: 125.8236 - mse: 125.8236 - val_loss: 114.0912 - val_mse: 114.0912\n",
      "Epoch 231/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 125.9128 - mse: 125.9128\n",
      "Epoch 00231: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 269us/sample - loss: 124.1865 - mse: 124.1865 - val_loss: 114.7208 - val_mse: 114.7208\n",
      "Epoch 232/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 119.8871 - mse: 119.8871\n",
      "Epoch 00232: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 276us/sample - loss: 121.0395 - mse: 121.0395 - val_loss: 116.4944 - val_mse: 116.4944\n",
      "Epoch 233/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 113.2893 - mse: 113.2893\n",
      "Epoch 00233: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 311us/sample - loss: 119.2784 - mse: 119.2784 - val_loss: 117.3015 - val_mse: 117.3015\n",
      "Epoch 234/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 120.8284 - mse: 120.8284\n",
      "Epoch 00234: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 302us/sample - loss: 119.5380 - mse: 119.5380 - val_loss: 112.5399 - val_mse: 112.5399\n",
      "Epoch 235/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 117.2503 - mse: 117.2503\n",
      "Epoch 00235: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 304us/sample - loss: 119.3597 - mse: 119.3597 - val_loss: 115.1305 - val_mse: 115.1305\n",
      "Epoch 236/300\n",
      "2208/2400 [==========================>...] - ETA: 0s - loss: 127.7222 - mse: 127.7222\n",
      "Epoch 00236: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 268us/sample - loss: 122.5016 - mse: 122.5017 - val_loss: 114.5625 - val_mse: 114.5625\n",
      "Epoch 237/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 125.3762 - mse: 125.3761\n",
      "Epoch 00237: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 265us/sample - loss: 121.8710 - mse: 121.8709 - val_loss: 121.0014 - val_mse: 121.0014\n",
      "Epoch 238/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 125.2869 - mse: 125.2869\n",
      "Epoch 00238: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 261us/sample - loss: 122.3187 - mse: 122.3187 - val_loss: 114.5268 - val_mse: 114.5268\n",
      "Epoch 239/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 118.3614 - mse: 118.3614\n",
      "Epoch 00239: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 263us/sample - loss: 118.1044 - mse: 118.1044 - val_loss: 123.0212 - val_mse: 123.0212\n",
      "Epoch 240/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 126.5317 - mse: 126.5317\n",
      "Epoch 00240: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 264us/sample - loss: 125.8463 - mse: 125.8463 - val_loss: 116.6334 - val_mse: 116.6334\n",
      "Epoch 241/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 124.5689 - mse: 124.5689\n",
      "Epoch 00241: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 263us/sample - loss: 123.6167 - mse: 123.6167 - val_loss: 114.1533 - val_mse: 114.1533\n",
      "Epoch 242/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 120.4156 - mse: 120.4156\n",
      "Epoch 00242: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 262us/sample - loss: 120.7509 - mse: 120.7509 - val_loss: 124.5155 - val_mse: 124.5155\n",
      "Epoch 243/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 126.1908 - mse: 126.1908\n",
      "Epoch 00243: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 261us/sample - loss: 122.8314 - mse: 122.8314 - val_loss: 116.9983 - val_mse: 116.9983\n",
      "Epoch 244/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 119.2125 - mse: 119.2125\n",
      "Epoch 00244: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 263us/sample - loss: 116.5055 - mse: 116.5055 - val_loss: 119.2393 - val_mse: 119.2392\n",
      "Epoch 245/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 114.9874 - mse: 114.9874\n",
      "Epoch 00245: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 263us/sample - loss: 114.1679 - mse: 114.1679 - val_loss: 125.4404 - val_mse: 125.4404\n",
      "Epoch 246/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 125.5452 - mse: 125.5452\n",
      "Epoch 00246: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 258us/sample - loss: 129.5106 - mse: 129.5106 - val_loss: 116.5502 - val_mse: 116.5502\n",
      "Epoch 247/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 128.5364 - mse: 128.5364\n",
      "Epoch 00247: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 258us/sample - loss: 126.2908 - mse: 126.2908 - val_loss: 112.6790 - val_mse: 112.6790\n",
      "Epoch 248/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 121.0440 - mse: 121.0440\n",
      "Epoch 00248: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 261us/sample - loss: 118.7867 - mse: 118.7867 - val_loss: 114.3267 - val_mse: 114.3267\n",
      "Epoch 249/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 113.1830 - mse: 113.1830\n",
      "Epoch 00249: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 259us/sample - loss: 113.1628 - mse: 113.1628 - val_loss: 120.1407 - val_mse: 120.1407\n",
      "Epoch 250/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 117.9820 - mse: 117.9820\n",
      "Epoch 00250: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 260us/sample - loss: 119.5475 - mse: 119.5475 - val_loss: 118.4736 - val_mse: 118.4736\n",
      "Epoch 251/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 129.7257 - mse: 129.7258\n",
      "Epoch 00251: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 277us/sample - loss: 127.7125 - mse: 127.7125 - val_loss: 119.2001 - val_mse: 119.2001\n",
      "Epoch 252/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 126.4935 - mse: 126.4935\n",
      "Epoch 00252: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 263us/sample - loss: 127.0426 - mse: 127.0426 - val_loss: 118.4055 - val_mse: 118.4055\n",
      "Epoch 253/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 122.7434 - mse: 122.7433\n",
      "Epoch 00253: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 279us/sample - loss: 121.9059 - mse: 121.9059 - val_loss: 123.1296 - val_mse: 123.1296\n",
      "Epoch 254/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 121.2638 - mse: 121.2638\n",
      "Epoch 00254: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 262us/sample - loss: 119.7552 - mse: 119.7552 - val_loss: 115.5533 - val_mse: 115.5533\n",
      "Epoch 255/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 112.9577 - mse: 112.9577\n",
      "Epoch 00255: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 260us/sample - loss: 117.4978 - mse: 117.4978 - val_loss: 115.9826 - val_mse: 115.9826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 118.9176 - mse: 118.9176\n",
      "Epoch 00256: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 258us/sample - loss: 117.6269 - mse: 117.6269 - val_loss: 120.5093 - val_mse: 120.5093\n",
      "Epoch 257/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 117.5281 - mse: 117.5281\n",
      "Epoch 00257: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 271us/sample - loss: 119.0247 - mse: 119.0247 - val_loss: 113.0362 - val_mse: 113.0362\n",
      "Epoch 258/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 114.3347 - mse: 114.3347\n",
      "Epoch 00258: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 325us/sample - loss: 114.8851 - mse: 114.8850 - val_loss: 120.2039 - val_mse: 120.2039\n",
      "Epoch 259/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 109.0917 - mse: 109.0917\n",
      "Epoch 00259: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 374us/sample - loss: 108.8658 - mse: 108.8658 - val_loss: 121.3099 - val_mse: 121.3099\n",
      "Epoch 260/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 113.3366 - mse: 113.3366\n",
      "Epoch 00260: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 367us/sample - loss: 113.2697 - mse: 113.2697 - val_loss: 116.7130 - val_mse: 116.7130\n",
      "Epoch 261/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 121.2988 - mse: 121.2988\n",
      "Epoch 00261: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 299us/sample - loss: 121.9709 - mse: 121.9709 - val_loss: 114.7289 - val_mse: 114.7289\n",
      "Epoch 262/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 115.5911 - mse: 115.5911\n",
      "Epoch 00262: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 297us/sample - loss: 120.0556 - mse: 120.0556 - val_loss: 113.9171 - val_mse: 113.9171\n",
      "Epoch 263/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 117.5170 - mse: 117.5170\n",
      "Epoch 00263: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 296us/sample - loss: 117.2362 - mse: 117.2362 - val_loss: 122.5849 - val_mse: 122.5849\n",
      "Epoch 264/300\n",
      "2208/2400 [==========================>...] - ETA: 0s - loss: 119.0808 - mse: 119.0808\n",
      "Epoch 00264: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 289us/sample - loss: 115.1565 - mse: 115.1565 - val_loss: 121.1444 - val_mse: 121.1444\n",
      "Epoch 265/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 116.5663 - mse: 116.5663\n",
      "Epoch 00265: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 310us/sample - loss: 116.3414 - mse: 116.3414 - val_loss: 115.2589 - val_mse: 115.2589\n",
      "Epoch 266/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 113.7902 - mse: 113.7902\n",
      "Epoch 00266: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 332us/sample - loss: 113.0474 - mse: 113.0474 - val_loss: 118.0394 - val_mse: 118.0394\n",
      "Epoch 267/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 114.3475 - mse: 114.3475\n",
      "Epoch 00267: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 293us/sample - loss: 111.5119 - mse: 111.5119 - val_loss: 114.5516 - val_mse: 114.5515\n",
      "Epoch 268/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 108.7112 - mse: 108.7112\n",
      "Epoch 00268: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 288us/sample - loss: 113.5998 - mse: 113.5998 - val_loss: 117.3503 - val_mse: 117.3503\n",
      "Epoch 269/300\n",
      "2208/2400 [==========================>...] - ETA: 0s - loss: 114.1234 - mse: 114.1234\n",
      "Epoch 00269: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 284us/sample - loss: 113.2449 - mse: 113.2449 - val_loss: 119.6913 - val_mse: 119.6913\n",
      "Epoch 270/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 122.7467 - mse: 122.7467\n",
      "Epoch 00270: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 288us/sample - loss: 122.9790 - mse: 122.9790 - val_loss: 116.8523 - val_mse: 116.8524\n",
      "Epoch 271/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 120.8869 - mse: 120.8869\n",
      "Epoch 00271: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 297us/sample - loss: 120.2588 - mse: 120.2588 - val_loss: 122.3743 - val_mse: 122.3743\n",
      "Epoch 272/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 119.6640 - mse: 119.6640\n",
      "Epoch 00272: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 288us/sample - loss: 120.1335 - mse: 120.1335 - val_loss: 117.3625 - val_mse: 117.3625\n",
      "Epoch 273/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 118.2627 - mse: 118.2627\n",
      "Epoch 00273: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 284us/sample - loss: 116.3503 - mse: 116.3503 - val_loss: 123.7942 - val_mse: 123.7942\n",
      "Epoch 274/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 113.4055 - mse: 113.4055\n",
      "Epoch 00274: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 327us/sample - loss: 113.4853 - mse: 113.4853 - val_loss: 117.1326 - val_mse: 117.1326\n",
      "Epoch 275/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 113.1150 - mse: 113.1150\n",
      "Epoch 00275: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 307us/sample - loss: 112.1129 - mse: 112.1129 - val_loss: 118.2038 - val_mse: 118.2038\n",
      "Epoch 276/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 106.9840 - mse: 106.9840\n",
      "Epoch 00276: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 287us/sample - loss: 106.8449 - mse: 106.8449 - val_loss: 118.5847 - val_mse: 118.5847\n",
      "Epoch 277/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 115.4373 - mse: 115.4373\n",
      "Epoch 00277: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 315us/sample - loss: 113.2616 - mse: 113.2616 - val_loss: 113.2688 - val_mse: 113.2688\n",
      "Epoch 278/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 115.8706 - mse: 115.8706\n",
      "Epoch 00278: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 288us/sample - loss: 113.9028 - mse: 113.9028 - val_loss: 120.0541 - val_mse: 120.0541\n",
      "Epoch 279/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 111.6296 - mse: 111.6296\n",
      "Epoch 00279: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 319us/sample - loss: 110.7707 - mse: 110.7707 - val_loss: 119.5043 - val_mse: 119.5043\n",
      "Epoch 280/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 108.1819 - mse: 108.1819\n",
      "Epoch 00280: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 321us/sample - loss: 107.0616 - mse: 107.0616 - val_loss: 116.8462 - val_mse: 116.8462\n",
      "Epoch 281/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 118.0562 - mse: 118.0562\n",
      "Epoch 00281: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 318us/sample - loss: 115.7162 - mse: 115.7162 - val_loss: 122.0517 - val_mse: 122.0517\n",
      "Epoch 282/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 109.4143 - mse: 109.4143\n",
      "Epoch 00282: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 333us/sample - loss: 108.2725 - mse: 108.2725 - val_loss: 120.3030 - val_mse: 120.3030\n",
      "Epoch 283/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 140.3206 - mse: 140.3206\n",
      "Epoch 00283: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 351us/sample - loss: 139.4050 - mse: 139.4050 - val_loss: 114.6734 - val_mse: 114.6734\n",
      "Epoch 284/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 124.7774 - mse: 124.7774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00284: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 360us/sample - loss: 124.5573 - mse: 124.5574 - val_loss: 120.2946 - val_mse: 120.2946\n",
      "Epoch 285/300\n",
      "2208/2400 [==========================>...] - ETA: 0s - loss: 114.8414 - mse: 114.8414\n",
      "Epoch 00285: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 342us/sample - loss: 114.7185 - mse: 114.7185 - val_loss: 120.4008 - val_mse: 120.4008\n",
      "Epoch 286/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 115.1586 - mse: 115.1586\n",
      "Epoch 00286: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 272us/sample - loss: 114.2346 - mse: 114.2346 - val_loss: 121.1875 - val_mse: 121.1875\n",
      "Epoch 287/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 116.8199 - mse: 116.8199\n",
      "Epoch 00287: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 259us/sample - loss: 114.8319 - mse: 114.8319 - val_loss: 119.8702 - val_mse: 119.8702\n",
      "Epoch 288/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 113.1260 - mse: 113.1260\n",
      "Epoch 00288: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 261us/sample - loss: 114.4666 - mse: 114.4666 - val_loss: 123.5506 - val_mse: 123.5506\n",
      "Epoch 289/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 108.5903 - mse: 108.5903\n",
      "Epoch 00289: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 268us/sample - loss: 112.2396 - mse: 112.2396 - val_loss: 121.1215 - val_mse: 121.1215\n",
      "Epoch 290/300\n",
      "2240/2400 [===========================>..] - ETA: 0s - loss: 114.9537 - mse: 114.9536\n",
      "Epoch 00290: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 268us/sample - loss: 112.9438 - mse: 112.9438 - val_loss: 119.7389 - val_mse: 119.7389\n",
      "Epoch 291/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 108.0757 - mse: 108.0757\n",
      "Epoch 00291: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 258us/sample - loss: 110.2304 - mse: 110.2304 - val_loss: 115.9459 - val_mse: 115.9459\n",
      "Epoch 292/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 112.2700 - mse: 112.2700\n",
      "Epoch 00292: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 258us/sample - loss: 111.0947 - mse: 111.0947 - val_loss: 121.5589 - val_mse: 121.5589\n",
      "Epoch 293/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 108.3259 - mse: 108.3259\n",
      "Epoch 00293: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 256us/sample - loss: 106.6022 - mse: 106.6022 - val_loss: 115.1768 - val_mse: 115.1768\n",
      "Epoch 294/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 106.3683 - mse: 106.3683\n",
      "Epoch 00294: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 265us/sample - loss: 105.8044 - mse: 105.8044 - val_loss: 119.2716 - val_mse: 119.2716\n",
      "Epoch 295/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 121.8150 - mse: 121.8150\n",
      "Epoch 00295: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 256us/sample - loss: 125.3383 - mse: 125.3383 - val_loss: 135.9891 - val_mse: 135.9891\n",
      "Epoch 296/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 121.3915 - mse: 121.3915\n",
      "Epoch 00296: val_loss did not improve from 111.88478\n",
      "2400/2400 [==============================] - 1s 257us/sample - loss: 122.8371 - mse: 122.8371 - val_loss: 147.8820 - val_mse: 147.8820\n",
      "Epoch 297/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 121.6935 - mse: 121.6935\n",
      "Epoch 00297: val_loss improved from 111.88478 to 111.46321, saving model to best_model_1.h5\n",
      "2400/2400 [==============================] - 1s 313us/sample - loss: 121.0217 - mse: 121.0217 - val_loss: 111.4632 - val_mse: 111.4632\n",
      "Epoch 298/300\n",
      "2208/2400 [==========================>...] - ETA: 0s - loss: 122.2926 - mse: 122.2926\n",
      "Epoch 00298: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 266us/sample - loss: 120.1095 - mse: 120.1095 - val_loss: 113.5520 - val_mse: 113.5520\n",
      "Epoch 299/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 117.8404 - mse: 117.8403\n",
      "Epoch 00299: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 261us/sample - loss: 118.6401 - mse: 118.6401 - val_loss: 113.0891 - val_mse: 113.0891\n",
      "Epoch 300/300\n",
      "2208/2400 [==========================>...] - ETA: 0s - loss: 112.6609 - mse: 112.6609\n",
      "Epoch 00300: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 265us/sample - loss: 115.4253 - mse: 115.4253 - val_loss: 125.4563 - val_mse: 125.4563\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4131 samples, validate on 1033 samples\n",
      "Epoch 1/200\n",
      "3648/4131 [=========================>....] - ETA: 0s - loss: 4.8129 - accuracy: 0.6146\n",
      "Epoch 00001: val_loss improved from inf to 0.09174, saving model to best_model_2.h5\n",
      "4131/4131 [==============================] - 1s 254us/sample - loss: 4.4299 - accuracy: 0.6209 - val_loss: 0.0917 - val_accuracy: 0.9545\n",
      "Epoch 2/200\n",
      "3424/4131 [=======================>......] - ETA: 0s - loss: 1.4834 - accuracy: 0.6387\n",
      "Epoch 00002: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 56us/sample - loss: 1.4320 - accuracy: 0.6420 - val_loss: 1.6017 - val_accuracy: 0.4550\n",
      "Epoch 3/200\n",
      "3424/4131 [=======================>......] - ETA: 0s - loss: 0.9772 - accuracy: 0.6717\n",
      "Epoch 00003: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 53us/sample - loss: 0.9742 - accuracy: 0.6720 - val_loss: 3.6402 - val_accuracy: 0.1442\n",
      "Epoch 4/200\n",
      "4064/4131 [============================>.] - ETA: 0s - loss: 0.8470 - accuracy: 0.6941\n",
      "Epoch 00004: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.8423 - accuracy: 0.6957 - val_loss: 0.6616 - val_accuracy: 0.7018\n",
      "Epoch 5/200\n",
      "3712/4131 [=========================>....] - ETA: 0s - loss: 0.8133 - accuracy: 0.7026\n",
      "Epoch 00005: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 51us/sample - loss: 0.7929 - accuracy: 0.7056 - val_loss: 0.4416 - val_accuracy: 0.7938\n",
      "Epoch 6/200\n",
      "4064/4131 [============================>.] - ETA: 0s - loss: 0.6618 - accuracy: 0.7249\n",
      "Epoch 00006: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.6642 - accuracy: 0.7231 - val_loss: 1.7545 - val_accuracy: 0.2314\n",
      "Epoch 7/200\n",
      "3808/4131 [==========================>...] - ETA: 0s - loss: 0.6226 - accuracy: 0.7319\n",
      "Epoch 00007: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 50us/sample - loss: 0.6151 - accuracy: 0.7354 - val_loss: 0.2862 - val_accuracy: 0.9080\n",
      "Epoch 8/200\n",
      "3872/4131 [===========================>..] - ETA: 0s - loss: 0.6784 - accuracy: 0.7211\n",
      "Epoch 00008: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.6998 - accuracy: 0.7175 - val_loss: 0.3496 - val_accuracy: 0.8490\n",
      "Epoch 9/200\n",
      "4096/4131 [============================>.] - ETA: 0s - loss: 0.5873 - accuracy: 0.7373\n",
      "Epoch 00009: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.5871 - accuracy: 0.7374 - val_loss: 0.1937 - val_accuracy: 0.9593\n",
      "Epoch 10/200\n",
      "3456/4131 [========================>.....] - ETA: 0s - loss: 0.5333 - accuracy: 0.7564\n",
      "Epoch 00010: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 55us/sample - loss: 0.5543 - accuracy: 0.7509 - val_loss: 0.2656 - val_accuracy: 0.9071\n",
      "Epoch 11/200\n",
      "4032/4131 [============================>.] - ETA: 0s - loss: 0.5620 - accuracy: 0.7478\n",
      "Epoch 00011: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.5633 - accuracy: 0.7492 - val_loss: 0.7661 - val_accuracy: 0.7919\n",
      "Epoch 12/200\n",
      "3872/4131 [===========================>..] - ETA: 0s - loss: 0.5257 - accuracy: 0.7583\n",
      "Epoch 00012: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.5236 - accuracy: 0.7596 - val_loss: 0.3546 - val_accuracy: 0.8703\n",
      "Epoch 13/200\n",
      "4032/4131 [============================>.] - ETA: 0s - loss: 0.5223 - accuracy: 0.7537\n",
      "Epoch 00013: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.5189 - accuracy: 0.7553 - val_loss: 0.1809 - val_accuracy: 0.9497\n",
      "Epoch 14/200\n",
      "4000/4131 [============================>.] - ETA: 0s - loss: 0.5750 - accuracy: 0.7487\n",
      "Epoch 00014: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.5745 - accuracy: 0.7482 - val_loss: 1.2790 - val_accuracy: 0.3320\n",
      "Epoch 15/200\n",
      "4000/4131 [============================>.] - ETA: 0s - loss: 0.6343 - accuracy: 0.7295\n",
      "Epoch 00015: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.6323 - accuracy: 0.7303 - val_loss: 0.6089 - val_accuracy: 0.7561\n",
      "Epoch 16/200\n",
      "4000/4131 [============================>.] - ETA: 0s - loss: 0.5137 - accuracy: 0.7713\n",
      "Epoch 00016: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.5224 - accuracy: 0.7698 - val_loss: 0.4359 - val_accuracy: 0.8877\n",
      "Epoch 17/200\n",
      "3872/4131 [===========================>..] - ETA: 0s - loss: 0.4856 - accuracy: 0.7714\n",
      "Epoch 00017: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.4859 - accuracy: 0.7698 - val_loss: 0.5097 - val_accuracy: 0.7473\n",
      "Epoch 18/200\n",
      "4032/4131 [============================>.] - ETA: 0s - loss: 0.4779 - accuracy: 0.7736\n",
      "Epoch 00018: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.4798 - accuracy: 0.7729 - val_loss: 0.1774 - val_accuracy: 0.9622\n",
      "Epoch 19/200\n",
      "4000/4131 [============================>.] - ETA: 0s - loss: 0.4741 - accuracy: 0.7753\n",
      "Epoch 00019: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.4777 - accuracy: 0.7729 - val_loss: 1.3835 - val_accuracy: 0.3020\n",
      "Epoch 20/200\n",
      "3872/4131 [===========================>..] - ETA: 0s - loss: 0.4718 - accuracy: 0.7740\n",
      "Epoch 00020: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.4714 - accuracy: 0.7754 - val_loss: 0.5150 - val_accuracy: 0.7425\n",
      "Epoch 21/200\n",
      "4032/4131 [============================>.] - ETA: 0s - loss: 0.4220 - accuracy: 0.7996\n",
      "Epoch 00021: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.4214 - accuracy: 0.8000 - val_loss: 0.2262 - val_accuracy: 0.9284\n",
      "Epoch 22/200\n",
      "3808/4131 [==========================>...] - ETA: 0s - loss: 0.5954 - accuracy: 0.7547\n",
      "Epoch 00022: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 50us/sample - loss: 0.5836 - accuracy: 0.7555 - val_loss: 0.4176 - val_accuracy: 0.8064\n",
      "Epoch 23/200\n",
      "4064/4131 [============================>.] - ETA: 0s - loss: 0.4467 - accuracy: 0.7985\n",
      "Epoch 00023: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.4469 - accuracy: 0.7976 - val_loss: 0.7574 - val_accuracy: 0.6302\n",
      "Epoch 24/200\n",
      "4064/4131 [============================>.] - ETA: 0s - loss: 0.4295 - accuracy: 0.8083\n",
      "Epoch 00024: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.4268 - accuracy: 0.8100 - val_loss: 0.1926 - val_accuracy: 0.9477\n",
      "Epoch 25/200\n",
      "3776/4131 [==========================>...] - ETA: 0s - loss: 0.4513 - accuracy: 0.7879\n",
      "Epoch 00025: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 50us/sample - loss: 0.4501 - accuracy: 0.7896 - val_loss: 0.3339 - val_accuracy: 0.8625\n",
      "Epoch 26/200\n",
      "4000/4131 [============================>.] - ETA: 0s - loss: 0.4101 - accuracy: 0.8085\n",
      "Epoch 00026: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.4092 - accuracy: 0.8088 - val_loss: 0.8129 - val_accuracy: 0.5324\n",
      "Epoch 27/200\n",
      "3936/4131 [===========================>..] - ETA: 0s - loss: 0.4788 - accuracy: 0.7815\n",
      "Epoch 00027: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.4761 - accuracy: 0.7826 - val_loss: 0.3640 - val_accuracy: 0.8548\n",
      "Epoch 28/200\n",
      "4000/4131 [============================>.] - ETA: 0s - loss: 0.4302 - accuracy: 0.7985\n",
      "Epoch 00028: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.4280 - accuracy: 0.8008 - val_loss: 0.8502 - val_accuracy: 0.6244\n",
      "Epoch 29/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4064/4131 [============================>.] - ETA: 0s - loss: 0.4404 - accuracy: 0.7970\n",
      "Epoch 00029: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.4408 - accuracy: 0.7974 - val_loss: 0.8225 - val_accuracy: 0.5479\n",
      "Epoch 30/200\n",
      "3936/4131 [===========================>..] - ETA: 0s - loss: 0.4301 - accuracy: 0.8031\n",
      "Epoch 00030: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.4299 - accuracy: 0.8025 - val_loss: 0.8219 - val_accuracy: 0.5324\n",
      "Epoch 31/200\n",
      "4000/4131 [============================>.] - ETA: 0s - loss: 0.3835 - accuracy: 0.8288\n",
      "Epoch 00031: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.3883 - accuracy: 0.8274 - val_loss: 0.8864 - val_accuracy: 0.5760\n",
      "Epoch 32/200\n",
      "3936/4131 [===========================>..] - ETA: 0s - loss: 0.4576 - accuracy: 0.8039\n",
      "Epoch 00032: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.4564 - accuracy: 0.8027 - val_loss: 1.4027 - val_accuracy: 0.3146\n",
      "Epoch 33/200\n",
      "4096/4131 [============================>.] - ETA: 0s - loss: 0.3810 - accuracy: 0.8213\n",
      "Epoch 00033: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 46us/sample - loss: 0.3816 - accuracy: 0.8214 - val_loss: 0.6717 - val_accuracy: 0.6689\n",
      "Epoch 34/200\n",
      "4064/4131 [============================>.] - ETA: 0s - loss: 0.4156 - accuracy: 0.8031\n",
      "Epoch 00034: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.4137 - accuracy: 0.8046 - val_loss: 0.6693 - val_accuracy: 0.6418\n",
      "Epoch 35/200\n",
      "3904/4131 [===========================>..] - ETA: 0s - loss: 0.4000 - accuracy: 0.8189\n",
      "Epoch 00035: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 50us/sample - loss: 0.4046 - accuracy: 0.8184 - val_loss: 0.5406 - val_accuracy: 0.7241\n",
      "Epoch 36/200\n",
      "4128/4131 [============================>.] - ETA: 0s - loss: 0.3848 - accuracy: 0.8268\n",
      "Epoch 00036: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 46us/sample - loss: 0.3848 - accuracy: 0.8267 - val_loss: 0.5524 - val_accuracy: 0.7173\n",
      "Epoch 37/200\n",
      "3776/4131 [==========================>...] - ETA: 0s - loss: 0.4119 - accuracy: 0.8159\n",
      "Epoch 00037: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 50us/sample - loss: 0.4157 - accuracy: 0.8131 - val_loss: 1.0454 - val_accuracy: 0.4627\n",
      "Epoch 38/200\n",
      "4064/4131 [============================>.] - ETA: 0s - loss: 0.3985 - accuracy: 0.8127\n",
      "Epoch 00038: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.3994 - accuracy: 0.8126 - val_loss: 0.4876 - val_accuracy: 0.7706\n",
      "Epoch 39/200\n",
      "4096/4131 [============================>.] - ETA: 0s - loss: 0.3590 - accuracy: 0.8440\n",
      "Epoch 00039: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 51us/sample - loss: 0.3590 - accuracy: 0.8439 - val_loss: 0.2984 - val_accuracy: 0.9032\n",
      "Epoch 40/200\n",
      "3360/4131 [=======================>......] - ETA: 0s - loss: 0.4787 - accuracy: 0.8045\n",
      "Epoch 00040: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 72us/sample - loss: 0.4820 - accuracy: 0.8013 - val_loss: 1.0667 - val_accuracy: 0.4443\n",
      "Epoch 41/200\n",
      "3776/4131 [==========================>...] - ETA: 0s - loss: 0.3555 - accuracy: 0.8456\n",
      "Epoch 00041: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 68us/sample - loss: 0.3559 - accuracy: 0.8451 - val_loss: 0.7159 - val_accuracy: 0.6002\n",
      "Epoch 42/200\n",
      "3840/4131 [==========================>...] - ETA: 0s - loss: 0.4002 - accuracy: 0.8391\n",
      "Epoch 00042: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 67us/sample - loss: 0.3984 - accuracy: 0.8376 - val_loss: 0.5863 - val_accuracy: 0.7212\n",
      "Epoch 43/200\n",
      "3904/4131 [===========================>..] - ETA: 0s - loss: 0.4893 - accuracy: 0.8074\n",
      "Epoch 00043: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 62us/sample - loss: 0.4805 - accuracy: 0.8092 - val_loss: 0.3317 - val_accuracy: 0.8606\n",
      "Epoch 44/200\n",
      "3488/4131 [========================>.....] - ETA: 0s - loss: 0.3718 - accuracy: 0.8306\n",
      "Epoch 00044: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 57us/sample - loss: 0.3704 - accuracy: 0.8313 - val_loss: 0.2908 - val_accuracy: 0.8945\n",
      "Epoch 45/200\n",
      "3360/4131 [=======================>......] - ETA: 0s - loss: 0.3435 - accuracy: 0.8527\n",
      "Epoch 00045: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 71us/sample - loss: 0.3444 - accuracy: 0.8511 - val_loss: 0.2735 - val_accuracy: 0.8916\n",
      "Epoch 46/200\n",
      "3680/4131 [=========================>....] - ETA: 0s - loss: 0.3445 - accuracy: 0.8438\n",
      "Epoch 00046: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 51us/sample - loss: 0.3461 - accuracy: 0.8429 - val_loss: 0.4283 - val_accuracy: 0.7880\n",
      "Epoch 47/200\n",
      "3776/4131 [==========================>...] - ETA: 0s - loss: 0.3169 - accuracy: 0.8631\n",
      "Epoch 00047: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 52us/sample - loss: 0.3252 - accuracy: 0.8608 - val_loss: 0.6365 - val_accuracy: 0.6786\n",
      "Epoch 48/200\n",
      "3872/4131 [===========================>..] - ETA: 0s - loss: 0.4311 - accuracy: 0.8223\n",
      "Epoch 00048: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.4244 - accuracy: 0.8245 - val_loss: 0.3809 - val_accuracy: 0.8306\n",
      "Epoch 49/200\n",
      "3968/4131 [===========================>..] - ETA: 0s - loss: 0.3300 - accuracy: 0.8574\n",
      "Epoch 00049: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.3342 - accuracy: 0.8560 - val_loss: 0.3128 - val_accuracy: 0.8790\n",
      "Epoch 50/200\n",
      "3680/4131 [=========================>....] - ETA: 0s - loss: 0.3475 - accuracy: 0.8519\n",
      "Epoch 00050: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 51us/sample - loss: 0.3430 - accuracy: 0.8526 - val_loss: 0.3710 - val_accuracy: 0.8412\n",
      "Epoch 51/200\n",
      "3936/4131 [===========================>..] - ETA: 0s - loss: 0.3063 - accuracy: 0.8709\n",
      "Epoch 00051: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.3065 - accuracy: 0.8688 - val_loss: 0.2883 - val_accuracy: 0.8887\n",
      "Epoch 52/200\n",
      "3808/4131 [==========================>...] - ETA: 0s - loss: 0.3547 - accuracy: 0.8466\n",
      "Epoch 00052: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 50us/sample - loss: 0.3525 - accuracy: 0.8477 - val_loss: 1.0212 - val_accuracy: 0.5421\n",
      "Epoch 53/200\n",
      "3904/4131 [===========================>..] - ETA: 0s - loss: 0.3274 - accuracy: 0.8635\n",
      "Epoch 00053: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.3264 - accuracy: 0.8637 - val_loss: 0.7896 - val_accuracy: 0.5808\n",
      "Epoch 54/200\n",
      "3872/4131 [===========================>..] - ETA: 0s - loss: 0.3398 - accuracy: 0.8487\n",
      "Epoch 00054: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 50us/sample - loss: 0.3357 - accuracy: 0.8511 - val_loss: 0.2743 - val_accuracy: 0.8877\n",
      "Epoch 55/200\n",
      "3840/4131 [==========================>...] - ETA: 0s - loss: 0.2932 - accuracy: 0.8781\n",
      "Epoch 00055: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 50us/sample - loss: 0.2947 - accuracy: 0.8770 - val_loss: 0.3167 - val_accuracy: 0.8742\n",
      "Epoch 56/200\n",
      "3936/4131 [===========================>..] - ETA: 0s - loss: 0.3085 - accuracy: 0.8727\n",
      "Epoch 00056: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.3053 - accuracy: 0.8746 - val_loss: 0.1868 - val_accuracy: 0.9400\n",
      "Epoch 57/200\n",
      "3584/4131 [=========================>....] - ETA: 0s - loss: 0.2826 - accuracy: 0.8800\n",
      "Epoch 00057: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 52us/sample - loss: 0.2902 - accuracy: 0.8768 - val_loss: 0.9045 - val_accuracy: 0.6302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "3968/4131 [===========================>..] - ETA: 0s - loss: 0.3055 - accuracy: 0.8649\n",
      "Epoch 00058: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.3071 - accuracy: 0.8640 - val_loss: 0.1830 - val_accuracy: 0.9477\n",
      "Epoch 59/200\n",
      "3872/4131 [===========================>..] - ETA: 0s - loss: 0.3154 - accuracy: 0.8667\n",
      "Epoch 00059: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 50us/sample - loss: 0.3130 - accuracy: 0.8671 - val_loss: 0.2553 - val_accuracy: 0.9109\n",
      "Epoch 60/200\n",
      "3776/4131 [==========================>...] - ETA: 0s - loss: 0.3483 - accuracy: 0.8549\n",
      "Epoch 00060: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 50us/sample - loss: 0.3448 - accuracy: 0.8565 - val_loss: 0.3282 - val_accuracy: 0.8703\n",
      "Epoch 61/200\n",
      "3968/4131 [===========================>..] - ETA: 0s - loss: 0.3215 - accuracy: 0.8621\n",
      "Epoch 00061: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.3255 - accuracy: 0.8601 - val_loss: 0.5143 - val_accuracy: 0.7289\n",
      "Epoch 62/200\n",
      "3648/4131 [=========================>....] - ETA: 0s - loss: 0.2833 - accuracy: 0.8802\n",
      "Epoch 00062: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 51us/sample - loss: 0.2923 - accuracy: 0.8761 - val_loss: 0.2902 - val_accuracy: 0.8867\n",
      "Epoch 63/200\n",
      "3904/4131 [===========================>..] - ETA: 0s - loss: 0.3139 - accuracy: 0.8691\n",
      "Epoch 00063: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.3099 - accuracy: 0.8705 - val_loss: 0.5981 - val_accuracy: 0.6941\n",
      "Epoch 64/200\n",
      "3808/4131 [==========================>...] - ETA: 0s - loss: 0.2905 - accuracy: 0.8747\n",
      "Epoch 00064: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 50us/sample - loss: 0.3023 - accuracy: 0.8695 - val_loss: 0.1370 - val_accuracy: 0.9642\n",
      "Epoch 65/200\n",
      "3936/4131 [===========================>..] - ETA: 0s - loss: 0.2786 - accuracy: 0.8796\n",
      "Epoch 00065: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.2828 - accuracy: 0.8763 - val_loss: 0.2483 - val_accuracy: 0.9051\n",
      "Epoch 66/200\n",
      "3968/4131 [===========================>..] - ETA: 0s - loss: 0.3058 - accuracy: 0.8697\n",
      "Epoch 00066: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.3060 - accuracy: 0.8690 - val_loss: 0.1659 - val_accuracy: 0.9603\n",
      "Epoch 67/200\n",
      "3680/4131 [=========================>....] - ETA: 0s - loss: 0.2790 - accuracy: 0.8878\n",
      "Epoch 00067: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 51us/sample - loss: 0.2776 - accuracy: 0.8891 - val_loss: 0.7107 - val_accuracy: 0.6244\n",
      "Epoch 68/200\n",
      "3456/4131 [========================>.....] - ETA: 0s - loss: 0.2637 - accuracy: 0.8825\n",
      "Epoch 00068: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 53us/sample - loss: 0.2648 - accuracy: 0.8819 - val_loss: 0.6130 - val_accuracy: 0.7057\n",
      "Epoch 69/200\n",
      "3424/4131 [=======================>......] - ETA: 0s - loss: 0.2869 - accuracy: 0.8782\n",
      "Epoch 00069: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 54us/sample - loss: 0.2968 - accuracy: 0.8758 - val_loss: 0.3862 - val_accuracy: 0.8267\n",
      "Epoch 70/200\n",
      "3904/4131 [===========================>..] - ETA: 0s - loss: 0.2879 - accuracy: 0.8763\n",
      "Epoch 00070: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.2856 - accuracy: 0.8773 - val_loss: 0.4332 - val_accuracy: 0.8015\n",
      "Epoch 71/200\n",
      "3872/4131 [===========================>..] - ETA: 0s - loss: 0.2783 - accuracy: 0.8799\n",
      "Epoch 00071: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 50us/sample - loss: 0.2810 - accuracy: 0.8790 - val_loss: 0.2493 - val_accuracy: 0.9080\n",
      "Epoch 72/200\n",
      "3904/4131 [===========================>..] - ETA: 0s - loss: 0.2610 - accuracy: 0.8852\n",
      "Epoch 00072: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.2619 - accuracy: 0.8862 - val_loss: 0.3393 - val_accuracy: 0.8761\n",
      "Epoch 73/200\n",
      "3904/4131 [===========================>..] - ETA: 0s - loss: 0.2807 - accuracy: 0.8753\n",
      "Epoch 00073: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.2804 - accuracy: 0.8753 - val_loss: 0.1105 - val_accuracy: 0.9748\n",
      "Epoch 74/200\n",
      "3776/4131 [==========================>...] - ETA: 0s - loss: 0.2783 - accuracy: 0.8867\n",
      "Epoch 00074: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 50us/sample - loss: 0.2766 - accuracy: 0.8874 - val_loss: 0.3555 - val_accuracy: 0.8577\n",
      "Epoch 75/200\n",
      "3968/4131 [===========================>..] - ETA: 0s - loss: 0.2823 - accuracy: 0.8810\n",
      "Epoch 00075: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.2835 - accuracy: 0.8802 - val_loss: 0.2674 - val_accuracy: 0.8838\n",
      "Epoch 76/200\n",
      "3744/4131 [==========================>...] - ETA: 0s - loss: 0.2495 - accuracy: 0.8913\n",
      "Epoch 00076: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 51us/sample - loss: 0.2579 - accuracy: 0.8884 - val_loss: 0.1972 - val_accuracy: 0.9400\n",
      "Epoch 77/200\n",
      "4032/4131 [============================>.] - ETA: 0s - loss: 0.2414 - accuracy: 0.8983\n",
      "Epoch 00077: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.2413 - accuracy: 0.8983 - val_loss: 0.2185 - val_accuracy: 0.9119\n",
      "Epoch 78/200\n",
      "3968/4131 [===========================>..] - ETA: 0s - loss: 0.2637 - accuracy: 0.8871\n",
      "Epoch 00078: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.2618 - accuracy: 0.8872 - val_loss: 0.1409 - val_accuracy: 0.9603\n",
      "Epoch 79/200\n",
      "3776/4131 [==========================>...] - ETA: 0s - loss: 0.2172 - accuracy: 0.9084\n",
      "Epoch 00079: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 50us/sample - loss: 0.2212 - accuracy: 0.9075 - val_loss: 0.1114 - val_accuracy: 0.9710\n",
      "Epoch 80/200\n",
      "4128/4131 [============================>.] - ETA: 0s - loss: 0.2967 - accuracy: 0.8762\n",
      "Epoch 00080: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 46us/sample - loss: 0.2966 - accuracy: 0.8763 - val_loss: 0.4111 - val_accuracy: 0.8374\n",
      "Epoch 81/200\n",
      "3680/4131 [=========================>....] - ETA: 0s - loss: 0.2390 - accuracy: 0.8957\n",
      "Epoch 00081: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 52us/sample - loss: 0.2411 - accuracy: 0.8962 - val_loss: 0.5499 - val_accuracy: 0.7348\n",
      "Epoch 82/200\n",
      "3936/4131 [===========================>..] - ETA: 0s - loss: 0.2415 - accuracy: 0.9014\n",
      "Epoch 00082: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.2415 - accuracy: 0.9017 - val_loss: 0.3615 - val_accuracy: 0.8480\n",
      "Epoch 83/200\n",
      "3872/4131 [===========================>..] - ETA: 0s - loss: 0.3131 - accuracy: 0.8673\n",
      "Epoch 00083: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 50us/sample - loss: 0.3077 - accuracy: 0.8690 - val_loss: 0.6296 - val_accuracy: 0.6989\n",
      "Epoch 84/200\n",
      "3712/4131 [=========================>....] - ETA: 0s - loss: 0.2526 - accuracy: 0.8952\n",
      "Epoch 00084: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 51us/sample - loss: 0.2554 - accuracy: 0.8932 - val_loss: 0.4602 - val_accuracy: 0.7803\n",
      "Epoch 85/200\n",
      "3936/4131 [===========================>..] - ETA: 0s - loss: 0.2158 - accuracy: 0.9093\n",
      "Epoch 00085: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.2158 - accuracy: 0.9095 - val_loss: 0.3781 - val_accuracy: 0.8306\n",
      "Epoch 86/200\n",
      "3616/4131 [=========================>....] - ETA: 0s - loss: 0.2123 - accuracy: 0.9112\n",
      "Epoch 00086: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 52us/sample - loss: 0.2117 - accuracy: 0.9114 - val_loss: 0.1481 - val_accuracy: 0.9497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/200\n",
      "3744/4131 [==========================>...] - ETA: 0s - loss: 0.2311 - accuracy: 0.8988\n",
      "Epoch 00087: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 50us/sample - loss: 0.2316 - accuracy: 0.8983 - val_loss: 0.5504 - val_accuracy: 0.7299\n",
      "Epoch 88/200\n",
      "3840/4131 [==========================>...] - ETA: 0s - loss: 0.2629 - accuracy: 0.8935\n",
      "Epoch 00088: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 50us/sample - loss: 0.2629 - accuracy: 0.8932 - val_loss: 0.1962 - val_accuracy: 0.9264\n",
      "Epoch 89/200\n",
      "3712/4131 [=========================>....] - ETA: 0s - loss: 0.3785 - accuracy: 0.8421\n",
      "Epoch 00089: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 51us/sample - loss: 0.3636 - accuracy: 0.8489 - val_loss: 0.4696 - val_accuracy: 0.7822\n",
      "Epoch 90/200\n",
      "3968/4131 [===========================>..] - ETA: 0s - loss: 0.2574 - accuracy: 0.8931\n",
      "Epoch 00090: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.2573 - accuracy: 0.8916 - val_loss: 0.2194 - val_accuracy: 0.9197\n",
      "Epoch 91/200\n",
      "3712/4131 [=========================>....] - ETA: 0s - loss: 0.2117 - accuracy: 0.9124\n",
      "Epoch 00091: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 51us/sample - loss: 0.2106 - accuracy: 0.9138 - val_loss: 0.6700 - val_accuracy: 0.6583\n",
      "Epoch 92/200\n",
      "3904/4131 [===========================>..] - ETA: 0s - loss: 0.2664 - accuracy: 0.8876\n",
      "Epoch 00092: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.2627 - accuracy: 0.8901 - val_loss: 0.4857 - val_accuracy: 0.7783\n",
      "Epoch 93/200\n",
      "3840/4131 [==========================>...] - ETA: 0s - loss: 0.2077 - accuracy: 0.9146\n",
      "Epoch 00093: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 50us/sample - loss: 0.2055 - accuracy: 0.9155 - val_loss: 0.2527 - val_accuracy: 0.8993\n",
      "Epoch 94/200\n",
      "3936/4131 [===========================>..] - ETA: 0s - loss: 0.2229 - accuracy: 0.9090\n",
      "Epoch 00094: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 52us/sample - loss: 0.2207 - accuracy: 0.9107 - val_loss: 0.1355 - val_accuracy: 0.9545\n",
      "Epoch 95/200\n",
      "3936/4131 [===========================>..] - ETA: 0s - loss: 0.1831 - accuracy: 0.9261\n",
      "Epoch 00095: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 50us/sample - loss: 0.1830 - accuracy: 0.9269 - val_loss: 0.2402 - val_accuracy: 0.9022\n",
      "Epoch 96/200\n",
      "3840/4131 [==========================>...] - ETA: 0s - loss: 0.2025 - accuracy: 0.9164\n",
      "Epoch 00096: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 50us/sample - loss: 0.2038 - accuracy: 0.9158 - val_loss: 0.3504 - val_accuracy: 0.8354\n",
      "Epoch 97/200\n",
      "4096/4131 [============================>.] - ETA: 0s - loss: 0.2037 - accuracy: 0.9172\n",
      "Epoch 00097: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.2036 - accuracy: 0.9172 - val_loss: 0.3069 - val_accuracy: 0.8616\n",
      "Epoch 98/200\n",
      "3936/4131 [===========================>..] - ETA: 0s - loss: 0.2690 - accuracy: 0.8989\n",
      "Epoch 00098: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.2651 - accuracy: 0.8998 - val_loss: 0.3037 - val_accuracy: 0.8606\n",
      "Epoch 99/200\n",
      "4032/4131 [============================>.] - ETA: 0s - loss: 0.2151 - accuracy: 0.9144\n",
      "Epoch 00099: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.2155 - accuracy: 0.9136 - val_loss: 0.1970 - val_accuracy: 0.9351\n",
      "Epoch 100/200\n",
      "4064/4131 [============================>.] - ETA: 0s - loss: 0.2661 - accuracy: 0.8920\n",
      "Epoch 00100: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.2649 - accuracy: 0.8925 - val_loss: 0.1854 - val_accuracy: 0.9361\n",
      "Epoch 101/200\n",
      "3648/4131 [=========================>....] - ETA: 0s - loss: 0.1618 - accuracy: 0.9389\n",
      "Epoch 00101: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 52us/sample - loss: 0.1631 - accuracy: 0.9383 - val_loss: 0.2692 - val_accuracy: 0.8955\n",
      "Epoch 102/200\n",
      "4032/4131 [============================>.] - ETA: 0s - loss: 0.1788 - accuracy: 0.9281\n",
      "Epoch 00102: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.1770 - accuracy: 0.9291 - val_loss: 0.2900 - val_accuracy: 0.8722\n",
      "Epoch 103/200\n",
      "3808/4131 [==========================>...] - ETA: 0s - loss: 0.1795 - accuracy: 0.9259\n",
      "Epoch 00103: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 50us/sample - loss: 0.1914 - accuracy: 0.9218 - val_loss: 0.8499 - val_accuracy: 0.6108\n",
      "Epoch 104/200\n",
      "4064/4131 [============================>.] - ETA: 0s - loss: 0.2315 - accuracy: 0.9001\n",
      "Epoch 00104: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.2302 - accuracy: 0.9008 - val_loss: 0.2935 - val_accuracy: 0.8751\n",
      "Epoch 105/200\n",
      "4032/4131 [============================>.] - ETA: 0s - loss: 0.1469 - accuracy: 0.9422\n",
      "Epoch 00105: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.1464 - accuracy: 0.9424 - val_loss: 0.3341 - val_accuracy: 0.8587\n",
      "Epoch 106/200\n",
      "3968/4131 [===========================>..] - ETA: 0s - loss: 0.1409 - accuracy: 0.9466\n",
      "Epoch 00106: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.1446 - accuracy: 0.9450 - val_loss: 0.1876 - val_accuracy: 0.9274\n",
      "Epoch 107/200\n",
      "4064/4131 [============================>.] - ETA: 0s - loss: 0.3176 - accuracy: 0.8743\n",
      "Epoch 00107: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.3144 - accuracy: 0.8751 - val_loss: 0.3941 - val_accuracy: 0.8267\n",
      "Epoch 108/200\n",
      "3360/4131 [=======================>......] - ETA: 0s - loss: 0.2047 - accuracy: 0.9164\n",
      "Epoch 00108: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 55us/sample - loss: 0.2007 - accuracy: 0.9189 - val_loss: 0.3063 - val_accuracy: 0.8742\n",
      "Epoch 109/200\n",
      "4096/4131 [============================>.] - ETA: 0s - loss: 0.1745 - accuracy: 0.9280\n",
      "Epoch 00109: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.1751 - accuracy: 0.9276 - val_loss: 0.2652 - val_accuracy: 0.8683\n",
      "Epoch 110/200\n",
      "4032/4131 [============================>.] - ETA: 0s - loss: 0.1970 - accuracy: 0.9194\n",
      "Epoch 00110: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.1981 - accuracy: 0.9191 - val_loss: 0.1894 - val_accuracy: 0.9361\n",
      "Epoch 111/200\n",
      "3968/4131 [===========================>..] - ETA: 0s - loss: 0.1873 - accuracy: 0.9211\n",
      "Epoch 00111: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.1869 - accuracy: 0.9216 - val_loss: 0.2691 - val_accuracy: 0.8703\n",
      "Epoch 112/200\n",
      "3936/4131 [===========================>..] - ETA: 0s - loss: 0.1526 - accuracy: 0.9413\n",
      "Epoch 00112: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.1508 - accuracy: 0.9417 - val_loss: 0.1058 - val_accuracy: 0.9758\n",
      "Epoch 113/200\n",
      "3744/4131 [==========================>...] - ETA: 0s - loss: 0.1544 - accuracy: 0.9412\n",
      "Epoch 00113: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 50us/sample - loss: 0.1534 - accuracy: 0.9409 - val_loss: 0.3641 - val_accuracy: 0.8470\n",
      "Epoch 114/200\n",
      "4064/4131 [============================>.] - ETA: 0s - loss: 0.1830 - accuracy: 0.9250\n",
      "Epoch 00114: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.1816 - accuracy: 0.9252 - val_loss: 0.1539 - val_accuracy: 0.9429\n",
      "Epoch 115/200\n",
      "3648/4131 [=========================>....] - ETA: 0s - loss: 0.1764 - accuracy: 0.9265\n",
      "Epoch 00115: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 56us/sample - loss: 0.1756 - accuracy: 0.9274 - val_loss: 0.2785 - val_accuracy: 0.8761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/200\n",
      "3840/4131 [==========================>...] - ETA: 0s - loss: 0.1480 - accuracy: 0.9383\n",
      "Epoch 00116: val_loss did not improve from 0.09174\n",
      "4131/4131 [==============================] - 0s 70us/sample - loss: 0.1503 - accuracy: 0.9383 - val_loss: 0.2377 - val_accuracy: 0.9061\n",
      "Epoch 117/200\n",
      "3584/4131 [=========================>....] - ETA: 0s - loss: 0.1600 - accuracy: 0.9314 ETA: 0s - loss: 0.1735 - accuracy: 0.\n",
      "Epoch 00117: val_loss improved from 0.09174 to 0.05836, saving model to best_model_2.h5\n",
      "4131/4131 [==============================] - 0s 77us/sample - loss: 0.1530 - accuracy: 0.9346 - val_loss: 0.0584 - val_accuracy: 0.9864\n",
      "Epoch 118/200\n",
      "3872/4131 [===========================>..] - ETA: 0s - loss: 0.1470 - accuracy: 0.9442\n",
      "Epoch 00118: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 64us/sample - loss: 0.1478 - accuracy: 0.9448 - val_loss: 0.2305 - val_accuracy: 0.9051\n",
      "Epoch 119/200\n",
      "3200/4131 [======================>.......] - ETA: 0s - loss: 0.2054 - accuracy: 0.9166\n",
      "Epoch 00119: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 55us/sample - loss: 0.1932 - accuracy: 0.9201 - val_loss: 0.1681 - val_accuracy: 0.9313\n",
      "Epoch 120/200\n",
      "3360/4131 [=======================>......] - ETA: 0s - loss: 0.1369 - accuracy: 0.9449\n",
      "Epoch 00120: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 60us/sample - loss: 0.1332 - accuracy: 0.9475 - val_loss: 0.1457 - val_accuracy: 0.9468\n",
      "Epoch 121/200\n",
      "3904/4131 [===========================>..] - ETA: 0s - loss: 0.1547 - accuracy: 0.9367\n",
      "Epoch 00121: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 63us/sample - loss: 0.1539 - accuracy: 0.9380 - val_loss: 0.6221 - val_accuracy: 0.7483\n",
      "Epoch 122/200\n",
      "3808/4131 [==========================>...] - ETA: 0s - loss: 0.1446 - accuracy: 0.9412\n",
      "Epoch 00122: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.1444 - accuracy: 0.9412 - val_loss: 0.1232 - val_accuracy: 0.9584\n",
      "Epoch 123/200\n",
      "3808/4131 [==========================>...] - ETA: 0s - loss: 0.1887 - accuracy: 0.9194\n",
      "Epoch 00123: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 50us/sample - loss: 0.1844 - accuracy: 0.9221 - val_loss: 0.1771 - val_accuracy: 0.9303\n",
      "Epoch 124/200\n",
      "3968/4131 [===========================>..] - ETA: 0s - loss: 0.2175 - accuracy: 0.9115\n",
      "Epoch 00124: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.2184 - accuracy: 0.9104 - val_loss: 0.9489 - val_accuracy: 0.6534\n",
      "Epoch 125/200\n",
      "4000/4131 [============================>.] - ETA: 0s - loss: 0.1927 - accuracy: 0.9275\n",
      "Epoch 00125: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.1890 - accuracy: 0.9288 - val_loss: 0.2562 - val_accuracy: 0.8887\n",
      "Epoch 126/200\n",
      "3776/4131 [==========================>...] - ETA: 0s - loss: 0.1390 - accuracy: 0.9441\n",
      "Epoch 00126: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 50us/sample - loss: 0.1373 - accuracy: 0.9448 - val_loss: 0.0855 - val_accuracy: 0.9787\n",
      "Epoch 127/200\n",
      "4000/4131 [============================>.] - ETA: 0s - loss: 0.1353 - accuracy: 0.9480\n",
      "Epoch 00127: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.1361 - accuracy: 0.9477 - val_loss: 0.0916 - val_accuracy: 0.9681\n",
      "Epoch 128/200\n",
      "3872/4131 [===========================>..] - ETA: 0s - loss: 0.1462 - accuracy: 0.9411\n",
      "Epoch 00128: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.1440 - accuracy: 0.9421 - val_loss: 0.1283 - val_accuracy: 0.9555\n",
      "Epoch 129/200\n",
      "4000/4131 [============================>.] - ETA: 0s - loss: 0.1833 - accuracy: 0.9315\n",
      "Epoch 00129: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.1808 - accuracy: 0.9329 - val_loss: 0.0860 - val_accuracy: 0.9700\n",
      "Epoch 130/200\n",
      "4064/4131 [============================>.] - ETA: 0s - loss: 0.1464 - accuracy: 0.9459\n",
      "Epoch 00130: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.1477 - accuracy: 0.9450 - val_loss: 0.3426 - val_accuracy: 0.8606\n",
      "Epoch 131/200\n",
      "3808/4131 [==========================>...] - ETA: 0s - loss: 0.1660 - accuracy: 0.9309\n",
      "Epoch 00131: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 50us/sample - loss: 0.1647 - accuracy: 0.9327 - val_loss: 0.3660 - val_accuracy: 0.8616\n",
      "Epoch 132/200\n",
      "4032/4131 [============================>.] - ETA: 0s - loss: 0.1356 - accuracy: 0.9464\n",
      "Epoch 00132: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.1348 - accuracy: 0.9472 - val_loss: 0.1690 - val_accuracy: 0.9390\n",
      "Epoch 133/200\n",
      "3776/4131 [==========================>...] - ETA: 0s - loss: 0.1065 - accuracy: 0.9582\n",
      "Epoch 00133: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 52us/sample - loss: 0.1095 - accuracy: 0.9562 - val_loss: 0.3017 - val_accuracy: 0.8693\n",
      "Epoch 134/200\n",
      "3968/4131 [===========================>..] - ETA: 0s - loss: 0.1013 - accuracy: 0.9647\n",
      "Epoch 00134: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.1020 - accuracy: 0.9649 - val_loss: 0.1177 - val_accuracy: 0.9535\n",
      "Epoch 135/200\n",
      "4032/4131 [============================>.] - ETA: 0s - loss: 0.1584 - accuracy: 0.9392\n",
      "Epoch 00135: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.1580 - accuracy: 0.9395 - val_loss: 0.0759 - val_accuracy: 0.9768\n",
      "Epoch 136/200\n",
      "3936/4131 [===========================>..] - ETA: 0s - loss: 0.1556 - accuracy: 0.9347\n",
      "Epoch 00136: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.1547 - accuracy: 0.9351 - val_loss: 0.1129 - val_accuracy: 0.9584\n",
      "Epoch 137/200\n",
      "2784/4131 [===================>..........] - ETA: 0s - loss: 0.1274 - accuracy: 0.9483\n",
      "Epoch 00137: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 46us/sample - loss: 0.1198 - accuracy: 0.9526 - val_loss: 0.1198 - val_accuracy: 0.9535\n",
      "Epoch 138/200\n",
      "3808/4131 [==========================>...] - ETA: 0s - loss: 0.1023 - accuracy: 0.9619\n",
      "Epoch 00138: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 50us/sample - loss: 0.1018 - accuracy: 0.9622 - val_loss: 0.1824 - val_accuracy: 0.9322\n",
      "Epoch 139/200\n",
      "4032/4131 [============================>.] - ETA: 0s - loss: 0.1406 - accuracy: 0.9437\n",
      "Epoch 00139: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.1390 - accuracy: 0.9448 - val_loss: 0.2346 - val_accuracy: 0.8935\n",
      "Epoch 140/200\n",
      "4064/4131 [============================>.] - ETA: 0s - loss: 0.1527 - accuracy: 0.9432\n",
      "Epoch 00140: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.1519 - accuracy: 0.9436 - val_loss: 0.2176 - val_accuracy: 0.9080\n",
      "Epoch 141/200\n",
      "3872/4131 [===========================>..] - ETA: 0s - loss: 0.1729 - accuracy: 0.9329\n",
      "Epoch 00141: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.1728 - accuracy: 0.9327 - val_loss: 0.1653 - val_accuracy: 0.9380\n",
      "Epoch 142/200\n",
      "3360/4131 [=======================>......] - ETA: 0s - loss: 0.1418 - accuracy: 0.9432\n",
      "Epoch 00142: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 54us/sample - loss: 0.1342 - accuracy: 0.9475 - val_loss: 0.4580 - val_accuracy: 0.7909\n",
      "Epoch 143/200\n",
      "3808/4131 [==========================>...] - ETA: 0s - loss: 0.0997 - accuracy: 0.9567\n",
      "Epoch 00143: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 51us/sample - loss: 0.0987 - accuracy: 0.9579 - val_loss: 0.6913 - val_accuracy: 0.7164\n",
      "Epoch 144/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4032/4131 [============================>.] - ETA: 0s - loss: 0.1153 - accuracy: 0.9571\n",
      "Epoch 00144: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.1141 - accuracy: 0.9574 - val_loss: 0.1996 - val_accuracy: 0.9187\n",
      "Epoch 145/200\n",
      "4032/4131 [============================>.] - ETA: 0s - loss: 0.2417 - accuracy: 0.9055\n",
      "Epoch 00145: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.2417 - accuracy: 0.9056 - val_loss: 0.1057 - val_accuracy: 0.9652\n",
      "Epoch 146/200\n",
      "3936/4131 [===========================>..] - ETA: 0s - loss: 0.1328 - accuracy: 0.9494\n",
      "Epoch 00146: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.1319 - accuracy: 0.9501 - val_loss: 0.1633 - val_accuracy: 0.9458\n",
      "Epoch 147/200\n",
      "4064/4131 [============================>.] - ETA: 0s - loss: 0.0826 - accuracy: 0.9705\n",
      "Epoch 00147: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.0838 - accuracy: 0.9697 - val_loss: 0.1665 - val_accuracy: 0.9351\n",
      "Epoch 148/200\n",
      "3776/4131 [==========================>...] - ETA: 0s - loss: 0.0951 - accuracy: 0.9632\n",
      "Epoch 00148: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 50us/sample - loss: 0.0961 - accuracy: 0.9637 - val_loss: 0.4107 - val_accuracy: 0.8209\n",
      "Epoch 149/200\n",
      "4000/4131 [============================>.] - ETA: 0s - loss: 0.1157 - accuracy: 0.9542\n",
      "Epoch 00149: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.1132 - accuracy: 0.9557 - val_loss: 0.1009 - val_accuracy: 0.9593\n",
      "Epoch 150/200\n",
      "3968/4131 [===========================>..] - ETA: 0s - loss: 0.0911 - accuracy: 0.9652\n",
      "Epoch 00150: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.0928 - accuracy: 0.9647 - val_loss: 0.1013 - val_accuracy: 0.9652\n",
      "Epoch 151/200\n",
      "4096/4131 [============================>.] - ETA: 0s - loss: 0.0846 - accuracy: 0.9666\n",
      "Epoch 00151: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 46us/sample - loss: 0.0856 - accuracy: 0.9659 - val_loss: 0.2314 - val_accuracy: 0.9071\n",
      "Epoch 152/200\n",
      "4064/4131 [============================>.] - ETA: 0s - loss: 0.0948 - accuracy: 0.9599\n",
      "Epoch 00152: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.0954 - accuracy: 0.9598 - val_loss: 0.1539 - val_accuracy: 0.9351\n",
      "Epoch 153/200\n",
      "3872/4131 [===========================>..] - ETA: 0s - loss: 0.1782 - accuracy: 0.9403\n",
      "Epoch 00153: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.1741 - accuracy: 0.9409 - val_loss: 0.2065 - val_accuracy: 0.9235\n",
      "Epoch 154/200\n",
      "2816/4131 [===================>..........] - ETA: 0s - loss: 0.1345 - accuracy: 0.9499\n",
      "Epoch 00154: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 45us/sample - loss: 0.1409 - accuracy: 0.9475 - val_loss: 0.3549 - val_accuracy: 0.8693\n",
      "Epoch 155/200\n",
      "4000/4131 [============================>.] - ETA: 0s - loss: 0.1393 - accuracy: 0.9475\n",
      "Epoch 00155: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.1379 - accuracy: 0.9477 - val_loss: 0.2475 - val_accuracy: 0.8848\n",
      "Epoch 156/200\n",
      "3968/4131 [===========================>..] - ETA: 0s - loss: 0.1363 - accuracy: 0.9493\n",
      "Epoch 00156: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.1396 - accuracy: 0.9492 - val_loss: 0.1842 - val_accuracy: 0.9351\n",
      "Epoch 157/200\n",
      "4064/4131 [============================>.] - ETA: 0s - loss: 0.0952 - accuracy: 0.9631\n",
      "Epoch 00157: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.0962 - accuracy: 0.9622 - val_loss: 1.0106 - val_accuracy: 0.5934\n",
      "Epoch 158/200\n",
      "3968/4131 [===========================>..] - ETA: 0s - loss: 0.2145 - accuracy: 0.9168\n",
      "Epoch 00158: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.2089 - accuracy: 0.9189 - val_loss: 0.2660 - val_accuracy: 0.8993\n",
      "Epoch 159/200\n",
      "4064/4131 [============================>.] - ETA: 0s - loss: 0.0893 - accuracy: 0.9688\n",
      "Epoch 00159: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.0896 - accuracy: 0.9683 - val_loss: 0.1598 - val_accuracy: 0.9487\n",
      "Epoch 160/200\n",
      "4032/4131 [============================>.] - ETA: 0s - loss: 0.0831 - accuracy: 0.9663\n",
      "Epoch 00160: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.0840 - accuracy: 0.9659 - val_loss: 0.2316 - val_accuracy: 0.9284\n",
      "Epoch 161/200\n",
      "3904/4131 [===========================>..] - ETA: 0s - loss: 0.0759 - accuracy: 0.9741\n",
      "Epoch 00161: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.0753 - accuracy: 0.9743 - val_loss: 0.3121 - val_accuracy: 0.8761\n",
      "Epoch 162/200\n",
      "2816/4131 [===================>..........] - ETA: 0s - loss: 0.0984 - accuracy: 0.9624\n",
      "Epoch 00162: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 45us/sample - loss: 0.1035 - accuracy: 0.9596 - val_loss: 0.2820 - val_accuracy: 0.8838\n",
      "Epoch 163/200\n",
      "3936/4131 [===========================>..] - ETA: 0s - loss: 0.1920 - accuracy: 0.9360\n",
      "Epoch 00163: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.2147 - accuracy: 0.9303 - val_loss: 0.8364 - val_accuracy: 0.6960\n",
      "Epoch 164/200\n",
      "4096/4131 [============================>.] - ETA: 0s - loss: 0.2117 - accuracy: 0.9177\n",
      "Epoch 00164: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.2110 - accuracy: 0.9179 - val_loss: 0.2324 - val_accuracy: 0.9197\n",
      "Epoch 165/200\n",
      "2816/4131 [===================>..........] - ETA: 0s - loss: 0.1609 - accuracy: 0.9318\n",
      "Epoch 00165: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 46us/sample - loss: 0.1533 - accuracy: 0.9388 - val_loss: 0.1948 - val_accuracy: 0.9148\n",
      "Epoch 166/200\n",
      "3840/4131 [==========================>...] - ETA: 0s - loss: 0.0914 - accuracy: 0.9667\n",
      "Epoch 00166: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.0934 - accuracy: 0.9659 - val_loss: 0.2049 - val_accuracy: 0.9206\n",
      "Epoch 167/200\n",
      "2816/4131 [===================>..........] - ETA: 0s - loss: 0.1343 - accuracy: 0.9513\n",
      "Epoch 00167: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 45us/sample - loss: 0.1204 - accuracy: 0.9555 - val_loss: 0.3014 - val_accuracy: 0.8761\n",
      "Epoch 168/200\n",
      "3904/4131 [===========================>..] - ETA: 0s - loss: 0.0793 - accuracy: 0.9716\n",
      "Epoch 00168: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.0798 - accuracy: 0.9714 - val_loss: 0.2400 - val_accuracy: 0.9003\n",
      "Epoch 169/200\n",
      "4064/4131 [============================>.] - ETA: 0s - loss: 0.0827 - accuracy: 0.9710\n",
      "Epoch 00169: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.0821 - accuracy: 0.9712 - val_loss: 0.1242 - val_accuracy: 0.9574\n",
      "Epoch 170/200\n",
      "3808/4131 [==========================>...] - ETA: 0s - loss: 0.0806 - accuracy: 0.9714\n",
      "Epoch 00170: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 53us/sample - loss: 0.0877 - accuracy: 0.9693 - val_loss: 0.2197 - val_accuracy: 0.9177\n",
      "Epoch 171/200\n",
      "3808/4131 [==========================>...] - ETA: 0s - loss: 0.0763 - accuracy: 0.9722\n",
      "Epoch 00171: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.0739 - accuracy: 0.9734 - val_loss: 0.1438 - val_accuracy: 0.9564\n",
      "Epoch 172/200\n",
      "2784/4131 [===================>..........] - ETA: 0s - loss: 0.0688 - accuracy: 0.9741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00172: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 45us/sample - loss: 0.0744 - accuracy: 0.9705 - val_loss: 0.1949 - val_accuracy: 0.9119\n",
      "Epoch 173/200\n",
      "4000/4131 [============================>.] - ETA: 0s - loss: 0.0750 - accuracy: 0.9700\n",
      "Epoch 00173: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.0777 - accuracy: 0.9688 - val_loss: 0.5107 - val_accuracy: 0.7967\n",
      "Epoch 174/200\n",
      "4096/4131 [============================>.] - ETA: 0s - loss: 0.1068 - accuracy: 0.9597\n",
      "Epoch 00174: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.1062 - accuracy: 0.9601 - val_loss: 0.0620 - val_accuracy: 0.9787\n",
      "Epoch 175/200\n",
      "4128/4131 [============================>.] - ETA: 0s - loss: 0.0841 - accuracy: 0.9685\n",
      "Epoch 00175: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 46us/sample - loss: 0.0841 - accuracy: 0.9685 - val_loss: 0.3643 - val_accuracy: 0.8742\n",
      "Epoch 176/200\n",
      "3712/4131 [=========================>....] - ETA: 0s - loss: 0.1532 - accuracy: 0.9397\n",
      "Epoch 00176: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 51us/sample - loss: 0.1487 - accuracy: 0.9421 - val_loss: 0.1975 - val_accuracy: 0.9206\n",
      "Epoch 177/200\n",
      "4032/4131 [============================>.] - ETA: 0s - loss: 0.1459 - accuracy: 0.9430\n",
      "Epoch 00177: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.1454 - accuracy: 0.9434 - val_loss: 0.1808 - val_accuracy: 0.9419\n",
      "Epoch 178/200\n",
      "4000/4131 [============================>.] - ETA: 0s - loss: 0.1443 - accuracy: 0.9488\n",
      "Epoch 00178: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.1428 - accuracy: 0.9496 - val_loss: 0.2986 - val_accuracy: 0.8790\n",
      "Epoch 179/200\n",
      "4000/4131 [============================>.] - ETA: 0s - loss: 0.0659 - accuracy: 0.9730\n",
      "Epoch 00179: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.0668 - accuracy: 0.9726 - val_loss: 0.1634 - val_accuracy: 0.9439\n",
      "Epoch 180/200\n",
      "2848/4131 [===================>..........] - ETA: 0s - loss: 0.1323 - accuracy: 0.9431\n",
      "Epoch 00180: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 46us/sample - loss: 0.1300 - accuracy: 0.9460 - val_loss: 0.0784 - val_accuracy: 0.9768\n",
      "Epoch 181/200\n",
      "3840/4131 [==========================>...] - ETA: 0s - loss: 0.0667 - accuracy: 0.9789\n",
      "Epoch 00181: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.0657 - accuracy: 0.9792 - val_loss: 0.1706 - val_accuracy: 0.9351\n",
      "Epoch 182/200\n",
      "4032/4131 [============================>.] - ETA: 0s - loss: 0.0702 - accuracy: 0.9747\n",
      "Epoch 00182: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.0703 - accuracy: 0.9746 - val_loss: 0.1114 - val_accuracy: 0.9652\n",
      "Epoch 183/200\n",
      "3872/4131 [===========================>..] - ETA: 0s - loss: 0.1007 - accuracy: 0.9602\n",
      "Epoch 00183: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 50us/sample - loss: 0.1012 - accuracy: 0.9601 - val_loss: 0.1727 - val_accuracy: 0.9342\n",
      "Epoch 184/200\n",
      "3904/4131 [===========================>..] - ETA: 0s - loss: 0.1529 - accuracy: 0.9424\n",
      "Epoch 00184: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.1509 - accuracy: 0.9424 - val_loss: 0.3237 - val_accuracy: 0.8596\n",
      "Epoch 185/200\n",
      "4064/4131 [============================>.] - ETA: 0s - loss: 0.0805 - accuracy: 0.9685\n",
      "Epoch 00185: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.0815 - accuracy: 0.9683 - val_loss: 0.0994 - val_accuracy: 0.9739\n",
      "Epoch 186/200\n",
      "3840/4131 [==========================>...] - ETA: 0s - loss: 0.0638 - accuracy: 0.9755\n",
      "Epoch 00186: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.0623 - accuracy: 0.9763 - val_loss: 0.1016 - val_accuracy: 0.9652\n",
      "Epoch 187/200\n",
      "4032/4131 [============================>.] - ETA: 0s - loss: 0.0859 - accuracy: 0.9663\n",
      "Epoch 00187: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.0844 - accuracy: 0.9671 - val_loss: 0.0964 - val_accuracy: 0.9622\n",
      "Epoch 188/200\n",
      "3904/4131 [===========================>..] - ETA: 0s - loss: 0.0810 - accuracy: 0.9688\n",
      "Epoch 00188: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 49us/sample - loss: 0.0822 - accuracy: 0.9678 - val_loss: 0.3469 - val_accuracy: 0.8645\n",
      "Epoch 189/200\n",
      "4096/4131 [============================>.] - ETA: 0s - loss: 0.1196 - accuracy: 0.9531\n",
      "Epoch 00189: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.1198 - accuracy: 0.9530 - val_loss: 0.3568 - val_accuracy: 0.8345\n",
      "Epoch 190/200\n",
      "2816/4131 [===================>..........] - ETA: 0s - loss: 0.1337 - accuracy: 0.9471\n",
      "Epoch 00190: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 45us/sample - loss: 0.1508 - accuracy: 0.9419 - val_loss: 0.2751 - val_accuracy: 0.9167\n",
      "Epoch 191/200\n",
      "3936/4131 [===========================>..] - ETA: 0s - loss: 0.1903 - accuracy: 0.9319\n",
      "Epoch 00191: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 48us/sample - loss: 0.1853 - accuracy: 0.9337 - val_loss: 0.1512 - val_accuracy: 0.9477\n",
      "Epoch 192/200\n",
      "2784/4131 [===================>..........] - ETA: 0s - loss: 0.0485 - accuracy: 0.9842\n",
      "Epoch 00192: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 45us/sample - loss: 0.0566 - accuracy: 0.9806 - val_loss: 0.2074 - val_accuracy: 0.9187\n",
      "Epoch 193/200\n",
      "3552/4131 [========================>.....] - ETA: 0s - loss: 0.0702 - accuracy: 0.9755\n",
      "Epoch 00193: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 57us/sample - loss: 0.0689 - accuracy: 0.9753 - val_loss: 0.2597 - val_accuracy: 0.8974\n",
      "Epoch 194/200\n",
      "4096/4131 [============================>.] - ETA: 0s - loss: 0.0465 - accuracy: 0.9839\n",
      "Epoch 00194: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 63us/sample - loss: 0.0468 - accuracy: 0.9835 - val_loss: 0.1166 - val_accuracy: 0.9613\n",
      "Epoch 195/200\n",
      "3904/4131 [===========================>..] - ETA: 0s - loss: 0.0699 - accuracy: 0.9736\n",
      "Epoch 00195: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 66us/sample - loss: 0.0683 - accuracy: 0.9741 - val_loss: 0.1673 - val_accuracy: 0.9371\n",
      "Epoch 196/200\n",
      "4032/4131 [============================>.] - ETA: 0s - loss: 0.0648 - accuracy: 0.9747\n",
      "Epoch 00196: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 64us/sample - loss: 0.0653 - accuracy: 0.9746 - val_loss: 0.0806 - val_accuracy: 0.9710\n",
      "Epoch 197/200\n",
      "3392/4131 [=======================>......] - ETA: 0s - loss: 0.0684 - accuracy: 0.9767\n",
      "Epoch 00197: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 53us/sample - loss: 0.0746 - accuracy: 0.9741 - val_loss: 0.1918 - val_accuracy: 0.9264\n",
      "Epoch 198/200\n",
      "3552/4131 [========================>.....] - ETA: 0s - loss: 0.2272 - accuracy: 0.9229\n",
      "Epoch 00198: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 56us/sample - loss: 0.2210 - accuracy: 0.9237 - val_loss: 0.7216 - val_accuracy: 0.7076\n",
      "Epoch 199/200\n",
      "3904/4131 [===========================>..] - ETA: 0s - loss: 0.1325 - accuracy: 0.9485\n",
      "Epoch 00199: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 61us/sample - loss: 0.1300 - accuracy: 0.9501 - val_loss: 0.2420 - val_accuracy: 0.9206\n",
      "Epoch 200/200\n",
      "4096/4131 [============================>.] - ETA: 0s - loss: 0.2859 - accuracy: 0.9065\n",
      "Epoch 00200: val_loss did not improve from 0.05836\n",
      "4131/4131 [==============================] - 0s 47us/sample - loss: 0.2835 - accuracy: 0.9073 - val_loss: 0.1070 - val_accuracy: 0.9690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on Fold 2\n",
      "# Getting train data set up\n",
      "# Getting test data set up\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 2400 samples, validate on 600 samples\n",
      "Epoch 1/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 194.0505 - mse: 194.0505\n",
      "Epoch 00001: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 2s 875us/sample - loss: 193.3361 - mse: 193.3361 - val_loss: 126.9743 - val_mse: 126.9743\n",
      "Epoch 2/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 170.0336 - mse: 170.0336\n",
      "Epoch 00002: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 435us/sample - loss: 170.8361 - mse: 170.8361 - val_loss: 119.1447 - val_mse: 119.1447\n",
      "Epoch 3/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 179.0253 - mse: 179.0254\n",
      "Epoch 00003: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 440us/sample - loss: 177.7054 - mse: 177.7054 - val_loss: 115.2302 - val_mse: 115.2302\n",
      "Epoch 4/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 174.4036 - mse: 174.4037\n",
      "Epoch 00004: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 446us/sample - loss: 171.4833 - mse: 171.4833 - val_loss: 151.6120 - val_mse: 151.6120\n",
      "Epoch 5/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 173.6265 - mse: 173.6265\n",
      "Epoch 00005: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 509us/sample - loss: 171.1681 - mse: 171.1682 - val_loss: 125.0101 - val_mse: 125.0101\n",
      "Epoch 6/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 171.0731 - mse: 171.0731\n",
      "Epoch 00006: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 494us/sample - loss: 169.4743 - mse: 169.4743 - val_loss: 128.3587 - val_mse: 128.3587\n",
      "Epoch 7/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 169.7647 - mse: 169.7647\n",
      "Epoch 00007: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 455us/sample - loss: 169.1899 - mse: 169.1899 - val_loss: 118.6707 - val_mse: 118.6707\n",
      "Epoch 8/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 173.0717 - mse: 173.0717\n",
      "Epoch 00008: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 452us/sample - loss: 171.5817 - mse: 171.5817 - val_loss: 118.5809 - val_mse: 118.5809\n",
      "Epoch 9/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 164.4070 - mse: 164.4070\n",
      "Epoch 00009: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 451us/sample - loss: 162.7711 - mse: 162.7711 - val_loss: 114.5641 - val_mse: 114.5640\n",
      "Epoch 10/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.3219 - mse: 162.3219\n",
      "Epoch 00010: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 456us/sample - loss: 166.3480 - mse: 166.3480 - val_loss: 116.2285 - val_mse: 116.2285\n",
      "Epoch 11/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.2498 - mse: 163.2498\n",
      "Epoch 00011: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 446us/sample - loss: 160.4799 - mse: 160.4799 - val_loss: 119.3131 - val_mse: 119.3131\n",
      "Epoch 12/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 160.2601 - mse: 160.2601\n",
      "Epoch 00012: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 436us/sample - loss: 158.7437 - mse: 158.7437 - val_loss: 115.1717 - val_mse: 115.1717\n",
      "Epoch 13/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 157.2678 - mse: 157.2678\n",
      "Epoch 00013: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 490us/sample - loss: 161.9270 - mse: 161.9270 - val_loss: 123.1349 - val_mse: 123.1349\n",
      "Epoch 14/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 157.3130 - mse: 157.3130\n",
      "Epoch 00014: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 597us/sample - loss: 157.1816 - mse: 157.1816 - val_loss: 117.6678 - val_mse: 117.6678\n",
      "Epoch 15/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 157.7279 - mse: 157.7279\n",
      "Epoch 00015: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 583us/sample - loss: 158.0445 - mse: 158.0445 - val_loss: 114.9847 - val_mse: 114.9847\n",
      "Epoch 16/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 154.6680 - mse: 154.6680\n",
      "Epoch 00016: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 2s 713us/sample - loss: 154.3836 - mse: 154.3837 - val_loss: 137.5497 - val_mse: 137.5497\n",
      "Epoch 17/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 158.6229 - mse: 158.6229\n",
      "Epoch 00017: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 615us/sample - loss: 157.5641 - mse: 157.5641 - val_loss: 127.3403 - val_mse: 127.3403\n",
      "Epoch 18/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 149.3642 - mse: 149.3642\n",
      "Epoch 00018: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 2s 625us/sample - loss: 155.5678 - mse: 155.5679 - val_loss: 118.6868 - val_mse: 118.6869\n",
      "Epoch 19/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 155.3561 - mse: 155.3562\n",
      "Epoch 00019: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 589us/sample - loss: 154.7604 - mse: 154.7604 - val_loss: 120.3399 - val_mse: 120.3399\n",
      "Epoch 20/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 150.6078 - mse: 150.6077\n",
      "Epoch 00020: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 564us/sample - loss: 150.8336 - mse: 150.8336 - val_loss: 115.2498 - val_mse: 115.2498\n",
      "Epoch 21/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 157.9448 - mse: 157.9448\n",
      "Epoch 00021: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 543us/sample - loss: 156.8648 - mse: 156.8648 - val_loss: 125.8479 - val_mse: 125.8479\n",
      "Epoch 22/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 164.8523 - mse: 164.8523\n",
      "Epoch 00022: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 556us/sample - loss: 164.2120 - mse: 164.2120 - val_loss: 125.4184 - val_mse: 125.4184\n",
      "Epoch 23/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 163.1784 - mse: 163.1784\n",
      "Epoch 00023: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 535us/sample - loss: 161.6902 - mse: 161.6902 - val_loss: 129.6946 - val_mse: 129.6946\n",
      "Epoch 24/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 160.9697 - mse: 160.9697\n",
      "Epoch 00024: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 543us/sample - loss: 162.5911 - mse: 162.5911 - val_loss: 123.8219 - val_mse: 123.8220\n",
      "Epoch 25/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 161.8500 - mse: 161.8500\n",
      "Epoch 00025: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 540us/sample - loss: 160.6202 - mse: 160.6202 - val_loss: 118.0388 - val_mse: 118.0388\n",
      "Epoch 26/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 156.9401 - mse: 156.9401\n",
      "Epoch 00026: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 562us/sample - loss: 157.1010 - mse: 157.1010 - val_loss: 115.4480 - val_mse: 115.4480\n",
      "Epoch 27/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 157.1411 - mse: 157.1411\n",
      "Epoch 00027: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 548us/sample - loss: 153.4082 - mse: 153.4082 - val_loss: 117.7459 - val_mse: 117.7459\n",
      "Epoch 28/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 159.8561 - mse: 159.8560\n",
      "Epoch 00028: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 535us/sample - loss: 157.5467 - mse: 157.5466 - val_loss: 138.1171 - val_mse: 138.1171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 150.1600 - mse: 150.1600\n",
      "Epoch 00029: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 542us/sample - loss: 149.1490 - mse: 149.1489 - val_loss: 161.9053 - val_mse: 161.9053\n",
      "Epoch 30/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 157.9519 - mse: 157.9519\n",
      "Epoch 00030: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 597us/sample - loss: 156.9549 - mse: 156.9549 - val_loss: 128.1807 - val_mse: 128.1807\n",
      "Epoch 31/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 160.4005 - mse: 160.4005\n",
      "Epoch 00031: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 590us/sample - loss: 158.4233 - mse: 158.4232 - val_loss: 121.2198 - val_mse: 121.2198\n",
      "Epoch 32/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 152.1921 - mse: 152.1921\n",
      "Epoch 00032: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 544us/sample - loss: 151.2823 - mse: 151.2823 - val_loss: 116.1060 - val_mse: 116.1060\n",
      "Epoch 33/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 156.4264 - mse: 156.4264\n",
      "Epoch 00033: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 556us/sample - loss: 155.0210 - mse: 155.0209 - val_loss: 112.2592 - val_mse: 112.2592\n",
      "Epoch 34/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 150.9893 - mse: 150.9893\n",
      "Epoch 00034: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 527us/sample - loss: 150.7855 - mse: 150.7855 - val_loss: 125.0600 - val_mse: 125.0600\n",
      "Epoch 35/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 156.4937 - mse: 156.4937\n",
      "Epoch 00035: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 534us/sample - loss: 155.4156 - mse: 155.4157 - val_loss: 114.8519 - val_mse: 114.8519\n",
      "Epoch 36/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 147.6298 - mse: 147.6298\n",
      "Epoch 00036: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 536us/sample - loss: 146.9012 - mse: 146.9012 - val_loss: 155.2498 - val_mse: 155.2498\n",
      "Epoch 37/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 148.7903 - mse: 148.7903\n",
      "Epoch 00037: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 506us/sample - loss: 148.3255 - mse: 148.3255 - val_loss: 135.2886 - val_mse: 135.2885\n",
      "Epoch 38/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 149.1226 - mse: 149.1226\n",
      "Epoch 00038: val_loss did not improve from 111.46321\n",
      "2400/2400 [==============================] - 1s 514us/sample - loss: 148.5695 - mse: 148.5695 - val_loss: 117.7340 - val_mse: 117.7340\n",
      "Epoch 39/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 148.4758 - mse: 148.4758\n",
      "Epoch 00039: val_loss improved from 111.46321 to 111.12089, saving model to best_model_1.h5\n",
      "2400/2400 [==============================] - 1s 584us/sample - loss: 148.5528 - mse: 148.5528 - val_loss: 111.1209 - val_mse: 111.1209\n",
      "Epoch 40/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 155.8493 - mse: 155.8492\n",
      "Epoch 00040: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 520us/sample - loss: 155.0432 - mse: 155.0432 - val_loss: 121.4552 - val_mse: 121.4552\n",
      "Epoch 41/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 158.7395 - mse: 158.7395\n",
      "Epoch 00041: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 554us/sample - loss: 163.8427 - mse: 163.8426 - val_loss: 123.8722 - val_mse: 123.8722\n",
      "Epoch 42/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 161.4651 - mse: 161.4651\n",
      "Epoch 00042: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 575us/sample - loss: 160.2501 - mse: 160.2501 - val_loss: 119.7790 - val_mse: 119.7790\n",
      "Epoch 43/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 155.4984 - mse: 155.4984\n",
      "Epoch 00043: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 578us/sample - loss: 156.5608 - mse: 156.5608 - val_loss: 128.5139 - val_mse: 128.5139\n",
      "Epoch 44/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 155.2218 - mse: 155.2217\n",
      "Epoch 00044: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 584us/sample - loss: 153.7968 - mse: 153.7968 - val_loss: 124.3979 - val_mse: 124.3979\n",
      "Epoch 45/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 142.1069 - mse: 142.1069\n",
      "Epoch 00045: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 583us/sample - loss: 147.3003 - mse: 147.3003 - val_loss: 161.0579 - val_mse: 161.0579\n",
      "Epoch 46/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 157.8771 - mse: 157.8771\n",
      "Epoch 00046: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 542us/sample - loss: 159.1585 - mse: 159.1585 - val_loss: 124.8010 - val_mse: 124.8009\n",
      "Epoch 47/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 158.9427 - mse: 158.9427\n",
      "Epoch 00047: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 555us/sample - loss: 156.2802 - mse: 156.2802 - val_loss: 119.8621 - val_mse: 119.8621\n",
      "Epoch 48/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 153.4071 - mse: 153.4071- ETA: 0s - loss: 160.4880 - m\n",
      "Epoch 00048: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 540us/sample - loss: 152.1936 - mse: 152.1936 - val_loss: 119.2066 - val_mse: 119.2066\n",
      "Epoch 49/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 154.4561 - mse: 154.4561- ETA: 0s - loss: 180.3755 - m\n",
      "Epoch 00049: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 547us/sample - loss: 152.4613 - mse: 152.4613 - val_loss: 127.1597 - val_mse: 127.1597\n",
      "Epoch 50/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 150.7993 - mse: 150.7993\n",
      "Epoch 00050: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 544us/sample - loss: 148.4081 - mse: 148.4081 - val_loss: 111.3116 - val_mse: 111.3116\n",
      "Epoch 51/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 148.4239 - mse: 148.4239\n",
      "Epoch 00051: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 542us/sample - loss: 147.0998 - mse: 147.0999 - val_loss: 113.2510 - val_mse: 113.2510\n",
      "Epoch 52/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 147.0492 - mse: 147.0492\n",
      "Epoch 00052: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 541us/sample - loss: 145.4368 - mse: 145.4368 - val_loss: 115.5228 - val_mse: 115.5228\n",
      "Epoch 53/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 149.0092 - mse: 149.0092\n",
      "Epoch 00053: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 542us/sample - loss: 146.9744 - mse: 146.9744 - val_loss: 118.3983 - val_mse: 118.3983\n",
      "Epoch 54/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 149.8591 - mse: 149.8591\n",
      "Epoch 00054: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 589us/sample - loss: 147.3734 - mse: 147.3734 - val_loss: 122.3478 - val_mse: 122.3478\n",
      "Epoch 55/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 152.6943 - mse: 152.6943\n",
      "Epoch 00055: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 593us/sample - loss: 152.9336 - mse: 152.9336 - val_loss: 113.0367 - val_mse: 113.0367\n",
      "Epoch 56/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 140.1629 - mse: 140.1629\n",
      "Epoch 00056: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 553us/sample - loss: 143.4164 - mse: 143.4164 - val_loss: 146.8151 - val_mse: 146.8151\n",
      "Epoch 57/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2272/2400 [===========================>..] - ETA: 0s - loss: 175.7833 - mse: 175.7833\n",
      "Epoch 00057: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 540us/sample - loss: 171.6074 - mse: 171.6074 - val_loss: 125.6576 - val_mse: 125.6576\n",
      "Epoch 58/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 164.8429 - mse: 164.8429\n",
      "Epoch 00058: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 535us/sample - loss: 164.1664 - mse: 164.1664 - val_loss: 121.1389 - val_mse: 121.1389\n",
      "Epoch 59/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 161.7513 - mse: 161.7513\n",
      "Epoch 00059: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 536us/sample - loss: 160.0232 - mse: 160.0232 - val_loss: 120.0125 - val_mse: 120.0125\n",
      "Epoch 60/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 163.2457 - mse: 163.2457\n",
      "Epoch 00060: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 526us/sample - loss: 160.2424 - mse: 160.2424 - val_loss: 118.2523 - val_mse: 118.2523\n",
      "Epoch 61/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 158.5077 - mse: 158.5077\n",
      "Epoch 00061: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 521us/sample - loss: 157.8307 - mse: 157.8307 - val_loss: 117.3606 - val_mse: 117.3606\n",
      "Epoch 62/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 156.4288 - mse: 156.4288\n",
      "Epoch 00062: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 528us/sample - loss: 155.9499 - mse: 155.9499 - val_loss: 120.1090 - val_mse: 120.1090\n",
      "Epoch 63/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 159.0725 - mse: 159.0726\n",
      "Epoch 00063: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 505us/sample - loss: 160.6246 - mse: 160.6247 - val_loss: 120.8187 - val_mse: 120.8187\n",
      "Epoch 64/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 149.8646 - mse: 149.8645\n",
      "Epoch 00064: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 521us/sample - loss: 156.3101 - mse: 156.3100 - val_loss: 117.7493 - val_mse: 117.7493\n",
      "Epoch 65/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 154.0479 - mse: 154.0479\n",
      "Epoch 00065: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 529us/sample - loss: 153.5325 - mse: 153.5325 - val_loss: 113.9790 - val_mse: 113.9790\n",
      "Epoch 66/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 149.9257 - mse: 149.9257\n",
      "Epoch 00066: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 567us/sample - loss: 150.6917 - mse: 150.6917 - val_loss: 140.1847 - val_mse: 140.1847\n",
      "Epoch 67/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 152.4930 - mse: 152.4930\n",
      "Epoch 00067: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 575us/sample - loss: 152.6527 - mse: 152.6527 - val_loss: 168.9097 - val_mse: 168.9097\n",
      "Epoch 68/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 167.7208 - mse: 167.7208\n",
      "Epoch 00068: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 584us/sample - loss: 167.2970 - mse: 167.2969 - val_loss: 121.9282 - val_mse: 121.9282\n",
      "Epoch 69/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.4296 - mse: 163.4296\n",
      "Epoch 00069: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 542us/sample - loss: 163.2575 - mse: 163.2575 - val_loss: 121.6315 - val_mse: 121.6315\n",
      "Epoch 70/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.8334 - mse: 163.8334\n",
      "Epoch 00070: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 532us/sample - loss: 161.8560 - mse: 161.8559 - val_loss: 121.7501 - val_mse: 121.7501\n",
      "Epoch 71/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.6056 - mse: 162.6056\n",
      "Epoch 00071: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 551us/sample - loss: 161.3103 - mse: 161.3103 - val_loss: 119.0371 - val_mse: 119.0371\n",
      "Epoch 72/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.5737 - mse: 162.5736\n",
      "Epoch 00072: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 546us/sample - loss: 161.0468 - mse: 161.0468 - val_loss: 117.0002 - val_mse: 117.0002\n",
      "Epoch 73/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 158.7395 - mse: 158.7396\n",
      "Epoch 00073: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 523us/sample - loss: 160.4332 - mse: 160.4332 - val_loss: 124.2412 - val_mse: 124.2412\n",
      "Epoch 74/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 164.6470 - mse: 164.6470\n",
      "Epoch 00074: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 528us/sample - loss: 163.1906 - mse: 163.1907 - val_loss: 123.1151 - val_mse: 123.1151\n",
      "Epoch 75/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 165.5733 - mse: 165.5733\n",
      "Epoch 00075: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 527us/sample - loss: 162.9216 - mse: 162.9216 - val_loss: 120.8163 - val_mse: 120.8163\n",
      "Epoch 76/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 160.2883 - mse: 160.2883\n",
      "Epoch 00076: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 521us/sample - loss: 158.1875 - mse: 158.1875 - val_loss: 119.8974 - val_mse: 119.8974\n",
      "Epoch 77/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 160.9122 - mse: 160.9122\n",
      "Epoch 00077: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 528us/sample - loss: 159.2752 - mse: 159.2752 - val_loss: 119.3733 - val_mse: 119.3733\n",
      "Epoch 78/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 155.7533 - mse: 155.7533\n",
      "Epoch 00078: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 582us/sample - loss: 155.2736 - mse: 155.2736 - val_loss: 119.4269 - val_mse: 119.4269\n",
      "Epoch 79/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 156.6164 - mse: 156.6164\n",
      "Epoch 00079: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 572us/sample - loss: 154.5005 - mse: 154.5005 - val_loss: 112.3408 - val_mse: 112.3408\n",
      "Epoch 80/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 154.1310 - mse: 154.1310\n",
      "Epoch 00080: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 591us/sample - loss: 153.5483 - mse: 153.5484 - val_loss: 111.5639 - val_mse: 111.5639\n",
      "Epoch 81/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 154.6469 - mse: 154.6469\n",
      "Epoch 00081: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 555us/sample - loss: 153.8026 - mse: 153.8026 - val_loss: 121.3529 - val_mse: 121.3529\n",
      "Epoch 82/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 156.0881 - mse: 156.0881\n",
      "Epoch 00082: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 542us/sample - loss: 153.7877 - mse: 153.7877 - val_loss: 120.6002 - val_mse: 120.6002\n",
      "Epoch 83/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 152.5734 - mse: 152.5734\n",
      "Epoch 00083: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 561us/sample - loss: 152.1806 - mse: 152.1806 - val_loss: 115.9661 - val_mse: 115.9661\n",
      "Epoch 84/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 162.5057 - mse: 162.5057\n",
      "Epoch 00084: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 562us/sample - loss: 161.4977 - mse: 161.4977 - val_loss: 126.6235 - val_mse: 126.6235\n",
      "Epoch 85/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 161.7441 - mse: 161.7440\n",
      "Epoch 00085: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 549us/sample - loss: 164.5371 - mse: 164.5371 - val_loss: 122.0885 - val_mse: 122.0885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 158.7357 - mse: 158.7357\n",
      "Epoch 00086: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 534us/sample - loss: 162.5544 - mse: 162.5544 - val_loss: 123.1490 - val_mse: 123.1490\n",
      "Epoch 87/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 157.9478 - mse: 157.9478\n",
      "Epoch 00087: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 541us/sample - loss: 164.2446 - mse: 164.2446 - val_loss: 122.0689 - val_mse: 122.0689\n",
      "Epoch 88/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 164.4492 - mse: 164.4492\n",
      "Epoch 00088: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 528us/sample - loss: 163.1261 - mse: 163.1261 - val_loss: 121.7797 - val_mse: 121.7797\n",
      "Epoch 89/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 162.2918 - mse: 162.2919\n",
      "Epoch 00089: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 528us/sample - loss: 161.2982 - mse: 161.2982 - val_loss: 122.1283 - val_mse: 122.1283\n",
      "Epoch 90/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.8678 - mse: 162.8678\n",
      "Epoch 00090: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 562us/sample - loss: 161.7174 - mse: 161.7174 - val_loss: 122.2159 - val_mse: 122.2159\n",
      "Epoch 91/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 160.3820 - mse: 160.3820\n",
      "Epoch 00091: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 577us/sample - loss: 161.2071 - mse: 161.2071 - val_loss: 121.9292 - val_mse: 121.9292\n",
      "Epoch 92/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 164.4433 - mse: 164.4433\n",
      "Epoch 00092: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 587us/sample - loss: 162.8932 - mse: 162.8932 - val_loss: 122.2915 - val_mse: 122.2914\n",
      "Epoch 93/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 159.9876 - mse: 159.9876\n",
      "Epoch 00093: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 543us/sample - loss: 161.8053 - mse: 161.8053 - val_loss: 122.3063 - val_mse: 122.3063\n",
      "Epoch 94/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.2263 - mse: 162.2263\n",
      "Epoch 00094: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 557us/sample - loss: 161.4534 - mse: 161.4534 - val_loss: 121.3556 - val_mse: 121.3556\n",
      "Epoch 95/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 160.2748 - mse: 160.2748\n",
      "Epoch 00095: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 547us/sample - loss: 161.8325 - mse: 161.8325 - val_loss: 126.8565 - val_mse: 126.8565\n",
      "Epoch 96/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 161.3209 - mse: 161.3209\n",
      "Epoch 00096: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 543us/sample - loss: 162.5076 - mse: 162.5076 - val_loss: 120.3805 - val_mse: 120.3805\n",
      "Epoch 97/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.5879 - mse: 163.5880\n",
      "Epoch 00097: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 560us/sample - loss: 161.3110 - mse: 161.3111 - val_loss: 120.5712 - val_mse: 120.5712\n",
      "Epoch 98/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 159.9130 - mse: 159.9131\n",
      "Epoch 00098: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 540us/sample - loss: 159.9363 - mse: 159.9363 - val_loss: 122.9417 - val_mse: 122.9416\n",
      "Epoch 99/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 164.9485 - mse: 164.9485- ETA: 0s - loss: 168.2193 -\n",
      "Epoch 00099: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 528us/sample - loss: 162.1106 - mse: 162.1106 - val_loss: 121.7878 - val_mse: 121.7878\n",
      "Epoch 100/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 164.5757 - mse: 164.5756\n",
      "Epoch 00100: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 543us/sample - loss: 162.6761 - mse: 162.6761 - val_loss: 122.2034 - val_mse: 122.2034\n",
      "Epoch 101/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.0354 - mse: 162.0354\n",
      "Epoch 00101: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 543us/sample - loss: 162.5973 - mse: 162.5973 - val_loss: 122.4193 - val_mse: 122.4193\n",
      "Epoch 102/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.6565 - mse: 162.6565\n",
      "Epoch 00102: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 573us/sample - loss: 162.0272 - mse: 162.0271 - val_loss: 122.2786 - val_mse: 122.2786\n",
      "Epoch 103/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 161.6974 - mse: 161.6974\n",
      "Epoch 00103: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 611us/sample - loss: 162.1941 - mse: 162.1940 - val_loss: 122.9910 - val_mse: 122.9910\n",
      "Epoch 104/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.7477 - mse: 162.7477\n",
      "Epoch 00104: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 575us/sample - loss: 162.5191 - mse: 162.5191 - val_loss: 122.1937 - val_mse: 122.1937\n",
      "Epoch 105/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 164.0561 - mse: 164.0561\n",
      "Epoch 00105: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 541us/sample - loss: 162.2705 - mse: 162.2705 - val_loss: 122.1211 - val_mse: 122.1211\n",
      "Epoch 106/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 164.5929 - mse: 164.5929\n",
      "Epoch 00106: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 521us/sample - loss: 162.2156 - mse: 162.2156 - val_loss: 122.6798 - val_mse: 122.6798\n",
      "Epoch 107/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 164.4566 - mse: 164.4566\n",
      "Epoch 00107: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 542us/sample - loss: 163.7637 - mse: 163.7637 - val_loss: 121.8474 - val_mse: 121.8474\n",
      "Epoch 108/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 161.4438 - mse: 161.4438\n",
      "Epoch 00108: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 534us/sample - loss: 161.6711 - mse: 161.6711 - val_loss: 123.2160 - val_mse: 123.2160\n",
      "Epoch 109/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.7522 - mse: 162.7522\n",
      "Epoch 00109: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 521us/sample - loss: 162.0017 - mse: 162.0017 - val_loss: 121.8375 - val_mse: 121.8375\n",
      "Epoch 110/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.8906 - mse: 162.8906\n",
      "Epoch 00110: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 527us/sample - loss: 162.3715 - mse: 162.3715 - val_loss: 123.4423 - val_mse: 123.4423\n",
      "Epoch 111/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 160.0470 - mse: 160.0470\n",
      "Epoch 00111: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 528us/sample - loss: 161.4323 - mse: 161.4323 - val_loss: 124.6003 - val_mse: 124.6003\n",
      "Epoch 112/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 155.1757 - mse: 155.1757\n",
      "Epoch 00112: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 585us/sample - loss: 160.9643 - mse: 160.9643 - val_loss: 123.1811 - val_mse: 123.1811\n",
      "Epoch 113/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 165.7644 - mse: 165.7644\n",
      "Epoch 00113: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 688us/sample - loss: 162.7923 - mse: 162.7922 - val_loss: 121.6431 - val_mse: 121.6431\n",
      "Epoch 114/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 161.9558 - mse: 161.9558\n",
      "Epoch 00114: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 516us/sample - loss: 161.6942 - mse: 161.6942 - val_loss: 124.3803 - val_mse: 124.3803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 160.2127 - mse: 160.2126\n",
      "Epoch 00115: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 549us/sample - loss: 162.0730 - mse: 162.0730 - val_loss: 122.8409 - val_mse: 122.8409\n",
      "Epoch 116/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 163.4510 - mse: 163.4511\n",
      "Epoch 00116: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 536us/sample - loss: 162.0649 - mse: 162.0649 - val_loss: 121.9802 - val_mse: 121.9803\n",
      "Epoch 117/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 162.6053 - mse: 162.6053\n",
      "Epoch 00117: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 534us/sample - loss: 161.9886 - mse: 161.9886 - val_loss: 121.9719 - val_mse: 121.9719\n",
      "Epoch 118/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 164.1119 - mse: 164.1119\n",
      "Epoch 00118: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 563us/sample - loss: 163.2736 - mse: 163.2736 - val_loss: 123.3133 - val_mse: 123.3133\n",
      "Epoch 119/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 164.8023 - mse: 164.8024\n",
      "Epoch 00119: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 524us/sample - loss: 162.6596 - mse: 162.6596 - val_loss: 121.7683 - val_mse: 121.7682\n",
      "Epoch 120/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.1171 - mse: 163.1171\n",
      "Epoch 00120: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 516us/sample - loss: 162.1446 - mse: 162.1446 - val_loss: 122.0818 - val_mse: 122.0818\n",
      "Epoch 121/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 166.1436 - mse: 166.1437\n",
      "Epoch 00121: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 513us/sample - loss: 163.4604 - mse: 163.4605 - val_loss: 122.8549 - val_mse: 122.8549\n",
      "Epoch 122/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 156.9200 - mse: 156.9200\n",
      "Epoch 00122: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 507us/sample - loss: 162.1365 - mse: 162.1365 - val_loss: 122.1679 - val_mse: 122.1679\n",
      "Epoch 123/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 155.8065 - mse: 155.8066\n",
      "Epoch 00123: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 506us/sample - loss: 163.2599 - mse: 163.2599 - val_loss: 122.1107 - val_mse: 122.1107\n",
      "Epoch 124/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 165.2021 - mse: 165.2021\n",
      "Epoch 00124: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 522us/sample - loss: 162.5160 - mse: 162.5160 - val_loss: 122.2480 - val_mse: 122.2480\n",
      "Epoch 125/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 163.7581 - mse: 163.7581\n",
      "Epoch 00125: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 512us/sample - loss: 162.7171 - mse: 162.7171 - val_loss: 122.5197 - val_mse: 122.5197\n",
      "Epoch 126/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 154.3195 - mse: 154.3194\n",
      "Epoch 00126: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 514us/sample - loss: 160.8810 - mse: 160.8810 - val_loss: 121.3548 - val_mse: 121.3548\n",
      "Epoch 127/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.7667 - mse: 163.7667\n",
      "Epoch 00127: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 577us/sample - loss: 162.9633 - mse: 162.9634 - val_loss: 120.9738 - val_mse: 120.9737\n",
      "Epoch 128/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.2453 - mse: 162.2453\n",
      "Epoch 00128: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 575us/sample - loss: 161.1602 - mse: 161.1602 - val_loss: 120.1527 - val_mse: 120.1527\n",
      "Epoch 129/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 160.1169 - mse: 160.1169\n",
      "Epoch 00129: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 563us/sample - loss: 161.9153 - mse: 161.9153 - val_loss: 118.1088 - val_mse: 118.1089\n",
      "Epoch 130/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 159.2702 - mse: 159.2702\n",
      "Epoch 00130: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 542us/sample - loss: 157.0149 - mse: 157.0148 - val_loss: 118.0665 - val_mse: 118.0665\n",
      "Epoch 131/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 159.7680 - mse: 159.7681\n",
      "Epoch 00131: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 535us/sample - loss: 159.8797 - mse: 159.8797 - val_loss: 119.6164 - val_mse: 119.6164\n",
      "Epoch 132/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 160.0865 - mse: 160.0865\n",
      "Epoch 00132: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 555us/sample - loss: 157.9598 - mse: 157.9598 - val_loss: 116.0612 - val_mse: 116.0612\n",
      "Epoch 133/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 163.7100 - mse: 163.7100- ETA: 0s - loss: 143.0556 - \n",
      "Epoch 00133: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 534us/sample - loss: 161.6047 - mse: 161.6047 - val_loss: 115.1784 - val_mse: 115.1784\n",
      "Epoch 134/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 154.2150 - mse: 154.2150\n",
      "Epoch 00134: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 527us/sample - loss: 158.3268 - mse: 158.3268 - val_loss: 123.6480 - val_mse: 123.6480\n",
      "Epoch 135/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 165.8999 - mse: 165.8999\n",
      "Epoch 00135: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 528us/sample - loss: 162.4669 - mse: 162.4669 - val_loss: 119.0358 - val_mse: 119.0358\n",
      "Epoch 136/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 148.6493 - mse: 148.6493\n",
      "Epoch 00136: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 535us/sample - loss: 157.8753 - mse: 157.8753 - val_loss: 129.1867 - val_mse: 129.1867\n",
      "Epoch 137/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 155.5086 - mse: 155.5086\n",
      "Epoch 00137: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 534us/sample - loss: 162.3453 - mse: 162.3452 - val_loss: 122.3448 - val_mse: 122.3448\n",
      "Epoch 138/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 163.4413 - mse: 163.4413\n",
      "Epoch 00138: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 541us/sample - loss: 163.6630 - mse: 163.6630 - val_loss: 121.9074 - val_mse: 121.9074\n",
      "Epoch 139/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.5725 - mse: 162.5725\n",
      "Epoch 00139: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 591us/sample - loss: 162.3482 - mse: 162.3482 - val_loss: 122.5069 - val_mse: 122.5069\n",
      "Epoch 140/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 156.4216 - mse: 156.4216\n",
      "Epoch 00140: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 582us/sample - loss: 162.9782 - mse: 162.9782 - val_loss: 122.9397 - val_mse: 122.9398\n",
      "Epoch 141/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 153.0703 - mse: 153.0703\n",
      "Epoch 00141: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 605us/sample - loss: 162.2502 - mse: 162.2502 - val_loss: 126.5166 - val_mse: 126.5166\n",
      "Epoch 142/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 165.4366 - mse: 165.4366\n",
      "Epoch 00142: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 556us/sample - loss: 163.0103 - mse: 163.0103 - val_loss: 121.9221 - val_mse: 121.9221\n",
      "Epoch 143/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 157.4095 - mse: 157.4095\n",
      "Epoch 00143: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 541us/sample - loss: 163.4215 - mse: 163.4216 - val_loss: 121.8813 - val_mse: 121.8813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 163.8688 - mse: 163.8688\n",
      "Epoch 00144: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 537us/sample - loss: 163.0243 - mse: 163.0243 - val_loss: 122.0675 - val_mse: 122.0675\n",
      "Epoch 145/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 162.8712 - mse: 162.8712\n",
      "Epoch 00145: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 528us/sample - loss: 161.7580 - mse: 161.7580 - val_loss: 122.9566 - val_mse: 122.9566\n",
      "Epoch 146/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 161.3871 - mse: 161.3871\n",
      "Epoch 00146: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 514us/sample - loss: 163.7835 - mse: 163.7835 - val_loss: 122.8498 - val_mse: 122.8498\n",
      "Epoch 147/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 163.3531 - mse: 163.3531\n",
      "Epoch 00147: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 520us/sample - loss: 163.2784 - mse: 163.2784 - val_loss: 121.9606 - val_mse: 121.9606\n",
      "Epoch 148/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.2021 - mse: 162.2021\n",
      "Epoch 00148: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 528us/sample - loss: 161.4003 - mse: 161.4003 - val_loss: 122.8951 - val_mse: 122.8951\n",
      "Epoch 149/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.1880 - mse: 163.1880\n",
      "Epoch 00149: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 548us/sample - loss: 163.4632 - mse: 163.4633 - val_loss: 121.8947 - val_mse: 121.8947\n",
      "Epoch 150/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 164.4932 - mse: 164.4932\n",
      "Epoch 00150: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 541us/sample - loss: 161.1855 - mse: 161.1855 - val_loss: 121.8593 - val_mse: 121.8593\n",
      "Epoch 151/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 158.0619 - mse: 158.0619\n",
      "Epoch 00151: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 556us/sample - loss: 160.9677 - mse: 160.9677 - val_loss: 121.5170 - val_mse: 121.5170\n",
      "Epoch 152/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 161.1480 - mse: 161.1480- ETA: 0s - loss: 145.4395 - mse: \n",
      "Epoch 00152: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 569us/sample - loss: 161.1624 - mse: 161.1624 - val_loss: 122.5161 - val_mse: 122.5161\n",
      "Epoch 153/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 164.4646 - mse: 164.4646\n",
      "Epoch 00153: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 561us/sample - loss: 161.7009 - mse: 161.7009 - val_loss: 123.7045 - val_mse: 123.7045\n",
      "Epoch 154/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 167.4653 - mse: 167.4653\n",
      "Epoch 00154: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 542us/sample - loss: 163.9572 - mse: 163.9572 - val_loss: 122.4611 - val_mse: 122.4611\n",
      "Epoch 155/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 161.0990 - mse: 161.0990\n",
      "Epoch 00155: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 514us/sample - loss: 162.3953 - mse: 162.3953 - val_loss: 121.9033 - val_mse: 121.9033\n",
      "Epoch 156/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 164.4344 - mse: 164.4344\n",
      "Epoch 00156: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 521us/sample - loss: 161.9357 - mse: 161.9358 - val_loss: 120.6898 - val_mse: 120.6898\n",
      "Epoch 157/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 157.9460 - mse: 157.9460\n",
      "Epoch 00157: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 514us/sample - loss: 160.6022 - mse: 160.6022 - val_loss: 118.8534 - val_mse: 118.8534\n",
      "Epoch 158/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 159.5945 - mse: 159.5945\n",
      "Epoch 00158: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 520us/sample - loss: 159.3958 - mse: 159.3958 - val_loss: 118.8891 - val_mse: 118.8891\n",
      "Epoch 159/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 160.1851 - mse: 160.1851- ETA: 0s - loss: 154.0766 - m\n",
      "Epoch 00159: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 521us/sample - loss: 157.4070 - mse: 157.4070 - val_loss: 121.2597 - val_mse: 121.2597\n",
      "Epoch 160/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 157.2030 - mse: 157.2030\n",
      "Epoch 00160: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 535us/sample - loss: 162.6173 - mse: 162.6174 - val_loss: 120.1226 - val_mse: 120.1226\n",
      "Epoch 161/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 164.4357 - mse: 164.4357\n",
      "Epoch 00161: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 519us/sample - loss: 160.6273 - mse: 160.6273 - val_loss: 120.0774 - val_mse: 120.0774\n",
      "Epoch 162/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 159.7170 - mse: 159.7170\n",
      "Epoch 00162: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 529us/sample - loss: 159.0234 - mse: 159.0234 - val_loss: 118.2901 - val_mse: 118.2901\n",
      "Epoch 163/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.3653 - mse: 162.3653\n",
      "Epoch 00163: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 548us/sample - loss: 160.4255 - mse: 160.4255 - val_loss: 119.4246 - val_mse: 119.4246\n",
      "Epoch 164/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 157.7016 - mse: 157.7016\n",
      "Epoch 00164: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 590us/sample - loss: 158.7651 - mse: 158.7651 - val_loss: 121.3204 - val_mse: 121.3204\n",
      "Epoch 165/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 161.8062 - mse: 161.8062\n",
      "Epoch 00165: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 604us/sample - loss: 160.4215 - mse: 160.4215 - val_loss: 121.7569 - val_mse: 121.7569\n",
      "Epoch 166/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 159.8654 - mse: 159.8654\n",
      "Epoch 00166: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 560us/sample - loss: 159.6812 - mse: 159.6812 - val_loss: 122.2313 - val_mse: 122.2313\n",
      "Epoch 167/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 160.6253 - mse: 160.6253\n",
      "Epoch 00167: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 563us/sample - loss: 161.0555 - mse: 161.0555 - val_loss: 119.2988 - val_mse: 119.2988\n",
      "Epoch 168/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.2910 - mse: 162.2910\n",
      "Epoch 00168: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 583us/sample - loss: 161.4213 - mse: 161.4213 - val_loss: 117.3094 - val_mse: 117.3094\n",
      "Epoch 169/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 158.5820 - mse: 158.5820\n",
      "Epoch 00169: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 567us/sample - loss: 159.0579 - mse: 159.0579 - val_loss: 115.3656 - val_mse: 115.3656\n",
      "Epoch 170/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 156.5629 - mse: 156.5630\n",
      "Epoch 00170: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 543us/sample - loss: 157.9095 - mse: 157.9096 - val_loss: 121.0182 - val_mse: 121.0182\n",
      "Epoch 171/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.0672 - mse: 162.0672\n",
      "Epoch 00171: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 536us/sample - loss: 160.9632 - mse: 160.9632 - val_loss: 122.8904 - val_mse: 122.8904\n",
      "Epoch 172/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2272/2400 [===========================>..] - ETA: 0s - loss: 155.1375 - mse: 155.1375\n",
      "Epoch 00172: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 547us/sample - loss: 161.1525 - mse: 161.1526 - val_loss: 121.8579 - val_mse: 121.8579\n",
      "Epoch 173/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 162.8030 - mse: 162.8030\n",
      "Epoch 00173: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 520us/sample - loss: 163.7728 - mse: 163.7728 - val_loss: 121.8392 - val_mse: 121.8392\n",
      "Epoch 174/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 165.1047 - mse: 165.1047\n",
      "Epoch 00174: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 529us/sample - loss: 164.4483 - mse: 164.4482 - val_loss: 121.8389 - val_mse: 121.8389\n",
      "Epoch 175/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.8039 - mse: 162.8039\n",
      "Epoch 00175: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 562us/sample - loss: 161.5233 - mse: 161.5234 - val_loss: 122.0563 - val_mse: 122.0563\n",
      "Epoch 176/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 156.0896 - mse: 156.0897\n",
      "Epoch 00176: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 580us/sample - loss: 162.4494 - mse: 162.4494 - val_loss: 124.4150 - val_mse: 124.4150\n",
      "Epoch 177/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 164.5095 - mse: 164.5096\n",
      "Epoch 00177: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 578us/sample - loss: 163.7861 - mse: 163.7861 - val_loss: 122.0342 - val_mse: 122.0342\n",
      "Epoch 178/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 161.0481 - mse: 161.0481\n",
      "Epoch 00178: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 546us/sample - loss: 161.9087 - mse: 161.9087 - val_loss: 123.7338 - val_mse: 123.7338\n",
      "Epoch 179/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.8158 - mse: 162.8159\n",
      "Epoch 00179: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 536us/sample - loss: 162.8289 - mse: 162.8290 - val_loss: 121.8566 - val_mse: 121.8567\n",
      "Epoch 180/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.2051 - mse: 163.2051\n",
      "Epoch 00180: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 534us/sample - loss: 161.9805 - mse: 161.9805 - val_loss: 121.9409 - val_mse: 121.9409\n",
      "Epoch 181/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.0148 - mse: 162.0148\n",
      "Epoch 00181: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 542us/sample - loss: 162.1184 - mse: 162.1184 - val_loss: 122.2163 - val_mse: 122.2163\n",
      "Epoch 182/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 165.7111 - mse: 165.7112\n",
      "Epoch 00182: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 513us/sample - loss: 162.7258 - mse: 162.7258 - val_loss: 121.8642 - val_mse: 121.8642\n",
      "Epoch 183/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.5807 - mse: 163.5807\n",
      "Epoch 00183: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 520us/sample - loss: 162.9858 - mse: 162.9858 - val_loss: 122.2664 - val_mse: 122.2664\n",
      "Epoch 184/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.6023 - mse: 163.6023\n",
      "Epoch 00184: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 521us/sample - loss: 162.2780 - mse: 162.2780 - val_loss: 122.5043 - val_mse: 122.5043\n",
      "Epoch 185/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.1972 - mse: 162.1971\n",
      "Epoch 00185: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 520us/sample - loss: 161.7364 - mse: 161.7364 - val_loss: 121.8771 - val_mse: 121.8771\n",
      "Epoch 186/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 167.0517 - mse: 167.0517\n",
      "Epoch 00186: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 507us/sample - loss: 161.8939 - mse: 161.8939 - val_loss: 122.6823 - val_mse: 122.6823\n",
      "Epoch 187/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 160.8064 - mse: 160.8064- ETA: 0s - loss: 213.4528 - \n",
      "Epoch 00187: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 513us/sample - loss: 162.7199 - mse: 162.7200 - val_loss: 122.6293 - val_mse: 122.6293\n",
      "Epoch 188/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 159.2703 - mse: 159.2703\n",
      "Epoch 00188: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 555us/sample - loss: 161.4625 - mse: 161.4625 - val_loss: 121.9261 - val_mse: 121.9261\n",
      "Epoch 189/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 165.0095 - mse: 165.0095\n",
      "Epoch 00189: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 562us/sample - loss: 163.6791 - mse: 163.6791 - val_loss: 121.9360 - val_mse: 121.9360\n",
      "Epoch 190/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.7199 - mse: 163.7199- ETA: 0s - loss: 160.8145 - mse:\n",
      "Epoch 00190: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 584us/sample - loss: 162.7738 - mse: 162.7738 - val_loss: 123.7716 - val_mse: 123.7716\n",
      "Epoch 191/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 164.6512 - mse: 164.6513\n",
      "Epoch 00191: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 542us/sample - loss: 162.3070 - mse: 162.3070 - val_loss: 122.4042 - val_mse: 122.4042\n",
      "Epoch 192/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 164.1474 - mse: 164.1474\n",
      "Epoch 00192: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 548us/sample - loss: 162.9892 - mse: 162.9892 - val_loss: 122.6645 - val_mse: 122.6645\n",
      "Epoch 193/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.2507 - mse: 163.2507\n",
      "Epoch 00193: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 529us/sample - loss: 162.5747 - mse: 162.5747 - val_loss: 121.9756 - val_mse: 121.9756\n",
      "Epoch 194/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 162.8334 - mse: 162.8334\n",
      "Epoch 00194: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 520us/sample - loss: 162.3112 - mse: 162.3112 - val_loss: 123.2923 - val_mse: 123.2923\n",
      "Epoch 195/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.7703 - mse: 163.7703\n",
      "Epoch 00195: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 541us/sample - loss: 162.2181 - mse: 162.2181 - val_loss: 122.2630 - val_mse: 122.2630\n",
      "Epoch 196/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 164.8546 - mse: 164.8546\n",
      "Epoch 00196: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 548us/sample - loss: 163.0461 - mse: 163.0461 - val_loss: 122.1331 - val_mse: 122.1331\n",
      "Epoch 197/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 164.1099 - mse: 164.1099\n",
      "Epoch 00197: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 520us/sample - loss: 162.8013 - mse: 162.8014 - val_loss: 122.6551 - val_mse: 122.6552\n",
      "Epoch 198/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 160.1417 - mse: 160.1416\n",
      "Epoch 00198: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 536us/sample - loss: 162.4865 - mse: 162.4865 - val_loss: 122.7911 - val_mse: 122.7911\n",
      "Epoch 199/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 159.3879 - mse: 159.3879\n",
      "Epoch 00199: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 540us/sample - loss: 162.2372 - mse: 162.2372 - val_loss: 122.1152 - val_mse: 122.1152\n",
      "Epoch 200/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.9817 - mse: 162.9816\n",
      "Epoch 00200: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 576us/sample - loss: 162.1026 - mse: 162.1026 - val_loss: 122.1232 - val_mse: 122.1232\n",
      "Epoch 201/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 161.9655 - mse: 161.9655\n",
      "Epoch 00201: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 577us/sample - loss: 160.6873 - mse: 160.6873 - val_loss: 122.0961 - val_mse: 122.0961\n",
      "Epoch 202/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 165.0758 - mse: 165.0758\n",
      "Epoch 00202: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 568us/sample - loss: 163.0604 - mse: 163.0604 - val_loss: 122.3625 - val_mse: 122.3625\n",
      "Epoch 203/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 162.8402 - mse: 162.8401\n",
      "Epoch 00203: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 561us/sample - loss: 161.8455 - mse: 161.8455 - val_loss: 123.1477 - val_mse: 123.1477\n",
      "Epoch 204/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 164.9322 - mse: 164.9322\n",
      "Epoch 00204: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 537us/sample - loss: 163.4664 - mse: 163.4664 - val_loss: 123.1653 - val_mse: 123.1653\n",
      "Epoch 205/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 164.5283 - mse: 164.5283- ETA: 0s - loss: 141.2564 - mse:\n",
      "Epoch 00205: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 548us/sample - loss: 163.9733 - mse: 163.9733 - val_loss: 123.9244 - val_mse: 123.9244\n",
      "Epoch 206/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 161.0003 - mse: 161.0003\n",
      "Epoch 00206: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 527us/sample - loss: 161.5004 - mse: 161.5004 - val_loss: 121.8651 - val_mse: 121.8651\n",
      "Epoch 207/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.2330 - mse: 163.2330\n",
      "Epoch 00207: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 513us/sample - loss: 162.8155 - mse: 162.8154 - val_loss: 123.2615 - val_mse: 123.2615\n",
      "Epoch 208/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 165.5893 - mse: 165.5893\n",
      "Epoch 00208: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 514us/sample - loss: 163.3828 - mse: 163.3828 - val_loss: 122.5167 - val_mse: 122.5167\n",
      "Epoch 209/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 166.8824 - mse: 166.8824\n",
      "Epoch 00209: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 527us/sample - loss: 163.6569 - mse: 163.6569 - val_loss: 124.2619 - val_mse: 124.2619\n",
      "Epoch 210/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 156.9960 - mse: 156.9960\n",
      "Epoch 00210: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 514us/sample - loss: 161.1725 - mse: 161.1725 - val_loss: 121.8859 - val_mse: 121.8859\n",
      "Epoch 211/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 161.9799 - mse: 161.9799\n",
      "Epoch 00211: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 528us/sample - loss: 163.0611 - mse: 163.0611 - val_loss: 122.3354 - val_mse: 122.3354\n",
      "Epoch 212/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.3244 - mse: 163.3244\n",
      "Epoch 00212: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 562us/sample - loss: 163.4876 - mse: 163.4875 - val_loss: 124.5917 - val_mse: 124.5917\n",
      "Epoch 213/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 167.5097 - mse: 167.509 - ETA: 0s - loss: 163.8415 - mse: 163.8415\n",
      "Epoch 00213: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 590us/sample - loss: 163.0530 - mse: 163.0530 - val_loss: 122.7457 - val_mse: 122.7457\n",
      "Epoch 214/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 162.7255 - mse: 162.7255\n",
      "Epoch 00214: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 576us/sample - loss: 161.9047 - mse: 161.9047 - val_loss: 124.0234 - val_mse: 124.0234\n",
      "Epoch 215/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.2592 - mse: 162.2593\n",
      "Epoch 00215: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 528us/sample - loss: 162.0898 - mse: 162.0898 - val_loss: 123.1912 - val_mse: 123.1912\n",
      "Epoch 216/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.5009 - mse: 163.5008\n",
      "Epoch 00216: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 539us/sample - loss: 162.9329 - mse: 162.9328 - val_loss: 122.1377 - val_mse: 122.1377\n",
      "Epoch 217/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 163.7631 - mse: 163.7630\n",
      "Epoch 00217: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 539us/sample - loss: 161.9956 - mse: 161.9956 - val_loss: 123.3363 - val_mse: 123.3363\n",
      "Epoch 218/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 160.6705 - mse: 160.6705\n",
      "Epoch 00218: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 549us/sample - loss: 161.1502 - mse: 161.1501 - val_loss: 122.5400 - val_mse: 122.5400\n",
      "Epoch 219/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 164.4362 - mse: 164.4362\n",
      "Epoch 00219: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 528us/sample - loss: 162.9381 - mse: 162.9381 - val_loss: 123.7519 - val_mse: 123.7519\n",
      "Epoch 220/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 161.4978 - mse: 161.4979\n",
      "Epoch 00220: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 533us/sample - loss: 162.0746 - mse: 162.0746 - val_loss: 124.4472 - val_mse: 124.4472\n",
      "Epoch 221/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 160.6475 - mse: 160.6475\n",
      "Epoch 00221: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 535us/sample - loss: 161.5092 - mse: 161.5092 - val_loss: 123.1172 - val_mse: 123.1172\n",
      "Epoch 222/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.5833 - mse: 162.5833\n",
      "Epoch 00222: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 542us/sample - loss: 161.7340 - mse: 161.7340 - val_loss: 121.9153 - val_mse: 121.9153\n",
      "Epoch 223/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.4549 - mse: 162.4549\n",
      "Epoch 00223: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 533us/sample - loss: 161.7365 - mse: 161.7366 - val_loss: 123.2016 - val_mse: 123.2016\n",
      "Epoch 224/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.1231 - mse: 163.1231\n",
      "Epoch 00224: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 579us/sample - loss: 161.7050 - mse: 161.7051 - val_loss: 122.8256 - val_mse: 122.8256\n",
      "Epoch 225/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 159.2070 - mse: 159.2071\n",
      "Epoch 00225: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 578us/sample - loss: 162.3203 - mse: 162.3204 - val_loss: 122.3147 - val_mse: 122.3148\n",
      "Epoch 226/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.5729 - mse: 163.5729\n",
      "Epoch 00226: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 590us/sample - loss: 162.3441 - mse: 162.3441 - val_loss: 122.0351 - val_mse: 122.0351\n",
      "Epoch 227/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 163.6614 - mse: 163.6614\n",
      "Epoch 00227: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 548us/sample - loss: 162.1405 - mse: 162.1405 - val_loss: 122.3168 - val_mse: 122.3168\n",
      "Epoch 228/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2272/2400 [===========================>..] - ETA: 0s - loss: 165.8038 - mse: 165.8038\n",
      "Epoch 00228: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 521us/sample - loss: 162.1969 - mse: 162.1969 - val_loss: 124.3823 - val_mse: 124.3823\n",
      "Epoch 229/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 164.2914 - mse: 164.2914\n",
      "Epoch 00229: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 535us/sample - loss: 162.8864 - mse: 162.8863 - val_loss: 121.9361 - val_mse: 121.9361\n",
      "Epoch 230/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.5945 - mse: 162.5946- ETA: 0s - loss: 138.6746 -\n",
      "Epoch 00230: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 570us/sample - loss: 162.0013 - mse: 162.0014 - val_loss: 122.7140 - val_mse: 122.7140\n",
      "Epoch 231/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.0647 - mse: 162.0647\n",
      "Epoch 00231: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 554us/sample - loss: 160.6316 - mse: 160.6316 - val_loss: 123.5342 - val_mse: 123.5342\n",
      "Epoch 232/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 161.8655 - mse: 161.8655\n",
      "Epoch 00232: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 535us/sample - loss: 162.7017 - mse: 162.7017 - val_loss: 122.7824 - val_mse: 122.7824\n",
      "Epoch 233/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.3083 - mse: 162.3083- ETA: 0s - loss: 180.0541 - mse: 180.\n",
      "Epoch 00233: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 527us/sample - loss: 161.8627 - mse: 161.8627 - val_loss: 121.8408 - val_mse: 121.8408\n",
      "Epoch 234/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 161.8603 - mse: 161.8603\n",
      "Epoch 00234: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 527us/sample - loss: 162.8617 - mse: 162.8617 - val_loss: 122.7143 - val_mse: 122.7143\n",
      "Epoch 235/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 160.8152 - mse: 160.8151- ETA: 0s - loss: 154.9877 - mse: 15\n",
      "Epoch 00235: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 549us/sample - loss: 160.6503 - mse: 160.6502 - val_loss: 123.5148 - val_mse: 123.5148\n",
      "Epoch 236/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 164.2960 - mse: 164.2960\n",
      "Epoch 00236: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 548us/sample - loss: 163.0767 - mse: 163.0767 - val_loss: 122.7127 - val_mse: 122.7127\n",
      "Epoch 237/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 164.1002 - mse: 164.1003\n",
      "Epoch 00237: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 572us/sample - loss: 161.3123 - mse: 161.3124 - val_loss: 123.0212 - val_mse: 123.0212\n",
      "Epoch 238/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.2383 - mse: 162.2383\n",
      "Epoch 00238: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 1s 590us/sample - loss: 160.9339 - mse: 160.9339 - val_loss: 123.8634 - val_mse: 123.8634\n",
      "Epoch 239/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.9713 - mse: 162.9713\n",
      "Epoch 00239: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 723us/sample - loss: 161.6261 - mse: 161.6261 - val_loss: 123.3278 - val_mse: 123.3278\n",
      "Epoch 00239: early stopping\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4150 samples, validate on 1038 samples\n",
      "Epoch 1/200\n",
      "3232/4150 [======================>.......] - ETA: 0s - loss: 3.2637 - accuracy: 0.6293\n",
      "Epoch 00001: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 1s 191us/sample - loss: 2.9576 - accuracy: 0.6243 - val_loss: 3.1502 - val_accuracy: 0.4162\n",
      "Epoch 2/200\n",
      "3456/4150 [=======================>......] - ETA: 0s - loss: 1.2815 - accuracy: 0.6690\n",
      "Epoch 00002: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 83us/sample - loss: 1.2181 - accuracy: 0.6745 - val_loss: 5.1879 - val_accuracy: 0.2216\n",
      "Epoch 3/200\n",
      "3712/4150 [=========================>....] - ETA: 0s - loss: 0.9704 - accuracy: 0.6862\n",
      "Epoch 00003: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 88us/sample - loss: 0.9495 - accuracy: 0.6923 - val_loss: 1.3121 - val_accuracy: 0.5376\n",
      "Epoch 4/200\n",
      "3488/4150 [========================>.....] - ETA: 0s - loss: 0.7942 - accuracy: 0.7162\n",
      "Epoch 00004: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 84us/sample - loss: 0.8220 - accuracy: 0.7092 - val_loss: 4.5723 - val_accuracy: 0.0568\n",
      "Epoch 5/200\n",
      "3904/4150 [===========================>..] - ETA: 0s - loss: 0.8254 - accuracy: 0.7185\n",
      "Epoch 00005: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 95us/sample - loss: 0.8399 - accuracy: 0.7137 - val_loss: 0.0763 - val_accuracy: 0.9759\n",
      "Epoch 6/200\n",
      "3776/4150 [==========================>...] - ETA: 0s - loss: 0.7860 - accuracy: 0.7105\n",
      "Epoch 00006: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 84us/sample - loss: 0.7702 - accuracy: 0.7125 - val_loss: 0.3208 - val_accuracy: 0.8738\n",
      "Epoch 7/200\n",
      "3968/4150 [===========================>..] - ETA: 0s - loss: 0.7079 - accuracy: 0.7238\n",
      "Epoch 00007: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 93us/sample - loss: 0.7034 - accuracy: 0.7253 - val_loss: 0.1714 - val_accuracy: 0.9566\n",
      "Epoch 8/200\n",
      "3520/4150 [========================>.....] - ETA: 0s - loss: 0.6946 - accuracy: 0.7176\n",
      "Epoch 00008: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 100us/sample - loss: 0.6974 - accuracy: 0.7186 - val_loss: 1.3563 - val_accuracy: 0.4489\n",
      "Epoch 9/200\n",
      "4064/4150 [============================>.] - ETA: 0s - loss: 0.6241 - accuracy: 0.7392\n",
      "Epoch 00009: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 87us/sample - loss: 0.6260 - accuracy: 0.7393 - val_loss: 0.5558 - val_accuracy: 0.7428\n",
      "Epoch 10/200\n",
      "3776/4150 [==========================>...] - ETA: 0s - loss: 0.6448 - accuracy: 0.7288\n",
      "Epoch 00010: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 80us/sample - loss: 0.6266 - accuracy: 0.7366 - val_loss: 0.5310 - val_accuracy: 0.7312\n",
      "Epoch 11/200\n",
      "3456/4150 [=======================>......] - ETA: 0s - loss: 0.5281 - accuracy: 0.7593\n",
      "Epoch 00011: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 68us/sample - loss: 0.5179 - accuracy: 0.7643 - val_loss: 0.7619 - val_accuracy: 0.6089\n",
      "Epoch 12/200\n",
      "3584/4150 [========================>.....] - ETA: 0s - loss: 0.4703 - accuracy: 0.7829\n",
      "Epoch 00012: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 68us/sample - loss: 0.4854 - accuracy: 0.7769 - val_loss: 0.2468 - val_accuracy: 0.8960\n",
      "Epoch 13/200\n",
      "3232/4150 [======================>.......] - ETA: 0s - loss: 0.5105 - accuracy: 0.7760\n",
      "Epoch 00013: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 71us/sample - loss: 0.5021 - accuracy: 0.7778 - val_loss: 0.8206 - val_accuracy: 0.5626\n",
      "Epoch 14/200\n",
      "3296/4150 [======================>.......] - ETA: 0s - loss: 0.5612 - accuracy: 0.7627\n",
      "Epoch 00014: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 82us/sample - loss: 0.5414 - accuracy: 0.7639 - val_loss: 1.0058 - val_accuracy: 0.5896\n",
      "Epoch 15/200\n",
      "3328/4150 [=======================>......] - ETA: 0s - loss: 0.4534 - accuracy: 0.7879\n",
      "Epoch 00015: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 69us/sample - loss: 0.4824 - accuracy: 0.7812 - val_loss: 0.2067 - val_accuracy: 0.9470\n",
      "Epoch 16/200\n",
      "3296/4150 [======================>.......] - ETA: 0s - loss: 0.5485 - accuracy: 0.7655\n",
      "Epoch 00016: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 68us/sample - loss: 0.5333 - accuracy: 0.7651 - val_loss: 0.2669 - val_accuracy: 0.8882\n",
      "Epoch 17/200\n",
      "3488/4150 [========================>.....] - ETA: 0s - loss: 0.4799 - accuracy: 0.7738\n",
      "Epoch 00017: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 80us/sample - loss: 0.4744 - accuracy: 0.7776 - val_loss: 0.2693 - val_accuracy: 0.8950\n",
      "Epoch 18/200\n",
      "3520/4150 [========================>.....] - ETA: 0s - loss: 0.4777 - accuracy: 0.7804\n",
      "Epoch 00018: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 67us/sample - loss: 0.4609 - accuracy: 0.7889 - val_loss: 0.4623 - val_accuracy: 0.7813\n",
      "Epoch 19/200\n",
      "3552/4150 [========================>.....] - ETA: 0s - loss: 0.4534 - accuracy: 0.7936\n",
      "Epoch 00019: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 68us/sample - loss: 0.4546 - accuracy: 0.7940 - val_loss: 0.4439 - val_accuracy: 0.8160\n",
      "Epoch 20/200\n",
      "3904/4150 [===========================>..] - ETA: 0s - loss: 0.4315 - accuracy: 0.8056\n",
      "Epoch 00020: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 64us/sample - loss: 0.4299 - accuracy: 0.8065 - val_loss: 0.7062 - val_accuracy: 0.6830\n",
      "Epoch 21/200\n",
      "3904/4150 [===========================>..] - ETA: 0s - loss: 0.4146 - accuracy: 0.8023\n",
      "Epoch 00021: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 64us/sample - loss: 0.4167 - accuracy: 0.8022 - val_loss: 0.4636 - val_accuracy: 0.7958\n",
      "Epoch 22/200\n",
      "3904/4150 [===========================>..] - ETA: 0s - loss: 0.4286 - accuracy: 0.8051\n",
      "Epoch 00022: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 65us/sample - loss: 0.4227 - accuracy: 0.8070 - val_loss: 0.6757 - val_accuracy: 0.6638\n",
      "Epoch 23/200\n",
      "3680/4150 [=========================>....] - ETA: 0s - loss: 0.3976 - accuracy: 0.8136\n",
      "Epoch 00023: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 65us/sample - loss: 0.3996 - accuracy: 0.8120 - val_loss: 2.4517 - val_accuracy: 0.1898\n",
      "Epoch 24/200\n",
      "3552/4150 [========================>.....] - ETA: 0s - loss: 0.4167 - accuracy: 0.8026\n",
      "Epoch 00024: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 68us/sample - loss: 0.4111 - accuracy: 0.8046 - val_loss: 0.7913 - val_accuracy: 0.5568\n",
      "Epoch 25/200\n",
      "3584/4150 [========================>.....] - ETA: 0s - loss: 0.4366 - accuracy: 0.8050\n",
      "Epoch 00025: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 70us/sample - loss: 0.4355 - accuracy: 0.8036 - val_loss: 0.3938 - val_accuracy: 0.8160\n",
      "Epoch 26/200\n",
      "3840/4150 [==========================>...] - ETA: 0s - loss: 0.4478 - accuracy: 0.8013\n",
      "Epoch 00026: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 65us/sample - loss: 0.4530 - accuracy: 0.7990 - val_loss: 0.3142 - val_accuracy: 0.8776\n",
      "Epoch 27/200\n",
      "3680/4150 [=========================>....] - ETA: 0s - loss: 0.5536 - accuracy: 0.7807\n",
      "Epoch 00027: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 66us/sample - loss: 0.5537 - accuracy: 0.7802 - val_loss: 0.4288 - val_accuracy: 0.8112\n",
      "Epoch 28/200\n",
      "4064/4150 [============================>.] - ETA: 0s - loss: 0.3986 - accuracy: 0.8155\n",
      "Epoch 00028: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 62us/sample - loss: 0.3978 - accuracy: 0.8154 - val_loss: 0.4938 - val_accuracy: 0.7543\n",
      "Epoch 29/200\n",
      "3648/4150 [=========================>....] - ETA: 0s - loss: 0.4031 - accuracy: 0.8224\n",
      "Epoch 00029: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 67us/sample - loss: 0.4038 - accuracy: 0.8214 - val_loss: 0.3194 - val_accuracy: 0.8805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200\n",
      "4000/4150 [===========================>..] - ETA: 0s - loss: 0.4146 - accuracy: 0.8080\n",
      "Epoch 00030: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 64us/sample - loss: 0.4166 - accuracy: 0.8077 - val_loss: 0.4669 - val_accuracy: 0.7707\n",
      "Epoch 31/200\n",
      "3840/4150 [==========================>...] - ETA: 0s - loss: 0.3756 - accuracy: 0.8326\n",
      "Epoch 00031: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 64us/sample - loss: 0.3775 - accuracy: 0.8316 - val_loss: 0.6180 - val_accuracy: 0.6493\n",
      "Epoch 32/200\n",
      "4128/4150 [============================>.] - ETA: 0s - loss: 0.4616 - accuracy: 0.7970\n",
      "Epoch 00032: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 61us/sample - loss: 0.4600 - accuracy: 0.7978 - val_loss: 0.7821 - val_accuracy: 0.5780\n",
      "Epoch 33/200\n",
      "3936/4150 [===========================>..] - ETA: 0s - loss: 0.4386 - accuracy: 0.8021\n",
      "Epoch 00033: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 64us/sample - loss: 0.4440 - accuracy: 0.7986 - val_loss: 1.1512 - val_accuracy: 0.4451\n",
      "Epoch 34/200\n",
      "3840/4150 [==========================>...] - ETA: 0s - loss: 0.3751 - accuracy: 0.8250\n",
      "Epoch 00034: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 64us/sample - loss: 0.3760 - accuracy: 0.8253 - val_loss: 1.0934 - val_accuracy: 0.3545\n",
      "Epoch 35/200\n",
      "3136/4150 [=====================>........] - ETA: 0s - loss: 0.3819 - accuracy: 0.8288\n",
      "Epoch 00035: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 58us/sample - loss: 0.3774 - accuracy: 0.8294 - val_loss: 0.6206 - val_accuracy: 0.6821\n",
      "Epoch 36/200\n",
      "3680/4150 [=========================>....] - ETA: 0s - loss: 0.3832 - accuracy: 0.8236\n",
      "Epoch 00036: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 65us/sample - loss: 0.3840 - accuracy: 0.8241 - val_loss: 0.5233 - val_accuracy: 0.7572\n",
      "Epoch 37/200\n",
      "3136/4150 [=====================>........] - ETA: 0s - loss: 0.3945 - accuracy: 0.8198\n",
      "Epoch 00037: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 55us/sample - loss: 0.3830 - accuracy: 0.8239 - val_loss: 0.3497 - val_accuracy: 0.8584\n",
      "Epoch 38/200\n",
      "3808/4150 [==========================>...] - ETA: 0s - loss: 0.3579 - accuracy: 0.8364\n",
      "Epoch 00038: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 67us/sample - loss: 0.3573 - accuracy: 0.8364 - val_loss: 0.2919 - val_accuracy: 0.8834\n",
      "Epoch 39/200\n",
      "3648/4150 [=========================>....] - ETA: 0s - loss: 0.3657 - accuracy: 0.8358\n",
      "Epoch 00039: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 78us/sample - loss: 0.3648 - accuracy: 0.8359 - val_loss: 0.5954 - val_accuracy: 0.7119\n",
      "Epoch 40/200\n",
      "3296/4150 [======================>.......] - ETA: 0s - loss: 0.3994 - accuracy: 0.8249\n",
      "Epoch 00040: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 67us/sample - loss: 0.3932 - accuracy: 0.8255 - val_loss: 0.3554 - val_accuracy: 0.8401\n",
      "Epoch 41/200\n",
      "3616/4150 [=========================>....] - ETA: 0s - loss: 0.3337 - accuracy: 0.8498\n",
      "Epoch 00041: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 68us/sample - loss: 0.3436 - accuracy: 0.8436 - val_loss: 0.8180 - val_accuracy: 0.5511\n",
      "Epoch 42/200\n",
      "3840/4150 [==========================>...] - ETA: 0s - loss: 0.3613 - accuracy: 0.8359\n",
      "Epoch 00042: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 66us/sample - loss: 0.3772 - accuracy: 0.8306 - val_loss: 0.0948 - val_accuracy: 0.9750\n",
      "Epoch 43/200\n",
      "4096/4150 [============================>.] - ETA: 0s - loss: 0.3995 - accuracy: 0.8213\n",
      "Epoch 00043: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 61us/sample - loss: 0.4000 - accuracy: 0.8207 - val_loss: 0.3151 - val_accuracy: 0.8642\n",
      "Epoch 44/200\n",
      "3808/4150 [==========================>...] - ETA: 0s - loss: 0.3662 - accuracy: 0.8406\n",
      "Epoch 00044: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 65us/sample - loss: 0.3634 - accuracy: 0.8395 - val_loss: 0.5752 - val_accuracy: 0.6985\n",
      "Epoch 45/200\n",
      "4000/4150 [===========================>..] - ETA: 0s - loss: 0.3404 - accuracy: 0.8455\n",
      "Epoch 00045: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 62us/sample - loss: 0.3400 - accuracy: 0.8455 - val_loss: 0.1817 - val_accuracy: 0.9576\n",
      "Epoch 46/200\n",
      "3776/4150 [==========================>...] - ETA: 0s - loss: 0.3418 - accuracy: 0.8419\n",
      "Epoch 00046: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 66us/sample - loss: 0.3389 - accuracy: 0.8427 - val_loss: 0.9198 - val_accuracy: 0.5568\n",
      "Epoch 47/200\n",
      "3104/4150 [=====================>........] - ETA: 0s - loss: 0.3211 - accuracy: 0.8547\n",
      "Epoch 00047: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 55us/sample - loss: 0.3142 - accuracy: 0.8590 - val_loss: 0.2766 - val_accuracy: 0.9162\n",
      "Epoch 48/200\n",
      "3360/4150 [=======================>......] - ETA: 0s - loss: 0.3670 - accuracy: 0.8420\n",
      "Epoch 00048: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 68us/sample - loss: 0.3657 - accuracy: 0.8407 - val_loss: 0.9511 - val_accuracy: 0.5202\n",
      "Epoch 49/200\n",
      "3648/4150 [=========================>....] - ETA: 0s - loss: 0.3235 - accuracy: 0.8569\n",
      "Epoch 00049: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 65us/sample - loss: 0.3250 - accuracy: 0.8571 - val_loss: 0.4125 - val_accuracy: 0.7890\n",
      "Epoch 50/200\n",
      "3936/4150 [===========================>..] - ETA: 0s - loss: 0.3415 - accuracy: 0.8481\n",
      "Epoch 00050: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 102us/sample - loss: 0.3423 - accuracy: 0.8482 - val_loss: 0.3057 - val_accuracy: 0.8719\n",
      "Epoch 51/200\n",
      "4032/4150 [============================>.] - ETA: 0s - loss: 0.3244 - accuracy: 0.8537\n",
      "Epoch 00051: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 89us/sample - loss: 0.3242 - accuracy: 0.8537 - val_loss: 0.7734 - val_accuracy: 0.6224\n",
      "Epoch 52/200\n",
      "4000/4150 [===========================>..] - ETA: 0s - loss: 0.3154 - accuracy: 0.8555\n",
      "Epoch 00052: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 64us/sample - loss: 0.3132 - accuracy: 0.8571 - val_loss: 0.5445 - val_accuracy: 0.7360\n",
      "Epoch 53/200\n",
      "3776/4150 [==========================>...] - ETA: 0s - loss: 0.3350 - accuracy: 0.8512\n",
      "Epoch 00053: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 65us/sample - loss: 0.3636 - accuracy: 0.8467 - val_loss: 1.8672 - val_accuracy: 0.4634\n",
      "Epoch 54/200\n",
      "4096/4150 [============================>.] - ETA: 0s - loss: 0.4232 - accuracy: 0.8301\n",
      "Epoch 00054: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 77us/sample - loss: 0.4219 - accuracy: 0.8306 - val_loss: 0.8686 - val_accuracy: 0.5906\n",
      "Epoch 55/200\n",
      "3584/4150 [========================>.....] - ETA: 0s - loss: 0.3269 - accuracy: 0.8552\n",
      "Epoch 00055: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 66us/sample - loss: 0.3319 - accuracy: 0.8542 - val_loss: 0.6138 - val_accuracy: 0.6830\n",
      "Epoch 56/200\n",
      "3936/4150 [===========================>..] - ETA: 0s - loss: 0.3198 - accuracy: 0.8506\n",
      "Epoch 00056: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 93us/sample - loss: 0.3221 - accuracy: 0.8499 - val_loss: 0.2172 - val_accuracy: 0.9191\n",
      "Epoch 57/200\n",
      "3520/4150 [========================>.....] - ETA: 0s - loss: 0.2905 - accuracy: 0.8662\n",
      "Epoch 00057: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 85us/sample - loss: 0.2974 - accuracy: 0.8631 - val_loss: 0.4433 - val_accuracy: 0.7813\n",
      "Epoch 58/200\n",
      "4096/4150 [============================>.] - ETA: 0s - loss: 0.2850 - accuracy: 0.8713\n",
      "Epoch 00058: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 92us/sample - loss: 0.2868 - accuracy: 0.8701 - val_loss: 0.5250 - val_accuracy: 0.7476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200\n",
      "3840/4150 [==========================>...] - ETA: 0s - loss: 0.2969 - accuracy: 0.8714\n",
      "Epoch 00059: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 80us/sample - loss: 0.2957 - accuracy: 0.8708 - val_loss: 0.4123 - val_accuracy: 0.8015\n",
      "Epoch 60/200\n",
      "3072/4150 [=====================>........] - ETA: 0s - loss: 0.3165 - accuracy: 0.8545\n",
      "Epoch 00060: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 73us/sample - loss: 0.3086 - accuracy: 0.8598 - val_loss: 0.5087 - val_accuracy: 0.7697\n",
      "Epoch 61/200\n",
      "3488/4150 [========================>.....] - ETA: 0s - loss: 0.3035 - accuracy: 0.8675\n",
      "Epoch 00061: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 84us/sample - loss: 0.3045 - accuracy: 0.8667 - val_loss: 0.4694 - val_accuracy: 0.7659\n",
      "Epoch 62/200\n",
      "3456/4150 [=======================>......] - ETA: 0s - loss: 0.2977 - accuracy: 0.8686\n",
      "Epoch 00062: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 92us/sample - loss: 0.3029 - accuracy: 0.8670 - val_loss: 0.4961 - val_accuracy: 0.7601\n",
      "Epoch 63/200\n",
      "3648/4150 [=========================>....] - ETA: 0s - loss: 0.2788 - accuracy: 0.8791\n",
      "Epoch 00063: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 69us/sample - loss: 0.2812 - accuracy: 0.8773 - val_loss: 0.3624 - val_accuracy: 0.8536\n",
      "Epoch 64/200\n",
      "3456/4150 [=======================>......] - ETA: 0s - loss: 0.2482 - accuracy: 0.8935\n",
      "Epoch 00064: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 67us/sample - loss: 0.2505 - accuracy: 0.8923 - val_loss: 0.3286 - val_accuracy: 0.8401\n",
      "Epoch 65/200\n",
      "3776/4150 [==========================>...] - ETA: 0s - loss: 0.2946 - accuracy: 0.8657\n",
      "Epoch 00065: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 66us/sample - loss: 0.2981 - accuracy: 0.8646 - val_loss: 0.2689 - val_accuracy: 0.8902\n",
      "Epoch 66/200\n",
      "3680/4150 [=========================>....] - ETA: 0s - loss: 0.2928 - accuracy: 0.8687\n",
      "Epoch 00066: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 66us/sample - loss: 0.2971 - accuracy: 0.8677 - val_loss: 0.3566 - val_accuracy: 0.8401\n",
      "Epoch 67/200\n",
      "3552/4150 [========================>.....] - ETA: 0s - loss: 0.2886 - accuracy: 0.8680\n",
      "Epoch 00067: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 65us/sample - loss: 0.2793 - accuracy: 0.8735 - val_loss: 0.2513 - val_accuracy: 0.8931\n",
      "Epoch 68/200\n",
      "3552/4150 [========================>.....] - ETA: 0s - loss: 0.2549 - accuracy: 0.8899\n",
      "Epoch 00068: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 71us/sample - loss: 0.2561 - accuracy: 0.8899 - val_loss: 0.1798 - val_accuracy: 0.9403\n",
      "Epoch 69/200\n",
      "3264/4150 [======================>.......] - ETA: 0s - loss: 0.2284 - accuracy: 0.9007\n",
      "Epoch 00069: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 69us/sample - loss: 0.2317 - accuracy: 0.8988 - val_loss: 0.2064 - val_accuracy: 0.9268\n",
      "Epoch 70/200\n",
      "3232/4150 [======================>.......] - ETA: 0s - loss: 0.2376 - accuracy: 0.8936\n",
      "Epoch 00070: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 68us/sample - loss: 0.2443 - accuracy: 0.8913 - val_loss: 1.1152 - val_accuracy: 0.5800\n",
      "Epoch 71/200\n",
      "3648/4150 [=========================>....] - ETA: 0s - loss: 0.2809 - accuracy: 0.8777\n",
      "Epoch 00071: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 68us/sample - loss: 0.2754 - accuracy: 0.8798 - val_loss: 0.4841 - val_accuracy: 0.7592\n",
      "Epoch 72/200\n",
      "3584/4150 [========================>.....] - ETA: 0s - loss: 0.2537 - accuracy: 0.8959\n",
      "Epoch 00072: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 67us/sample - loss: 0.2546 - accuracy: 0.8961 - val_loss: 0.3350 - val_accuracy: 0.8565\n",
      "Epoch 73/200\n",
      "3680/4150 [=========================>....] - ETA: 0s - loss: 0.2335 - accuracy: 0.8995\n",
      "Epoch 00073: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 67us/sample - loss: 0.2300 - accuracy: 0.9014 - val_loss: 0.1903 - val_accuracy: 0.9249\n",
      "Epoch 74/200\n",
      "4000/4150 [===========================>..] - ETA: 0s - loss: 0.2097 - accuracy: 0.9082\n",
      "Epoch 00074: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 62us/sample - loss: 0.2100 - accuracy: 0.9084 - val_loss: 0.1718 - val_accuracy: 0.9383\n",
      "Epoch 75/200\n",
      "4032/4150 [============================>.] - ETA: 0s - loss: 0.2397 - accuracy: 0.8951\n",
      "Epoch 00075: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 63us/sample - loss: 0.2446 - accuracy: 0.8935 - val_loss: 0.4491 - val_accuracy: 0.7919\n",
      "Epoch 76/200\n",
      "3424/4150 [=======================>......] - ETA: 0s - loss: 0.2607 - accuracy: 0.8922\n",
      "Epoch 00076: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 68us/sample - loss: 0.2558 - accuracy: 0.8930 - val_loss: 0.3227 - val_accuracy: 0.8796\n",
      "Epoch 77/200\n",
      "4128/4150 [============================>.] - ETA: 0s - loss: 0.3486 - accuracy: 0.8631\n",
      "Epoch 00077: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 75us/sample - loss: 0.3492 - accuracy: 0.8631 - val_loss: 0.3973 - val_accuracy: 0.8237\n",
      "Epoch 78/200\n",
      "3520/4150 [========================>.....] - ETA: 0s - loss: 0.2366 - accuracy: 0.8920\n",
      "Epoch 00078: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 73us/sample - loss: 0.2287 - accuracy: 0.8966 - val_loss: 0.0869 - val_accuracy: 0.9865\n",
      "Epoch 79/200\n",
      "4064/4150 [============================>.] - ETA: 0s - loss: 0.2072 - accuracy: 0.9109\n",
      "Epoch 00079: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 76us/sample - loss: 0.2068 - accuracy: 0.9108 - val_loss: 0.3633 - val_accuracy: 0.8574\n",
      "Epoch 80/200\n",
      "3488/4150 [========================>.....] - ETA: 0s - loss: 0.2444 - accuracy: 0.8982\n",
      "Epoch 00080: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 83us/sample - loss: 0.2391 - accuracy: 0.9002 - val_loss: 0.3194 - val_accuracy: 0.8661\n",
      "Epoch 81/200\n",
      "4032/4150 [============================>.] - ETA: 0s - loss: 0.3194 - accuracy: 0.8596\n",
      "Epoch 00081: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 79us/sample - loss: 0.3161 - accuracy: 0.8610 - val_loss: 0.2523 - val_accuracy: 0.9017\n",
      "Epoch 82/200\n",
      "3648/4150 [=========================>....] - ETA: 0s - loss: 0.1991 - accuracy: 0.9191\n",
      "Epoch 00082: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 66us/sample - loss: 0.1943 - accuracy: 0.9205 - val_loss: 0.2605 - val_accuracy: 0.9008\n",
      "Epoch 83/200\n",
      "3328/4150 [=======================>......] - ETA: 0s - loss: 0.1912 - accuracy: 0.9174\n",
      "Epoch 00083: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 71us/sample - loss: 0.2083 - accuracy: 0.9118 - val_loss: 0.4604 - val_accuracy: 0.7640\n",
      "Epoch 84/200\n",
      "3968/4150 [===========================>..] - ETA: 0s - loss: 0.2044 - accuracy: 0.9168\n",
      "Epoch 00084: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 79us/sample - loss: 0.2084 - accuracy: 0.9145 - val_loss: 0.0801 - val_accuracy: 0.9778\n",
      "Epoch 85/200\n",
      "3776/4150 [==========================>...] - ETA: 0s - loss: 0.1974 - accuracy: 0.9198\n",
      "Epoch 00085: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 67us/sample - loss: 0.1943 - accuracy: 0.9210 - val_loss: 0.1945 - val_accuracy: 0.9229\n",
      "Epoch 86/200\n",
      "3616/4150 [=========================>....] - ETA: 0s - loss: 0.1886 - accuracy: 0.9259\n",
      "Epoch 00086: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 66us/sample - loss: 0.1959 - accuracy: 0.9224 - val_loss: 0.2835 - val_accuracy: 0.8748\n",
      "Epoch 87/200\n",
      "3136/4150 [=====================>........] - ETA: 0s - loss: 0.1832 - accuracy: 0.9238\n",
      "Epoch 00087: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 58us/sample - loss: 0.1803 - accuracy: 0.9258 - val_loss: 0.3129 - val_accuracy: 0.8661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/200\n",
      "3712/4150 [=========================>....] - ETA: 0s - loss: 0.1638 - accuracy: 0.9337\n",
      "Epoch 00088: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 73us/sample - loss: 0.1774 - accuracy: 0.9275 - val_loss: 0.2522 - val_accuracy: 0.9056\n",
      "Epoch 89/200\n",
      "3424/4150 [=======================>......] - ETA: 0s - loss: 0.1815 - accuracy: 0.9209\n",
      "Epoch 00089: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 67us/sample - loss: 0.1858 - accuracy: 0.9205 - val_loss: 0.3043 - val_accuracy: 0.8738\n",
      "Epoch 90/200\n",
      "3840/4150 [==========================>...] - ETA: 0s - loss: 0.2214 - accuracy: 0.9081\n",
      "Epoch 00090: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 65us/sample - loss: 0.2303 - accuracy: 0.9046 - val_loss: 0.4757 - val_accuracy: 0.7755\n",
      "Epoch 91/200\n",
      "3840/4150 [==========================>...] - ETA: 0s - loss: 0.2354 - accuracy: 0.9000\n",
      "Epoch 00091: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 64us/sample - loss: 0.2316 - accuracy: 0.9012 - val_loss: 0.2926 - val_accuracy: 0.8661\n",
      "Epoch 92/200\n",
      "3936/4150 [===========================>..] - ETA: 0s - loss: 0.1659 - accuracy: 0.9352\n",
      "Epoch 00092: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 94us/sample - loss: 0.1648 - accuracy: 0.9354 - val_loss: 0.2035 - val_accuracy: 0.9056\n",
      "Epoch 93/200\n",
      "4000/4150 [===========================>..] - ETA: 0s - loss: 0.1665 - accuracy: 0.9293\n",
      "Epoch 00093: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 96us/sample - loss: 0.1669 - accuracy: 0.9301 - val_loss: 0.5794 - val_accuracy: 0.7553\n",
      "Epoch 94/200\n",
      "3648/4150 [=========================>....] - ETA: 0s - loss: 0.2635 - accuracy: 0.8953\n",
      "Epoch 00094: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 66us/sample - loss: 0.2592 - accuracy: 0.8961 - val_loss: 0.2072 - val_accuracy: 0.9056\n",
      "Epoch 95/200\n",
      "4128/4150 [============================>.] - ETA: 0s - loss: 0.1995 - accuracy: 0.9201\n",
      "Epoch 00095: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 60us/sample - loss: 0.2028 - accuracy: 0.9195 - val_loss: 0.3215 - val_accuracy: 0.8622\n",
      "Epoch 96/200\n",
      "3776/4150 [==========================>...] - ETA: 0s - loss: 0.1707 - accuracy: 0.9309\n",
      "Epoch 00096: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 70us/sample - loss: 0.1723 - accuracy: 0.9294 - val_loss: 0.4003 - val_accuracy: 0.8247\n",
      "Epoch 97/200\n",
      "3520/4150 [========================>.....] - ETA: 0s - loss: 0.1772 - accuracy: 0.9276\n",
      "Epoch 00097: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 68us/sample - loss: 0.1707 - accuracy: 0.9294 - val_loss: 0.2002 - val_accuracy: 0.9200\n",
      "Epoch 98/200\n",
      "3360/4150 [=======================>......] - ETA: 0s - loss: 0.1465 - accuracy: 0.9455\n",
      "Epoch 00098: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 68us/sample - loss: 0.1562 - accuracy: 0.9400 - val_loss: 0.2154 - val_accuracy: 0.9191\n",
      "Epoch 99/200\n",
      "3456/4150 [=======================>......] - ETA: 0s - loss: 0.1285 - accuracy: 0.9531\n",
      "Epoch 00099: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 68us/sample - loss: 0.1329 - accuracy: 0.9504 - val_loss: 0.1291 - val_accuracy: 0.9509\n",
      "Epoch 100/200\n",
      "3776/4150 [==========================>...] - ETA: 0s - loss: 0.1636 - accuracy: 0.9338\n",
      "Epoch 00100: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 64us/sample - loss: 0.1713 - accuracy: 0.9316 - val_loss: 0.0757 - val_accuracy: 0.9778\n",
      "Epoch 101/200\n",
      "3424/4150 [=======================>......] - ETA: 0s - loss: 0.1823 - accuracy: 0.9305\n",
      "Epoch 00101: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 68us/sample - loss: 0.1816 - accuracy: 0.9287 - val_loss: 0.0785 - val_accuracy: 0.9740\n",
      "Epoch 102/200\n",
      "3648/4150 [=========================>....] - ETA: 0s - loss: 0.2401 - accuracy: 0.9104\n",
      "Epoch 00102: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 66us/sample - loss: 0.2464 - accuracy: 0.9060 - val_loss: 0.4533 - val_accuracy: 0.8179\n",
      "Epoch 103/200\n",
      "4096/4150 [============================>.] - ETA: 0s - loss: 0.1977 - accuracy: 0.9243\n",
      "Epoch 00103: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 62us/sample - loss: 0.1957 - accuracy: 0.9251 - val_loss: 0.2030 - val_accuracy: 0.9152\n",
      "Epoch 104/200\n",
      "3648/4150 [=========================>....] - ETA: 0s - loss: 0.1221 - accuracy: 0.9534\n",
      "Epoch 00104: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 67us/sample - loss: 0.1290 - accuracy: 0.9501 - val_loss: 0.1589 - val_accuracy: 0.9422\n",
      "Epoch 105/200\n",
      "3968/4150 [===========================>..] - ETA: 0s - loss: 0.1240 - accuracy: 0.9529\n",
      "Epoch 00105: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 63us/sample - loss: 0.1253 - accuracy: 0.9518 - val_loss: 0.1045 - val_accuracy: 0.9711\n",
      "Epoch 106/200\n",
      "4128/4150 [============================>.] - ETA: 0s - loss: 0.2017 - accuracy: 0.9254\n",
      "Epoch 00106: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 62us/sample - loss: 0.2022 - accuracy: 0.9251 - val_loss: 0.5831 - val_accuracy: 0.7225\n",
      "Epoch 107/200\n",
      "3776/4150 [==========================>...] - ETA: 0s - loss: 0.1887 - accuracy: 0.9232\n",
      "Epoch 00107: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 65us/sample - loss: 0.1873 - accuracy: 0.9239 - val_loss: 0.1243 - val_accuracy: 0.9538\n",
      "Epoch 108/200\n",
      "4096/4150 [============================>.] - ETA: 0s - loss: 0.1412 - accuracy: 0.9470\n",
      "Epoch 00108: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 62us/sample - loss: 0.1414 - accuracy: 0.9467 - val_loss: 0.2015 - val_accuracy: 0.9297\n",
      "Epoch 109/200\n",
      "3104/4150 [=====================>........] - ETA: 0s - loss: 0.1846 - accuracy: 0.9246\n",
      "Epoch 00109: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 55us/sample - loss: 0.1794 - accuracy: 0.9258 - val_loss: 0.3453 - val_accuracy: 0.8507\n",
      "Epoch 110/200\n",
      "3552/4150 [========================>.....] - ETA: 0s - loss: 0.1780 - accuracy: 0.9282\n",
      "Epoch 00110: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 88us/sample - loss: 0.1760 - accuracy: 0.9284 - val_loss: 0.4662 - val_accuracy: 0.8073\n",
      "Epoch 111/200\n",
      "3552/4150 [========================>.....] - ETA: 0s - loss: 0.1931 - accuracy: 0.9178\n",
      "Epoch 00111: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 81us/sample - loss: 0.1832 - accuracy: 0.9219 - val_loss: 0.2865 - val_accuracy: 0.8690\n",
      "Epoch 112/200\n",
      "3968/4150 [===========================>..] - ETA: 0s - loss: 0.1441 - accuracy: 0.9415\n",
      "Epoch 00112: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 92us/sample - loss: 0.1435 - accuracy: 0.9412 - val_loss: 0.2499 - val_accuracy: 0.9066\n",
      "Epoch 113/200\n",
      "3744/4150 [==========================>...] - ETA: 0s - loss: 0.1163 - accuracy: 0.9570\n",
      "Epoch 00113: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 81us/sample - loss: 0.1176 - accuracy: 0.9557 - val_loss: 0.1226 - val_accuracy: 0.9586\n",
      "Epoch 114/200\n",
      "3744/4150 [==========================>...] - ETA: 0s - loss: 0.2035 - accuracy: 0.9279\n",
      "Epoch 00114: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 80us/sample - loss: 0.2155 - accuracy: 0.9229 - val_loss: 0.5924 - val_accuracy: 0.8006\n",
      "Epoch 115/200\n",
      "3904/4150 [===========================>..] - ETA: 0s - loss: 0.3437 - accuracy: 0.8768\n",
      "Epoch 00115: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 82us/sample - loss: 0.3334 - accuracy: 0.8802 - val_loss: 0.1821 - val_accuracy: 0.9258\n",
      "Epoch 116/200\n",
      "3552/4150 [========================>.....] - ETA: 0s - loss: 0.1581 - accuracy: 0.9383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00116: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 82us/sample - loss: 0.1530 - accuracy: 0.9405 - val_loss: 0.2858 - val_accuracy: 0.8902\n",
      "Epoch 117/200\n",
      "3168/4150 [=====================>........] - ETA: 0s - loss: 0.1240 - accuracy: 0.9508\n",
      "Epoch 00117: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 68us/sample - loss: 0.1262 - accuracy: 0.9484 - val_loss: 0.3326 - val_accuracy: 0.8526\n",
      "Epoch 118/200\n",
      "3360/4150 [=======================>......] - ETA: 0s - loss: 0.1031 - accuracy: 0.9628\n",
      "Epoch 00118: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 72us/sample - loss: 0.1081 - accuracy: 0.9607 - val_loss: 0.1880 - val_accuracy: 0.9287\n",
      "Epoch 119/200\n",
      "3712/4150 [=========================>....] - ETA: 0s - loss: 0.1292 - accuracy: 0.9488\n",
      "Epoch 00119: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 66us/sample - loss: 0.1311 - accuracy: 0.9484 - val_loss: 0.2186 - val_accuracy: 0.9162\n",
      "Epoch 120/200\n",
      "3936/4150 [===========================>..] - ETA: 0s - loss: 0.1137 - accuracy: 0.9560\n",
      "Epoch 00120: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 63us/sample - loss: 0.1126 - accuracy: 0.9564 - val_loss: 0.4034 - val_accuracy: 0.8198\n",
      "Epoch 121/200\n",
      "3648/4150 [=========================>....] - ETA: 0s - loss: 0.1928 - accuracy: 0.9241\n",
      "Epoch 00121: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 66us/sample - loss: 0.1910 - accuracy: 0.9239 - val_loss: 0.4124 - val_accuracy: 0.8603\n",
      "Epoch 122/200\n",
      "3712/4150 [=========================>....] - ETA: 0s - loss: 0.1715 - accuracy: 0.9294\n",
      "Epoch 00122: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 66us/sample - loss: 0.1715 - accuracy: 0.9289 - val_loss: 0.3277 - val_accuracy: 0.8699\n",
      "Epoch 123/200\n",
      "4032/4150 [============================>.] - ETA: 0s - loss: 0.1327 - accuracy: 0.9462\n",
      "Epoch 00123: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 62us/sample - loss: 0.1345 - accuracy: 0.9453 - val_loss: 0.2165 - val_accuracy: 0.9133\n",
      "Epoch 124/200\n",
      "3712/4150 [=========================>....] - ETA: 0s - loss: 0.1518 - accuracy: 0.9450\n",
      "Epoch 00124: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 66us/sample - loss: 0.1470 - accuracy: 0.9465 - val_loss: 0.1113 - val_accuracy: 0.9595\n",
      "Epoch 125/200\n",
      "3936/4150 [===========================>..] - ETA: 0s - loss: 0.1125 - accuracy: 0.9520\n",
      "Epoch 00125: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 64us/sample - loss: 0.1128 - accuracy: 0.9518 - val_loss: 0.1098 - val_accuracy: 0.9566\n",
      "Epoch 126/200\n",
      "3808/4150 [==========================>...] - ETA: 0s - loss: 0.1841 - accuracy: 0.9357\n",
      "Epoch 00126: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 64us/sample - loss: 0.1846 - accuracy: 0.9342 - val_loss: 0.3587 - val_accuracy: 0.8459\n",
      "Epoch 127/200\n",
      "3968/4150 [===========================>..] - ETA: 0s - loss: 0.1433 - accuracy: 0.9458\n",
      "Epoch 00127: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 63us/sample - loss: 0.1469 - accuracy: 0.9424 - val_loss: 0.2850 - val_accuracy: 0.8699\n",
      "Epoch 128/200\n",
      "4064/4150 [============================>.] - ETA: 0s - loss: 0.2197 - accuracy: 0.9178\n",
      "Epoch 00128: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 62us/sample - loss: 0.2175 - accuracy: 0.9188 - val_loss: 0.3444 - val_accuracy: 0.8410\n",
      "Epoch 129/200\n",
      "3648/4150 [=========================>....] - ETA: 0s - loss: 0.1032 - accuracy: 0.9608\n",
      "Epoch 00129: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 67us/sample - loss: 0.1015 - accuracy: 0.9619 - val_loss: 0.1084 - val_accuracy: 0.9586\n",
      "Epoch 130/200\n",
      "3584/4150 [========================>.....] - ETA: 0s - loss: 0.0893 - accuracy: 0.9665\n",
      "Epoch 00130: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 66us/sample - loss: 0.0899 - accuracy: 0.9670 - val_loss: 0.1240 - val_accuracy: 0.9595\n",
      "Epoch 131/200\n",
      "3648/4150 [=========================>....] - ETA: 0s - loss: 0.1136 - accuracy: 0.9581\n",
      "Epoch 00131: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 68us/sample - loss: 0.1146 - accuracy: 0.9559 - val_loss: 0.2438 - val_accuracy: 0.9181\n",
      "Epoch 132/200\n",
      "3456/4150 [=======================>......] - ETA: 0s - loss: 0.1288 - accuracy: 0.9494\n",
      "Epoch 00132: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 67us/sample - loss: 0.1293 - accuracy: 0.9508 - val_loss: 0.2368 - val_accuracy: 0.9104\n",
      "Epoch 133/200\n",
      "3584/4150 [========================>.....] - ETA: 0s - loss: 0.1355 - accuracy: 0.9467\n",
      "Epoch 00133: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 67us/sample - loss: 0.1334 - accuracy: 0.9472 - val_loss: 0.3295 - val_accuracy: 0.8603\n",
      "Epoch 134/200\n",
      "4032/4150 [============================>.] - ETA: 0s - loss: 0.0934 - accuracy: 0.9630\n",
      "Epoch 00134: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 63us/sample - loss: 0.0933 - accuracy: 0.9627 - val_loss: 0.2893 - val_accuracy: 0.8863\n",
      "Epoch 135/200\n",
      "3520/4150 [========================>.....] - ETA: 0s - loss: 0.1395 - accuracy: 0.9435\n",
      "Epoch 00135: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 67us/sample - loss: 0.1337 - accuracy: 0.9453 - val_loss: 0.1470 - val_accuracy: 0.9480\n",
      "Epoch 136/200\n",
      "3616/4150 [=========================>....] - ETA: 0s - loss: 0.0970 - accuracy: 0.9621\n",
      "Epoch 00136: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 66us/sample - loss: 0.1045 - accuracy: 0.9583 - val_loss: 0.4538 - val_accuracy: 0.8227\n",
      "Epoch 137/200\n",
      "4096/4150 [============================>.] - ETA: 0s - loss: 0.1278 - accuracy: 0.9490\n",
      "Epoch 00137: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 62us/sample - loss: 0.1276 - accuracy: 0.9487 - val_loss: 0.1505 - val_accuracy: 0.9518\n",
      "Epoch 138/200\n",
      "3680/4150 [=========================>....] - ETA: 0s - loss: 0.0871 - accuracy: 0.9696\n",
      "Epoch 00138: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 65us/sample - loss: 0.0865 - accuracy: 0.9699 - val_loss: 0.3979 - val_accuracy: 0.8304\n",
      "Epoch 139/200\n",
      "4032/4150 [============================>.] - ETA: 0s - loss: 0.1231 - accuracy: 0.9544\n",
      "Epoch 00139: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 62us/sample - loss: 0.1249 - accuracy: 0.9530 - val_loss: 0.2698 - val_accuracy: 0.8940\n",
      "Epoch 140/200\n",
      "3712/4150 [=========================>....] - ETA: 0s - loss: 0.2216 - accuracy: 0.9165\n",
      "Epoch 00140: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 65us/sample - loss: 0.2178 - accuracy: 0.9178 - val_loss: 0.2856 - val_accuracy: 0.8911\n",
      "Epoch 141/200\n",
      "4128/4150 [============================>.] - ETA: 0s - loss: 0.1315 - accuracy: 0.9462\n",
      "Epoch 00141: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 62us/sample - loss: 0.1318 - accuracy: 0.9458 - val_loss: 0.2807 - val_accuracy: 0.8979\n",
      "Epoch 142/200\n",
      "3584/4150 [========================>.....] - ETA: 0s - loss: 0.0925 - accuracy: 0.9648\n",
      "Epoch 00142: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 67us/sample - loss: 0.0972 - accuracy: 0.9629 - val_loss: 0.1922 - val_accuracy: 0.9383\n",
      "Epoch 143/200\n",
      "3776/4150 [==========================>...] - ETA: 0s - loss: 0.1394 - accuracy: 0.9473\n",
      "Epoch 00143: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 64us/sample - loss: 0.1378 - accuracy: 0.9487 - val_loss: 0.1834 - val_accuracy: 0.9412\n",
      "Epoch 144/200\n",
      "3136/4150 [=====================>........] - ETA: 0s - loss: 0.1069 - accuracy: 0.9589\n",
      "Epoch 00144: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 60us/sample - loss: 0.1043 - accuracy: 0.9600 - val_loss: 0.1969 - val_accuracy: 0.9258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/200\n",
      "3552/4150 [========================>.....] - ETA: 0s - loss: 0.1387 - accuracy: 0.9454\n",
      "Epoch 00145: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 65us/sample - loss: 0.1360 - accuracy: 0.9467 - val_loss: 0.2960 - val_accuracy: 0.8931\n",
      "Epoch 146/200\n",
      "3104/4150 [=====================>........] - ETA: 0s - loss: 0.1051 - accuracy: 0.9613\n",
      "Epoch 00146: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 55us/sample - loss: 0.1018 - accuracy: 0.9631 - val_loss: 0.1542 - val_accuracy: 0.9461\n",
      "Epoch 147/200\n",
      "3872/4150 [==========================>...] - ETA: 0s - loss: 0.0661 - accuracy: 0.9749\n",
      "Epoch 00147: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 67us/sample - loss: 0.0660 - accuracy: 0.9754 - val_loss: 0.0882 - val_accuracy: 0.9778\n",
      "Epoch 148/200\n",
      "3744/4150 [==========================>...] - ETA: 0s - loss: 0.0946 - accuracy: 0.9642\n",
      "Epoch 00148: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 65us/sample - loss: 0.0927 - accuracy: 0.9648 - val_loss: 0.2689 - val_accuracy: 0.8969\n",
      "Epoch 149/200\n",
      "3136/4150 [=====================>........] - ETA: 0s - loss: 0.1031 - accuracy: 0.9589\n",
      "Epoch 00149: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 56us/sample - loss: 0.1249 - accuracy: 0.9537 - val_loss: 1.0692 - val_accuracy: 0.7100\n",
      "Epoch 150/200\n",
      "3488/4150 [========================>.....] - ETA: 0s - loss: 0.1848 - accuracy: 0.9286\n",
      "Epoch 00150: val_loss did not improve from 0.05836\n",
      "4150/4150 [==============================] - 0s 68us/sample - loss: 0.1708 - accuracy: 0.9352 - val_loss: 0.2344 - val_accuracy: 0.8979\n",
      "Epoch 151/200\n",
      "3392/4150 [=======================>......] - ETA: 0s - loss: 0.0707 - accuracy: 0.9741\n",
      "Epoch 00151: val_loss improved from 0.05836 to 0.04715, saving model to best_model_2.h5\n",
      "4150/4150 [==============================] - 0s 68us/sample - loss: 0.0712 - accuracy: 0.9742 - val_loss: 0.0471 - val_accuracy: 0.9836\n",
      "Epoch 152/200\n",
      "3424/4150 [=======================>......] - ETA: 0s - loss: 0.1431 - accuracy: 0.9492\n",
      "Epoch 00152: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 68us/sample - loss: 0.1849 - accuracy: 0.9376 - val_loss: 0.8206 - val_accuracy: 0.8064\n",
      "Epoch 153/200\n",
      "3648/4150 [=========================>....] - ETA: 0s - loss: 0.3627 - accuracy: 0.8838\n",
      "Epoch 00153: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 67us/sample - loss: 0.3399 - accuracy: 0.8896 - val_loss: 0.1947 - val_accuracy: 0.9133\n",
      "Epoch 154/200\n",
      "4096/4150 [============================>.] - ETA: 0s - loss: 0.1524 - accuracy: 0.9390\n",
      "Epoch 00154: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 61us/sample - loss: 0.1521 - accuracy: 0.9390 - val_loss: 0.1608 - val_accuracy: 0.9528\n",
      "Epoch 155/200\n",
      "3072/4150 [=====================>........] - ETA: 0s - loss: 0.0961 - accuracy: 0.9609\n",
      "Epoch 00155: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 70us/sample - loss: 0.0910 - accuracy: 0.9646 - val_loss: 0.1316 - val_accuracy: 0.9576\n",
      "Epoch 156/200\n",
      "3680/4150 [=========================>....] - ETA: 0s - loss: 0.0662 - accuracy: 0.9758\n",
      "Epoch 00156: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 67us/sample - loss: 0.0658 - accuracy: 0.9759 - val_loss: 0.1433 - val_accuracy: 0.9499\n",
      "Epoch 157/200\n",
      "4064/4150 [============================>.] - ETA: 0s - loss: 0.0548 - accuracy: 0.9813\n",
      "Epoch 00157: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 63us/sample - loss: 0.0554 - accuracy: 0.9810 - val_loss: 0.1461 - val_accuracy: 0.9432\n",
      "Epoch 158/200\n",
      "3744/4150 [==========================>...] - ETA: 0s - loss: 0.0723 - accuracy: 0.9706\n",
      "Epoch 00158: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 65us/sample - loss: 0.0771 - accuracy: 0.9689 - val_loss: 0.0820 - val_accuracy: 0.9730\n",
      "Epoch 159/200\n",
      "3840/4150 [==========================>...] - ETA: 0s - loss: 0.0952 - accuracy: 0.9620\n",
      "Epoch 00159: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 65us/sample - loss: 0.0958 - accuracy: 0.9624 - val_loss: 0.1850 - val_accuracy: 0.9306\n",
      "Epoch 160/200\n",
      "3072/4150 [=====================>........] - ETA: 0s - loss: 0.0881 - accuracy: 0.9652\n",
      "Epoch 00160: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 57us/sample - loss: 0.0772 - accuracy: 0.9704 - val_loss: 0.1663 - val_accuracy: 0.9374\n",
      "Epoch 161/200\n",
      "3520/4150 [========================>.....] - ETA: 0s - loss: 0.0640 - accuracy: 0.9778\n",
      "Epoch 00161: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 69us/sample - loss: 0.0668 - accuracy: 0.9759 - val_loss: 0.1676 - val_accuracy: 0.9316\n",
      "Epoch 162/200\n",
      "3904/4150 [===========================>..] - ETA: 0s - loss: 0.0626 - accuracy: 0.9759\n",
      "Epoch 00162: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 64us/sample - loss: 0.0625 - accuracy: 0.9761 - val_loss: 0.1481 - val_accuracy: 0.9538\n",
      "Epoch 163/200\n",
      "4032/4150 [============================>.] - ETA: 0s - loss: 0.0625 - accuracy: 0.9777\n",
      "Epoch 00163: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 62us/sample - loss: 0.0618 - accuracy: 0.9783 - val_loss: 0.2916 - val_accuracy: 0.8960\n",
      "Epoch 164/200\n",
      "3648/4150 [=========================>....] - ETA: 0s - loss: 0.0721 - accuracy: 0.9737\n",
      "Epoch 00164: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 66us/sample - loss: 0.0701 - accuracy: 0.9745 - val_loss: 0.1719 - val_accuracy: 0.9374\n",
      "Epoch 165/200\n",
      "4064/4150 [============================>.] - ETA: 0s - loss: 0.1060 - accuracy: 0.9604\n",
      "Epoch 00165: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 62us/sample - loss: 0.1059 - accuracy: 0.9602 - val_loss: 0.1278 - val_accuracy: 0.9595\n",
      "Epoch 166/200\n",
      "3712/4150 [=========================>....] - ETA: 0s - loss: 0.1628 - accuracy: 0.9421\n",
      "Epoch 00166: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 66us/sample - loss: 0.1557 - accuracy: 0.9443 - val_loss: 0.4369 - val_accuracy: 0.8333\n",
      "Epoch 167/200\n",
      "4096/4150 [============================>.] - ETA: 0s - loss: 0.1795 - accuracy: 0.9348\n",
      "Epoch 00167: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 64us/sample - loss: 0.1776 - accuracy: 0.9354 - val_loss: 0.1298 - val_accuracy: 0.9518\n",
      "Epoch 168/200\n",
      "4032/4150 [============================>.] - ETA: 0s - loss: 0.0874 - accuracy: 0.9675\n",
      "Epoch 00168: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 78us/sample - loss: 0.0856 - accuracy: 0.9682 - val_loss: 0.1216 - val_accuracy: 0.9624\n",
      "Epoch 169/200\n",
      "3520/4150 [========================>.....] - ETA: 0s - loss: 0.0924 - accuracy: 0.9628\n",
      "Epoch 00169: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 85us/sample - loss: 0.1031 - accuracy: 0.9600 - val_loss: 0.2214 - val_accuracy: 0.9123\n",
      "Epoch 170/200\n",
      "3552/4150 [========================>.....] - ETA: 0s - loss: 0.2123 - accuracy: 0.9369\n",
      "Epoch 00170: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 84us/sample - loss: 0.1987 - accuracy: 0.9393 - val_loss: 0.1301 - val_accuracy: 0.9624\n",
      "Epoch 171/200\n",
      "3680/4150 [=========================>....] - ETA: 0s - loss: 0.0729 - accuracy: 0.9758\n",
      "Epoch 00171: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 85us/sample - loss: 0.0722 - accuracy: 0.9754 - val_loss: 0.2039 - val_accuracy: 0.9345\n",
      "Epoch 172/200\n",
      "3424/4150 [=======================>......] - ETA: 0s - loss: 0.0558 - accuracy: 0.9801\n",
      "Epoch 00172: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 72us/sample - loss: 0.0570 - accuracy: 0.9786 - val_loss: 0.1378 - val_accuracy: 0.9538\n",
      "Epoch 173/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4150 [===========================>..] - ETA: 0s - loss: 0.0442 - accuracy: 0.9852\n",
      "Epoch 00173: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 92us/sample - loss: 0.0440 - accuracy: 0.9851 - val_loss: 0.0923 - val_accuracy: 0.9711\n",
      "Epoch 174/200\n",
      "3232/4150 [======================>.......] - ETA: 0s - loss: 0.0422 - accuracy: 0.9889\n",
      "Epoch 00174: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 68us/sample - loss: 0.0432 - accuracy: 0.9884 - val_loss: 0.1566 - val_accuracy: 0.9412\n",
      "Epoch 175/200\n",
      "3488/4150 [========================>.....] - ETA: 0s - loss: 0.0517 - accuracy: 0.9811\n",
      "Epoch 00175: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 71us/sample - loss: 0.0539 - accuracy: 0.9805 - val_loss: 0.2207 - val_accuracy: 0.9326\n",
      "Epoch 176/200\n",
      "4032/4150 [============================>.] - ETA: 0s - loss: 0.0820 - accuracy: 0.9680\n",
      "Epoch 00176: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 63us/sample - loss: 0.0848 - accuracy: 0.9672 - val_loss: 0.6232 - val_accuracy: 0.8035\n",
      "Epoch 177/200\n",
      "3968/4150 [===========================>..] - ETA: 0s - loss: 0.1181 - accuracy: 0.9569\n",
      "Epoch 00177: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 63us/sample - loss: 0.1158 - accuracy: 0.9573 - val_loss: 0.1609 - val_accuracy: 0.9547\n",
      "Epoch 178/200\n",
      "3936/4150 [===========================>..] - ETA: 0s - loss: 0.0983 - accuracy: 0.9627\n",
      "Epoch 00178: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 63us/sample - loss: 0.0980 - accuracy: 0.9631 - val_loss: 0.0912 - val_accuracy: 0.9740\n",
      "Epoch 179/200\n",
      "4032/4150 [============================>.] - ETA: 0s - loss: 0.0680 - accuracy: 0.9772\n",
      "Epoch 00179: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 63us/sample - loss: 0.0686 - accuracy: 0.9769 - val_loss: 0.2009 - val_accuracy: 0.9412\n",
      "Epoch 180/200\n",
      "3616/4150 [=========================>....] - ETA: 0s - loss: 0.1006 - accuracy: 0.9616\n",
      "Epoch 00180: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 67us/sample - loss: 0.0963 - accuracy: 0.9631 - val_loss: 0.0852 - val_accuracy: 0.9701\n",
      "Epoch 181/200\n",
      "3904/4150 [===========================>..] - ETA: 0s - loss: 0.0531 - accuracy: 0.9810\n",
      "Epoch 00181: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 64us/sample - loss: 0.0531 - accuracy: 0.9812 - val_loss: 0.0864 - val_accuracy: 0.9740\n",
      "Epoch 182/200\n",
      "3648/4150 [=========================>....] - ETA: 0s - loss: 0.0855 - accuracy: 0.9646\n",
      "Epoch 00182: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 80us/sample - loss: 0.0861 - accuracy: 0.9643 - val_loss: 0.0859 - val_accuracy: 0.9624\n",
      "Epoch 183/200\n",
      "4000/4150 [===========================>..] - ETA: 0s - loss: 0.1033 - accuracy: 0.9617\n",
      "Epoch 00183: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 77us/sample - loss: 0.1011 - accuracy: 0.9627 - val_loss: 0.1194 - val_accuracy: 0.9586\n",
      "Epoch 184/200\n",
      "3904/4150 [===========================>..] - ETA: 0s - loss: 0.1102 - accuracy: 0.9618\n",
      "Epoch 00184: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 64us/sample - loss: 0.1144 - accuracy: 0.9602 - val_loss: 0.2139 - val_accuracy: 0.9181\n",
      "Epoch 185/200\n",
      "3424/4150 [=======================>......] - ETA: 0s - loss: 0.0740 - accuracy: 0.9734\n",
      "Epoch 00185: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 70us/sample - loss: 0.0769 - accuracy: 0.9725 - val_loss: 0.0643 - val_accuracy: 0.9798\n",
      "Epoch 186/200\n",
      "4096/4150 [============================>.] - ETA: 0s - loss: 0.0870 - accuracy: 0.9651\n",
      "Epoch 00186: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 74us/sample - loss: 0.0887 - accuracy: 0.9651 - val_loss: 0.1136 - val_accuracy: 0.9576\n",
      "Epoch 187/200\n",
      "4096/4150 [============================>.] - ETA: 0s - loss: 0.1201 - accuracy: 0.9548\n",
      "Epoch 00187: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 75us/sample - loss: 0.1200 - accuracy: 0.9549 - val_loss: 0.2044 - val_accuracy: 0.9268\n",
      "Epoch 188/200\n",
      "3776/4150 [==========================>...] - ETA: 0s - loss: 0.0883 - accuracy: 0.9674\n",
      "Epoch 00188: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 65us/sample - loss: 0.0880 - accuracy: 0.9667 - val_loss: 0.1785 - val_accuracy: 0.9374\n",
      "Epoch 189/200\n",
      "3616/4150 [=========================>....] - ETA: 0s - loss: 0.0437 - accuracy: 0.9851\n",
      "Epoch 00189: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 68us/sample - loss: 0.0430 - accuracy: 0.9853 - val_loss: 0.0579 - val_accuracy: 0.9778\n",
      "Epoch 190/200\n",
      "3392/4150 [=======================>......] - ETA: 0s - loss: 0.0516 - accuracy: 0.9832\n",
      "Epoch 00190: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 70us/sample - loss: 0.0489 - accuracy: 0.9841 - val_loss: 0.0757 - val_accuracy: 0.9692\n",
      "Epoch 191/200\n",
      "3776/4150 [==========================>...] - ETA: 0s - loss: 0.0453 - accuracy: 0.9865\n",
      "Epoch 00191: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 84us/sample - loss: 0.0479 - accuracy: 0.9855 - val_loss: 0.1297 - val_accuracy: 0.9538\n",
      "Epoch 192/200\n",
      "4032/4150 [============================>.] - ETA: 0s - loss: 0.1762 - accuracy: 0.9435\n",
      "Epoch 00192: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 77us/sample - loss: 0.1823 - accuracy: 0.9414 - val_loss: 1.0624 - val_accuracy: 0.6753\n",
      "Epoch 193/200\n",
      "4032/4150 [============================>.] - ETA: 0s - loss: 0.4002 - accuracy: 0.8772\n",
      "Epoch 00193: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 78us/sample - loss: 0.3930 - accuracy: 0.8790 - val_loss: 0.2197 - val_accuracy: 0.9277\n",
      "Epoch 194/200\n",
      "4096/4150 [============================>.] - ETA: 0s - loss: 0.1055 - accuracy: 0.9604\n",
      "Epoch 00194: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 107us/sample - loss: 0.1047 - accuracy: 0.9607 - val_loss: 0.2236 - val_accuracy: 0.9229\n",
      "Epoch 195/200\n",
      "3680/4150 [=========================>....] - ETA: 0s - loss: 0.0665 - accuracy: 0.9793\n",
      "Epoch 00195: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 103us/sample - loss: 0.0640 - accuracy: 0.9798 - val_loss: 0.0716 - val_accuracy: 0.9778\n",
      "Epoch 196/200\n",
      "3616/4150 [=========================>....] - ETA: 0s - loss: 0.0378 - accuracy: 0.9884\n",
      "Epoch 00196: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 52us/sample - loss: 0.0400 - accuracy: 0.9875 - val_loss: 0.1890 - val_accuracy: 0.9345\n",
      "Epoch 197/200\n",
      "3360/4150 [=======================>......] - ETA: 0s - loss: 0.0698 - accuracy: 0.9762\n",
      "Epoch 00197: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 56us/sample - loss: 0.0634 - accuracy: 0.9783 - val_loss: 0.1437 - val_accuracy: 0.9595\n",
      "Epoch 198/200\n",
      "3264/4150 [======================>.......] - ETA: 0s - loss: 0.0643 - accuracy: 0.9727\n",
      "Epoch 00198: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 56us/sample - loss: 0.0665 - accuracy: 0.9733 - val_loss: 0.0508 - val_accuracy: 0.9827\n",
      "Epoch 199/200\n",
      "3808/4150 [==========================>...] - ETA: 0s - loss: 0.0722 - accuracy: 0.9745\n",
      "Epoch 00199: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 50us/sample - loss: 0.0712 - accuracy: 0.9749 - val_loss: 0.0954 - val_accuracy: 0.9701\n",
      "Epoch 200/200\n",
      "3904/4150 [===========================>..] - ETA: 0s - loss: 0.0638 - accuracy: 0.9793\n",
      "Epoch 00200: val_loss did not improve from 0.04715\n",
      "4150/4150 [==============================] - 0s 48us/sample - loss: 0.0619 - accuracy: 0.9802 - val_loss: 0.2645 - val_accuracy: 0.9027\n",
      "Testing on Fold 3\n",
      "# Getting train data set up\n",
      "# Getting test data set up\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2400 samples, validate on 600 samples\n",
      "Epoch 1/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 192.6307 - mse: 192.6307\n",
      "Epoch 00001: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 4s 2ms/sample - loss: 191.3546 - mse: 191.3546 - val_loss: 136.3155 - val_mse: 136.3155\n",
      "Epoch 2/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 169.8871 - mse: 169.8871\n",
      "Epoch 00002: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 917us/sample - loss: 173.8672 - mse: 173.8672 - val_loss: 112.7022 - val_mse: 112.7022\n",
      "Epoch 3/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 165.7134 - mse: 165.7134\n",
      "Epoch 00003: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 674us/sample - loss: 166.4826 - mse: 166.4826 - val_loss: 117.2998 - val_mse: 117.2998\n",
      "Epoch 4/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 158.1730 - mse: 158.1730- ETA: 0s - loss: 150.5263 - ms\n",
      "Epoch 00004: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 814us/sample - loss: 159.6364 - mse: 159.6364 - val_loss: 114.3195 - val_mse: 114.3195\n",
      "Epoch 5/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 160.5405 - mse: 160.5405\n",
      "Epoch 00005: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 767us/sample - loss: 158.4694 - mse: 158.4694 - val_loss: 138.6964 - val_mse: 138.6965\n",
      "Epoch 6/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 161.1521 - mse: 161.1521\n",
      "Epoch 00006: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 785us/sample - loss: 161.0121 - mse: 161.0121 - val_loss: 120.2608 - val_mse: 120.2608\n",
      "Epoch 7/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 163.1197 - mse: 163.1197\n",
      "Epoch 00007: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 811us/sample - loss: 161.9584 - mse: 161.9583 - val_loss: 115.6909 - val_mse: 115.6909\n",
      "Epoch 8/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 154.2139 - mse: 154.2139\n",
      "Epoch 00008: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 764us/sample - loss: 153.0801 - mse: 153.0801 - val_loss: 118.8275 - val_mse: 118.8275\n",
      "Epoch 9/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 152.4071 - mse: 152.4071\n",
      "Epoch 00009: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 730us/sample - loss: 151.2422 - mse: 151.2422 - val_loss: 115.0790 - val_mse: 115.0790\n",
      "Epoch 10/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 149.2932 - mse: 149.2932\n",
      "Epoch 00010: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 751us/sample - loss: 148.5256 - mse: 148.5256 - val_loss: 130.9828 - val_mse: 130.9828\n",
      "Epoch 11/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 151.8965 - mse: 151.8965\n",
      "Epoch 00011: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 721us/sample - loss: 151.5511 - mse: 151.5511 - val_loss: 141.0099 - val_mse: 141.0099\n",
      "Epoch 12/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 155.6764 - mse: 155.6764\n",
      "Epoch 00012: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 713us/sample - loss: 153.6630 - mse: 153.6630 - val_loss: 114.1198 - val_mse: 114.1197\n",
      "Epoch 13/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 147.8707 - mse: 147.8708\n",
      "Epoch 00013: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 736us/sample - loss: 149.7029 - mse: 149.7029 - val_loss: 124.4326 - val_mse: 124.4326\n",
      "Epoch 14/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 151.0645 - mse: 151.0645\n",
      "Epoch 00014: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 759us/sample - loss: 151.7724 - mse: 151.7724 - val_loss: 120.0350 - val_mse: 120.0350\n",
      "Epoch 15/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 158.6493 - mse: 158.6493- ETA: 1s - loss: 155.0\n",
      "Epoch 00015: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 758us/sample - loss: 159.1556 - mse: 159.1556 - val_loss: 119.5833 - val_mse: 119.5833\n",
      "Epoch 16/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 160.2747 - mse: 160.2747\n",
      "Epoch 00016: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 830us/sample - loss: 165.0587 - mse: 165.0587 - val_loss: 122.1984 - val_mse: 122.1983\n",
      "Epoch 17/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 167.8514 - mse: 167.8514\n",
      "Epoch 00017: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 737us/sample - loss: 165.4476 - mse: 165.4476 - val_loss: 122.6537 - val_mse: 122.6537\n",
      "Epoch 18/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 168.0877 - mse: 168.0877\n",
      "Epoch 00018: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 712us/sample - loss: 166.1790 - mse: 166.1790 - val_loss: 122.7420 - val_mse: 122.7420\n",
      "Epoch 19/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.6933 - mse: 162.6933\n",
      "Epoch 00019: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 722us/sample - loss: 161.8603 - mse: 161.8603 - val_loss: 134.6883 - val_mse: 134.6883\n",
      "Epoch 20/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 160.0676 - mse: 160.0677\n",
      "Epoch 00020: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 743us/sample - loss: 159.3702 - mse: 159.3703 - val_loss: 120.8831 - val_mse: 120.8831\n",
      "Epoch 21/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 153.8686 - mse: 153.8686\n",
      "Epoch 00021: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 763us/sample - loss: 152.3709 - mse: 152.3709 - val_loss: 120.5568 - val_mse: 120.5568\n",
      "Epoch 22/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 159.7181 - mse: 159.7181\n",
      "Epoch 00022: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 735us/sample - loss: 156.9708 - mse: 156.9708 - val_loss: 117.8036 - val_mse: 117.8036\n",
      "Epoch 23/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 157.6622 - mse: 157.6622\n",
      "Epoch 00023: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 730us/sample - loss: 156.9800 - mse: 156.9799 - val_loss: 123.4298 - val_mse: 123.4298\n",
      "Epoch 24/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 159.5453 - mse: 159.5453\n",
      "Epoch 00024: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 776us/sample - loss: 158.8699 - mse: 158.8699 - val_loss: 116.4963 - val_mse: 116.4963\n",
      "Epoch 25/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 159.6052 - mse: 159.6053\n",
      "Epoch 00025: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 761us/sample - loss: 157.4734 - mse: 157.4734 - val_loss: 123.1884 - val_mse: 123.1884\n",
      "Epoch 26/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 155.2619 - mse: 155.2619\n",
      "Epoch 00026: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 735us/sample - loss: 154.2339 - mse: 154.2339 - val_loss: 113.8526 - val_mse: 113.8526\n",
      "Epoch 27/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 160.7607 - mse: 160.7607\n",
      "Epoch 00027: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 752us/sample - loss: 159.6284 - mse: 159.6284 - val_loss: 120.4541 - val_mse: 120.4541\n",
      "Epoch 28/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 164.0873 - mse: 164.0873\n",
      "Epoch 00028: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 742us/sample - loss: 161.7522 - mse: 161.7522 - val_loss: 120.5592 - val_mse: 120.5592\n",
      "Epoch 29/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2368/2400 [============================>.] - ETA: 0s - loss: 157.4029 - mse: 157.4029\n",
      "Epoch 00029: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 735us/sample - loss: 157.5194 - mse: 157.5194 - val_loss: 143.0723 - val_mse: 143.0723\n",
      "Epoch 30/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 151.1254 - mse: 151.1255\n",
      "Epoch 00030: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 710us/sample - loss: 158.5314 - mse: 158.5314 - val_loss: 128.8830 - val_mse: 128.8830\n",
      "Epoch 31/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 154.5881 - mse: 154.5882\n",
      "Epoch 00031: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 720us/sample - loss: 155.3672 - mse: 155.3673 - val_loss: 118.0738 - val_mse: 118.0738\n",
      "Epoch 32/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 153.2976 - mse: 153.2976\n",
      "Epoch 00032: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 717us/sample - loss: 155.1495 - mse: 155.1495 - val_loss: 129.6623 - val_mse: 129.6623\n",
      "Epoch 33/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 154.5584 - mse: 154.5584\n",
      "Epoch 00033: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 785us/sample - loss: 153.2643 - mse: 153.2643 - val_loss: 115.4780 - val_mse: 115.4780\n",
      "Epoch 34/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 157.5263 - mse: 157.5262\n",
      "Epoch 00034: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 806us/sample - loss: 156.0715 - mse: 156.0715 - val_loss: 121.3532 - val_mse: 121.3532\n",
      "Epoch 35/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 155.3591 - mse: 155.3591\n",
      "Epoch 00035: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 749us/sample - loss: 154.8772 - mse: 154.8772 - val_loss: 116.0185 - val_mse: 116.0185\n",
      "Epoch 36/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 155.0509 - mse: 155.0509\n",
      "Epoch 00036: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 734us/sample - loss: 154.6574 - mse: 154.6574 - val_loss: 131.8214 - val_mse: 131.8214\n",
      "Epoch 37/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 148.2815 - mse: 148.2816\n",
      "Epoch 00037: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 742us/sample - loss: 148.5246 - mse: 148.5247 - val_loss: 153.9786 - val_mse: 153.9786\n",
      "Epoch 38/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 151.4648 - mse: 151.4648\n",
      "Epoch 00038: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 713us/sample - loss: 152.4253 - mse: 152.4253 - val_loss: 115.8447 - val_mse: 115.8447\n",
      "Epoch 39/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 147.5286 - mse: 147.5286\n",
      "Epoch 00039: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 722us/sample - loss: 146.8517 - mse: 146.8517 - val_loss: 113.3951 - val_mse: 113.3951\n",
      "Epoch 40/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 143.2057 - mse: 143.2057\n",
      "Epoch 00040: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 787us/sample - loss: 147.3209 - mse: 147.3209 - val_loss: 116.1533 - val_mse: 116.1533\n",
      "Epoch 41/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 151.0403 - mse: 151.0403\n",
      "Epoch 00041: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 798us/sample - loss: 150.1480 - mse: 150.1480 - val_loss: 123.3701 - val_mse: 123.3701\n",
      "Epoch 42/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 151.2900 - mse: 151.2900\n",
      "Epoch 00042: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 827us/sample - loss: 148.5727 - mse: 148.5727 - val_loss: 112.9895 - val_mse: 112.9895\n",
      "Epoch 43/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 153.4153 - mse: 153.4153\n",
      "Epoch 00043: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 807us/sample - loss: 151.8392 - mse: 151.8393 - val_loss: 121.1201 - val_mse: 121.1201\n",
      "Epoch 44/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 141.2375 - mse: 141.2374\n",
      "Epoch 00044: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 757us/sample - loss: 145.7192 - mse: 145.7192 - val_loss: 137.3977 - val_mse: 137.3977\n",
      "Epoch 45/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 146.3049 - mse: 146.3049\n",
      "Epoch 00045: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 798us/sample - loss: 146.1531 - mse: 146.1532 - val_loss: 126.3879 - val_mse: 126.3878\n",
      "Epoch 46/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 153.7506 - mse: 153.7506\n",
      "Epoch 00046: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 871us/sample - loss: 152.5062 - mse: 152.5062 - val_loss: 116.5572 - val_mse: 116.5572\n",
      "Epoch 47/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 149.9180 - mse: 149.9181\n",
      "Epoch 00047: val_loss did not improve from 111.12089\n",
      "2400/2400 [==============================] - 2s 806us/sample - loss: 146.1334 - mse: 146.1334 - val_loss: 130.9172 - val_mse: 130.9172\n",
      "Epoch 48/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 143.6968 - mse: 143.6967\n",
      "Epoch 00048: val_loss improved from 111.12089 to 110.39956, saving model to best_model_1.h5\n",
      "2400/2400 [==============================] - 2s 814us/sample - loss: 144.3967 - mse: 144.3967 - val_loss: 110.3996 - val_mse: 110.3996\n",
      "Epoch 49/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 149.7406 - mse: 149.7406\n",
      "Epoch 00049: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 757us/sample - loss: 146.2418 - mse: 146.2419 - val_loss: 111.6435 - val_mse: 111.6435\n",
      "Epoch 50/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 148.6887 - mse: 148.6887\n",
      "Epoch 00050: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 834us/sample - loss: 147.9548 - mse: 147.9548 - val_loss: 116.8457 - val_mse: 116.8457\n",
      "Epoch 51/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 148.4739 - mse: 148.4738\n",
      "Epoch 00051: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 876us/sample - loss: 146.7057 - mse: 146.7057 - val_loss: 157.7446 - val_mse: 157.7446\n",
      "Epoch 52/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 149.6472 - mse: 149.6472\n",
      "Epoch 00052: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 778us/sample - loss: 148.6020 - mse: 148.6020 - val_loss: 121.4780 - val_mse: 121.4781\n",
      "Epoch 53/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 146.1411 - mse: 146.1411\n",
      "Epoch 00053: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 769us/sample - loss: 143.8441 - mse: 143.8441 - val_loss: 116.6677 - val_mse: 116.6676\n",
      "Epoch 54/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 152.6870 - mse: 152.6870\n",
      "Epoch 00054: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 767us/sample - loss: 149.3820 - mse: 149.3820 - val_loss: 114.5603 - val_mse: 114.5603\n",
      "Epoch 55/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 148.3033 - mse: 148.3032\n",
      "Epoch 00055: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 750us/sample - loss: 147.2483 - mse: 147.2483 - val_loss: 122.1218 - val_mse: 122.1218\n",
      "Epoch 56/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 143.5421 - mse: 143.5421\n",
      "Epoch 00056: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 757us/sample - loss: 141.2698 - mse: 141.2698 - val_loss: 140.7268 - val_mse: 140.7268\n",
      "Epoch 57/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 145.4928 - mse: 145.4928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00057: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 736us/sample - loss: 146.5114 - mse: 146.5114 - val_loss: 120.0278 - val_mse: 120.0278\n",
      "Epoch 58/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 143.9574 - mse: 143.9575\n",
      "Epoch 00058: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 732us/sample - loss: 143.8667 - mse: 143.8667 - val_loss: 111.7701 - val_mse: 111.7701\n",
      "Epoch 59/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 148.6362 - mse: 148.6363\n",
      "Epoch 00059: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 805us/sample - loss: 146.3318 - mse: 146.3318 - val_loss: 126.4993 - val_mse: 126.4993\n",
      "Epoch 60/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 137.5600 - mse: 137.5600\n",
      "Epoch 00060: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 817us/sample - loss: 144.5059 - mse: 144.5059 - val_loss: 112.2204 - val_mse: 112.2204\n",
      "Epoch 61/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 142.0960 - mse: 142.0959\n",
      "Epoch 00061: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 739us/sample - loss: 141.4831 - mse: 141.4831 - val_loss: 115.6952 - val_mse: 115.6952\n",
      "Epoch 62/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 154.5630 - mse: 154.5630\n",
      "Epoch 00062: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 740us/sample - loss: 155.3604 - mse: 155.3604 - val_loss: 118.5203 - val_mse: 118.5203\n",
      "Epoch 63/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 153.3305 - mse: 153.3305\n",
      "Epoch 00063: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 730us/sample - loss: 154.5571 - mse: 154.5571 - val_loss: 122.0733 - val_mse: 122.0733\n",
      "Epoch 64/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 149.2290 - mse: 149.2290\n",
      "Epoch 00064: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 726us/sample - loss: 148.4587 - mse: 148.4587 - val_loss: 118.7998 - val_mse: 118.7998\n",
      "Epoch 65/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 153.6803 - mse: 153.6803\n",
      "Epoch 00065: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 725us/sample - loss: 154.1697 - mse: 154.1697 - val_loss: 138.7107 - val_mse: 138.7107\n",
      "Epoch 66/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 147.9063 - mse: 147.9063\n",
      "Epoch 00066: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 714us/sample - loss: 146.0730 - mse: 146.0730 - val_loss: 120.7645 - val_mse: 120.7645\n",
      "Epoch 67/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 147.2893 - mse: 147.2893\n",
      "Epoch 00067: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 756us/sample - loss: 144.2487 - mse: 144.2487 - val_loss: 113.0510 - val_mse: 113.0510\n",
      "Epoch 68/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 150.6673 - mse: 150.6674\n",
      "Epoch 00068: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 786us/sample - loss: 152.9712 - mse: 152.9713 - val_loss: 121.3711 - val_mse: 121.3711\n",
      "Epoch 69/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 154.2982 - mse: 154.2982\n",
      "Epoch 00069: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 763us/sample - loss: 153.7900 - mse: 153.7900 - val_loss: 113.5062 - val_mse: 113.5062\n",
      "Epoch 70/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 153.0432 - mse: 153.0432\n",
      "Epoch 00070: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 732us/sample - loss: 152.5674 - mse: 152.5675 - val_loss: 119.5019 - val_mse: 119.5019\n",
      "Epoch 71/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 154.8558 - mse: 154.8557\n",
      "Epoch 00071: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 728us/sample - loss: 157.9043 - mse: 157.9043 - val_loss: 123.5979 - val_mse: 123.5980\n",
      "Epoch 72/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 151.2457 - mse: 151.2457\n",
      "Epoch 00072: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 711us/sample - loss: 148.8029 - mse: 148.8029 - val_loss: 119.8979 - val_mse: 119.8979\n",
      "Epoch 73/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 159.2156 - mse: 159.2155\n",
      "Epoch 00073: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 723us/sample - loss: 158.0978 - mse: 158.0978 - val_loss: 124.8871 - val_mse: 124.8871\n",
      "Epoch 74/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 150.3655 - mse: 150.3655\n",
      "Epoch 00074: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 733us/sample - loss: 152.2037 - mse: 152.2037 - val_loss: 117.6859 - val_mse: 117.6858\n",
      "Epoch 75/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 141.6773 - mse: 141.6773- ETA: 0s - loss: 146.5514 - mse: 1\n",
      "Epoch 00075: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 712us/sample - loss: 149.8399 - mse: 149.8399 - val_loss: 149.9904 - val_mse: 149.9904\n",
      "Epoch 76/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 148.6871 - mse: 148.6871\n",
      "Epoch 00076: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 756us/sample - loss: 150.2035 - mse: 150.2035 - val_loss: 114.7978 - val_mse: 114.7979\n",
      "Epoch 77/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 148.4699 - mse: 148.4699\n",
      "Epoch 00077: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 799us/sample - loss: 148.6866 - mse: 148.6866 - val_loss: 115.1095 - val_mse: 115.1095\n",
      "Epoch 78/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 158.8564 - mse: 158.8564\n",
      "Epoch 00078: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 756us/sample - loss: 157.1416 - mse: 157.1416 - val_loss: 119.6581 - val_mse: 119.6581\n",
      "Epoch 79/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 148.4305 - mse: 148.4305\n",
      "Epoch 00079: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 736us/sample - loss: 151.7426 - mse: 151.7426 - val_loss: 134.3863 - val_mse: 134.3863\n",
      "Epoch 80/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 157.2984 - mse: 157.2985\n",
      "Epoch 00080: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 791us/sample - loss: 163.7030 - mse: 163.7030 - val_loss: 121.9656 - val_mse: 121.9655\n",
      "Epoch 81/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 160.7296 - mse: 160.7296\n",
      "Epoch 00081: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 749us/sample - loss: 161.7699 - mse: 161.7699 - val_loss: 122.3215 - val_mse: 122.3215\n",
      "Epoch 82/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 161.8762 - mse: 161.8763\n",
      "Epoch 00082: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 743us/sample - loss: 161.0405 - mse: 161.0406 - val_loss: 122.8214 - val_mse: 122.8214\n",
      "Epoch 83/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.3253 - mse: 163.3254\n",
      "Epoch 00083: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 736us/sample - loss: 163.5822 - mse: 163.5823 - val_loss: 124.0508 - val_mse: 124.0508\n",
      "Epoch 84/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 164.3565 - mse: 164.3565\n",
      "Epoch 00084: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 735us/sample - loss: 162.0731 - mse: 162.0731 - val_loss: 121.8425 - val_mse: 121.8425\n",
      "Epoch 85/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.2153 - mse: 162.2153\n",
      "Epoch 00085: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 791us/sample - loss: 162.2437 - mse: 162.2437 - val_loss: 122.2163 - val_mse: 122.2162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 161.8802 - mse: 161.8802\n",
      "Epoch 00086: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 799us/sample - loss: 161.4293 - mse: 161.4292 - val_loss: 123.0474 - val_mse: 123.0474\n",
      "Epoch 87/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 158.6352 - mse: 158.6352\n",
      "Epoch 00087: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 763us/sample - loss: 162.0617 - mse: 162.0617 - val_loss: 122.9458 - val_mse: 122.9458\n",
      "Epoch 88/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 163.8172 - mse: 163.8172\n",
      "Epoch 00088: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 728us/sample - loss: 161.5960 - mse: 161.5960 - val_loss: 123.9715 - val_mse: 123.9715\n",
      "Epoch 89/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 162.9292 - mse: 162.9293\n",
      "Epoch 00089: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 750us/sample - loss: 161.9531 - mse: 161.9531 - val_loss: 123.9798 - val_mse: 123.9798\n",
      "Epoch 90/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 159.7805 - mse: 159.7805\n",
      "Epoch 00090: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 737us/sample - loss: 161.5877 - mse: 161.5877 - val_loss: 122.1079 - val_mse: 122.1079\n",
      "Epoch 91/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 161.1375 - mse: 161.1375\n",
      "Epoch 00091: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 723us/sample - loss: 161.1506 - mse: 161.1506 - val_loss: 122.1910 - val_mse: 122.1910\n",
      "Epoch 92/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 150.4468 - mse: 150.4468\n",
      "Epoch 00092: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 729us/sample - loss: 161.8952 - mse: 161.8951 - val_loss: 122.1108 - val_mse: 122.1107\n",
      "Epoch 93/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 165.3767 - mse: 165.3767\n",
      "Epoch 00093: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 730us/sample - loss: 162.9641 - mse: 162.9641 - val_loss: 121.9889 - val_mse: 121.9889\n",
      "Epoch 94/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.3411 - mse: 162.3411\n",
      "Epoch 00094: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 871us/sample - loss: 162.2691 - mse: 162.2691 - val_loss: 122.5973 - val_mse: 122.5973\n",
      "Epoch 95/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 157.4548 - mse: 157.4548\n",
      "Epoch 00095: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 831us/sample - loss: 162.2722 - mse: 162.2722 - val_loss: 122.3185 - val_mse: 122.3185\n",
      "Epoch 96/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 152.2161 - mse: 152.2160\n",
      "Epoch 00096: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 784us/sample - loss: 161.8770 - mse: 161.8770 - val_loss: 122.5204 - val_mse: 122.5204\n",
      "Epoch 97/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 161.3765 - mse: 161.3764\n",
      "Epoch 00097: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 775us/sample - loss: 161.5567 - mse: 161.5567 - val_loss: 123.3071 - val_mse: 123.3071\n",
      "Epoch 98/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 161.0318 - mse: 161.0318\n",
      "Epoch 00098: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 761us/sample - loss: 160.3924 - mse: 160.3924 - val_loss: 122.8008 - val_mse: 122.8008\n",
      "Epoch 99/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 161.6506 - mse: 161.6506\n",
      "Epoch 00099: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 810us/sample - loss: 162.2801 - mse: 162.2801 - val_loss: 122.1737 - val_mse: 122.1736\n",
      "Epoch 100/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 161.2713 - mse: 161.2713\n",
      "Epoch 00100: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 761us/sample - loss: 160.3106 - mse: 160.3106 - val_loss: 125.5760 - val_mse: 125.5760\n",
      "Epoch 101/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 152.1364 - mse: 152.1364\n",
      "Epoch 00101: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 757us/sample - loss: 161.7631 - mse: 161.7631 - val_loss: 122.6623 - val_mse: 122.6623\n",
      "Epoch 102/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 153.4149 - mse: 153.4149\n",
      "Epoch 00102: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 784us/sample - loss: 159.7265 - mse: 159.7265 - val_loss: 133.9704 - val_mse: 133.9704\n",
      "Epoch 103/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.6715 - mse: 162.6715\n",
      "Epoch 00103: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 823us/sample - loss: 161.9713 - mse: 161.9713 - val_loss: 122.3260 - val_mse: 122.3260\n",
      "Epoch 104/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.8145 - mse: 162.8145\n",
      "Epoch 00104: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 806us/sample - loss: 162.1449 - mse: 162.1449 - val_loss: 123.3441 - val_mse: 123.3441\n",
      "Epoch 105/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.8906 - mse: 162.8906\n",
      "Epoch 00105: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 786us/sample - loss: 161.3503 - mse: 161.3503 - val_loss: 122.4601 - val_mse: 122.4601\n",
      "Epoch 106/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 163.2329 - mse: 163.2329\n",
      "Epoch 00106: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 817us/sample - loss: 161.2815 - mse: 161.2815 - val_loss: 127.8604 - val_mse: 127.8604\n",
      "Epoch 107/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 160.3777 - mse: 160.3777\n",
      "Epoch 00107: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 831us/sample - loss: 162.1235 - mse: 162.1235 - val_loss: 119.2348 - val_mse: 119.2348\n",
      "Epoch 108/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 161.4083 - mse: 161.4084\n",
      "Epoch 00108: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 762us/sample - loss: 161.2598 - mse: 161.2598 - val_loss: 120.2087 - val_mse: 120.2087\n",
      "Epoch 109/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 153.5475 - mse: 153.5475\n",
      "Epoch 00109: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 776us/sample - loss: 159.6175 - mse: 159.6175 - val_loss: 124.5270 - val_mse: 124.5270\n",
      "Epoch 110/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.3769 - mse: 163.3769\n",
      "Epoch 00110: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 750us/sample - loss: 161.8730 - mse: 161.8730 - val_loss: 120.8874 - val_mse: 120.8875\n",
      "Epoch 111/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 159.1663 - mse: 159.1663\n",
      "Epoch 00111: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 826us/sample - loss: 158.4585 - mse: 158.4585 - val_loss: 115.8164 - val_mse: 115.8164\n",
      "Epoch 112/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 150.5148 - mse: 150.5147\n",
      "Epoch 00112: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 860us/sample - loss: 153.7660 - mse: 153.7659 - val_loss: 129.7612 - val_mse: 129.7612\n",
      "Epoch 113/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.2003 - mse: 163.2003\n",
      "Epoch 00113: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 799us/sample - loss: 162.4703 - mse: 162.4703 - val_loss: 122.1032 - val_mse: 122.1032\n",
      "Epoch 114/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 152.6086 - mse: 152.6087\n",
      "Epoch 00114: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 1ms/sample - loss: 161.2837 - mse: 161.2837 - val_loss: 121.8822 - val_mse: 121.8822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 154.2459 - mse: 154.2459\n",
      "Epoch 00115: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 894us/sample - loss: 161.9190 - mse: 161.9190 - val_loss: 123.3431 - val_mse: 123.3431\n",
      "Epoch 116/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.5706 - mse: 162.5706\n",
      "Epoch 00116: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 730us/sample - loss: 161.0975 - mse: 161.0974 - val_loss: 122.1165 - val_mse: 122.1165\n",
      "Epoch 117/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 163.9637 - mse: 163.9637\n",
      "Epoch 00117: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 749us/sample - loss: 162.5955 - mse: 162.5955 - val_loss: 122.2734 - val_mse: 122.2734\n",
      "Epoch 118/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.1993 - mse: 162.1992\n",
      "Epoch 00118: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 899us/sample - loss: 162.2191 - mse: 162.2191 - val_loss: 122.2860 - val_mse: 122.2860\n",
      "Epoch 119/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.4500 - mse: 162.4500\n",
      "Epoch 00119: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 856us/sample - loss: 161.6528 - mse: 161.6528 - val_loss: 122.5222 - val_mse: 122.5222\n",
      "Epoch 120/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 160.6885 - mse: 160.6884\n",
      "Epoch 00120: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 877us/sample - loss: 160.0061 - mse: 160.0060 - val_loss: 123.1411 - val_mse: 123.1411\n",
      "Epoch 121/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 161.7727 - mse: 161.7727\n",
      "Epoch 00121: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 860us/sample - loss: 161.0013 - mse: 161.0013 - val_loss: 121.9591 - val_mse: 121.9591\n",
      "Epoch 122/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.0271 - mse: 163.0271\n",
      "Epoch 00122: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 788us/sample - loss: 162.2753 - mse: 162.2753 - val_loss: 123.3613 - val_mse: 123.3613\n",
      "Epoch 123/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.8158 - mse: 162.8159\n",
      "Epoch 00123: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 777us/sample - loss: 162.2121 - mse: 162.2121 - val_loss: 122.2249 - val_mse: 122.2249\n",
      "Epoch 124/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.8962 - mse: 162.8962\n",
      "Epoch 00124: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 763us/sample - loss: 160.7758 - mse: 160.7757 - val_loss: 123.9710 - val_mse: 123.9710\n",
      "Epoch 125/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 163.7377 - mse: 163.7377\n",
      "Epoch 00125: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 757us/sample - loss: 162.7602 - mse: 162.7602 - val_loss: 122.6537 - val_mse: 122.6537\n",
      "Epoch 126/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 163.3932 - mse: 163.3932\n",
      "Epoch 00126: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 725us/sample - loss: 162.1775 - mse: 162.1775 - val_loss: 121.8611 - val_mse: 121.8611\n",
      "Epoch 127/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 161.7090 - mse: 161.7090\n",
      "Epoch 00127: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 847us/sample - loss: 160.7203 - mse: 160.7203 - val_loss: 125.2014 - val_mse: 125.2014\n",
      "Epoch 128/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 158.9366 - mse: 158.9366- ETA: 0s - loss: 162.5576 - mse: 162.5\n",
      "Epoch 00128: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 883us/sample - loss: 162.4535 - mse: 162.4534 - val_loss: 122.7687 - val_mse: 122.7687\n",
      "Epoch 129/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 165.5431 - mse: 165.5431\n",
      "Epoch 00129: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 822us/sample - loss: 162.3376 - mse: 162.3376 - val_loss: 122.4492 - val_mse: 122.4492\n",
      "Epoch 130/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 159.2016 - mse: 159.2016\n",
      "Epoch 00130: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 772us/sample - loss: 162.5258 - mse: 162.5258 - val_loss: 122.3755 - val_mse: 122.3755\n",
      "Epoch 131/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 157.4146 - mse: 157.4146\n",
      "Epoch 00131: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 784us/sample - loss: 161.1104 - mse: 161.1105 - val_loss: 122.4530 - val_mse: 122.4530\n",
      "Epoch 132/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 159.8484 - mse: 159.8484\n",
      "Epoch 00132: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 771us/sample - loss: 160.2634 - mse: 160.2633 - val_loss: 124.1118 - val_mse: 124.1118\n",
      "Epoch 133/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 164.8484 - mse: 164.8484\n",
      "Epoch 00133: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 759us/sample - loss: 161.4080 - mse: 161.4079 - val_loss: 121.9657 - val_mse: 121.9657\n",
      "Epoch 134/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 161.7594 - mse: 161.7595\n",
      "Epoch 00134: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 771us/sample - loss: 161.8885 - mse: 161.8885 - val_loss: 121.9712 - val_mse: 121.9712\n",
      "Epoch 135/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.4813 - mse: 163.4814\n",
      "Epoch 00135: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 816us/sample - loss: 162.3658 - mse: 162.3658 - val_loss: 124.0332 - val_mse: 124.0332\n",
      "Epoch 136/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.4035 - mse: 163.4035\n",
      "Epoch 00136: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 892us/sample - loss: 162.1426 - mse: 162.1426 - val_loss: 121.9289 - val_mse: 121.9289\n",
      "Epoch 137/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 160.0978 - mse: 160.0979\n",
      "Epoch 00137: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 868us/sample - loss: 161.4588 - mse: 161.4588 - val_loss: 122.9381 - val_mse: 122.9381\n",
      "Epoch 138/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 161.7991 - mse: 161.7991\n",
      "Epoch 00138: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 773us/sample - loss: 161.8598 - mse: 161.8598 - val_loss: 122.1675 - val_mse: 122.1675\n",
      "Epoch 139/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 161.5361 - mse: 161.5361\n",
      "Epoch 00139: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 764us/sample - loss: 161.3477 - mse: 161.3477 - val_loss: 123.3557 - val_mse: 123.3557\n",
      "Epoch 140/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.5871 - mse: 162.5871\n",
      "Epoch 00140: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 829us/sample - loss: 160.5244 - mse: 160.5244 - val_loss: 121.8887 - val_mse: 121.8887\n",
      "Epoch 141/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 160.8458 - mse: 160.8458\n",
      "Epoch 00141: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 781us/sample - loss: 161.3683 - mse: 161.3683 - val_loss: 122.8270 - val_mse: 122.8270\n",
      "Epoch 142/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.3829 - mse: 163.3829\n",
      "Epoch 00142: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 748us/sample - loss: 162.4782 - mse: 162.4781 - val_loss: 122.4967 - val_mse: 122.4967\n",
      "Epoch 143/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2336/2400 [============================>.] - ETA: 0s - loss: 165.1351 - mse: 165.1351\n",
      "Epoch 00143: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 773us/sample - loss: 162.3066 - mse: 162.3065 - val_loss: 123.2172 - val_mse: 123.2172\n",
      "Epoch 144/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 159.6737 - mse: 159.6737\n",
      "Epoch 00144: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 816us/sample - loss: 161.4608 - mse: 161.4608 - val_loss: 122.8128 - val_mse: 122.8127\n",
      "Epoch 145/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.9130 - mse: 162.9131\n",
      "Epoch 00145: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 757us/sample - loss: 160.9058 - mse: 160.9059 - val_loss: 122.6009 - val_mse: 122.6009\n",
      "Epoch 146/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.5635 - mse: 162.5635\n",
      "Epoch 00146: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 750us/sample - loss: 161.3498 - mse: 161.3498 - val_loss: 122.7338 - val_mse: 122.7338\n",
      "Epoch 147/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.9928 - mse: 162.9928\n",
      "Epoch 00147: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 729us/sample - loss: 161.5866 - mse: 161.5866 - val_loss: 122.3934 - val_mse: 122.3934\n",
      "Epoch 148/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 160.1537 - mse: 160.1537- ETA: 1s - loss: 11\n",
      "Epoch 00148: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 714us/sample - loss: 162.2561 - mse: 162.2561 - val_loss: 121.9696 - val_mse: 121.9696\n",
      "Epoch 149/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 164.3860 - mse: 164.3860\n",
      "Epoch 00149: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 722us/sample - loss: 162.2064 - mse: 162.2063 - val_loss: 123.6518 - val_mse: 123.6517\n",
      "Epoch 150/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.1812 - mse: 162.1811\n",
      "Epoch 00150: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 719us/sample - loss: 162.6898 - mse: 162.6898 - val_loss: 122.6571 - val_mse: 122.6571\n",
      "Epoch 151/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 161.7733 - mse: 161.7733\n",
      "Epoch 00151: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 709us/sample - loss: 162.1570 - mse: 162.1570 - val_loss: 123.3002 - val_mse: 123.3002\n",
      "Epoch 152/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 161.1716 - mse: 161.1716\n",
      "Epoch 00152: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 825us/sample - loss: 160.9635 - mse: 160.9635 - val_loss: 122.4839 - val_mse: 122.4839\n",
      "Epoch 153/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 159.9045 - mse: 159.9045\n",
      "Epoch 00153: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 689us/sample - loss: 160.7166 - mse: 160.7166 - val_loss: 122.0758 - val_mse: 122.0758\n",
      "Epoch 154/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.4193 - mse: 162.4194\n",
      "Epoch 00154: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 633us/sample - loss: 161.3798 - mse: 161.3798 - val_loss: 123.3201 - val_mse: 123.3201\n",
      "Epoch 155/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 164.0479 - mse: 164.0479- ETA: 0s - loss: 157.7343 - mse: \n",
      "Epoch 00155: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 625us/sample - loss: 161.3545 - mse: 161.3545 - val_loss: 123.1140 - val_mse: 123.1141\n",
      "Epoch 156/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.7688 - mse: 163.7688\n",
      "Epoch 00156: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 623us/sample - loss: 162.0235 - mse: 162.0235 - val_loss: 122.6317 - val_mse: 122.6317\n",
      "Epoch 157/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.5199 - mse: 162.5199\n",
      "Epoch 00157: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 620us/sample - loss: 161.6088 - mse: 161.6088 - val_loss: 123.4609 - val_mse: 123.4609\n",
      "Epoch 158/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 155.9190 - mse: 155.9190\n",
      "Epoch 00158: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 614us/sample - loss: 161.7269 - mse: 161.7269 - val_loss: 122.2418 - val_mse: 122.2418\n",
      "Epoch 159/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.5525 - mse: 163.5525\n",
      "Epoch 00159: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 611us/sample - loss: 161.5896 - mse: 161.5896 - val_loss: 121.8529 - val_mse: 121.8529\n",
      "Epoch 160/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 160.2329 - mse: 160.2329\n",
      "Epoch 00160: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 619us/sample - loss: 161.4254 - mse: 161.4254 - val_loss: 122.1749 - val_mse: 122.1749\n",
      "Epoch 161/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 162.3277 - mse: 162.3277\n",
      "Epoch 00161: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 612us/sample - loss: 162.3428 - mse: 162.3428 - val_loss: 124.5507 - val_mse: 124.5507\n",
      "Epoch 162/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 163.2881 - mse: 163.2881\n",
      "Epoch 00162: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 625us/sample - loss: 160.6756 - mse: 160.6756 - val_loss: 122.3954 - val_mse: 122.3954\n",
      "Epoch 163/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 166.1376 - mse: 166.1376\n",
      "Epoch 00163: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 678us/sample - loss: 161.7916 - mse: 161.7916 - val_loss: 122.0196 - val_mse: 122.0196\n",
      "Epoch 164/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 160.3151 - mse: 160.3151\n",
      "Epoch 00164: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 658us/sample - loss: 161.0316 - mse: 161.0316 - val_loss: 123.9245 - val_mse: 123.9245\n",
      "Epoch 165/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.3543 - mse: 162.3543\n",
      "Epoch 00165: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 618us/sample - loss: 160.7330 - mse: 160.7330 - val_loss: 123.4851 - val_mse: 123.4851\n",
      "Epoch 166/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 164.9920 - mse: 164.9920\n",
      "Epoch 00166: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 625us/sample - loss: 161.3115 - mse: 161.3116 - val_loss: 122.6246 - val_mse: 122.6246\n",
      "Epoch 167/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 161.9385 - mse: 161.9385\n",
      "Epoch 00167: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 621us/sample - loss: 161.7727 - mse: 161.7727 - val_loss: 122.7061 - val_mse: 122.7061\n",
      "Epoch 168/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 164.8029 - mse: 164.8029\n",
      "Epoch 00168: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 608us/sample - loss: 161.1582 - mse: 161.1582 - val_loss: 122.2443 - val_mse: 122.2443\n",
      "Epoch 169/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.9382 - mse: 162.9382\n",
      "Epoch 00169: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 610us/sample - loss: 161.9640 - mse: 161.9640 - val_loss: 122.5553 - val_mse: 122.5553\n",
      "Epoch 170/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 162.5684 - mse: 162.5684\n",
      "Epoch 00170: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 621us/sample - loss: 161.1834 - mse: 161.1834 - val_loss: 122.5713 - val_mse: 122.5713\n",
      "Epoch 171/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.2214 - mse: 162.2214\n",
      "Epoch 00171: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 614us/sample - loss: 161.4477 - mse: 161.4477 - val_loss: 123.1406 - val_mse: 123.1406\n",
      "Epoch 172/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 163.2054 - mse: 163.2054\n",
      "Epoch 00172: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 613us/sample - loss: 162.0616 - mse: 162.0616 - val_loss: 122.2085 - val_mse: 122.2085\n",
      "Epoch 173/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 160.4053 - mse: 160.4053\n",
      "Epoch 00173: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 682us/sample - loss: 161.5658 - mse: 161.5658 - val_loss: 122.6811 - val_mse: 122.6811\n",
      "Epoch 174/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.2046 - mse: 162.2046\n",
      "Epoch 00174: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 687us/sample - loss: 161.0308 - mse: 161.0308 - val_loss: 121.9696 - val_mse: 121.9696\n",
      "Epoch 175/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 158.7472 - mse: 158.7471\n",
      "Epoch 00175: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 621us/sample - loss: 161.5109 - mse: 161.5109 - val_loss: 122.1605 - val_mse: 122.1605\n",
      "Epoch 176/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 164.5097 - mse: 164.5097\n",
      "Epoch 00176: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 621us/sample - loss: 161.0226 - mse: 161.0225 - val_loss: 123.0137 - val_mse: 123.0137\n",
      "Epoch 177/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 163.0348 - mse: 163.0348\n",
      "Epoch 00177: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 635us/sample - loss: 161.8821 - mse: 161.8820 - val_loss: 122.3230 - val_mse: 122.3230\n",
      "Epoch 178/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 161.9121 - mse: 161.9121\n",
      "Epoch 00178: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 619us/sample - loss: 161.4618 - mse: 161.4619 - val_loss: 122.5991 - val_mse: 122.5991\n",
      "Epoch 179/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.9428 - mse: 163.9429\n",
      "Epoch 00179: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 605us/sample - loss: 161.8481 - mse: 161.8482 - val_loss: 121.8543 - val_mse: 121.8543\n",
      "Epoch 180/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 159.7196 - mse: 159.7196\n",
      "Epoch 00180: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 609us/sample - loss: 160.3283 - mse: 160.3283 - val_loss: 122.0713 - val_mse: 122.0713\n",
      "Epoch 181/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 152.6636 - mse: 152.6636\n",
      "Epoch 00181: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 604us/sample - loss: 160.6580 - mse: 160.6580 - val_loss: 122.4141 - val_mse: 122.4142\n",
      "Epoch 182/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 164.0827 - mse: 164.0826\n",
      "Epoch 00182: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 609us/sample - loss: 160.7321 - mse: 160.7321 - val_loss: 124.1579 - val_mse: 124.1579\n",
      "Epoch 183/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.4969 - mse: 163.4969\n",
      "Epoch 00183: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 603us/sample - loss: 161.2222 - mse: 161.2223 - val_loss: 122.6983 - val_mse: 122.6983\n",
      "Epoch 184/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.6500 - mse: 163.6500\n",
      "Epoch 00184: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 673us/sample - loss: 160.8212 - mse: 160.8212 - val_loss: 122.5609 - val_mse: 122.5609\n",
      "Epoch 185/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 163.2121 - mse: 163.2121\n",
      "Epoch 00185: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 655us/sample - loss: 160.7172 - mse: 160.7171 - val_loss: 123.1265 - val_mse: 123.1265\n",
      "Epoch 186/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.4663 - mse: 162.4663\n",
      "Epoch 00186: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 625us/sample - loss: 161.1903 - mse: 161.1903 - val_loss: 122.6172 - val_mse: 122.6172\n",
      "Epoch 187/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 163.2898 - mse: 163.2898\n",
      "Epoch 00187: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 626us/sample - loss: 160.5103 - mse: 160.5103 - val_loss: 121.9442 - val_mse: 121.9442\n",
      "Epoch 188/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 160.9404 - mse: 160.9404\n",
      "Epoch 00188: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 623us/sample - loss: 159.9336 - mse: 159.9336 - val_loss: 122.0283 - val_mse: 122.0283\n",
      "Epoch 189/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 158.4760 - mse: 158.4760\n",
      "Epoch 00189: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 613us/sample - loss: 161.8883 - mse: 161.8883 - val_loss: 122.1067 - val_mse: 122.1067\n",
      "Epoch 190/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.7892 - mse: 162.7892\n",
      "Epoch 00190: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 611us/sample - loss: 162.2857 - mse: 162.2857 - val_loss: 122.3769 - val_mse: 122.3769\n",
      "Epoch 191/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 164.7451 - mse: 164.7452\n",
      "Epoch 00191: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 605us/sample - loss: 161.8205 - mse: 161.8205 - val_loss: 122.0153 - val_mse: 122.0153\n",
      "Epoch 192/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.6723 - mse: 162.6724\n",
      "Epoch 00192: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 612us/sample - loss: 162.0974 - mse: 162.0975 - val_loss: 122.2715 - val_mse: 122.2715\n",
      "Epoch 193/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 158.3825 - mse: 158.3825\n",
      "Epoch 00193: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 606us/sample - loss: 161.2117 - mse: 161.2117 - val_loss: 122.1075 - val_mse: 122.1075\n",
      "Epoch 194/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.3708 - mse: 162.3708\n",
      "Epoch 00194: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 634us/sample - loss: 161.3403 - mse: 161.3403 - val_loss: 122.0606 - val_mse: 122.0606\n",
      "Epoch 195/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 155.4822 - mse: 155.4822\n",
      "Epoch 00195: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 675us/sample - loss: 161.1165 - mse: 161.1165 - val_loss: 122.2893 - val_mse: 122.2893\n",
      "Epoch 196/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.3580 - mse: 163.3580\n",
      "Epoch 00196: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 636us/sample - loss: 161.1660 - mse: 161.1660 - val_loss: 122.5697 - val_mse: 122.5697\n",
      "Epoch 197/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 163.0513 - mse: 163.0513\n",
      "Epoch 00197: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 611us/sample - loss: 160.8566 - mse: 160.8566 - val_loss: 123.0661 - val_mse: 123.0661\n",
      "Epoch 198/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.3847 - mse: 162.3847\n",
      "Epoch 00198: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 618us/sample - loss: 162.4502 - mse: 162.4502 - val_loss: 123.1018 - val_mse: 123.1018\n",
      "Epoch 199/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 161.9967 - mse: 161.9967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00199: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 624us/sample - loss: 161.1832 - mse: 161.1832 - val_loss: 122.2258 - val_mse: 122.2258\n",
      "Epoch 200/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 155.2056 - mse: 155.2056\n",
      "Epoch 00200: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 609us/sample - loss: 161.3115 - mse: 161.3115 - val_loss: 122.2647 - val_mse: 122.2647\n",
      "Epoch 201/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 164.0913 - mse: 164.0914\n",
      "Epoch 00201: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 612us/sample - loss: 161.1637 - mse: 161.1638 - val_loss: 122.2895 - val_mse: 122.2895\n",
      "Epoch 202/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 164.1459 - mse: 164.1459\n",
      "Epoch 00202: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 618us/sample - loss: 161.3676 - mse: 161.3677 - val_loss: 123.6471 - val_mse: 123.6471\n",
      "Epoch 203/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.7147 - mse: 162.7147\n",
      "Epoch 00203: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 616us/sample - loss: 162.1928 - mse: 162.1928 - val_loss: 122.4148 - val_mse: 122.4147\n",
      "Epoch 204/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 160.6925 - mse: 160.6925\n",
      "Epoch 00204: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 613us/sample - loss: 161.5389 - mse: 161.5389 - val_loss: 122.3722 - val_mse: 122.3722\n",
      "Epoch 205/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 161.5169 - mse: 161.5168\n",
      "Epoch 00205: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 651us/sample - loss: 160.6730 - mse: 160.6729 - val_loss: 124.0831 - val_mse: 124.0830\n",
      "Epoch 206/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 164.4196 - mse: 164.4196\n",
      "Epoch 00206: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 690us/sample - loss: 161.6733 - mse: 161.6733 - val_loss: 121.8541 - val_mse: 121.8541\n",
      "Epoch 207/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.5398 - mse: 163.5398\n",
      "Epoch 00207: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 610us/sample - loss: 162.1293 - mse: 162.1293 - val_loss: 122.6374 - val_mse: 122.6374\n",
      "Epoch 208/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.8135 - mse: 162.8135\n",
      "Epoch 00208: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 615us/sample - loss: 161.5427 - mse: 161.5427 - val_loss: 122.2826 - val_mse: 122.2825\n",
      "Epoch 209/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 161.4302 - mse: 161.4303\n",
      "Epoch 00209: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 618us/sample - loss: 161.6287 - mse: 161.6287 - val_loss: 122.0595 - val_mse: 122.0595\n",
      "Epoch 210/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 164.3133 - mse: 164.3133\n",
      "Epoch 00210: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 617us/sample - loss: 161.4789 - mse: 161.4789 - val_loss: 123.6789 - val_mse: 123.6789\n",
      "Epoch 211/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.6753 - mse: 162.6753\n",
      "Epoch 00211: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 602us/sample - loss: 161.1612 - mse: 161.1612 - val_loss: 122.7577 - val_mse: 122.7577\n",
      "Epoch 212/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 166.8353 - mse: 166.8353\n",
      "Epoch 00212: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 602us/sample - loss: 163.4470 - mse: 163.4470 - val_loss: 122.1326 - val_mse: 122.1326\n",
      "Epoch 213/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 161.0068 - mse: 161.0068\n",
      "Epoch 00213: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 619us/sample - loss: 161.5009 - mse: 161.5009 - val_loss: 122.2432 - val_mse: 122.2432\n",
      "Epoch 214/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.0888 - mse: 162.0888- ETA: 0s - loss: 176.1005 -\n",
      "Epoch 00214: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 596us/sample - loss: 160.9746 - mse: 160.9747 - val_loss: 122.9256 - val_mse: 122.9256\n",
      "Epoch 215/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 154.8877 - mse: 154.8876\n",
      "Epoch 00215: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 591us/sample - loss: 160.4467 - mse: 160.4467 - val_loss: 122.2159 - val_mse: 122.2159\n",
      "Epoch 216/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 166.6184 - mse: 166.6184\n",
      "Epoch 00216: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 666us/sample - loss: 162.7732 - mse: 162.7732 - val_loss: 123.2452 - val_mse: 123.2452\n",
      "Epoch 217/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.1259 - mse: 163.1260\n",
      "Epoch 00217: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 666us/sample - loss: 161.3831 - mse: 161.3832 - val_loss: 122.0237 - val_mse: 122.0237\n",
      "Epoch 218/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 156.0399 - mse: 156.0399\n",
      "Epoch 00218: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 615us/sample - loss: 161.8809 - mse: 161.8809 - val_loss: 122.4537 - val_mse: 122.4537\n",
      "Epoch 219/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.5876 - mse: 163.5876\n",
      "Epoch 00219: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 651us/sample - loss: 161.1344 - mse: 161.1344 - val_loss: 122.2256 - val_mse: 122.2256\n",
      "Epoch 220/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 161.7724 - mse: 161.7724\n",
      "Epoch 00220: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 644us/sample - loss: 160.7480 - mse: 160.7480 - val_loss: 121.8751 - val_mse: 121.8751\n",
      "Epoch 221/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 163.4481 - mse: 163.4481\n",
      "Epoch 00221: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 646us/sample - loss: 161.5910 - mse: 161.5910 - val_loss: 122.4919 - val_mse: 122.4919\n",
      "Epoch 222/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 159.8924 - mse: 159.8924\n",
      "Epoch 00222: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 648us/sample - loss: 161.1824 - mse: 161.1824 - val_loss: 123.2290 - val_mse: 123.2289\n",
      "Epoch 223/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 161.4682 - mse: 161.4682\n",
      "Epoch 00223: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 599us/sample - loss: 161.7644 - mse: 161.7643 - val_loss: 122.2126 - val_mse: 122.2126\n",
      "Epoch 224/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.7513 - mse: 162.7513\n",
      "Epoch 00224: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 615us/sample - loss: 160.3953 - mse: 160.3953 - val_loss: 121.9017 - val_mse: 121.9017\n",
      "Epoch 225/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 161.5509 - mse: 161.5509\n",
      "Epoch 00225: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 604us/sample - loss: 160.8312 - mse: 160.8313 - val_loss: 123.1079 - val_mse: 123.1078\n",
      "Epoch 226/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 160.6977 - mse: 160.6977\n",
      "Epoch 00226: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 634us/sample - loss: 160.4334 - mse: 160.4334 - val_loss: 122.5196 - val_mse: 122.5196\n",
      "Epoch 227/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 162.2657 - mse: 162.2657\n",
      "Epoch 00227: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 676us/sample - loss: 160.7724 - mse: 160.7724 - val_loss: 121.9049 - val_mse: 121.9049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 228/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 160.8730 - mse: 160.8729\n",
      "Epoch 00228: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 619us/sample - loss: 161.9245 - mse: 161.9245 - val_loss: 122.7761 - val_mse: 122.7761\n",
      "Epoch 229/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 164.0823 - mse: 164.0824\n",
      "Epoch 00229: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 603us/sample - loss: 160.4410 - mse: 160.4410 - val_loss: 122.3411 - val_mse: 122.3411\n",
      "Epoch 230/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 159.9276 - mse: 159.9276\n",
      "Epoch 00230: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 603us/sample - loss: 161.4301 - mse: 161.4301 - val_loss: 122.6382 - val_mse: 122.6382\n",
      "Epoch 231/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 160.8424 - mse: 160.8424\n",
      "Epoch 00231: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 602us/sample - loss: 161.8614 - mse: 161.8614 - val_loss: 122.8005 - val_mse: 122.8005\n",
      "Epoch 232/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 148.6732 - mse: 148.6732\n",
      "Epoch 00232: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 603us/sample - loss: 161.5609 - mse: 161.5609 - val_loss: 122.1235 - val_mse: 122.1235\n",
      "Epoch 233/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 164.4007 - mse: 164.4007\n",
      "Epoch 00233: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 589us/sample - loss: 162.2822 - mse: 162.2822 - val_loss: 122.8595 - val_mse: 122.8595\n",
      "Epoch 234/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 165.0020 - mse: 165.0020\n",
      "Epoch 00234: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 587us/sample - loss: 161.1966 - mse: 161.1966 - val_loss: 121.8835 - val_mse: 121.8835\n",
      "Epoch 235/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 164.4625 - mse: 164.4624\n",
      "Epoch 00235: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 600us/sample - loss: 161.8887 - mse: 161.8887 - val_loss: 122.3981 - val_mse: 122.3981\n",
      "Epoch 236/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 161.5261 - mse: 161.5261\n",
      "Epoch 00236: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 594us/sample - loss: 161.2679 - mse: 161.2679 - val_loss: 123.2062 - val_mse: 123.2062\n",
      "Epoch 237/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 164.0981 - mse: 164.0981\n",
      "Epoch 00237: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 620us/sample - loss: 161.1625 - mse: 161.1625 - val_loss: 122.0134 - val_mse: 122.0134\n",
      "Epoch 238/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 153.1088 - mse: 153.1088\n",
      "Epoch 00238: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 673us/sample - loss: 160.8851 - mse: 160.8851 - val_loss: 123.8866 - val_mse: 123.8866\n",
      "Epoch 239/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 156.7820 - mse: 156.7820\n",
      "Epoch 00239: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 620us/sample - loss: 161.1909 - mse: 161.1908 - val_loss: 122.9245 - val_mse: 122.9245\n",
      "Epoch 240/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 163.7796 - mse: 163.7796\n",
      "Epoch 00240: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 607us/sample - loss: 161.7436 - mse: 161.7436 - val_loss: 122.4829 - val_mse: 122.4829\n",
      "Epoch 241/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 164.3816 - mse: 164.3816\n",
      "Epoch 00241: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 606us/sample - loss: 161.5254 - mse: 161.5254 - val_loss: 122.0064 - val_mse: 122.0064\n",
      "Epoch 242/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 154.6353 - mse: 154.6353\n",
      "Epoch 00242: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 612us/sample - loss: 161.6825 - mse: 161.6825 - val_loss: 122.4657 - val_mse: 122.4657\n",
      "Epoch 243/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.9796 - mse: 162.9796\n",
      "Epoch 00243: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 594us/sample - loss: 161.2686 - mse: 161.2686 - val_loss: 122.2585 - val_mse: 122.2585\n",
      "Epoch 244/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 153.8836 - mse: 153.8837\n",
      "Epoch 00244: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 591us/sample - loss: 162.5450 - mse: 162.5450 - val_loss: 123.0547 - val_mse: 123.0547\n",
      "Epoch 245/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 158.0266 - mse: 158.0266\n",
      "Epoch 00245: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 597us/sample - loss: 160.3393 - mse: 160.3393 - val_loss: 122.3769 - val_mse: 122.3769\n",
      "Epoch 246/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 156.8021 - mse: 156.8021\n",
      "Epoch 00246: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 591us/sample - loss: 161.2476 - mse: 161.2476 - val_loss: 124.1134 - val_mse: 124.1134\n",
      "Epoch 247/300\n",
      "2272/2400 [===========================>..] - ETA: 0s - loss: 157.2563 - mse: 157.2563\n",
      "Epoch 00247: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 1s 592us/sample - loss: 160.9479 - mse: 160.9478 - val_loss: 124.1539 - val_mse: 124.1539\n",
      "Epoch 248/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.2379 - mse: 163.2379\n",
      "Epoch 00248: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 643us/sample - loss: 161.4348 - mse: 161.4348 - val_loss: 122.6437 - val_mse: 122.6437\n",
      "Epoch 00248: early stopping\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4140 samples, validate on 1036 samples\n",
      "Epoch 1/200\n",
      "4000/4140 [===========================>..] - ETA: 0s - loss: 2.6911 - accuracy: 0.6290\n",
      "Epoch 00001: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 1s 219us/sample - loss: 2.6538 - accuracy: 0.6319 - val_loss: 1.0356 - val_accuracy: 0.8205\n",
      "Epoch 2/200\n",
      "3392/4140 [=======================>......] - ETA: 0s - loss: 1.3145 - accuracy: 0.6728\n",
      "Epoch 00002: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 79us/sample - loss: 1.2912 - accuracy: 0.6722 - val_loss: 0.6485 - val_accuracy: 0.8282\n",
      "Epoch 3/200\n",
      "3040/4140 [=====================>........] - ETA: 0s - loss: 1.1586 - accuracy: 0.6852\n",
      "Epoch 00003: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 1.1798 - accuracy: 0.6804 - val_loss: 0.5785 - val_accuracy: 0.8581\n",
      "Epoch 4/200\n",
      "2944/4140 [====================>.........] - ETA: 0s - loss: 1.0308 - accuracy: 0.7045\n",
      "Epoch 00004: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 1.1329 - accuracy: 0.6920 - val_loss: 1.9356 - val_accuracy: 0.4624\n",
      "Epoch 5/200\n",
      "2880/4140 [===================>..........] - ETA: 0s - loss: 0.9164 - accuracy: 0.7076\n",
      "Epoch 00005: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.9201 - accuracy: 0.7085 - val_loss: 0.6881 - val_accuracy: 0.7278\n",
      "Epoch 6/200\n",
      "2912/4140 [====================>.........] - ETA: 0s - loss: 0.8091 - accuracy: 0.7139\n",
      "Epoch 00006: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.8282 - accuracy: 0.7143 - val_loss: 2.1831 - val_accuracy: 0.4112\n",
      "Epoch 7/200\n",
      "4128/4140 [============================>.] - ETA: 0s - loss: 0.6947 - accuracy: 0.7427\n",
      "Epoch 00007: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 51us/sample - loss: 0.6938 - accuracy: 0.7430 - val_loss: 0.9497 - val_accuracy: 0.6486\n",
      "Epoch 8/200\n",
      "3072/4140 [=====================>........] - ETA: 0s - loss: 0.7794 - accuracy: 0.7249\n",
      "Epoch 00008: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.8002 - accuracy: 0.7234 - val_loss: 1.4806 - val_accuracy: 0.3649\n",
      "Epoch 9/200\n",
      "2880/4140 [===================>..........] - ETA: 0s - loss: 0.6339 - accuracy: 0.7417\n",
      "Epoch 00009: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.6547 - accuracy: 0.7331 - val_loss: 1.8377 - val_accuracy: 0.2876\n",
      "Epoch 10/200\n",
      "2848/4140 [===================>..........] - ETA: 0s - loss: 0.7197 - accuracy: 0.7149\n",
      "Epoch 00010: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.6734 - accuracy: 0.7273 - val_loss: 0.6976 - val_accuracy: 0.7008\n",
      "Epoch 11/200\n",
      "2912/4140 [====================>.........] - ETA: 0s - loss: 0.5982 - accuracy: 0.7569\n",
      "Epoch 00011: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.5963 - accuracy: 0.7556 - val_loss: 0.1155 - val_accuracy: 0.9710\n",
      "Epoch 12/200\n",
      "3008/4140 [====================>.........] - ETA: 0s - loss: 0.7352 - accuracy: 0.7224\n",
      "Epoch 00012: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 50us/sample - loss: 0.6956 - accuracy: 0.7261 - val_loss: 0.4955 - val_accuracy: 0.7790\n",
      "Epoch 13/200\n",
      "3200/4140 [======================>.......] - ETA: 0s - loss: 0.5446 - accuracy: 0.7672\n",
      "Epoch 00013: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 46us/sample - loss: 0.5339 - accuracy: 0.7691 - val_loss: 0.8148 - val_accuracy: 0.5830\n",
      "Epoch 14/200\n",
      "3200/4140 [======================>.......] - ETA: 0s - loss: 0.5468 - accuracy: 0.7594\n",
      "Epoch 00014: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.5496 - accuracy: 0.7589 - val_loss: 0.0631 - val_accuracy: 0.9903\n",
      "Epoch 15/200\n",
      "3232/4140 [======================>.......] - ETA: 0s - loss: 0.6043 - accuracy: 0.7503\n",
      "Epoch 00015: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.6525 - accuracy: 0.7408 - val_loss: 0.4515 - val_accuracy: 0.8156\n",
      "Epoch 16/200\n",
      "2816/4140 [===================>..........] - ETA: 0s - loss: 0.6393 - accuracy: 0.7344\n",
      "Epoch 00016: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.6186 - accuracy: 0.7389 - val_loss: 0.7538 - val_accuracy: 0.6622\n",
      "Epoch 17/200\n",
      "4128/4140 [============================>.] - ETA: 0s - loss: 0.5746 - accuracy: 0.7599\n",
      "Epoch 00017: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 51us/sample - loss: 0.5748 - accuracy: 0.7599 - val_loss: 0.7147 - val_accuracy: 0.6612\n",
      "Epoch 18/200\n",
      "3104/4140 [=====================>........] - ETA: 0s - loss: 0.5030 - accuracy: 0.7726\n",
      "Epoch 00018: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.5052 - accuracy: 0.7783 - val_loss: 1.2098 - val_accuracy: 0.4083\n",
      "Epoch 19/200\n",
      "3072/4140 [=====================>........] - ETA: 0s - loss: 0.5670 - accuracy: 0.7575\n",
      "Epoch 00019: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 45us/sample - loss: 0.5486 - accuracy: 0.7609 - val_loss: 0.4721 - val_accuracy: 0.8069\n",
      "Epoch 20/200\n",
      "3168/4140 [=====================>........] - ETA: 0s - loss: 0.4614 - accuracy: 0.7936\n",
      "Epoch 00020: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 50us/sample - loss: 0.4595 - accuracy: 0.7918 - val_loss: 1.0962 - val_accuracy: 0.4237\n",
      "Epoch 21/200\n",
      "3232/4140 [======================>.......] - ETA: 0s - loss: 0.5035 - accuracy: 0.7887\n",
      "Epoch 00021: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.5475 - accuracy: 0.7742 - val_loss: 0.5993 - val_accuracy: 0.7819\n",
      "Epoch 22/200\n",
      "3872/4140 [===========================>..] - ETA: 0s - loss: 0.5356 - accuracy: 0.7745\n",
      "Epoch 00022: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 51us/sample - loss: 0.5328 - accuracy: 0.7751 - val_loss: 0.8317 - val_accuracy: 0.5840\n",
      "Epoch 23/200\n",
      "2784/4140 [===================>..........] - ETA: 0s - loss: 0.4973 - accuracy: 0.7665\n",
      "Epoch 00023: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.4821 - accuracy: 0.7761 - val_loss: 0.8328 - val_accuracy: 0.5705\n",
      "Epoch 24/200\n",
      "2880/4140 [===================>..........] - ETA: 0s - loss: 0.4342 - accuracy: 0.8031\n",
      "Epoch 00024: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.4650 - accuracy: 0.7903 - val_loss: 0.4071 - val_accuracy: 0.8311\n",
      "Epoch 25/200\n",
      "2912/4140 [====================>.........] - ETA: 0s - loss: 0.4580 - accuracy: 0.7916\n",
      "Epoch 00025: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.4890 - accuracy: 0.7812 - val_loss: 0.7272 - val_accuracy: 0.6554\n",
      "Epoch 26/200\n",
      "2944/4140 [====================>.........] - ETA: 0s - loss: 0.4324 - accuracy: 0.7945\n",
      "Epoch 00026: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.4410 - accuracy: 0.7944 - val_loss: 0.3100 - val_accuracy: 0.8793\n",
      "Epoch 27/200\n",
      "3936/4140 [===========================>..] - ETA: 0s - loss: 0.4487 - accuracy: 0.8054\n",
      "Epoch 00027: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 47us/sample - loss: 0.4507 - accuracy: 0.8036 - val_loss: 0.4405 - val_accuracy: 0.8002\n",
      "Epoch 28/200\n",
      "3072/4140 [=====================>........] - ETA: 0s - loss: 0.4669 - accuracy: 0.7923\n",
      "Epoch 00028: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.4604 - accuracy: 0.7940 - val_loss: 0.2867 - val_accuracy: 0.8967\n",
      "Epoch 29/200\n",
      "3136/4140 [=====================>........] - ETA: 0s - loss: 0.4409 - accuracy: 0.7956\n",
      "Epoch 00029: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 45us/sample - loss: 0.4444 - accuracy: 0.7995 - val_loss: 1.4028 - val_accuracy: 0.4392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200\n",
      "2816/4140 [===================>..........] - ETA: 0s - loss: 0.4698 - accuracy: 0.7940\n",
      "Epoch 00030: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.5630 - accuracy: 0.7684 - val_loss: 1.3650 - val_accuracy: 0.4054\n",
      "Epoch 31/200\n",
      "3808/4140 [==========================>...] - ETA: 0s - loss: 0.4525 - accuracy: 0.7973\n",
      "Epoch 00031: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 60us/sample - loss: 0.4481 - accuracy: 0.7990 - val_loss: 0.7290 - val_accuracy: 0.6062\n",
      "Epoch 32/200\n",
      "3584/4140 [========================>.....] - ETA: 0s - loss: 0.5093 - accuracy: 0.7762\n",
      "Epoch 00032: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 64us/sample - loss: 0.4944 - accuracy: 0.7826 - val_loss: 1.1146 - val_accuracy: 0.3900\n",
      "Epoch 33/200\n",
      "3456/4140 [========================>.....] - ETA: 0s - loss: 0.4687 - accuracy: 0.7859\n",
      "Epoch 00033: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 79us/sample - loss: 0.4774 - accuracy: 0.7843 - val_loss: 0.9182 - val_accuracy: 0.4923\n",
      "Epoch 34/200\n",
      "3360/4140 [=======================>......] - ETA: 0s - loss: 0.4040 - accuracy: 0.8152\n",
      "Epoch 00034: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 64us/sample - loss: 0.4059 - accuracy: 0.8188 - val_loss: 0.3631 - val_accuracy: 0.8822\n",
      "Epoch 35/200\n",
      "3520/4140 [========================>.....] - ETA: 0s - loss: 0.4000 - accuracy: 0.8224\n",
      "Epoch 00035: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 56us/sample - loss: 0.4107 - accuracy: 0.8198 - val_loss: 0.4135 - val_accuracy: 0.8234\n",
      "Epoch 36/200\n",
      "3840/4140 [==========================>...] - ETA: 0s - loss: 0.4209 - accuracy: 0.8057\n",
      "Epoch 00036: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 102us/sample - loss: 0.4171 - accuracy: 0.8070 - val_loss: 0.9188 - val_accuracy: 0.5415\n",
      "Epoch 37/200\n",
      "3840/4140 [==========================>...] - ETA: 0s - loss: 0.4095 - accuracy: 0.8245\n",
      "Epoch 00037: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 118us/sample - loss: 0.4118 - accuracy: 0.8225 - val_loss: 0.4150 - val_accuracy: 0.8137\n",
      "Epoch 38/200\n",
      "4000/4140 [===========================>..] - ETA: 0s - loss: 0.4453 - accuracy: 0.8023\n",
      "Epoch 00038: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 74us/sample - loss: 0.4444 - accuracy: 0.8027 - val_loss: 0.2347 - val_accuracy: 0.9218\n",
      "Epoch 39/200\n",
      "3744/4140 [==========================>...] - ETA: 0s - loss: 0.3927 - accuracy: 0.8205\n",
      "Epoch 00039: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 69us/sample - loss: 0.3854 - accuracy: 0.8234 - val_loss: 0.7453 - val_accuracy: 0.6313\n",
      "Epoch 40/200\n",
      "3232/4140 [======================>.......] - ETA: 0s - loss: 0.4383 - accuracy: 0.8131\n",
      "Epoch 00040: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 57us/sample - loss: 0.4150 - accuracy: 0.8200 - val_loss: 0.5623 - val_accuracy: 0.7423\n",
      "Epoch 41/200\n",
      "3200/4140 [======================>.......] - ETA: 0s - loss: 0.3998 - accuracy: 0.8234\n",
      "Epoch 00041: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 57us/sample - loss: 0.3978 - accuracy: 0.8225 - val_loss: 0.3754 - val_accuracy: 0.8456\n",
      "Epoch 42/200\n",
      "3648/4140 [=========================>....] - ETA: 0s - loss: 0.3637 - accuracy: 0.8374\n",
      "Epoch 00042: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 52us/sample - loss: 0.3691 - accuracy: 0.8365 - val_loss: 1.4027 - val_accuracy: 0.3407\n",
      "Epoch 43/200\n",
      "3712/4140 [=========================>....] - ETA: 0s - loss: 0.3556 - accuracy: 0.8367\n",
      "Epoch 00043: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 51us/sample - loss: 0.3553 - accuracy: 0.8374 - val_loss: 0.6514 - val_accuracy: 0.6515\n",
      "Epoch 44/200\n",
      "3808/4140 [==========================>...] - ETA: 0s - loss: 0.3720 - accuracy: 0.8348\n",
      "Epoch 00044: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 50us/sample - loss: 0.3753 - accuracy: 0.8338 - val_loss: 0.8139 - val_accuracy: 0.5898\n",
      "Epoch 45/200\n",
      "3744/4140 [==========================>...] - ETA: 0s - loss: 0.3554 - accuracy: 0.8456\n",
      "Epoch 00045: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 51us/sample - loss: 0.3542 - accuracy: 0.8461 - val_loss: 0.2964 - val_accuracy: 0.8900\n",
      "Epoch 46/200\n",
      "3808/4140 [==========================>...] - ETA: 0s - loss: 0.4134 - accuracy: 0.8251\n",
      "Epoch 00046: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 51us/sample - loss: 0.4125 - accuracy: 0.8246 - val_loss: 0.1660 - val_accuracy: 0.9624\n",
      "Epoch 47/200\n",
      "3712/4140 [=========================>....] - ETA: 0s - loss: 0.3601 - accuracy: 0.8349\n",
      "Epoch 00047: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 52us/sample - loss: 0.3641 - accuracy: 0.8353 - val_loss: 0.9979 - val_accuracy: 0.5174\n",
      "Epoch 48/200\n",
      "3872/4140 [===========================>..] - ETA: 0s - loss: 0.3677 - accuracy: 0.8339\n",
      "Epoch 00048: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 50us/sample - loss: 0.3653 - accuracy: 0.8357 - val_loss: 0.2474 - val_accuracy: 0.9237\n",
      "Epoch 49/200\n",
      "3872/4140 [===========================>..] - ETA: 0s - loss: 0.3641 - accuracy: 0.8427\n",
      "Epoch 00049: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.3650 - accuracy: 0.8428 - val_loss: 0.9942 - val_accuracy: 0.5174\n",
      "Epoch 50/200\n",
      "3744/4140 [==========================>...] - ETA: 0s - loss: 0.3629 - accuracy: 0.8392\n",
      "Epoch 00050: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 50us/sample - loss: 0.3850 - accuracy: 0.8326 - val_loss: 1.0402 - val_accuracy: 0.4942\n",
      "Epoch 51/200\n",
      "3488/4140 [========================>.....] - ETA: 0s - loss: 0.4105 - accuracy: 0.8228\n",
      "Epoch 00051: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 56us/sample - loss: 0.4014 - accuracy: 0.8261 - val_loss: 0.4141 - val_accuracy: 0.8012\n",
      "Epoch 52/200\n",
      "3264/4140 [======================>.......] - ETA: 0s - loss: 0.3690 - accuracy: 0.8361\n",
      "Epoch 00052: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 56us/sample - loss: 0.3772 - accuracy: 0.8302 - val_loss: 0.5330 - val_accuracy: 0.7375\n",
      "Epoch 53/200\n",
      "3808/4140 [==========================>...] - ETA: 0s - loss: 0.3326 - accuracy: 0.8569\n",
      "Epoch 00053: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 50us/sample - loss: 0.3325 - accuracy: 0.8558 - val_loss: 0.6312 - val_accuracy: 0.6718\n",
      "Epoch 54/200\n",
      "3936/4140 [===========================>..] - ETA: 0s - loss: 0.3255 - accuracy: 0.8565\n",
      "Epoch 00054: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.3271 - accuracy: 0.8546 - val_loss: 0.1859 - val_accuracy: 0.9431\n",
      "Epoch 55/200\n",
      "3648/4140 [=========================>....] - ETA: 0s - loss: 0.3503 - accuracy: 0.8490\n",
      "Epoch 00055: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 51us/sample - loss: 0.3469 - accuracy: 0.8500 - val_loss: 0.5612 - val_accuracy: 0.7365\n",
      "Epoch 56/200\n",
      "3840/4140 [==========================>...] - ETA: 0s - loss: 0.3924 - accuracy: 0.8328\n",
      "Epoch 00056: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 50us/sample - loss: 0.3875 - accuracy: 0.8374 - val_loss: 0.3892 - val_accuracy: 0.8736\n",
      "Epoch 57/200\n",
      "3808/4140 [==========================>...] - ETA: 0s - loss: 0.3248 - accuracy: 0.8606\n",
      "Epoch 00057: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 51us/sample - loss: 0.3249 - accuracy: 0.8609 - val_loss: 0.2307 - val_accuracy: 0.9199\n",
      "Epoch 58/200\n",
      "3904/4140 [===========================>..] - ETA: 0s - loss: 0.3143 - accuracy: 0.8591\n",
      "Epoch 00058: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.3138 - accuracy: 0.8589 - val_loss: 0.8593 - val_accuracy: 0.6670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200\n",
      "3616/4140 [=========================>....] - ETA: 0s - loss: 0.3293 - accuracy: 0.8554\n",
      "Epoch 00059: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 52us/sample - loss: 0.3218 - accuracy: 0.8585 - val_loss: 0.3683 - val_accuracy: 0.8176\n",
      "Epoch 60/200\n",
      "3488/4140 [========================>.....] - ETA: 0s - loss: 0.2971 - accuracy: 0.8655\n",
      "Epoch 00060: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 53us/sample - loss: 0.3013 - accuracy: 0.8664 - val_loss: 0.4019 - val_accuracy: 0.8272\n",
      "Epoch 61/200\n",
      "3712/4140 [=========================>....] - ETA: 0s - loss: 0.2602 - accuracy: 0.8901\n",
      "Epoch 00061: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 51us/sample - loss: 0.2655 - accuracy: 0.8874 - val_loss: 0.7402 - val_accuracy: 0.6795\n",
      "Epoch 62/200\n",
      "3776/4140 [==========================>...] - ETA: 0s - loss: 0.3127 - accuracy: 0.8700\n",
      "Epoch 00062: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 51us/sample - loss: 0.3148 - accuracy: 0.8703 - val_loss: 0.9778 - val_accuracy: 0.5647\n",
      "Epoch 63/200\n",
      "3808/4140 [==========================>...] - ETA: 0s - loss: 0.2916 - accuracy: 0.8732\n",
      "Epoch 00063: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 50us/sample - loss: 0.2936 - accuracy: 0.8703 - val_loss: 0.1412 - val_accuracy: 0.9537\n",
      "Epoch 64/200\n",
      "3776/4140 [==========================>...] - ETA: 0s - loss: 0.3233 - accuracy: 0.8581\n",
      "Epoch 00064: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 50us/sample - loss: 0.3210 - accuracy: 0.8589 - val_loss: 0.2296 - val_accuracy: 0.9122\n",
      "Epoch 65/200\n",
      "3648/4140 [=========================>....] - ETA: 0s - loss: 0.2907 - accuracy: 0.8731\n",
      "Epoch 00065: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 52us/sample - loss: 0.2851 - accuracy: 0.8758 - val_loss: 0.3074 - val_accuracy: 0.8726\n",
      "Epoch 66/200\n",
      "3744/4140 [==========================>...] - ETA: 0s - loss: 0.3386 - accuracy: 0.8614\n",
      "Epoch 00066: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 51us/sample - loss: 0.3378 - accuracy: 0.8623 - val_loss: 0.2757 - val_accuracy: 0.9025\n",
      "Epoch 67/200\n",
      "3648/4140 [=========================>....] - ETA: 0s - loss: 0.3087 - accuracy: 0.8627\n",
      "Epoch 00067: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 51us/sample - loss: 0.3089 - accuracy: 0.8626 - val_loss: 0.3513 - val_accuracy: 0.8610\n",
      "Epoch 68/200\n",
      "3872/4140 [===========================>..] - ETA: 0s - loss: 0.3146 - accuracy: 0.8580\n",
      "Epoch 00068: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 50us/sample - loss: 0.3118 - accuracy: 0.8599 - val_loss: 0.3182 - val_accuracy: 0.8639\n",
      "Epoch 69/200\n",
      "3456/4140 [========================>.....] - ETA: 0s - loss: 0.2700 - accuracy: 0.8805\n",
      "Epoch 00069: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 67us/sample - loss: 0.2762 - accuracy: 0.8800 - val_loss: 0.5230 - val_accuracy: 0.7577\n",
      "Epoch 70/200\n",
      "3776/4140 [==========================>...] - ETA: 0s - loss: 0.3075 - accuracy: 0.8689\n",
      "Epoch 00070: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 50us/sample - loss: 0.3086 - accuracy: 0.8674 - val_loss: 0.1597 - val_accuracy: 0.9469\n",
      "Epoch 71/200\n",
      "3840/4140 [==========================>...] - ETA: 0s - loss: 0.2725 - accuracy: 0.8828\n",
      "Epoch 00071: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 51us/sample - loss: 0.2706 - accuracy: 0.8833 - val_loss: 0.1999 - val_accuracy: 0.9392\n",
      "Epoch 72/200\n",
      "3648/4140 [=========================>....] - ETA: 0s - loss: 0.2527 - accuracy: 0.8931\n",
      "Epoch 00072: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 53us/sample - loss: 0.2531 - accuracy: 0.8937 - val_loss: 0.3834 - val_accuracy: 0.8311\n",
      "Epoch 73/200\n",
      "3776/4140 [==========================>...] - ETA: 0s - loss: 0.2811 - accuracy: 0.8837\n",
      "Epoch 00073: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 50us/sample - loss: 0.2809 - accuracy: 0.8821 - val_loss: 1.4750 - val_accuracy: 0.4083\n",
      "Epoch 74/200\n",
      "3744/4140 [==========================>...] - ETA: 0s - loss: 0.3082 - accuracy: 0.8691\n",
      "Epoch 00074: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 50us/sample - loss: 0.3036 - accuracy: 0.8720 - val_loss: 0.3396 - val_accuracy: 0.8475\n",
      "Epoch 75/200\n",
      "3936/4140 [===========================>..] - ETA: 0s - loss: 0.2796 - accuracy: 0.8793\n",
      "Epoch 00075: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.2745 - accuracy: 0.8824 - val_loss: 0.4122 - val_accuracy: 0.8137\n",
      "Epoch 76/200\n",
      "3424/4140 [=======================>......] - ETA: 0s - loss: 0.2652 - accuracy: 0.8852\n",
      "Epoch 00076: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 54us/sample - loss: 0.2770 - accuracy: 0.8812 - val_loss: 0.3853 - val_accuracy: 0.8523\n",
      "Epoch 77/200\n",
      "3808/4140 [==========================>...] - ETA: 0s - loss: 0.3403 - accuracy: 0.8621\n",
      "Epoch 00077: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.3350 - accuracy: 0.8628 - val_loss: 0.4861 - val_accuracy: 0.7693\n",
      "Epoch 78/200\n",
      "3936/4140 [===========================>..] - ETA: 0s - loss: 0.2683 - accuracy: 0.8859\n",
      "Epoch 00078: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.2659 - accuracy: 0.8865 - val_loss: 0.5889 - val_accuracy: 0.7114\n",
      "Epoch 79/200\n",
      "3808/4140 [==========================>...] - ETA: 0s - loss: 0.2572 - accuracy: 0.8934\n",
      "Epoch 00079: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 50us/sample - loss: 0.2556 - accuracy: 0.8952 - val_loss: 0.1752 - val_accuracy: 0.9363\n",
      "Epoch 80/200\n",
      "3808/4140 [==========================>...] - ETA: 0s - loss: 0.2731 - accuracy: 0.8871\n",
      "Epoch 00080: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 51us/sample - loss: 0.2659 - accuracy: 0.8889 - val_loss: 0.2739 - val_accuracy: 0.9025\n",
      "Epoch 81/200\n",
      "3776/4140 [==========================>...] - ETA: 0s - loss: 0.2293 - accuracy: 0.8996\n",
      "Epoch 00081: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 51us/sample - loss: 0.2426 - accuracy: 0.8947 - val_loss: 0.6472 - val_accuracy: 0.6583\n",
      "Epoch 82/200\n",
      "3808/4140 [==========================>...] - ETA: 0s - loss: 0.2337 - accuracy: 0.9005\n",
      "Epoch 00082: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.2321 - accuracy: 0.9019 - val_loss: 0.3527 - val_accuracy: 0.8446\n",
      "Epoch 83/200\n",
      "3840/4140 [==========================>...] - ETA: 0s - loss: 0.2555 - accuracy: 0.8971\n",
      "Epoch 00083: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.2528 - accuracy: 0.8986 - val_loss: 0.2484 - val_accuracy: 0.8938\n",
      "Epoch 84/200\n",
      "3616/4140 [=========================>....] - ETA: 0s - loss: 0.2339 - accuracy: 0.8996\n",
      "Epoch 00084: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 52us/sample - loss: 0.2393 - accuracy: 0.8981 - val_loss: 0.5690 - val_accuracy: 0.7519\n",
      "Epoch 85/200\n",
      "3232/4140 [======================>.......] - ETA: 0s - loss: 0.2615 - accuracy: 0.8911\n",
      "Epoch 00085: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 57us/sample - loss: 0.2650 - accuracy: 0.8889 - val_loss: 0.2485 - val_accuracy: 0.8958\n",
      "Epoch 86/200\n",
      "3680/4140 [=========================>....] - ETA: 0s - loss: 0.2175 - accuracy: 0.9082\n",
      "Epoch 00086: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 53us/sample - loss: 0.2171 - accuracy: 0.9099 - val_loss: 0.2265 - val_accuracy: 0.9189\n",
      "Epoch 87/200\n",
      "3104/4140 [=====================>........] - ETA: 0s - loss: 0.2462 - accuracy: 0.8943\n",
      "Epoch 00087: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 59us/sample - loss: 0.2354 - accuracy: 0.8988 - val_loss: 0.3345 - val_accuracy: 0.8542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/200\n",
      "3936/4140 [===========================>..] - ETA: 0s - loss: 0.2363 - accuracy: 0.8938\n",
      "Epoch 00088: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 63us/sample - loss: 0.2357 - accuracy: 0.8935 - val_loss: 0.5483 - val_accuracy: 0.7539\n",
      "Epoch 89/200\n",
      "3872/4140 [===========================>..] - ETA: 0s - loss: 0.2167 - accuracy: 0.9044\n",
      "Epoch 00089: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.2177 - accuracy: 0.9051 - val_loss: 0.2518 - val_accuracy: 0.8736\n",
      "Epoch 90/200\n",
      "3776/4140 [==========================>...] - ETA: 0s - loss: 0.2162 - accuracy: 0.9081\n",
      "Epoch 00090: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.2139 - accuracy: 0.9097 - val_loss: 0.1386 - val_accuracy: 0.9556\n",
      "Epoch 91/200\n",
      "3840/4140 [==========================>...] - ETA: 0s - loss: 0.2087 - accuracy: 0.9112\n",
      "Epoch 00091: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.2124 - accuracy: 0.9106 - val_loss: 0.4346 - val_accuracy: 0.7963\n",
      "Epoch 92/200\n",
      "3936/4140 [===========================>..] - ETA: 0s - loss: 0.2190 - accuracy: 0.9121\n",
      "Epoch 00092: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.2182 - accuracy: 0.9109 - val_loss: 0.4317 - val_accuracy: 0.7809\n",
      "Epoch 93/200\n",
      "3776/4140 [==========================>...] - ETA: 0s - loss: 0.2104 - accuracy: 0.9121\n",
      "Epoch 00093: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 50us/sample - loss: 0.2124 - accuracy: 0.9126 - val_loss: 0.2112 - val_accuracy: 0.9180\n",
      "Epoch 94/200\n",
      "3776/4140 [==========================>...] - ETA: 0s - loss: 0.2108 - accuracy: 0.9118\n",
      "Epoch 00094: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 50us/sample - loss: 0.2165 - accuracy: 0.9101 - val_loss: 0.5090 - val_accuracy: 0.7529\n",
      "Epoch 95/200\n",
      "3392/4140 [=======================>......] - ETA: 0s - loss: 0.2862 - accuracy: 0.8900\n",
      "Epoch 00095: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 73us/sample - loss: 0.2768 - accuracy: 0.8923 - val_loss: 0.6930 - val_accuracy: 0.7181\n",
      "Epoch 96/200\n",
      "3392/4140 [=======================>......] - ETA: 0s - loss: 0.2257 - accuracy: 0.9071\n",
      "Epoch 00096: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 56us/sample - loss: 0.2156 - accuracy: 0.9111 - val_loss: 0.3098 - val_accuracy: 0.8851\n",
      "Epoch 97/200\n",
      "3872/4140 [===========================>..] - ETA: 0s - loss: 0.1944 - accuracy: 0.9220\n",
      "Epoch 00097: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 83us/sample - loss: 0.1987 - accuracy: 0.9186 - val_loss: 0.6350 - val_accuracy: 0.7259\n",
      "Epoch 98/200\n",
      "3040/4140 [=====================>........] - ETA: 0s - loss: 0.1879 - accuracy: 0.9243\n",
      "Epoch 00098: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 73us/sample - loss: 0.1915 - accuracy: 0.9215 - val_loss: 0.2828 - val_accuracy: 0.8832\n",
      "Epoch 99/200\n",
      "3328/4140 [=======================>......] - ETA: 0s - loss: 0.1845 - accuracy: 0.9210\n",
      "Epoch 00099: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 59us/sample - loss: 0.1856 - accuracy: 0.9188 - val_loss: 0.1595 - val_accuracy: 0.9450\n",
      "Epoch 100/200\n",
      "3360/4140 [=======================>......] - ETA: 0s - loss: 0.1562 - accuracy: 0.9330\n",
      "Epoch 00100: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 73us/sample - loss: 0.1648 - accuracy: 0.9312 - val_loss: 0.1399 - val_accuracy: 0.9595\n",
      "Epoch 101/200\n",
      "3488/4140 [========================>.....] - ETA: 0s - loss: 0.1915 - accuracy: 0.9212\n",
      "Epoch 00101: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 73us/sample - loss: 0.1906 - accuracy: 0.9217 - val_loss: 0.3521 - val_accuracy: 0.8562\n",
      "Epoch 102/200\n",
      "3584/4140 [========================>.....] - ETA: 0s - loss: 0.2164 - accuracy: 0.9110\n",
      "Epoch 00102: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 72us/sample - loss: 0.2194 - accuracy: 0.9114 - val_loss: 0.1328 - val_accuracy: 0.9498\n",
      "Epoch 103/200\n",
      "3744/4140 [==========================>...] - ETA: 0s - loss: 0.2036 - accuracy: 0.9177\n",
      "Epoch 00103: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 68us/sample - loss: 0.2097 - accuracy: 0.9164 - val_loss: 0.3723 - val_accuracy: 0.8292\n",
      "Epoch 104/200\n",
      "3488/4140 [========================>.....] - ETA: 0s - loss: 0.1863 - accuracy: 0.9249\n",
      "Epoch 00104: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 56us/sample - loss: 0.1846 - accuracy: 0.9251 - val_loss: 0.1447 - val_accuracy: 0.9440\n",
      "Epoch 105/200\n",
      "3776/4140 [==========================>...] - ETA: 0s - loss: 0.1871 - accuracy: 0.9285\n",
      "Epoch 00105: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 67us/sample - loss: 0.1847 - accuracy: 0.9283 - val_loss: 0.3760 - val_accuracy: 0.8253\n",
      "Epoch 106/200\n",
      "3648/4140 [=========================>....] - ETA: 0s - loss: 0.1555 - accuracy: 0.9334\n",
      "Epoch 00106: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 98us/sample - loss: 0.1530 - accuracy: 0.9355 - val_loss: 0.2306 - val_accuracy: 0.9064\n",
      "Epoch 107/200\n",
      "3680/4140 [=========================>....] - ETA: 0s - loss: 0.1501 - accuracy: 0.9391\n",
      "Epoch 00107: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 66us/sample - loss: 0.1507 - accuracy: 0.9386 - val_loss: 0.7672 - val_accuracy: 0.6766\n",
      "Epoch 108/200\n",
      "3296/4140 [======================>.......] - ETA: 0s - loss: 0.2155 - accuracy: 0.9053\n",
      "Epoch 00108: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 59us/sample - loss: 0.2142 - accuracy: 0.9065 - val_loss: 0.3414 - val_accuracy: 0.8552\n",
      "Epoch 109/200\n",
      "3136/4140 [=====================>........] - ETA: 0s - loss: 0.1760 - accuracy: 0.9283\n",
      "Epoch 00109: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 60us/sample - loss: 0.2020 - accuracy: 0.9171 - val_loss: 0.1136 - val_accuracy: 0.9720\n",
      "Epoch 110/200\n",
      "3712/4140 [=========================>....] - ETA: 0s - loss: 0.2243 - accuracy: 0.9054\n",
      "Epoch 00110: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 52us/sample - loss: 0.2158 - accuracy: 0.9092 - val_loss: 0.1227 - val_accuracy: 0.9614\n",
      "Epoch 111/200\n",
      "3648/4140 [=========================>....] - ETA: 0s - loss: 0.2000 - accuracy: 0.9194\n",
      "Epoch 00111: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 68us/sample - loss: 0.1981 - accuracy: 0.9193 - val_loss: 0.3897 - val_accuracy: 0.8263\n",
      "Epoch 112/200\n",
      "3104/4140 [=====================>........] - ETA: 0s - loss: 0.1856 - accuracy: 0.9253\n",
      "Epoch 00112: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 59us/sample - loss: 0.1799 - accuracy: 0.9290 - val_loss: 0.2418 - val_accuracy: 0.9015\n",
      "Epoch 113/200\n",
      "3584/4140 [========================>.....] - ETA: 0s - loss: 0.1661 - accuracy: 0.9297\n",
      "Epoch 00113: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 52us/sample - loss: 0.1661 - accuracy: 0.9309 - val_loss: 0.2394 - val_accuracy: 0.9015\n",
      "Epoch 114/200\n",
      "3616/4140 [=========================>....] - ETA: 0s - loss: 0.1920 - accuracy: 0.9143\n",
      "Epoch 00114: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 52us/sample - loss: 0.1840 - accuracy: 0.9193 - val_loss: 0.0559 - val_accuracy: 0.9903\n",
      "Epoch 115/200\n",
      "3680/4140 [=========================>....] - ETA: 0s - loss: 0.1515 - accuracy: 0.9424\n",
      "Epoch 00115: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 51us/sample - loss: 0.1534 - accuracy: 0.9415 - val_loss: 0.6353 - val_accuracy: 0.8311\n",
      "Epoch 116/200\n",
      "3712/4140 [=========================>....] - ETA: 0s - loss: 0.1686 - accuracy: 0.9367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00116: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 51us/sample - loss: 0.1747 - accuracy: 0.9345 - val_loss: 0.3946 - val_accuracy: 0.8533\n",
      "Epoch 117/200\n",
      "3744/4140 [==========================>...] - ETA: 0s - loss: 0.2153 - accuracy: 0.9132\n",
      "Epoch 00117: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 51us/sample - loss: 0.2261 - accuracy: 0.9114 - val_loss: 0.2065 - val_accuracy: 0.9450\n",
      "Epoch 118/200\n",
      "3840/4140 [==========================>...] - ETA: 0s - loss: 0.1822 - accuracy: 0.9247\n",
      "Epoch 00118: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 50us/sample - loss: 0.1800 - accuracy: 0.9251 - val_loss: 0.1068 - val_accuracy: 0.9701\n",
      "Epoch 119/200\n",
      "3744/4140 [==========================>...] - ETA: 0s - loss: 0.1568 - accuracy: 0.9316\n",
      "Epoch 00119: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 51us/sample - loss: 0.1586 - accuracy: 0.9316 - val_loss: 0.1607 - val_accuracy: 0.9469\n",
      "Epoch 120/200\n",
      "3808/4140 [==========================>...] - ETA: 0s - loss: 0.1491 - accuracy: 0.9378\n",
      "Epoch 00120: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 50us/sample - loss: 0.1530 - accuracy: 0.9355 - val_loss: 0.1068 - val_accuracy: 0.9614\n",
      "Epoch 121/200\n",
      "3744/4140 [==========================>...] - ETA: 0s - loss: 0.1803 - accuracy: 0.9199\n",
      "Epoch 00121: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 51us/sample - loss: 0.1846 - accuracy: 0.9191 - val_loss: 0.2697 - val_accuracy: 0.8851\n",
      "Epoch 122/200\n",
      "3776/4140 [==========================>...] - ETA: 0s - loss: 0.1402 - accuracy: 0.9425\n",
      "Epoch 00122: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 50us/sample - loss: 0.1377 - accuracy: 0.9437 - val_loss: 0.3621 - val_accuracy: 0.8475\n",
      "Epoch 123/200\n",
      "3808/4140 [==========================>...] - ETA: 0s - loss: 0.1209 - accuracy: 0.9538\n",
      "Epoch 00123: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 50us/sample - loss: 0.1238 - accuracy: 0.9512 - val_loss: 0.1915 - val_accuracy: 0.9257\n",
      "Epoch 124/200\n",
      "3744/4140 [==========================>...] - ETA: 0s - loss: 0.1625 - accuracy: 0.9391\n",
      "Epoch 00124: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 51us/sample - loss: 0.1632 - accuracy: 0.9384 - val_loss: 0.1811 - val_accuracy: 0.9276\n",
      "Epoch 125/200\n",
      "3776/4140 [==========================>...] - ETA: 0s - loss: 0.1392 - accuracy: 0.9420\n",
      "Epoch 00125: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 50us/sample - loss: 0.1441 - accuracy: 0.9399 - val_loss: 0.3002 - val_accuracy: 0.8803\n",
      "Epoch 126/200\n",
      "3744/4140 [==========================>...] - ETA: 0s - loss: 0.1708 - accuracy: 0.9351\n",
      "Epoch 00126: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 51us/sample - loss: 0.1651 - accuracy: 0.9372 - val_loss: 0.3203 - val_accuracy: 0.8871\n",
      "Epoch 127/200\n",
      "3776/4140 [==========================>...] - ETA: 0s - loss: 0.1436 - accuracy: 0.9399\n",
      "Epoch 00127: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 50us/sample - loss: 0.1433 - accuracy: 0.9399 - val_loss: 0.0858 - val_accuracy: 0.9720\n",
      "Epoch 128/200\n",
      "3392/4140 [=======================>......] - ETA: 0s - loss: 0.1276 - accuracy: 0.9496\n",
      "Epoch 00128: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 56us/sample - loss: 0.1286 - accuracy: 0.9490 - val_loss: 0.0959 - val_accuracy: 0.9749\n",
      "Epoch 129/200\n",
      "3360/4140 [=======================>......] - ETA: 0s - loss: 0.1245 - accuracy: 0.9545\n",
      "Epoch 00129: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 56us/sample - loss: 0.1862 - accuracy: 0.9357 - val_loss: 0.7678 - val_accuracy: 0.8388\n",
      "Epoch 130/200\n",
      "3712/4140 [=========================>....] - ETA: 0s - loss: 0.2579 - accuracy: 0.9011\n",
      "Epoch 00130: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 51us/sample - loss: 0.2495 - accuracy: 0.9041 - val_loss: 0.5962 - val_accuracy: 0.7394\n",
      "Epoch 131/200\n",
      "3872/4140 [===========================>..] - ETA: 0s - loss: 0.1642 - accuracy: 0.9362\n",
      "Epoch 00131: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.1618 - accuracy: 0.9374 - val_loss: 0.1811 - val_accuracy: 0.9392\n",
      "Epoch 132/200\n",
      "3840/4140 [==========================>...] - ETA: 0s - loss: 0.1203 - accuracy: 0.9563\n",
      "Epoch 00132: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 50us/sample - loss: 0.1231 - accuracy: 0.9558 - val_loss: 0.1006 - val_accuracy: 0.9720\n",
      "Epoch 133/200\n",
      "3904/4140 [===========================>..] - ETA: 0s - loss: 0.1259 - accuracy: 0.9480\n",
      "Epoch 00133: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 48us/sample - loss: 0.1284 - accuracy: 0.9481 - val_loss: 0.3347 - val_accuracy: 0.8832\n",
      "Epoch 134/200\n",
      "3808/4140 [==========================>...] - ETA: 0s - loss: 0.1189 - accuracy: 0.9527\n",
      "Epoch 00134: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.1180 - accuracy: 0.9534 - val_loss: 0.1561 - val_accuracy: 0.9373\n",
      "Epoch 135/200\n",
      "3776/4140 [==========================>...] - ETA: 0s - loss: 0.1492 - accuracy: 0.9428\n",
      "Epoch 00135: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 50us/sample - loss: 0.1453 - accuracy: 0.9437 - val_loss: 0.3010 - val_accuracy: 0.8803\n",
      "Epoch 136/200\n",
      "3904/4140 [===========================>..] - ETA: 0s - loss: 0.1055 - accuracy: 0.9603\n",
      "Epoch 00136: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.1048 - accuracy: 0.9609 - val_loss: 0.1149 - val_accuracy: 0.9807\n",
      "Epoch 137/200\n",
      "3872/4140 [===========================>..] - ETA: 0s - loss: 0.1150 - accuracy: 0.9530\n",
      "Epoch 00137: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 50us/sample - loss: 0.1159 - accuracy: 0.9527 - val_loss: 0.1380 - val_accuracy: 0.9421\n",
      "Epoch 138/200\n",
      "3744/4140 [==========================>...] - ETA: 0s - loss: 0.1094 - accuracy: 0.9594\n",
      "Epoch 00138: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 50us/sample - loss: 0.1120 - accuracy: 0.9575 - val_loss: 0.6242 - val_accuracy: 0.7722\n",
      "Epoch 139/200\n",
      "3840/4140 [==========================>...] - ETA: 0s - loss: 0.1522 - accuracy: 0.9396\n",
      "Epoch 00139: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.1576 - accuracy: 0.9365 - val_loss: 0.5039 - val_accuracy: 0.7770\n",
      "Epoch 140/200\n",
      "3808/4140 [==========================>...] - ETA: 0s - loss: 0.2450 - accuracy: 0.9049\n",
      "Epoch 00140: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 50us/sample - loss: 0.2394 - accuracy: 0.9072 - val_loss: 0.1868 - val_accuracy: 0.9286\n",
      "Epoch 141/200\n",
      "3936/4140 [===========================>..] - ETA: 0s - loss: 0.1348 - accuracy: 0.9459\n",
      "Epoch 00141: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 49us/sample - loss: 0.1337 - accuracy: 0.9464 - val_loss: 0.1947 - val_accuracy: 0.9189\n",
      "Epoch 142/200\n",
      "3904/4140 [===========================>..] - ETA: 0s - loss: 0.1129 - accuracy: 0.9521\n",
      "Epoch 00142: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 60us/sample - loss: 0.1127 - accuracy: 0.9517 - val_loss: 0.1660 - val_accuracy: 0.9373\n",
      "Epoch 143/200\n",
      "4032/4140 [============================>.] - ETA: 0s - loss: 0.1275 - accuracy: 0.9482\n",
      "Epoch 00143: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 92us/sample - loss: 0.1276 - accuracy: 0.9483 - val_loss: 0.1330 - val_accuracy: 0.9527\n",
      "Epoch 144/200\n",
      "3616/4140 [=========================>....] - ETA: 0s - loss: 0.1800 - accuracy: 0.9289\n",
      "Epoch 00144: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 82us/sample - loss: 0.2026 - accuracy: 0.9203 - val_loss: 0.4387 - val_accuracy: 0.8108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/200\n",
      "3328/4140 [=======================>......] - ETA: 0s - loss: 0.1419 - accuracy: 0.9438\n",
      "Epoch 00145: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 68us/sample - loss: 0.1387 - accuracy: 0.9459 - val_loss: 0.0830 - val_accuracy: 0.9768\n",
      "Epoch 146/200\n",
      "3232/4140 [======================>.......] - ETA: 0s - loss: 0.1946 - accuracy: 0.9186\n",
      "Epoch 00146: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 69us/sample - loss: 0.1760 - accuracy: 0.9273 - val_loss: 0.2160 - val_accuracy: 0.9247\n",
      "Epoch 147/200\n",
      "3904/4140 [===========================>..] - ETA: 0s - loss: 0.1744 - accuracy: 0.9326\n",
      "Epoch 00147: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 77us/sample - loss: 0.1721 - accuracy: 0.9329 - val_loss: 0.1443 - val_accuracy: 0.9479\n",
      "Epoch 148/200\n",
      "3968/4140 [===========================>..] - ETA: 0s - loss: 0.0943 - accuracy: 0.9642\n",
      "Epoch 00148: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 65us/sample - loss: 0.0954 - accuracy: 0.9635 - val_loss: 0.0831 - val_accuracy: 0.9691\n",
      "Epoch 149/200\n",
      "3872/4140 [===========================>..] - ETA: 0s - loss: 0.1308 - accuracy: 0.9468\n",
      "Epoch 00149: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 65us/sample - loss: 0.1291 - accuracy: 0.9478 - val_loss: 0.1960 - val_accuracy: 0.9295\n",
      "Epoch 150/200\n",
      "3808/4140 [==========================>...] - ETA: 0s - loss: 0.1050 - accuracy: 0.9603\n",
      "Epoch 00150: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 76us/sample - loss: 0.1061 - accuracy: 0.9601 - val_loss: 0.0512 - val_accuracy: 0.9884\n",
      "Epoch 151/200\n",
      "4000/4140 [===========================>..] - ETA: 0s - loss: 0.1141 - accuracy: 0.9542\n",
      "Epoch 00151: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 76us/sample - loss: 0.1133 - accuracy: 0.9543 - val_loss: 0.1521 - val_accuracy: 0.9479\n",
      "Epoch 152/200\n",
      "3968/4140 [===========================>..] - ETA: 0s - loss: 0.1153 - accuracy: 0.9567\n",
      "Epoch 00152: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 78us/sample - loss: 0.1139 - accuracy: 0.9563 - val_loss: 0.0682 - val_accuracy: 0.9768\n",
      "Epoch 153/200\n",
      "3168/4140 [=====================>........] - ETA: 0s - loss: 0.0761 - accuracy: 0.9725\n",
      "Epoch 00153: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 68us/sample - loss: 0.0848 - accuracy: 0.9708 - val_loss: 0.2554 - val_accuracy: 0.9064\n",
      "Epoch 154/200\n",
      "3456/4140 [========================>.....] - ETA: 0s - loss: 0.2525 - accuracy: 0.9175\n",
      "Epoch 00154: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 89us/sample - loss: 0.2407 - accuracy: 0.9164 - val_loss: 0.1246 - val_accuracy: 0.9537\n",
      "Epoch 155/200\n",
      "4128/4140 [============================>.] - ETA: 0s - loss: 0.1084 - accuracy: 0.9608\n",
      "Epoch 00155: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 88us/sample - loss: 0.1082 - accuracy: 0.9609 - val_loss: 0.1358 - val_accuracy: 0.9537\n",
      "Epoch 156/200\n",
      "3840/4140 [==========================>...] - ETA: 0s - loss: 0.0962 - accuracy: 0.9633\n",
      "Epoch 00156: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 89us/sample - loss: 0.0959 - accuracy: 0.9630 - val_loss: 0.1424 - val_accuracy: 0.9459\n",
      "Epoch 157/200\n",
      "3968/4140 [===========================>..] - ETA: 0s - loss: 0.1050 - accuracy: 0.9597\n",
      "Epoch 00157: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 77us/sample - loss: 0.1082 - accuracy: 0.9592 - val_loss: 0.1705 - val_accuracy: 0.9411\n",
      "Epoch 158/200\n",
      "3904/4140 [===========================>..] - ETA: 0s - loss: 0.0838 - accuracy: 0.9662\n",
      "Epoch 00158: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 80us/sample - loss: 0.0831 - accuracy: 0.9671 - val_loss: 0.0789 - val_accuracy: 0.9817\n",
      "Epoch 159/200\n",
      "3680/4140 [=========================>....] - ETA: 0s - loss: 0.0735 - accuracy: 0.9755\n",
      "Epoch 00159: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 67us/sample - loss: 0.0721 - accuracy: 0.9758 - val_loss: 0.0657 - val_accuracy: 0.9807\n",
      "Epoch 160/200\n",
      "3648/4140 [=========================>....] - ETA: 0s - loss: 0.0635 - accuracy: 0.9770\n",
      "Epoch 00160: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 67us/sample - loss: 0.0682 - accuracy: 0.9763 - val_loss: 0.1127 - val_accuracy: 0.9730\n",
      "Epoch 161/200\n",
      "3712/4140 [=========================>....] - ETA: 0s - loss: 0.1137 - accuracy: 0.9561\n",
      "Epoch 00161: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 66us/sample - loss: 0.1127 - accuracy: 0.9565 - val_loss: 0.1813 - val_accuracy: 0.9218\n",
      "Epoch 162/200\n",
      "3552/4140 [========================>.....] - ETA: 0s - loss: 0.1239 - accuracy: 0.9530\n",
      "Epoch 00162: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 85us/sample - loss: 0.1358 - accuracy: 0.9483 - val_loss: 0.1807 - val_accuracy: 0.9537\n",
      "Epoch 163/200\n",
      "3680/4140 [=========================>....] - ETA: 0s - loss: 0.2042 - accuracy: 0.9171\n",
      "Epoch 00163: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 107us/sample - loss: 0.1998 - accuracy: 0.9181 - val_loss: 0.1901 - val_accuracy: 0.9276\n",
      "Epoch 164/200\n",
      "4000/4140 [===========================>..] - ETA: 0s - loss: 0.1314 - accuracy: 0.9477\n",
      "Epoch 00164: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 99us/sample - loss: 0.1383 - accuracy: 0.9464 - val_loss: 0.1117 - val_accuracy: 0.9585\n",
      "Epoch 165/200\n",
      "3712/4140 [=========================>....] - ETA: 0s - loss: 0.1591 - accuracy: 0.9351\n",
      "Epoch 00165: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 97us/sample - loss: 0.1590 - accuracy: 0.9338 - val_loss: 0.1250 - val_accuracy: 0.9469\n",
      "Epoch 166/200\n",
      "3648/4140 [=========================>....] - ETA: 0s - loss: 0.1444 - accuracy: 0.9457\n",
      "Epoch 00166: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 86us/sample - loss: 0.1461 - accuracy: 0.9435 - val_loss: 0.1885 - val_accuracy: 0.9170\n",
      "Epoch 167/200\n",
      "3520/4140 [========================>.....] - ETA: 0s - loss: 0.0806 - accuracy: 0.9693\n",
      "Epoch 00167: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 102us/sample - loss: 0.0810 - accuracy: 0.9686 - val_loss: 0.1395 - val_accuracy: 0.9469\n",
      "Epoch 168/200\n",
      "3744/4140 [==========================>...] - ETA: 0s - loss: 0.0852 - accuracy: 0.9685\n",
      "Epoch 00168: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 100us/sample - loss: 0.0856 - accuracy: 0.9679 - val_loss: 0.2249 - val_accuracy: 0.9131\n",
      "Epoch 169/200\n",
      "3392/4140 [=======================>......] - ETA: 0s - loss: 0.0716 - accuracy: 0.9755\n",
      "Epoch 00169: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 83us/sample - loss: 0.0713 - accuracy: 0.9754 - val_loss: 0.1491 - val_accuracy: 0.9527\n",
      "Epoch 170/200\n",
      "4128/4140 [============================>.] - ETA: 0s - loss: 0.0879 - accuracy: 0.9654\n",
      "Epoch 00170: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 76us/sample - loss: 0.0883 - accuracy: 0.9652 - val_loss: 0.4897 - val_accuracy: 0.8108\n",
      "Epoch 171/200\n",
      "3552/4140 [========================>.....] - ETA: 0s - loss: 0.1385 - accuracy: 0.9493\n",
      "Epoch 00171: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 68us/sample - loss: 0.1393 - accuracy: 0.9486 - val_loss: 0.0748 - val_accuracy: 0.9778\n",
      "Epoch 172/200\n",
      "4064/4140 [============================>.] - ETA: 0s - loss: 0.0885 - accuracy: 0.9663\n",
      "Epoch 00172: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 76us/sample - loss: 0.0907 - accuracy: 0.9657 - val_loss: 0.2342 - val_accuracy: 0.9064\n",
      "Epoch 173/200\n",
      "4000/4140 [===========================>..] - ETA: 0s - loss: 0.1241 - accuracy: 0.9528\n",
      "Epoch 00173: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 78us/sample - loss: 0.1253 - accuracy: 0.9524 - val_loss: 0.2081 - val_accuracy: 0.9247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174/200\n",
      "3776/4140 [==========================>...] - ETA: 0s - loss: 0.1027 - accuracy: 0.9613\n",
      "Epoch 00174: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 65us/sample - loss: 0.1015 - accuracy: 0.9611 - val_loss: 0.0999 - val_accuracy: 0.9672\n",
      "Epoch 175/200\n",
      "3808/4140 [==========================>...] - ETA: 0s - loss: 0.0667 - accuracy: 0.9782\n",
      "Epoch 00175: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 65us/sample - loss: 0.0678 - accuracy: 0.9768 - val_loss: 0.0996 - val_accuracy: 0.9595\n",
      "Epoch 176/200\n",
      "3552/4140 [========================>.....] - ETA: 0s - loss: 0.0956 - accuracy: 0.9657\n",
      "Epoch 00176: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 68us/sample - loss: 0.0927 - accuracy: 0.9671 - val_loss: 0.1428 - val_accuracy: 0.9527\n",
      "Epoch 177/200\n",
      "4064/4140 [============================>.] - ETA: 0s - loss: 0.0695 - accuracy: 0.9766\n",
      "Epoch 00177: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 75us/sample - loss: 0.0744 - accuracy: 0.9751 - val_loss: 0.2466 - val_accuracy: 0.9064\n",
      "Epoch 178/200\n",
      "3520/4140 [========================>.....] - ETA: 0s - loss: 0.1675 - accuracy: 0.9366\n",
      "Epoch 00178: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 67us/sample - loss: 0.1548 - accuracy: 0.9408 - val_loss: 0.2214 - val_accuracy: 0.9093\n",
      "Epoch 179/200\n",
      "3392/4140 [=======================>......] - ETA: 0s - loss: 0.0900 - accuracy: 0.9658\n",
      "Epoch 00179: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 70us/sample - loss: 0.0870 - accuracy: 0.9679 - val_loss: 0.1303 - val_accuracy: 0.9604\n",
      "Epoch 180/200\n",
      "3264/4140 [======================>.......] - ETA: 0s - loss: 0.0455 - accuracy: 0.9874\n",
      "Epoch 00180: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 68us/sample - loss: 0.0460 - accuracy: 0.9865 - val_loss: 0.0501 - val_accuracy: 0.9875\n",
      "Epoch 181/200\n",
      "4128/4140 [============================>.] - ETA: 0s - loss: 0.1273 - accuracy: 0.9542\n",
      "Epoch 00181: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 75us/sample - loss: 0.1270 - accuracy: 0.9543 - val_loss: 0.6461 - val_accuracy: 0.7770\n",
      "Epoch 182/200\n",
      "3904/4140 [===========================>..] - ETA: 0s - loss: 0.2461 - accuracy: 0.9191\n",
      "Epoch 00182: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 68us/sample - loss: 0.2428 - accuracy: 0.9196 - val_loss: 0.4058 - val_accuracy: 0.8292\n",
      "Epoch 183/200\n",
      "3840/4140 [==========================>...] - ETA: 0s - loss: 0.1375 - accuracy: 0.9424\n",
      "Epoch 00183: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 67us/sample - loss: 0.1362 - accuracy: 0.9435 - val_loss: 0.2473 - val_accuracy: 0.9054\n",
      "Epoch 184/200\n",
      "3776/4140 [==========================>...] - ETA: 0s - loss: 0.1006 - accuracy: 0.9650\n",
      "Epoch 00184: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 68us/sample - loss: 0.1003 - accuracy: 0.9647 - val_loss: 0.1110 - val_accuracy: 0.9701\n",
      "Epoch 185/200\n",
      "3456/4140 [========================>.....] - ETA: 0s - loss: 0.0632 - accuracy: 0.9803\n",
      "Epoch 00185: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 69us/sample - loss: 0.0635 - accuracy: 0.9797 - val_loss: 0.0702 - val_accuracy: 0.9797\n",
      "Epoch 186/200\n",
      "3456/4140 [========================>.....] - ETA: 0s - loss: 0.0740 - accuracy: 0.9714\n",
      "Epoch 00186: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 70us/sample - loss: 0.0718 - accuracy: 0.9729 - val_loss: 0.1360 - val_accuracy: 0.9537\n",
      "Epoch 187/200\n",
      "3968/4140 [===========================>..] - ETA: 0s - loss: 0.0529 - accuracy: 0.9829\n",
      "Epoch 00187: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 75us/sample - loss: 0.0520 - accuracy: 0.9831 - val_loss: 0.1323 - val_accuracy: 0.9546\n",
      "Epoch 188/200\n",
      "3520/4140 [========================>.....] - ETA: 0s - loss: 0.0552 - accuracy: 0.9827\n",
      "Epoch 00188: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 68us/sample - loss: 0.0550 - accuracy: 0.9824 - val_loss: 0.1794 - val_accuracy: 0.9228\n",
      "Epoch 189/200\n",
      "3488/4140 [========================>.....] - ETA: 0s - loss: 0.1287 - accuracy: 0.9561\n",
      "Epoch 00189: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 72us/sample - loss: 0.1314 - accuracy: 0.9558 - val_loss: 0.3231 - val_accuracy: 0.8813\n",
      "Epoch 190/200\n",
      "3648/4140 [=========================>....] - ETA: 0s - loss: 0.1265 - accuracy: 0.9512\n",
      "Epoch 00190: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 67us/sample - loss: 0.1207 - accuracy: 0.9519 - val_loss: 0.0896 - val_accuracy: 0.9797\n",
      "Epoch 191/200\n",
      "3360/4140 [=======================>......] - ETA: 0s - loss: 0.1393 - accuracy: 0.9467\n",
      "Epoch 00191: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 69us/sample - loss: 0.1382 - accuracy: 0.9473 - val_loss: 0.1472 - val_accuracy: 0.9382\n",
      "Epoch 192/200\n",
      "3584/4140 [========================>.....] - ETA: 0s - loss: 0.1191 - accuracy: 0.9562\n",
      "Epoch 00192: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 68us/sample - loss: 0.1136 - accuracy: 0.9585 - val_loss: 0.1057 - val_accuracy: 0.9691\n",
      "Epoch 193/200\n",
      "3552/4140 [========================>.....] - ETA: 0s - loss: 0.0773 - accuracy: 0.9688\n",
      "Epoch 00193: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 67us/sample - loss: 0.0787 - accuracy: 0.9691 - val_loss: 0.1199 - val_accuracy: 0.9479\n",
      "Epoch 194/200\n",
      "3840/4140 [==========================>...] - ETA: 0s - loss: 0.0550 - accuracy: 0.9828\n",
      "Epoch 00194: val_loss did not improve from 0.04715\n",
      "4140/4140 [==============================] - 0s 65us/sample - loss: 0.0567 - accuracy: 0.9819 - val_loss: 0.0984 - val_accuracy: 0.9720\n",
      "Epoch 195/200\n",
      "3680/4140 [=========================>....] - ETA: 0s - loss: 0.0427 - accuracy: 0.9859\n",
      "Epoch 00195: val_loss improved from 0.04715 to 0.04632, saving model to best_model_2.h5\n",
      "4140/4140 [==============================] - 0s 73us/sample - loss: 0.0421 - accuracy: 0.9865 - val_loss: 0.0463 - val_accuracy: 0.9894\n",
      "Epoch 196/200\n",
      "3456/4140 [========================>.....] - ETA: 0s - loss: 0.0380 - accuracy: 0.9881\n",
      "Epoch 00196: val_loss did not improve from 0.04632\n",
      "4140/4140 [==============================] - 0s 69us/sample - loss: 0.0419 - accuracy: 0.9874 - val_loss: 0.1898 - val_accuracy: 0.9344\n",
      "Epoch 197/200\n",
      "4128/4140 [============================>.] - ETA: 0s - loss: 0.0635 - accuracy: 0.9797\n",
      "Epoch 00197: val_loss did not improve from 0.04632\n",
      "4140/4140 [==============================] - 0s 76us/sample - loss: 0.0654 - accuracy: 0.9790 - val_loss: 0.3455 - val_accuracy: 0.8832\n",
      "Epoch 198/200\n",
      "3488/4140 [========================>.....] - ETA: 0s - loss: 0.4251 - accuracy: 0.8802\n",
      "Epoch 00198: val_loss did not improve from 0.04632\n",
      "4140/4140 [==============================] - 0s 68us/sample - loss: 0.3812 - accuracy: 0.8915 - val_loss: 0.1639 - val_accuracy: 0.9373\n",
      "Epoch 199/200\n",
      "3552/4140 [========================>.....] - ETA: 0s - loss: 0.1228 - accuracy: 0.9524\n",
      "Epoch 00199: val_loss did not improve from 0.04632\n",
      "4140/4140 [==============================] - 0s 71us/sample - loss: 0.1222 - accuracy: 0.9527 - val_loss: 0.2077 - val_accuracy: 0.9180\n",
      "Epoch 200/200\n",
      "3616/4140 [=========================>....] - ETA: 0s - loss: 0.1194 - accuracy: 0.9588\n",
      "Epoch 00200: val_loss did not improve from 0.04632\n",
      "4140/4140 [==============================] - 0s 66us/sample - loss: 0.1131 - accuracy: 0.9601 - val_loss: 0.3666 - val_accuracy: 0.9170\n",
      "Testing on Fold 4\n",
      "# Getting train data set up\n",
      "# Getting test data set up\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 2400 samples, validate on 600 samples\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2368/2400 [============================>.] - ETA: 0s - loss: 184.1818 - mse: 184.1818\n",
      "Epoch 00001: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 4s 2ms/sample - loss: 183.5705 - mse: 183.5704 - val_loss: 138.9953 - val_mse: 138.9953\n",
      "Epoch 2/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 176.4492 - mse: 176.4492\n",
      "Epoch 00002: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 819us/sample - loss: 173.8007 - mse: 173.8007 - val_loss: 154.3021 - val_mse: 154.3021\n",
      "Epoch 3/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 166.6715 - mse: 166.6716\n",
      "Epoch 00003: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 919us/sample - loss: 165.5025 - mse: 165.5025 - val_loss: 150.2068 - val_mse: 150.2068\n",
      "Epoch 4/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 176.9716 - mse: 176.9716\n",
      "Epoch 00004: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 852us/sample - loss: 176.7522 - mse: 176.7523 - val_loss: 140.6176 - val_mse: 140.6176\n",
      "Epoch 5/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 169.5191 - mse: 169.5191\n",
      "Epoch 00005: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 831us/sample - loss: 167.8317 - mse: 167.8317 - val_loss: 161.3771 - val_mse: 161.3771\n",
      "Epoch 6/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 165.2837 - mse: 165.2836\n",
      "Epoch 00006: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 869us/sample - loss: 162.5147 - mse: 162.5147 - val_loss: 149.8513 - val_mse: 149.8513\n",
      "Epoch 7/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 158.2568 - mse: 158.2568\n",
      "Epoch 00007: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 856us/sample - loss: 159.4063 - mse: 159.4063 - val_loss: 131.2538 - val_mse: 131.2538\n",
      "Epoch 8/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 153.5046 - mse: 153.5045\n",
      "Epoch 00008: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 823us/sample - loss: 160.1557 - mse: 160.1556 - val_loss: 138.4331 - val_mse: 138.4331\n",
      "Epoch 9/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 162.2688 - mse: 162.2687\n",
      "Epoch 00009: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 830us/sample - loss: 159.4637 - mse: 159.4636 - val_loss: 139.8174 - val_mse: 139.8174\n",
      "Epoch 10/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.3239 - mse: 163.3239\n",
      "Epoch 00010: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 841us/sample - loss: 162.0970 - mse: 162.0970 - val_loss: 144.6563 - val_mse: 144.6563\n",
      "Epoch 11/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 155.1871 - mse: 155.1871\n",
      "Epoch 00011: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 958us/sample - loss: 154.9588 - mse: 154.9588 - val_loss: 138.6792 - val_mse: 138.6792\n",
      "Epoch 12/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 155.1437 - mse: 155.1438\n",
      "Epoch 00012: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 1ms/sample - loss: 155.0019 - mse: 155.0019 - val_loss: 136.7385 - val_mse: 136.7385\n",
      "Epoch 13/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 152.2766 - mse: 152.2766\n",
      "Epoch 00013: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 3s 1ms/sample - loss: 153.9410 - mse: 153.9409 - val_loss: 129.3125 - val_mse: 129.3125\n",
      "Epoch 14/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 154.4659 - mse: 154.4659\n",
      "Epoch 00014: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 1ms/sample - loss: 155.9032 - mse: 155.9032 - val_loss: 130.7992 - val_mse: 130.7992\n",
      "Epoch 15/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 159.3124 - mse: 159.3124\n",
      "Epoch 00015: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 3s 1ms/sample - loss: 159.2853 - mse: 159.2853 - val_loss: 136.6968 - val_mse: 136.6968\n",
      "Epoch 16/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 152.2254 - mse: 152.2254\n",
      "Epoch 00016: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 901us/sample - loss: 151.1680 - mse: 151.1681 - val_loss: 134.3406 - val_mse: 134.3406\n",
      "Epoch 17/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 157.6867 - mse: 157.6867\n",
      "Epoch 00017: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 3s 1ms/sample - loss: 157.2182 - mse: 157.2182 - val_loss: 145.1769 - val_mse: 145.1769\n",
      "Epoch 18/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 157.9702 - mse: 157.9702\n",
      "Epoch 00018: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 3s 1ms/sample - loss: 157.0063 - mse: 157.0063 - val_loss: 139.9404 - val_mse: 139.9404\n",
      "Epoch 19/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 142.9835 - mse: 142.9835\n",
      "Epoch 00019: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 3s 1ms/sample - loss: 141.9423 - mse: 141.9422 - val_loss: 139.2667 - val_mse: 139.2667\n",
      "Epoch 20/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 152.0597 - mse: 152.0597\n",
      "Epoch 00020: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 3s 1ms/sample - loss: 150.6516 - mse: 150.6516 - val_loss: 146.1315 - val_mse: 146.1315\n",
      "Epoch 21/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 152.9016 - mse: 152.9016\n",
      "Epoch 00021: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 3s 1ms/sample - loss: 151.0313 - mse: 151.0313 - val_loss: 130.0465 - val_mse: 130.0466\n",
      "Epoch 22/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 145.7136 - mse: 145.7136\n",
      "Epoch 00022: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 934us/sample - loss: 148.8202 - mse: 148.8202 - val_loss: 150.7541 - val_mse: 150.7541\n",
      "Epoch 23/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 147.8416 - mse: 147.8416\n",
      "Epoch 00023: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 898us/sample - loss: 151.2912 - mse: 151.2912 - val_loss: 133.8858 - val_mse: 133.8858\n",
      "Epoch 24/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 150.1170 - mse: 150.1171\n",
      "Epoch 00024: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 864us/sample - loss: 148.2634 - mse: 148.2634 - val_loss: 129.5557 - val_mse: 129.5557\n",
      "Epoch 25/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 152.3665 - mse: 152.3665\n",
      "Epoch 00025: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 843us/sample - loss: 151.8268 - mse: 151.8268 - val_loss: 146.3968 - val_mse: 146.3968\n",
      "Epoch 26/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 150.6451 - mse: 150.6451\n",
      "Epoch 00026: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 850us/sample - loss: 151.6954 - mse: 151.6954 - val_loss: 153.6624 - val_mse: 153.6624\n",
      "Epoch 27/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 151.8450 - mse: 151.8450\n",
      "Epoch 00027: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 872us/sample - loss: 149.0981 - mse: 149.0981 - val_loss: 139.5233 - val_mse: 139.5233\n",
      "Epoch 28/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 153.6217 - mse: 153.6217\n",
      "Epoch 00028: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 825us/sample - loss: 152.6634 - mse: 152.6634 - val_loss: 190.2684 - val_mse: 190.2684\n",
      "Epoch 29/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 150.9928 - mse: 150.9929\n",
      "Epoch 00029: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 816us/sample - loss: 148.8955 - mse: 148.8956 - val_loss: 136.4609 - val_mse: 136.4609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 155.7276 - mse: 155.7276\n",
      "Epoch 00030: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 823us/sample - loss: 154.2517 - mse: 154.2517 - val_loss: 140.6818 - val_mse: 140.6818\n",
      "Epoch 31/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 145.8787 - mse: 145.8786\n",
      "Epoch 00031: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 906us/sample - loss: 147.3764 - mse: 147.3764 - val_loss: 146.5970 - val_mse: 146.5970\n",
      "Epoch 32/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 147.0556 - mse: 147.0557\n",
      "Epoch 00032: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 887us/sample - loss: 149.7104 - mse: 149.7104 - val_loss: 155.7263 - val_mse: 155.7263\n",
      "Epoch 33/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 140.3670 - mse: 140.3670\n",
      "Epoch 00033: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 843us/sample - loss: 149.2022 - mse: 149.2022 - val_loss: 142.2698 - val_mse: 142.2698\n",
      "Epoch 34/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 145.7461 - mse: 145.7461\n",
      "Epoch 00034: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 838us/sample - loss: 146.4180 - mse: 146.4180 - val_loss: 151.2092 - val_mse: 151.2092\n",
      "Epoch 35/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 146.2364 - mse: 146.2364\n",
      "Epoch 00035: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 815us/sample - loss: 145.4567 - mse: 145.4567 - val_loss: 137.2857 - val_mse: 137.2858\n",
      "Epoch 36/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 146.2935 - mse: 146.2935\n",
      "Epoch 00036: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 833us/sample - loss: 144.5490 - mse: 144.5490 - val_loss: 135.8029 - val_mse: 135.8029\n",
      "Epoch 37/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 146.7544 - mse: 146.7544\n",
      "Epoch 00037: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 807us/sample - loss: 145.1809 - mse: 145.1809 - val_loss: 133.3424 - val_mse: 133.3424\n",
      "Epoch 38/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 145.3909 - mse: 145.3909\n",
      "Epoch 00038: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 812us/sample - loss: 145.1796 - mse: 145.1796 - val_loss: 143.5222 - val_mse: 143.5222\n",
      "Epoch 39/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 159.0619 - mse: 159.0619\n",
      "Epoch 00039: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 917us/sample - loss: 156.6705 - mse: 156.6705 - val_loss: 151.8674 - val_mse: 151.8674\n",
      "Epoch 40/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 164.4521 - mse: 164.4521\n",
      "Epoch 00040: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 836us/sample - loss: 165.2109 - mse: 165.2109 - val_loss: 152.9455 - val_mse: 152.9455\n",
      "Epoch 41/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 165.9883 - mse: 165.9883\n",
      "Epoch 00041: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 813us/sample - loss: 163.7272 - mse: 163.7272 - val_loss: 155.1735 - val_mse: 155.1735\n",
      "Epoch 42/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.8422 - mse: 162.8423\n",
      "Epoch 00042: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 817us/sample - loss: 163.4506 - mse: 163.4506 - val_loss: 151.3947 - val_mse: 151.3947\n",
      "Epoch 43/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 165.2867 - mse: 165.2867\n",
      "Epoch 00043: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 796us/sample - loss: 163.3056 - mse: 163.3056 - val_loss: 155.5060 - val_mse: 155.5060\n",
      "Epoch 44/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 165.0995 - mse: 165.0995\n",
      "Epoch 00044: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 800us/sample - loss: 164.0090 - mse: 164.0090 - val_loss: 153.8619 - val_mse: 153.8619\n",
      "Epoch 45/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 161.0854 - mse: 161.0854\n",
      "Epoch 00045: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 800us/sample - loss: 161.4630 - mse: 161.4630 - val_loss: 150.6119 - val_mse: 150.6119\n",
      "Epoch 46/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 159.0149 - mse: 159.0149\n",
      "Epoch 00046: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 806us/sample - loss: 161.6402 - mse: 161.6402 - val_loss: 151.5412 - val_mse: 151.5412\n",
      "Epoch 47/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 161.0336 - mse: 161.0337\n",
      "Epoch 00047: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 891us/sample - loss: 161.0221 - mse: 161.0221 - val_loss: 147.2764 - val_mse: 147.2764\n",
      "Epoch 48/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 157.2069 - mse: 157.2069\n",
      "Epoch 00048: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 850us/sample - loss: 161.0457 - mse: 161.0457 - val_loss: 152.1745 - val_mse: 152.1745\n",
      "Epoch 49/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 162.6431 - mse: 162.6430\n",
      "Epoch 00049: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 828us/sample - loss: 160.7451 - mse: 160.7451 - val_loss: 150.5446 - val_mse: 150.5446\n",
      "Epoch 50/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 161.4620 - mse: 161.4620- ETA: 1s - loss: 168\n",
      "Epoch 00050: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 832us/sample - loss: 159.5657 - mse: 159.5657 - val_loss: 148.5323 - val_mse: 148.5323\n",
      "Epoch 51/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 155.5532 - mse: 155.5532\n",
      "Epoch 00051: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 1ms/sample - loss: 155.4125 - mse: 155.4125 - val_loss: 141.9652 - val_mse: 141.9651\n",
      "Epoch 52/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 156.7861 - mse: 156.7861\n",
      "Epoch 00052: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 3s 1ms/sample - loss: 156.0023 - mse: 156.0023 - val_loss: 147.9737 - val_mse: 147.9737\n",
      "Epoch 53/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 156.9909 - mse: 156.9909\n",
      "Epoch 00053: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 988us/sample - loss: 155.7655 - mse: 155.7655 - val_loss: 147.9940 - val_mse: 147.9940\n",
      "Epoch 54/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 155.1361 - mse: 155.1360\n",
      "Epoch 00054: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 1ms/sample - loss: 153.6235 - mse: 153.6235 - val_loss: 143.4306 - val_mse: 143.4306\n",
      "Epoch 55/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 158.3133 - mse: 158.3133\n",
      "Epoch 00055: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 887us/sample - loss: 158.9640 - mse: 158.9639 - val_loss: 167.5441 - val_mse: 167.5441\n",
      "Epoch 56/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 155.4488 - mse: 155.4489\n",
      "Epoch 00056: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 807us/sample - loss: 157.3005 - mse: 157.3005 - val_loss: 138.0626 - val_mse: 138.0626\n",
      "Epoch 57/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 151.2003 - mse: 151.2003\n",
      "Epoch 00057: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 811us/sample - loss: 152.1369 - mse: 152.1369 - val_loss: 139.7301 - val_mse: 139.7301\n",
      "Epoch 58/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 152.1458 - mse: 152.1459\n",
      "Epoch 00058: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 796us/sample - loss: 150.9004 - mse: 150.9004 - val_loss: 140.0304 - val_mse: 140.0304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 155.0736 - mse: 155.0736\n",
      "Epoch 00059: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 789us/sample - loss: 153.8528 - mse: 153.8528 - val_loss: 137.2815 - val_mse: 137.2814\n",
      "Epoch 60/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 151.5273 - mse: 151.5274\n",
      "Epoch 00060: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 794us/sample - loss: 150.7768 - mse: 150.7768 - val_loss: 136.0360 - val_mse: 136.0360\n",
      "Epoch 61/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.0547 - mse: 162.0547\n",
      "Epoch 00061: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 807us/sample - loss: 159.8644 - mse: 159.8644 - val_loss: 146.2613 - val_mse: 146.2613\n",
      "Epoch 62/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 154.8376 - mse: 154.8376\n",
      "Epoch 00062: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 880us/sample - loss: 151.5554 - mse: 151.5553 - val_loss: 148.5378 - val_mse: 148.5378\n",
      "Epoch 63/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 149.3166 - mse: 149.3166\n",
      "Epoch 00063: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 861us/sample - loss: 148.8562 - mse: 148.8562 - val_loss: 146.9209 - val_mse: 146.9209\n",
      "Epoch 64/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 151.3440 - mse: 151.3440\n",
      "Epoch 00064: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 817us/sample - loss: 150.7827 - mse: 150.7827 - val_loss: 143.3267 - val_mse: 143.3267\n",
      "Epoch 65/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 150.7194 - mse: 150.7193\n",
      "Epoch 00065: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 813us/sample - loss: 149.6347 - mse: 149.6347 - val_loss: 152.4929 - val_mse: 152.4929\n",
      "Epoch 66/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 154.4063 - mse: 154.4062\n",
      "Epoch 00066: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 801us/sample - loss: 154.0662 - mse: 154.0662 - val_loss: 149.9535 - val_mse: 149.9535\n",
      "Epoch 67/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 154.0452 - mse: 154.0453\n",
      "Epoch 00067: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 791us/sample - loss: 154.8002 - mse: 154.8002 - val_loss: 161.7441 - val_mse: 161.7441\n",
      "Epoch 68/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 161.5577 - mse: 161.5577\n",
      "Epoch 00068: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 796us/sample - loss: 164.0728 - mse: 164.0728 - val_loss: 153.8867 - val_mse: 153.8867\n",
      "Epoch 69/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 165.1434 - mse: 165.1434\n",
      "Epoch 00069: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 790us/sample - loss: 164.2307 - mse: 164.2307 - val_loss: 153.1964 - val_mse: 153.1964\n",
      "Epoch 70/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 164.6930 - mse: 164.6931\n",
      "Epoch 00070: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 844us/sample - loss: 164.5139 - mse: 164.5139 - val_loss: 152.9049 - val_mse: 152.9049\n",
      "Epoch 71/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 164.1262 - mse: 164.1263\n",
      "Epoch 00071: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 860us/sample - loss: 162.8533 - mse: 162.8533 - val_loss: 152.0306 - val_mse: 152.0306\n",
      "Epoch 72/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.4059 - mse: 162.4059\n",
      "Epoch 00072: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 816us/sample - loss: 162.4293 - mse: 162.4293 - val_loss: 149.4151 - val_mse: 149.4151\n",
      "Epoch 73/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 155.2533 - mse: 155.2533\n",
      "Epoch 00073: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 813us/sample - loss: 161.8948 - mse: 161.8948 - val_loss: 153.0605 - val_mse: 153.0604\n",
      "Epoch 74/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.8267 - mse: 162.8266\n",
      "Epoch 00074: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 797us/sample - loss: 161.5550 - mse: 161.5549 - val_loss: 153.3289 - val_mse: 153.3289\n",
      "Epoch 75/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.9265 - mse: 162.9265\n",
      "Epoch 00075: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 811us/sample - loss: 162.3175 - mse: 162.3174 - val_loss: 153.9286 - val_mse: 153.9285\n",
      "Epoch 76/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.9213 - mse: 162.9214\n",
      "Epoch 00076: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 802us/sample - loss: 162.5397 - mse: 162.5397 - val_loss: 153.4896 - val_mse: 153.4896\n",
      "Epoch 77/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.6674 - mse: 163.6674\n",
      "Epoch 00077: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 810us/sample - loss: 162.8961 - mse: 162.8961 - val_loss: 153.2915 - val_mse: 153.2915\n",
      "Epoch 78/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 164.9338 - mse: 164.9338\n",
      "Epoch 00078: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 853us/sample - loss: 163.3457 - mse: 163.3457 - val_loss: 153.4901 - val_mse: 153.4901\n",
      "Epoch 79/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 163.3858 - mse: 163.3858\n",
      "Epoch 00079: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 861us/sample - loss: 162.9291 - mse: 162.9291 - val_loss: 153.7869 - val_mse: 153.7869\n",
      "Epoch 80/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 163.8186 - mse: 163.8186\n",
      "Epoch 00080: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 826us/sample - loss: 162.0190 - mse: 162.0190 - val_loss: 153.2878 - val_mse: 153.2878\n",
      "Epoch 81/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 161.3367 - mse: 161.3367\n",
      "Epoch 00081: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 802us/sample - loss: 162.8657 - mse: 162.8656 - val_loss: 153.2828 - val_mse: 153.2828\n",
      "Epoch 82/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 159.2956 - mse: 159.2955\n",
      "Epoch 00082: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 815us/sample - loss: 161.7462 - mse: 161.7462 - val_loss: 153.5342 - val_mse: 153.5342\n",
      "Epoch 83/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.1909 - mse: 163.1909\n",
      "Epoch 00083: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 789us/sample - loss: 161.9953 - mse: 161.9952 - val_loss: 153.4175 - val_mse: 153.4175\n",
      "Epoch 84/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.3990 - mse: 163.3991\n",
      "Epoch 00084: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 804us/sample - loss: 163.1484 - mse: 163.1484 - val_loss: 154.1402 - val_mse: 154.1402\n",
      "Epoch 85/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.4790 - mse: 162.4790\n",
      "Epoch 00085: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 782us/sample - loss: 162.2006 - mse: 162.2006 - val_loss: 153.9031 - val_mse: 153.9031\n",
      "Epoch 86/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.3582 - mse: 163.3582\n",
      "Epoch 00086: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 3s 1ms/sample - loss: 161.9970 - mse: 161.9970 - val_loss: 153.2818 - val_mse: 153.2818\n",
      "Epoch 87/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.2678 - mse: 163.2678\n",
      "Epoch 00087: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 3s 1ms/sample - loss: 163.1902 - mse: 163.1902 - val_loss: 153.8506 - val_mse: 153.8506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.7413 - mse: 163.7413\n",
      "Epoch 00088: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 870us/sample - loss: 163.8518 - mse: 163.8517 - val_loss: 153.3118 - val_mse: 153.3118\n",
      "Epoch 89/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.0510 - mse: 163.0510\n",
      "Epoch 00089: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 812us/sample - loss: 162.9817 - mse: 162.9818 - val_loss: 153.5337 - val_mse: 153.5337\n",
      "Epoch 90/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.4687 - mse: 162.4688\n",
      "Epoch 00090: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 799us/sample - loss: 161.6784 - mse: 161.6785 - val_loss: 153.4721 - val_mse: 153.4721\n",
      "Epoch 91/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 164.6604 - mse: 164.6604\n",
      "Epoch 00091: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 809us/sample - loss: 163.6362 - mse: 163.6362 - val_loss: 153.9258 - val_mse: 153.9258\n",
      "Epoch 92/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 164.2426 - mse: 164.2426\n",
      "Epoch 00092: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 799us/sample - loss: 162.7581 - mse: 162.7580 - val_loss: 153.3036 - val_mse: 153.3036\n",
      "Epoch 93/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 160.2352 - mse: 160.2353\n",
      "Epoch 00093: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 813us/sample - loss: 162.7782 - mse: 162.7782 - val_loss: 153.2959 - val_mse: 153.2959\n",
      "Epoch 94/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.5593 - mse: 163.5593\n",
      "Epoch 00094: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 885us/sample - loss: 162.9871 - mse: 162.9872 - val_loss: 153.5823 - val_mse: 153.5823\n",
      "Epoch 95/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 163.5505 - mse: 163.5506\n",
      "Epoch 00095: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 834us/sample - loss: 162.9659 - mse: 162.9659 - val_loss: 153.7552 - val_mse: 153.7552\n",
      "Epoch 96/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.5483 - mse: 162.5483\n",
      "Epoch 00096: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 812us/sample - loss: 162.4544 - mse: 162.4544 - val_loss: 153.5431 - val_mse: 153.5431\n",
      "Epoch 97/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 164.4220 - mse: 164.4220\n",
      "Epoch 00097: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 796us/sample - loss: 163.2716 - mse: 163.2716 - val_loss: 153.5554 - val_mse: 153.5554\n",
      "Epoch 98/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 157.4274 - mse: 157.4274\n",
      "Epoch 00098: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 796us/sample - loss: 162.7263 - mse: 162.7263 - val_loss: 153.6191 - val_mse: 153.6191\n",
      "Epoch 99/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 162.9681 - mse: 162.9681\n",
      "Epoch 00099: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 786us/sample - loss: 162.2528 - mse: 162.2528 - val_loss: 153.2886 - val_mse: 153.2886\n",
      "Epoch 100/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 166.0086 - mse: 166.0086\n",
      "Epoch 00100: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 788us/sample - loss: 163.6191 - mse: 163.6191 - val_loss: 153.9248 - val_mse: 153.9248\n",
      "Epoch 101/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 157.7518 - mse: 157.7518\n",
      "Epoch 00101: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 808us/sample - loss: 161.1754 - mse: 161.1754 - val_loss: 154.2161 - val_mse: 154.2161\n",
      "Epoch 102/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 159.8266 - mse: 159.8266\n",
      "Epoch 00102: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 850us/sample - loss: 161.1224 - mse: 161.1224 - val_loss: 153.4004 - val_mse: 153.4004\n",
      "Epoch 103/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 161.0574 - mse: 161.0575\n",
      "Epoch 00103: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 837us/sample - loss: 162.7376 - mse: 162.7377 - val_loss: 154.6819 - val_mse: 154.6819\n",
      "Epoch 104/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.4351 - mse: 162.4350\n",
      "Epoch 00104: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 824us/sample - loss: 163.3593 - mse: 163.3593 - val_loss: 153.6022 - val_mse: 153.6022\n",
      "Epoch 105/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.8833 - mse: 162.8832\n",
      "Epoch 00105: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 827us/sample - loss: 161.5646 - mse: 161.5646 - val_loss: 153.3116 - val_mse: 153.3116\n",
      "Epoch 106/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.7337 - mse: 162.7337\n",
      "Epoch 00106: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 809us/sample - loss: 162.1359 - mse: 162.1359 - val_loss: 153.3103 - val_mse: 153.3103\n",
      "Epoch 107/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 164.2233 - mse: 164.2234\n",
      "Epoch 00107: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 793us/sample - loss: 162.0848 - mse: 162.0849 - val_loss: 153.8220 - val_mse: 153.8220\n",
      "Epoch 108/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 164.1336 - mse: 164.1336\n",
      "Epoch 00108: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 799us/sample - loss: 163.3096 - mse: 163.3096 - val_loss: 153.5420 - val_mse: 153.5420\n",
      "Epoch 109/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.5211 - mse: 163.5211\n",
      "Epoch 00109: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 800us/sample - loss: 162.4757 - mse: 162.4757 - val_loss: 153.4115 - val_mse: 153.4115\n",
      "Epoch 110/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 161.9327 - mse: 161.9327\n",
      "Epoch 00110: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 868us/sample - loss: 161.7800 - mse: 161.7800 - val_loss: 153.5372 - val_mse: 153.5372\n",
      "Epoch 111/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 164.5164 - mse: 164.5164\n",
      "Epoch 00111: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 851us/sample - loss: 162.4538 - mse: 162.4539 - val_loss: 153.4181 - val_mse: 153.4181\n",
      "Epoch 112/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 164.1142 - mse: 164.1142\n",
      "Epoch 00112: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 800us/sample - loss: 163.3934 - mse: 163.3934 - val_loss: 153.4896 - val_mse: 153.4896\n",
      "Epoch 113/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 155.1724 - mse: 155.1723\n",
      "Epoch 00113: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 799us/sample - loss: 162.2506 - mse: 162.2506 - val_loss: 153.5039 - val_mse: 153.5039\n",
      "Epoch 114/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.5607 - mse: 162.5607\n",
      "Epoch 00114: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 805us/sample - loss: 162.6582 - mse: 162.6581 - val_loss: 153.4300 - val_mse: 153.4300\n",
      "Epoch 115/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.6531 - mse: 162.6531\n",
      "Epoch 00115: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 782us/sample - loss: 161.0759 - mse: 161.0759 - val_loss: 153.4949 - val_mse: 153.4950\n",
      "Epoch 116/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 160.8485 - mse: 160.8485\n",
      "Epoch 00116: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 790us/sample - loss: 162.9181 - mse: 162.9181 - val_loss: 153.2865 - val_mse: 153.2865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 158.7169 - mse: 158.7169\n",
      "Epoch 00117: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 783us/sample - loss: 162.0975 - mse: 162.0975 - val_loss: 153.3179 - val_mse: 153.3179\n",
      "Epoch 118/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 161.9024 - mse: 161.9024\n",
      "Epoch 00118: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 834us/sample - loss: 162.4962 - mse: 162.4961 - val_loss: 153.3260 - val_mse: 153.3260\n",
      "Epoch 119/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 164.1092 - mse: 164.1092\n",
      "Epoch 00119: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 863us/sample - loss: 161.6711 - mse: 161.6711 - val_loss: 154.9690 - val_mse: 154.9690\n",
      "Epoch 120/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.8298 - mse: 162.8298\n",
      "Epoch 00120: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 810us/sample - loss: 162.6952 - mse: 162.6951 - val_loss: 153.3133 - val_mse: 153.3133\n",
      "Epoch 121/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.2168 - mse: 163.2169\n",
      "Epoch 00121: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 810us/sample - loss: 161.0586 - mse: 161.0586 - val_loss: 153.3966 - val_mse: 153.3966\n",
      "Epoch 122/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.2770 - mse: 163.2770\n",
      "Epoch 00122: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 810us/sample - loss: 162.1251 - mse: 162.1251 - val_loss: 153.2975 - val_mse: 153.2975\n",
      "Epoch 123/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 164.3644 - mse: 164.3644\n",
      "Epoch 00123: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 787us/sample - loss: 162.8085 - mse: 162.8085 - val_loss: 153.3327 - val_mse: 153.3327\n",
      "Epoch 124/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 165.0566 - mse: 165.0566\n",
      "Epoch 00124: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 791us/sample - loss: 162.7567 - mse: 162.7567 - val_loss: 153.2822 - val_mse: 153.2822\n",
      "Epoch 125/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.4056 - mse: 163.4056\n",
      "Epoch 00125: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 792us/sample - loss: 162.0907 - mse: 162.0907 - val_loss: 153.2913 - val_mse: 153.2913\n",
      "Epoch 126/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.0566 - mse: 163.0566\n",
      "Epoch 00126: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 805us/sample - loss: 162.2129 - mse: 162.2129 - val_loss: 153.3050 - val_mse: 153.3049\n",
      "Epoch 127/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 156.6396 - mse: 156.6396\n",
      "Epoch 00127: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 893us/sample - loss: 164.3983 - mse: 164.3984 - val_loss: 153.3795 - val_mse: 153.3795\n",
      "Epoch 128/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 160.5272 - mse: 160.5273\n",
      "Epoch 00128: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 817us/sample - loss: 161.5173 - mse: 161.5173 - val_loss: 153.3197 - val_mse: 153.3197\n",
      "Epoch 129/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.8389 - mse: 163.8389\n",
      "Epoch 00129: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 808us/sample - loss: 162.3269 - mse: 162.3269 - val_loss: 153.6225 - val_mse: 153.6225\n",
      "Epoch 130/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.1801 - mse: 163.1801\n",
      "Epoch 00130: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 796us/sample - loss: 162.5188 - mse: 162.5188 - val_loss: 153.3665 - val_mse: 153.3665\n",
      "Epoch 131/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 165.5365 - mse: 165.5364\n",
      "Epoch 00131: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 798us/sample - loss: 163.3710 - mse: 163.3710 - val_loss: 153.9880 - val_mse: 153.9880\n",
      "Epoch 132/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.9780 - mse: 163.9780\n",
      "Epoch 00132: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 798us/sample - loss: 161.8182 - mse: 161.8182 - val_loss: 153.2938 - val_mse: 153.2938\n",
      "Epoch 133/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.4939 - mse: 162.4940\n",
      "Epoch 00133: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 806us/sample - loss: 161.7508 - mse: 161.7508 - val_loss: 153.3849 - val_mse: 153.3849\n",
      "Epoch 134/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 161.0851 - mse: 161.0852\n",
      "Epoch 00134: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 806us/sample - loss: 162.0098 - mse: 162.0098 - val_loss: 153.4374 - val_mse: 153.4374\n",
      "Epoch 135/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.2441 - mse: 163.2441\n",
      "Epoch 00135: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 888us/sample - loss: 162.3329 - mse: 162.3329 - val_loss: 153.7325 - val_mse: 153.7325\n",
      "Epoch 136/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.3921 - mse: 163.3921\n",
      "Epoch 00136: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 847us/sample - loss: 162.3008 - mse: 162.3008 - val_loss: 153.2814 - val_mse: 153.2814\n",
      "Epoch 137/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.2474 - mse: 162.2474\n",
      "Epoch 00137: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 1ms/sample - loss: 161.9298 - mse: 161.9297 - val_loss: 153.3450 - val_mse: 153.3450\n",
      "Epoch 138/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 164.3640 - mse: 164.3639\n",
      "Epoch 00138: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 1ms/sample - loss: 161.3827 - mse: 161.3827 - val_loss: 153.2807 - val_mse: 153.2806\n",
      "Epoch 139/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.1617 - mse: 162.1618\n",
      "Epoch 00139: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 932us/sample - loss: 162.0683 - mse: 162.0684 - val_loss: 153.5880 - val_mse: 153.5880\n",
      "Epoch 140/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.4976 - mse: 163.4975\n",
      "Epoch 00140: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 994us/sample - loss: 163.2442 - mse: 163.2442 - val_loss: 153.3783 - val_mse: 153.3783\n",
      "Epoch 141/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.6768 - mse: 162.6767\n",
      "Epoch 00141: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 999us/sample - loss: 161.7601 - mse: 161.7601 - val_loss: 154.0077 - val_mse: 154.0076\n",
      "Epoch 142/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.5235 - mse: 162.5235\n",
      "Epoch 00142: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 3s 1ms/sample - loss: 162.1348 - mse: 162.1349 - val_loss: 153.3982 - val_mse: 153.3982\n",
      "Epoch 143/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 165.3134 - mse: 165.3134\n",
      "Epoch 00143: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 3s 1ms/sample - loss: 162.6146 - mse: 162.6146 - val_loss: 153.3669 - val_mse: 153.3669\n",
      "Epoch 144/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.0039 - mse: 162.0039- ETA: 0s - loss: 158.9518 - ms\n",
      "Epoch 00144: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 1ms/sample - loss: 162.5259 - mse: 162.5259 - val_loss: 153.7353 - val_mse: 153.7353\n",
      "Epoch 145/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 161.4603 - mse: 161.4604\n",
      "Epoch 00145: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 3s 1ms/sample - loss: 161.4296 - mse: 161.4296 - val_loss: 153.3002 - val_mse: 153.3002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.2252 - mse: 162.2253\n",
      "Epoch 00146: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 1ms/sample - loss: 160.9157 - mse: 160.9158 - val_loss: 153.4458 - val_mse: 153.4458\n",
      "Epoch 147/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 165.1097 - mse: 165.1097\n",
      "Epoch 00147: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 1ms/sample - loss: 162.7683 - mse: 162.7682 - val_loss: 153.2972 - val_mse: 153.2972\n",
      "Epoch 148/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 161.5550 - mse: 161.5550\n",
      "Epoch 00148: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 886us/sample - loss: 161.4946 - mse: 161.4946 - val_loss: 153.3244 - val_mse: 153.3244\n",
      "Epoch 149/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.6814 - mse: 163.6814\n",
      "Epoch 00149: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 860us/sample - loss: 162.5288 - mse: 162.5287 - val_loss: 153.2981 - val_mse: 153.2981\n",
      "Epoch 150/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.1348 - mse: 163.1348\n",
      "Epoch 00150: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 837us/sample - loss: 162.5364 - mse: 162.5364 - val_loss: 153.4234 - val_mse: 153.4234\n",
      "Epoch 151/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.7151 - mse: 163.7151\n",
      "Epoch 00151: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 944us/sample - loss: 162.8561 - mse: 162.8561 - val_loss: 153.3412 - val_mse: 153.3412\n",
      "Epoch 152/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 160.8269 - mse: 160.8269\n",
      "Epoch 00152: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 3s 1ms/sample - loss: 162.3588 - mse: 162.3588 - val_loss: 153.2998 - val_mse: 153.2998\n",
      "Epoch 153/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 165.5284 - mse: 165.5283\n",
      "Epoch 00153: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 885us/sample - loss: 162.9221 - mse: 162.9220 - val_loss: 153.3055 - val_mse: 153.3055\n",
      "Epoch 154/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 156.8732 - mse: 156.8733\n",
      "Epoch 00154: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 836us/sample - loss: 162.1781 - mse: 162.1782 - val_loss: 153.5578 - val_mse: 153.5578\n",
      "Epoch 155/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 153.6914 - mse: 153.6914\n",
      "Epoch 00155: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 864us/sample - loss: 161.4638 - mse: 161.4639 - val_loss: 153.3705 - val_mse: 153.3705\n",
      "Epoch 156/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 164.2358 - mse: 164.2358\n",
      "Epoch 00156: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 904us/sample - loss: 162.5916 - mse: 162.5916 - val_loss: 153.6433 - val_mse: 153.6433\n",
      "Epoch 157/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 164.3177 - mse: 164.3177\n",
      "Epoch 00157: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 832us/sample - loss: 161.9478 - mse: 161.9478 - val_loss: 153.3869 - val_mse: 153.3869\n",
      "Epoch 158/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 156.3385 - mse: 156.3385\n",
      "Epoch 00158: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 834us/sample - loss: 161.5443 - mse: 161.5443 - val_loss: 153.3520 - val_mse: 153.3520\n",
      "Epoch 159/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 160.1449 - mse: 160.1449\n",
      "Epoch 00159: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 872us/sample - loss: 161.9187 - mse: 161.9187 - val_loss: 153.3749 - val_mse: 153.3749\n",
      "Epoch 160/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 161.5797 - mse: 161.5797\n",
      "Epoch 00160: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 804us/sample - loss: 160.9030 - mse: 160.9030 - val_loss: 153.4091 - val_mse: 153.4091\n",
      "Epoch 161/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.6236 - mse: 163.6236\n",
      "Epoch 00161: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 868us/sample - loss: 162.3250 - mse: 162.3251 - val_loss: 153.3328 - val_mse: 153.3327\n",
      "Epoch 162/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.0255 - mse: 162.0254\n",
      "Epoch 00162: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 817us/sample - loss: 161.5111 - mse: 161.5111 - val_loss: 153.4512 - val_mse: 153.4512\n",
      "Epoch 163/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 161.7014 - mse: 161.7015\n",
      "Epoch 00163: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 976us/sample - loss: 162.9198 - mse: 162.9198 - val_loss: 154.0029 - val_mse: 154.0029\n",
      "Epoch 164/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 155.8573 - mse: 155.8572\n",
      "Epoch 00164: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 918us/sample - loss: 162.2359 - mse: 162.2359 - val_loss: 153.3199 - val_mse: 153.3199\n",
      "Epoch 165/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.7637 - mse: 163.7637\n",
      "Epoch 00165: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 855us/sample - loss: 161.0118 - mse: 161.0118 - val_loss: 153.3467 - val_mse: 153.3466\n",
      "Epoch 166/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 157.2281 - mse: 157.2280\n",
      "Epoch 00166: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 809us/sample - loss: 162.4901 - mse: 162.4901 - val_loss: 153.3053 - val_mse: 153.3053\n",
      "Epoch 167/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 167.9495 - mse: 167.9495\n",
      "Epoch 00167: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 799us/sample - loss: 163.7514 - mse: 163.7514 - val_loss: 153.3029 - val_mse: 153.3029\n",
      "Epoch 168/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 160.6592 - mse: 160.6593\n",
      "Epoch 00168: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 784us/sample - loss: 162.4564 - mse: 162.4564 - val_loss: 153.2823 - val_mse: 153.2823\n",
      "Epoch 169/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 162.3200 - mse: 162.3200\n",
      "Epoch 00169: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 783us/sample - loss: 162.2800 - mse: 162.2800 - val_loss: 153.7149 - val_mse: 153.7149\n",
      "Epoch 170/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.7306 - mse: 162.7306\n",
      "Epoch 00170: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 779us/sample - loss: 161.6945 - mse: 161.6945 - val_loss: 153.3369 - val_mse: 153.3369\n",
      "Epoch 171/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.4183 - mse: 162.4184\n",
      "Epoch 00171: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 834us/sample - loss: 162.3852 - mse: 162.3852 - val_loss: 153.3320 - val_mse: 153.3320\n",
      "Epoch 172/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.8467 - mse: 163.8467\n",
      "Epoch 00172: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 858us/sample - loss: 161.4532 - mse: 161.4532 - val_loss: 153.3239 - val_mse: 153.3239\n",
      "Epoch 173/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 166.3419 - mse: 166.3419\n",
      "Epoch 00173: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 825us/sample - loss: 163.7618 - mse: 163.7618 - val_loss: 153.9242 - val_mse: 153.9242\n",
      "Epoch 174/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.4616 - mse: 162.4616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00174: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 813us/sample - loss: 161.8179 - mse: 161.8179 - val_loss: 153.6385 - val_mse: 153.6385\n",
      "Epoch 175/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.4825 - mse: 163.4826\n",
      "Epoch 00175: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 798us/sample - loss: 160.9094 - mse: 160.9095 - val_loss: 154.1828 - val_mse: 154.1828\n",
      "Epoch 176/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 165.9631 - mse: 165.9630\n",
      "Epoch 00176: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 794us/sample - loss: 163.7099 - mse: 163.7099 - val_loss: 153.4928 - val_mse: 153.4928\n",
      "Epoch 177/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 160.7211 - mse: 160.7211\n",
      "Epoch 00177: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 811us/sample - loss: 161.7540 - mse: 161.7540 - val_loss: 153.2806 - val_mse: 153.2806\n",
      "Epoch 178/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 161.5312 - mse: 161.5312\n",
      "Epoch 00178: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 787us/sample - loss: 161.6905 - mse: 161.6905 - val_loss: 153.3266 - val_mse: 153.3266\n",
      "Epoch 179/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 161.5878 - mse: 161.5878\n",
      "Epoch 00179: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 827us/sample - loss: 161.3889 - mse: 161.3889 - val_loss: 153.2805 - val_mse: 153.2805\n",
      "Epoch 180/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 156.3069 - mse: 156.3070\n",
      "Epoch 00180: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 896us/sample - loss: 161.9693 - mse: 161.9693 - val_loss: 153.2950 - val_mse: 153.2950\n",
      "Epoch 181/300\n",
      "2304/2400 [===========================>..] - ETA: 0s - loss: 164.9608 - mse: 164.9608\n",
      "Epoch 00181: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 811us/sample - loss: 162.1408 - mse: 162.1408 - val_loss: 153.3219 - val_mse: 153.3219\n",
      "Epoch 182/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.4587 - mse: 162.4586\n",
      "Epoch 00182: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 1ms/sample - loss: 162.6205 - mse: 162.6205 - val_loss: 153.3404 - val_mse: 153.3403\n",
      "Epoch 183/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.4395 - mse: 162.4395\n",
      "Epoch 00183: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 985us/sample - loss: 162.8368 - mse: 162.8368 - val_loss: 153.3250 - val_mse: 153.3250\n",
      "Epoch 184/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 151.0317 - mse: 151.0317- ETA: 0s - loss: 149.4047 -\n",
      "Epoch 00184: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 963us/sample - loss: 161.7119 - mse: 161.7119 - val_loss: 153.2857 - val_mse: 153.2857\n",
      "Epoch 185/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.3468 - mse: 162.3468\n",
      "Epoch 00185: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 845us/sample - loss: 161.1097 - mse: 161.1097 - val_loss: 153.3231 - val_mse: 153.3231\n",
      "Epoch 186/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.1751 - mse: 162.1751\n",
      "Epoch 00186: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 817us/sample - loss: 161.4515 - mse: 161.4515 - val_loss: 153.3222 - val_mse: 153.3222\n",
      "Epoch 187/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.2383 - mse: 162.2383\n",
      "Epoch 00187: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 915us/sample - loss: 161.1726 - mse: 161.1727 - val_loss: 153.2848 - val_mse: 153.2849\n",
      "Epoch 188/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 150.0116 - mse: 150.0116\n",
      "Epoch 00188: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 841us/sample - loss: 162.3425 - mse: 162.3425 - val_loss: 153.8068 - val_mse: 153.8068\n",
      "Epoch 189/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.8345 - mse: 162.8345\n",
      "Epoch 00189: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 828us/sample - loss: 161.7611 - mse: 161.7611 - val_loss: 153.8133 - val_mse: 153.8133\n",
      "Epoch 190/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.4979 - mse: 163.4979\n",
      "Epoch 00190: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 813us/sample - loss: 161.5109 - mse: 161.5109 - val_loss: 153.2867 - val_mse: 153.2867\n",
      "Epoch 191/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.2633 - mse: 162.2633\n",
      "Epoch 00191: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 798us/sample - loss: 161.4708 - mse: 161.4708 - val_loss: 153.4569 - val_mse: 153.4569\n",
      "Epoch 192/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.8745 - mse: 163.8745\n",
      "Epoch 00192: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 799us/sample - loss: 162.3714 - mse: 162.3714 - val_loss: 153.2854 - val_mse: 153.2854\n",
      "Epoch 193/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.4954 - mse: 162.4954\n",
      "Epoch 00193: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 902us/sample - loss: 162.0261 - mse: 162.0261 - val_loss: 153.2818 - val_mse: 153.2818\n",
      "Epoch 194/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.5586 - mse: 163.5586\n",
      "Epoch 00194: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 997us/sample - loss: 162.3741 - mse: 162.3741 - val_loss: 153.2904 - val_mse: 153.2904\n",
      "Epoch 195/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 160.9277 - mse: 160.9276\n",
      "Epoch 00195: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 3s 1ms/sample - loss: 161.6921 - mse: 161.6920 - val_loss: 153.3265 - val_mse: 153.3265\n",
      "Epoch 196/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 161.5409 - mse: 161.5409\n",
      "Epoch 00196: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 3s 1ms/sample - loss: 162.1320 - mse: 162.1320 - val_loss: 153.2894 - val_mse: 153.2894\n",
      "Epoch 197/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 161.9486 - mse: 161.9486\n",
      "Epoch 00197: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 3s 1ms/sample - loss: 162.8305 - mse: 162.8305 - val_loss: 153.3012 - val_mse: 153.3012\n",
      "Epoch 198/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.6116 - mse: 162.6115\n",
      "Epoch 00198: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 1ms/sample - loss: 161.8719 - mse: 161.8719 - val_loss: 153.4355 - val_mse: 153.4355\n",
      "Epoch 199/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.2108 - mse: 162.2109\n",
      "Epoch 00199: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 930us/sample - loss: 161.8830 - mse: 161.8830 - val_loss: 154.0254 - val_mse: 154.0254\n",
      "Epoch 200/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.0836 - mse: 162.0836\n",
      "Epoch 00200: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 1ms/sample - loss: 162.6869 - mse: 162.6869 - val_loss: 153.4256 - val_mse: 153.4256\n",
      "Epoch 201/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.1237 - mse: 163.1237- ETA: 0s - loss: 171.8006\n",
      "Epoch 00201: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 1ms/sample - loss: 162.1242 - mse: 162.1242 - val_loss: 154.0152 - val_mse: 154.0153\n",
      "Epoch 202/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 163.0274 - mse: 163.0275\n",
      "Epoch 00202: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 3s 1ms/sample - loss: 161.8458 - mse: 161.8458 - val_loss: 153.7135 - val_mse: 153.7135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 164.3129 - mse: 164.3129\n",
      "Epoch 00203: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 998us/sample - loss: 163.6030 - mse: 163.6030 - val_loss: 153.4596 - val_mse: 153.4596\n",
      "Epoch 204/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 159.5368 - mse: 159.5368\n",
      "Epoch 00204: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 938us/sample - loss: 162.4121 - mse: 162.4121 - val_loss: 153.3172 - val_mse: 153.3172\n",
      "Epoch 205/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 161.9276 - mse: 161.9276\n",
      "Epoch 00205: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 989us/sample - loss: 162.5113 - mse: 162.5113 - val_loss: 153.4217 - val_mse: 153.4217\n",
      "Epoch 206/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 162.5796 - mse: 162.5796\n",
      "Epoch 00206: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 873us/sample - loss: 161.6724 - mse: 161.6725 - val_loss: 153.5524 - val_mse: 153.5524\n",
      "Epoch 207/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 163.1417 - mse: 163.1416\n",
      "Epoch 00207: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 926us/sample - loss: 161.7734 - mse: 161.7733 - val_loss: 153.4220 - val_mse: 153.4220\n",
      "Epoch 208/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 164.1924 - mse: 164.1923\n",
      "Epoch 00208: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 958us/sample - loss: 162.6448 - mse: 162.6447 - val_loss: 153.4113 - val_mse: 153.4113\n",
      "Epoch 209/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.8517 - mse: 162.8517\n",
      "Epoch 00209: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 873us/sample - loss: 161.6634 - mse: 161.6635 - val_loss: 153.3036 - val_mse: 153.3036\n",
      "Epoch 210/300\n",
      "2336/2400 [============================>.] - ETA: 0s - loss: 157.9706 - mse: 157.9706\n",
      "Epoch 00210: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 879us/sample - loss: 162.7997 - mse: 162.7997 - val_loss: 153.2791 - val_mse: 153.2791\n",
      "Epoch 211/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 161.4807 - mse: 161.4807\n",
      "Epoch 00211: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 947us/sample - loss: 161.9516 - mse: 161.9516 - val_loss: 153.2941 - val_mse: 153.2941\n",
      "Epoch 212/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 161.8258 - mse: 161.8258\n",
      "Epoch 00212: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 812us/sample - loss: 161.7810 - mse: 161.7809 - val_loss: 153.3642 - val_mse: 153.3642\n",
      "Epoch 213/300\n",
      "2368/2400 [============================>.] - ETA: 0s - loss: 162.2300 - mse: 162.2300\n",
      "Epoch 00213: val_loss did not improve from 110.39956\n",
      "2400/2400 [==============================] - 2s 839us/sample - loss: 162.3335 - mse: 162.3334 - val_loss: 153.6024 - val_mse: 153.6024\n",
      "Epoch 00213: early stopping\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samue\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4118 samples, validate on 1030 samples\n",
      "Epoch 1/200\n",
      "3776/4118 [==========================>...] - ETA: 0s - loss: 3.5421 - accuracy: 0.6224\n",
      "Epoch 00001: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 1s 203us/sample - loss: 3.3556 - accuracy: 0.6272 - val_loss: 1.4110 - val_accuracy: 0.6670\n",
      "Epoch 2/200\n",
      "4032/4118 [============================>.] - ETA: 0s - loss: 1.4235 - accuracy: 0.6687\n",
      "Epoch 00002: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 63us/sample - loss: 1.4194 - accuracy: 0.6685 - val_loss: 1.0019 - val_accuracy: 0.7039\n",
      "Epoch 3/200\n",
      "3328/4118 [=======================>......] - ETA: 0s - loss: 1.1847 - accuracy: 0.6944\n",
      "Epoch 00003: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 54us/sample - loss: 1.1299 - accuracy: 0.6940 - val_loss: 1.7989 - val_accuracy: 0.4184\n",
      "Epoch 4/200\n",
      "4096/4118 [============================>.] - ETA: 0s - loss: 0.9744 - accuracy: 0.6975\n",
      "Epoch 00004: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 61us/sample - loss: 0.9773 - accuracy: 0.6969 - val_loss: 1.0894 - val_accuracy: 0.6233\n",
      "Epoch 5/200\n",
      "3200/4118 [======================>.......] - ETA: 0s - loss: 0.9119 - accuracy: 0.7119\n",
      "Epoch 00005: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 53us/sample - loss: 0.8639 - accuracy: 0.7200 - val_loss: 0.5903 - val_accuracy: 0.7534\n",
      "Epoch 6/200\n",
      "4096/4118 [============================>.] - ETA: 0s - loss: 1.1787 - accuracy: 0.6804\n",
      "Epoch 00006: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 65us/sample - loss: 1.1763 - accuracy: 0.6804 - val_loss: 0.7879 - val_accuracy: 0.6903\n",
      "Epoch 7/200\n",
      "3328/4118 [=======================>......] - ETA: 0s - loss: 0.9815 - accuracy: 0.7064\n",
      "Epoch 00007: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 53us/sample - loss: 0.9769 - accuracy: 0.7064 - val_loss: 0.9436 - val_accuracy: 0.6777\n",
      "Epoch 8/200\n",
      "3648/4118 [=========================>....] - ETA: 0s - loss: 0.6672 - accuracy: 0.7390\n",
      "Epoch 00008: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 67us/sample - loss: 0.6794 - accuracy: 0.7377 - val_loss: 2.5473 - val_accuracy: 0.1874\n",
      "Epoch 9/200\n",
      "3168/4118 [======================>.......] - ETA: 0s - loss: 0.8117 - accuracy: 0.7150\n",
      "Epoch 00009: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 58us/sample - loss: 0.7718 - accuracy: 0.7234 - val_loss: 1.0169 - val_accuracy: 0.5718\n",
      "Epoch 10/200\n",
      "3872/4118 [===========================>..] - ETA: 0s - loss: 0.6512 - accuracy: 0.7386\n",
      "Epoch 00010: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 81us/sample - loss: 0.6501 - accuracy: 0.7368 - val_loss: 1.6394 - val_accuracy: 0.4796\n",
      "Epoch 11/200\n",
      "4064/4118 [============================>.] - ETA: 0s - loss: 0.6326 - accuracy: 0.7431\n",
      "Epoch 00011: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 94us/sample - loss: 0.6315 - accuracy: 0.7436 - val_loss: 0.7757 - val_accuracy: 0.6388\n",
      "Epoch 12/200\n",
      "3680/4118 [=========================>....] - ETA: 0s - loss: 0.6738 - accuracy: 0.7486\n",
      "Epoch 00012: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 83us/sample - loss: 0.6873 - accuracy: 0.7443 - val_loss: 0.1869 - val_accuracy: 0.9359\n",
      "Epoch 13/200\n",
      "3616/4118 [=========================>....] - ETA: 0s - loss: 0.5779 - accuracy: 0.7633\n",
      "Epoch 00013: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 85us/sample - loss: 0.5900 - accuracy: 0.7591 - val_loss: 0.3289 - val_accuracy: 0.8563\n",
      "Epoch 14/200\n",
      "3680/4118 [=========================>....] - ETA: 0s - loss: 0.6620 - accuracy: 0.7484\n",
      "Epoch 00014: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 85us/sample - loss: 0.6694 - accuracy: 0.7472 - val_loss: 0.6006 - val_accuracy: 0.7107\n",
      "Epoch 15/200\n",
      "3680/4118 [=========================>....] - ETA: 0s - loss: 0.8016 - accuracy: 0.7261\n",
      "Epoch 00015: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 1s 129us/sample - loss: 0.7692 - accuracy: 0.7329 - val_loss: 0.6347 - val_accuracy: 0.7058\n",
      "Epoch 16/200\n",
      "3520/4118 [========================>.....] - ETA: 0s - loss: 0.5629 - accuracy: 0.7594\n",
      "Epoch 00016: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 81us/sample - loss: 0.5792 - accuracy: 0.7584 - val_loss: 0.3740 - val_accuracy: 0.8505\n",
      "Epoch 17/200\n",
      "3328/4118 [=======================>......] - ETA: 0s - loss: 0.5584 - accuracy: 0.7611\n",
      "Epoch 00017: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 72us/sample - loss: 0.5457 - accuracy: 0.7647 - val_loss: 0.4854 - val_accuracy: 0.7883\n",
      "Epoch 18/200\n",
      "4000/4118 [============================>.] - ETA: 0s - loss: 0.4949 - accuracy: 0.7908\n",
      "Epoch 00018: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 76us/sample - loss: 0.4964 - accuracy: 0.7926 - val_loss: 0.6157 - val_accuracy: 0.7301\n",
      "Epoch 19/200\n",
      "3264/4118 [======================>.......] - ETA: 0s - loss: 0.5830 - accuracy: 0.7665\n",
      "Epoch 00019: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 82us/sample - loss: 0.5630 - accuracy: 0.7739 - val_loss: 0.8675 - val_accuracy: 0.5913\n",
      "Epoch 20/200\n",
      "3904/4118 [===========================>..] - ETA: 0s - loss: 0.5734 - accuracy: 0.7702\n",
      "Epoch 00020: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 76us/sample - loss: 0.5709 - accuracy: 0.7703 - val_loss: 0.6492 - val_accuracy: 0.6398\n",
      "Epoch 21/200\n",
      "3392/4118 [=======================>......] - ETA: 0s - loss: 0.5889 - accuracy: 0.7630\n",
      "Epoch 00021: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 70us/sample - loss: 0.5648 - accuracy: 0.7705 - val_loss: 0.8288 - val_accuracy: 0.5388\n",
      "Epoch 22/200\n",
      "3328/4118 [=======================>......] - ETA: 0s - loss: 0.4695 - accuracy: 0.7993\n",
      "Epoch 00022: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 69us/sample - loss: 0.5020 - accuracy: 0.7839 - val_loss: 0.5611 - val_accuracy: 0.9398\n",
      "Epoch 23/200\n",
      "3776/4118 [==========================>...] - ETA: 0s - loss: 0.4894 - accuracy: 0.7871\n",
      "Epoch 00023: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 87us/sample - loss: 0.4954 - accuracy: 0.7865 - val_loss: 2.5263 - val_accuracy: 0.2544\n",
      "Epoch 24/200\n",
      "3296/4118 [=======================>......] - ETA: 0s - loss: 0.5811 - accuracy: 0.7649\n",
      "Epoch 00024: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 70us/sample - loss: 0.5973 - accuracy: 0.7574 - val_loss: 1.2935 - val_accuracy: 0.6796\n",
      "Epoch 25/200\n",
      "3296/4118 [=======================>......] - ETA: 0s - loss: 0.5491 - accuracy: 0.7667\n",
      "Epoch 00025: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 69us/sample - loss: 0.5334 - accuracy: 0.7712 - val_loss: 0.8058 - val_accuracy: 0.6553\n",
      "Epoch 26/200\n",
      "3328/4118 [=======================>......] - ETA: 0s - loss: 0.5040 - accuracy: 0.7951\n",
      "Epoch 00026: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 73us/sample - loss: 0.5494 - accuracy: 0.7817 - val_loss: 0.1592 - val_accuracy: 0.9631\n",
      "Epoch 27/200\n",
      "3392/4118 [=======================>......] - ETA: 0s - loss: 0.4686 - accuracy: 0.7927\n",
      "Epoch 00027: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 69us/sample - loss: 0.4681 - accuracy: 0.7946 - val_loss: 0.7099 - val_accuracy: 0.6398\n",
      "Epoch 28/200\n",
      "4096/4118 [============================>.] - ETA: 0s - loss: 0.4414 - accuracy: 0.8105\n",
      "Epoch 00028: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 75us/sample - loss: 0.4403 - accuracy: 0.8108 - val_loss: 0.3554 - val_accuracy: 0.8524\n",
      "Epoch 29/200\n",
      "3456/4118 [========================>.....] - ETA: 0s - loss: 0.4538 - accuracy: 0.8056\n",
      "Epoch 00029: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 69us/sample - loss: 0.4511 - accuracy: 0.8082 - val_loss: 0.6013 - val_accuracy: 0.7311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200\n",
      "3488/4118 [========================>.....] - ETA: 0s - loss: 0.4566 - accuracy: 0.8157\n",
      "Epoch 00030: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 66us/sample - loss: 0.4462 - accuracy: 0.8169 - val_loss: 0.3598 - val_accuracy: 0.8398\n",
      "Epoch 31/200\n",
      "3168/4118 [======================>.......] - ETA: 0s - loss: 0.4199 - accuracy: 0.8242 ETA: 0s - loss: 0.4434 - accuracy: 0.\n",
      "Epoch 00031: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 76us/sample - loss: 0.4119 - accuracy: 0.8242 - val_loss: 0.8812 - val_accuracy: 0.5602\n",
      "Epoch 32/200\n",
      "3520/4118 [========================>.....] - ETA: 0s - loss: 0.3985 - accuracy: 0.8318\n",
      "Epoch 00032: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 69us/sample - loss: 0.4058 - accuracy: 0.8283 - val_loss: 1.2407 - val_accuracy: 0.3689\n",
      "Epoch 33/200\n",
      "3392/4118 [=======================>......] - ETA: 0s - loss: 0.4411 - accuracy: 0.8110\n",
      "Epoch 00033: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 70us/sample - loss: 0.4290 - accuracy: 0.8169 - val_loss: 0.4426 - val_accuracy: 0.8398\n",
      "Epoch 34/200\n",
      "3520/4118 [========================>.....] - ETA: 0s - loss: 0.3937 - accuracy: 0.8247\n",
      "Epoch 00034: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 67us/sample - loss: 0.3937 - accuracy: 0.8244 - val_loss: 0.1177 - val_accuracy: 0.9748\n",
      "Epoch 35/200\n",
      "3552/4118 [========================>.....] - ETA: 0s - loss: 0.3816 - accuracy: 0.8345\n",
      "Epoch 00035: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 67us/sample - loss: 0.3863 - accuracy: 0.8310 - val_loss: 0.6827 - val_accuracy: 0.6883\n",
      "Epoch 36/200\n",
      "3968/4118 [===========================>..] - ETA: 0s - loss: 0.4017 - accuracy: 0.8309\n",
      "Epoch 00036: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 62us/sample - loss: 0.4068 - accuracy: 0.8298 - val_loss: 1.0854 - val_accuracy: 0.5204\n",
      "Epoch 37/200\n",
      "3552/4118 [========================>.....] - ETA: 0s - loss: 0.4151 - accuracy: 0.8224\n",
      "Epoch 00037: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 68us/sample - loss: 0.4109 - accuracy: 0.8220 - val_loss: 0.4306 - val_accuracy: 0.7922\n",
      "Epoch 38/200\n",
      "4064/4118 [============================>.] - ETA: 0s - loss: 0.4169 - accuracy: 0.8292\n",
      "Epoch 00038: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 62us/sample - loss: 0.4192 - accuracy: 0.8281 - val_loss: 0.3615 - val_accuracy: 0.8612\n",
      "Epoch 39/200\n",
      "3936/4118 [===========================>..] - ETA: 0s - loss: 0.3861 - accuracy: 0.8265\n",
      "Epoch 00039: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 64us/sample - loss: 0.3833 - accuracy: 0.8273 - val_loss: 0.5888 - val_accuracy: 0.7078\n",
      "Epoch 40/200\n",
      "4000/4118 [============================>.] - ETA: 0s - loss: 0.3524 - accuracy: 0.8443\n",
      "Epoch 00040: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 63us/sample - loss: 0.3542 - accuracy: 0.8443 - val_loss: 0.1049 - val_accuracy: 0.9728\n",
      "Epoch 41/200\n",
      "3520/4118 [========================>.....] - ETA: 0s - loss: 0.5031 - accuracy: 0.7869\n",
      "Epoch 00041: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 67us/sample - loss: 0.5349 - accuracy: 0.7788 - val_loss: 0.0757 - val_accuracy: 0.9942\n",
      "Epoch 42/200\n",
      "3904/4118 [===========================>..] - ETA: 0s - loss: 0.4097 - accuracy: 0.8217\n",
      "Epoch 00042: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 63us/sample - loss: 0.4052 - accuracy: 0.8237 - val_loss: 0.5028 - val_accuracy: 0.7786\n",
      "Epoch 43/200\n",
      "3520/4118 [========================>.....] - ETA: 0s - loss: 0.3776 - accuracy: 0.8352\n",
      "Epoch 00043: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 68us/sample - loss: 0.3752 - accuracy: 0.8361 - val_loss: 0.7382 - val_accuracy: 0.6573\n",
      "Epoch 44/200\n",
      "3616/4118 [=========================>....] - ETA: 0s - loss: 0.3458 - accuracy: 0.8515\n",
      "Epoch 00044: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 66us/sample - loss: 0.3713 - accuracy: 0.8434 - val_loss: 0.6367 - val_accuracy: 0.7155\n",
      "Epoch 45/200\n",
      "3936/4118 [===========================>..] - ETA: 0s - loss: 0.3870 - accuracy: 0.8371\n",
      "Epoch 00045: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 63us/sample - loss: 0.3864 - accuracy: 0.8375 - val_loss: 0.7129 - val_accuracy: 0.6650\n",
      "Epoch 46/200\n",
      "3488/4118 [========================>.....] - ETA: 0s - loss: 0.3632 - accuracy: 0.8423\n",
      "Epoch 00046: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 64us/sample - loss: 0.3655 - accuracy: 0.8407 - val_loss: 0.3367 - val_accuracy: 0.8854\n",
      "Epoch 47/200\n",
      "3488/4118 [========================>.....] - ETA: 0s - loss: 0.3932 - accuracy: 0.8340\n",
      "Epoch 00047: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 71us/sample - loss: 0.3879 - accuracy: 0.8388 - val_loss: 0.7158 - val_accuracy: 0.7175\n",
      "Epoch 48/200\n",
      "3712/4118 [==========================>...] - ETA: 0s - loss: 0.3422 - accuracy: 0.8559\n",
      "Epoch 00048: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 66us/sample - loss: 0.3442 - accuracy: 0.8528 - val_loss: 0.8886 - val_accuracy: 0.6223\n",
      "Epoch 49/200\n",
      "3616/4118 [=========================>....] - ETA: 0s - loss: 0.4109 - accuracy: 0.8302\n",
      "Epoch 00049: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 67us/sample - loss: 0.4102 - accuracy: 0.8293 - val_loss: 0.8188 - val_accuracy: 0.6398\n",
      "Epoch 50/200\n",
      "3680/4118 [=========================>....] - ETA: 0s - loss: 0.3823 - accuracy: 0.8250\n",
      "Epoch 00050: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 67us/sample - loss: 0.3869 - accuracy: 0.8259 - val_loss: 0.5092 - val_accuracy: 0.8466\n",
      "Epoch 51/200\n",
      "3840/4118 [==========================>...] - ETA: 0s - loss: 0.3255 - accuracy: 0.8589\n",
      "Epoch 00051: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 65us/sample - loss: 0.3234 - accuracy: 0.8582 - val_loss: 1.1072 - val_accuracy: 0.6447\n",
      "Epoch 52/200\n",
      "3904/4118 [===========================>..] - ETA: 0s - loss: 0.3971 - accuracy: 0.8450\n",
      "Epoch 00052: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 64us/sample - loss: 0.3937 - accuracy: 0.8446 - val_loss: 0.6523 - val_accuracy: 0.6748\n",
      "Epoch 53/200\n",
      "3936/4118 [===========================>..] - ETA: 0s - loss: 0.3296 - accuracy: 0.8610\n",
      "Epoch 00053: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 64us/sample - loss: 0.3250 - accuracy: 0.8626 - val_loss: 0.7725 - val_accuracy: 0.6748\n",
      "Epoch 54/200\n",
      "3520/4118 [========================>.....] - ETA: 0s - loss: 0.3392 - accuracy: 0.8477\n",
      "Epoch 00054: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 68us/sample - loss: 0.3411 - accuracy: 0.8473 - val_loss: 0.8900 - val_accuracy: 0.6689\n",
      "Epoch 55/200\n",
      "3200/4118 [======================>.......] - ETA: 0s - loss: 0.3635 - accuracy: 0.8487\n",
      "Epoch 00055: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 70us/sample - loss: 0.3712 - accuracy: 0.8475 - val_loss: 0.1990 - val_accuracy: 0.9670\n",
      "Epoch 56/200\n",
      "3360/4118 [=======================>......] - ETA: 0s - loss: 0.3909 - accuracy: 0.8351\n",
      "Epoch 00056: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 84us/sample - loss: 0.3832 - accuracy: 0.8349 - val_loss: 0.4785 - val_accuracy: 0.7942\n",
      "Epoch 57/200\n",
      "3488/4118 [========================>.....] - ETA: 0s - loss: 0.3395 - accuracy: 0.8478\n",
      "Epoch 00057: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 68us/sample - loss: 0.3394 - accuracy: 0.8473 - val_loss: 0.6473 - val_accuracy: 0.6951\n",
      "Epoch 58/200\n",
      "3520/4118 [========================>.....] - ETA: 0s - loss: 0.3517 - accuracy: 0.8477\n",
      "Epoch 00058: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 68us/sample - loss: 0.3676 - accuracy: 0.8470 - val_loss: 0.3613 - val_accuracy: 0.8738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200\n",
      "3584/4118 [=========================>....] - ETA: 0s - loss: 0.3326 - accuracy: 0.8502\n",
      "Epoch 00059: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 70us/sample - loss: 0.3261 - accuracy: 0.8555 - val_loss: 0.7524 - val_accuracy: 0.6350\n",
      "Epoch 60/200\n",
      "3968/4118 [===========================>..] - ETA: 0s - loss: 0.3430 - accuracy: 0.8511\n",
      "Epoch 00060: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 64us/sample - loss: 0.3446 - accuracy: 0.8504 - val_loss: 1.0872 - val_accuracy: 0.5058\n",
      "Epoch 61/200\n",
      "3456/4118 [========================>.....] - ETA: 0s - loss: 0.3300 - accuracy: 0.8573\n",
      "Epoch 00061: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 86us/sample - loss: 0.3393 - accuracy: 0.8536 - val_loss: 0.3852 - val_accuracy: 0.8563\n",
      "Epoch 62/200\n",
      "3840/4118 [==========================>...] - ETA: 0s - loss: 0.3329 - accuracy: 0.8565\n",
      "Epoch 00062: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 68us/sample - loss: 0.3376 - accuracy: 0.8541 - val_loss: 0.3777 - val_accuracy: 0.8184\n",
      "Epoch 63/200\n",
      "3360/4118 [=======================>......] - ETA: 0s - loss: 0.3015 - accuracy: 0.8786\n",
      "Epoch 00063: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 73us/sample - loss: 0.3074 - accuracy: 0.8752 - val_loss: 0.3898 - val_accuracy: 0.8718\n",
      "Epoch 64/200\n",
      "3776/4118 [==========================>...] - ETA: 0s - loss: 0.3030 - accuracy: 0.8678\n",
      "Epoch 00064: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 82us/sample - loss: 0.3089 - accuracy: 0.8657 - val_loss: 0.7315 - val_accuracy: 0.6350\n",
      "Epoch 65/200\n",
      "3680/4118 [=========================>....] - ETA: 0s - loss: 0.3300 - accuracy: 0.8524\n",
      "Epoch 00065: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 83us/sample - loss: 0.3254 - accuracy: 0.8533 - val_loss: 0.6399 - val_accuracy: 0.6612\n",
      "Epoch 66/200\n",
      "3584/4118 [=========================>....] - ETA: 0s - loss: 0.3630 - accuracy: 0.8435\n",
      "Epoch 00066: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 85us/sample - loss: 0.3556 - accuracy: 0.8477 - val_loss: 0.1609 - val_accuracy: 0.9417\n",
      "Epoch 67/200\n",
      "3520/4118 [========================>.....] - ETA: 0s - loss: 0.2793 - accuracy: 0.8739\n",
      "Epoch 00067: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 88us/sample - loss: 0.2765 - accuracy: 0.8786 - val_loss: 0.2037 - val_accuracy: 0.9379\n",
      "Epoch 68/200\n",
      "3680/4118 [=========================>....] - ETA: 0s - loss: 0.3450 - accuracy: 0.8655\n",
      "Epoch 00068: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 102us/sample - loss: 0.3398 - accuracy: 0.8657 - val_loss: 0.3192 - val_accuracy: 0.8990\n",
      "Epoch 69/200\n",
      "3840/4118 [==========================>...] - ETA: 0s - loss: 0.3040 - accuracy: 0.8771\n",
      "Epoch 00069: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 91us/sample - loss: 0.3018 - accuracy: 0.8762 - val_loss: 0.2024 - val_accuracy: 0.9350\n",
      "Epoch 70/200\n",
      "3552/4118 [========================>.....] - ETA: 0s - loss: 0.2789 - accuracy: 0.8826\n",
      "Epoch 00070: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 67us/sample - loss: 0.2851 - accuracy: 0.8815 - val_loss: 0.2878 - val_accuracy: 0.8922\n",
      "Epoch 71/200\n",
      "3168/4118 [======================>.......] - ETA: 0s - loss: 0.3062 - accuracy: 0.8703\n",
      "Epoch 00071: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 72us/sample - loss: 0.3011 - accuracy: 0.8725 - val_loss: 0.0963 - val_accuracy: 0.9883\n",
      "Epoch 72/200\n",
      "3520/4118 [========================>.....] - ETA: 0s - loss: 0.3528 - accuracy: 0.8554\n",
      "Epoch 00072: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 69us/sample - loss: 0.3393 - accuracy: 0.8599 - val_loss: 0.3388 - val_accuracy: 0.8903\n",
      "Epoch 73/200\n",
      "3456/4118 [========================>.....] - ETA: 0s - loss: 0.3004 - accuracy: 0.8715\n",
      "Epoch 00073: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 69us/sample - loss: 0.2968 - accuracy: 0.8703 - val_loss: 0.4265 - val_accuracy: 0.8544\n",
      "Epoch 74/200\n",
      "3904/4118 [===========================>..] - ETA: 0s - loss: 0.2613 - accuracy: 0.8942\n",
      "Epoch 00074: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 82us/sample - loss: 0.2639 - accuracy: 0.8927 - val_loss: 0.5358 - val_accuracy: 0.8777\n",
      "Epoch 75/200\n",
      "3584/4118 [=========================>....] - ETA: 0s - loss: 0.2956 - accuracy: 0.8814\n",
      "Epoch 00075: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 71us/sample - loss: 0.3034 - accuracy: 0.8771 - val_loss: 0.6451 - val_accuracy: 0.7243\n",
      "Epoch 76/200\n",
      "3392/4118 [=======================>......] - ETA: 0s - loss: 0.2919 - accuracy: 0.8774\n",
      "Epoch 00076: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 70us/sample - loss: 0.2842 - accuracy: 0.8800 - val_loss: 0.2642 - val_accuracy: 0.9078\n",
      "Epoch 77/200\n",
      "4064/4118 [============================>.] - ETA: 0s - loss: 0.2476 - accuracy: 0.8949\n",
      "Epoch 00077: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 75us/sample - loss: 0.2475 - accuracy: 0.8946 - val_loss: 0.3001 - val_accuracy: 0.9330\n",
      "Epoch 78/200\n",
      "3328/4118 [=======================>......] - ETA: 0s - loss: 0.2524 - accuracy: 0.8882\n",
      "Epoch 00078: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 70us/sample - loss: 0.2556 - accuracy: 0.8881 - val_loss: 0.2177 - val_accuracy: 0.9214\n",
      "Epoch 79/200\n",
      "3904/4118 [===========================>..] - ETA: 0s - loss: 0.2889 - accuracy: 0.8742\n",
      "Epoch 00079: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 78us/sample - loss: 0.2900 - accuracy: 0.8757 - val_loss: 0.3642 - val_accuracy: 0.8485\n",
      "Epoch 80/200\n",
      "3456/4118 [========================>.....] - ETA: 0s - loss: 0.3053 - accuracy: 0.8724\n",
      "Epoch 00080: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 67us/sample - loss: 0.2932 - accuracy: 0.8759 - val_loss: 0.5350 - val_accuracy: 0.7748\n",
      "Epoch 81/200\n",
      "3232/4118 [======================>.......] - ETA: 0s - loss: 0.2276 - accuracy: 0.9038\n",
      "Epoch 00081: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 73us/sample - loss: 0.2316 - accuracy: 0.9029 - val_loss: 0.3949 - val_accuracy: 0.8495\n",
      "Epoch 82/200\n",
      "3360/4118 [=======================>......] - ETA: 0s - loss: 0.2060 - accuracy: 0.9116\n",
      "Epoch 00082: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 72us/sample - loss: 0.2090 - accuracy: 0.9109 - val_loss: 0.1892 - val_accuracy: 0.9437\n",
      "Epoch 83/200\n",
      "3424/4118 [=======================>......] - ETA: 0s - loss: 0.2887 - accuracy: 0.8838\n",
      "Epoch 00083: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 69us/sample - loss: 0.2892 - accuracy: 0.8844 - val_loss: 0.5144 - val_accuracy: 0.7631\n",
      "Epoch 84/200\n",
      "3392/4118 [=======================>......] - ETA: 0s - loss: 0.3228 - accuracy: 0.8688\n",
      "Epoch 00084: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 68us/sample - loss: 0.3112 - accuracy: 0.8715 - val_loss: 0.3673 - val_accuracy: 0.8563\n",
      "Epoch 85/200\n",
      "3200/4118 [======================>.......] - ETA: 0s - loss: 0.2512 - accuracy: 0.8984\n",
      "Epoch 00085: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 73us/sample - loss: 0.2460 - accuracy: 0.8995 - val_loss: 0.3876 - val_accuracy: 0.9049\n",
      "Epoch 86/200\n",
      "3776/4118 [==========================>...] - ETA: 0s - loss: 0.2188 - accuracy: 0.9115\n",
      "Epoch 00086: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 81us/sample - loss: 0.2192 - accuracy: 0.9111 - val_loss: 0.3462 - val_accuracy: 0.8388\n",
      "Epoch 87/200\n",
      "3552/4118 [========================>.....] - ETA: 0s - loss: 0.2022 - accuracy: 0.9192\n",
      "Epoch 00087: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 68us/sample - loss: 0.2002 - accuracy: 0.9199 - val_loss: 0.3069 - val_accuracy: 0.8524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/200\n",
      "3520/4118 [========================>.....] - ETA: 0s - loss: 0.2600 - accuracy: 0.8920\n",
      "Epoch 00088: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 65us/sample - loss: 0.2583 - accuracy: 0.8932 - val_loss: 0.2555 - val_accuracy: 0.9243\n",
      "Epoch 89/200\n",
      "3552/4118 [========================>.....] - ETA: 0s - loss: 0.2313 - accuracy: 0.9037\n",
      "Epoch 00089: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 69us/sample - loss: 0.2440 - accuracy: 0.9007 - val_loss: 0.5955 - val_accuracy: 0.7049\n",
      "Epoch 90/200\n",
      "3616/4118 [=========================>....] - ETA: 0s - loss: 0.2348 - accuracy: 0.8982\n",
      "Epoch 00090: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 69us/sample - loss: 0.2321 - accuracy: 0.8980 - val_loss: 0.1637 - val_accuracy: 0.9408\n",
      "Epoch 91/200\n",
      "3552/4118 [========================>.....] - ETA: 0s - loss: 0.2371 - accuracy: 0.9029\n",
      "Epoch 00091: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 68us/sample - loss: 0.2395 - accuracy: 0.9000 - val_loss: 0.4953 - val_accuracy: 0.8252\n",
      "Epoch 92/200\n",
      "3392/4118 [=======================>......] - ETA: 0s - loss: 0.2169 - accuracy: 0.9142\n",
      "Epoch 00092: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 69us/sample - loss: 0.2125 - accuracy: 0.9153 - val_loss: 0.2175 - val_accuracy: 0.9214\n",
      "Epoch 93/200\n",
      "3648/4118 [=========================>....] - ETA: 0s - loss: 0.2583 - accuracy: 0.8958\n",
      "Epoch 00093: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 67us/sample - loss: 0.2675 - accuracy: 0.8929 - val_loss: 0.7730 - val_accuracy: 0.8019\n",
      "Epoch 94/200\n",
      "3584/4118 [=========================>....] - ETA: 0s - loss: 0.2665 - accuracy: 0.8915\n",
      "Epoch 00094: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 67us/sample - loss: 0.2616 - accuracy: 0.8932 - val_loss: 0.3507 - val_accuracy: 0.8592\n",
      "Epoch 95/200\n",
      "3616/4118 [=========================>....] - ETA: 0s - loss: 0.1860 - accuracy: 0.9267\n",
      "Epoch 00095: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 66us/sample - loss: 0.1867 - accuracy: 0.9269 - val_loss: 0.1854 - val_accuracy: 0.9340\n",
      "Epoch 96/200\n",
      "3840/4118 [==========================>...] - ETA: 0s - loss: 0.2394 - accuracy: 0.8992\n",
      "Epoch 00096: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 64us/sample - loss: 0.2433 - accuracy: 0.8970 - val_loss: 0.4650 - val_accuracy: 0.7650\n",
      "Epoch 97/200\n",
      "3520/4118 [========================>.....] - ETA: 0s - loss: 0.2721 - accuracy: 0.8878\n",
      "Epoch 00097: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 68us/sample - loss: 0.2723 - accuracy: 0.8881 - val_loss: 0.3970 - val_accuracy: 0.8252\n",
      "Epoch 98/200\n",
      "3552/4118 [========================>.....] - ETA: 0s - loss: 0.2152 - accuracy: 0.9057\n",
      "Epoch 00098: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 67us/sample - loss: 0.2286 - accuracy: 0.9038 - val_loss: 0.6154 - val_accuracy: 0.8233\n",
      "Epoch 99/200\n",
      "3584/4118 [=========================>....] - ETA: 0s - loss: 0.1984 - accuracy: 0.9180\n",
      "Epoch 00099: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 69us/sample - loss: 0.1963 - accuracy: 0.9194 - val_loss: 0.1930 - val_accuracy: 0.9233\n",
      "Epoch 100/200\n",
      "3552/4118 [========================>.....] - ETA: 0s - loss: 0.1783 - accuracy: 0.9245\n",
      "Epoch 00100: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 68us/sample - loss: 0.1784 - accuracy: 0.9254 - val_loss: 0.6771 - val_accuracy: 0.7456\n",
      "Epoch 101/200\n",
      "3424/4118 [=======================>......] - ETA: 0s - loss: 0.1753 - accuracy: 0.9282\n",
      "Epoch 00101: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 69us/sample - loss: 0.1847 - accuracy: 0.9233 - val_loss: 0.1150 - val_accuracy: 0.9718\n",
      "Epoch 102/200\n",
      "3488/4118 [========================>.....] - ETA: 0s - loss: 0.1881 - accuracy: 0.9260\n",
      "Epoch 00102: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 71us/sample - loss: 0.1844 - accuracy: 0.9281 - val_loss: 0.2877 - val_accuracy: 0.8767\n",
      "Epoch 103/200\n",
      "3872/4118 [===========================>..] - ETA: 0s - loss: 0.1710 - accuracy: 0.9313\n",
      "Epoch 00103: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 65us/sample - loss: 0.1696 - accuracy: 0.9318 - val_loss: 0.2484 - val_accuracy: 0.8932\n",
      "Epoch 104/200\n",
      "3584/4118 [=========================>....] - ETA: 0s - loss: 0.1791 - accuracy: 0.9272\n",
      "Epoch 00104: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 68us/sample - loss: 0.1715 - accuracy: 0.9313 - val_loss: 0.1336 - val_accuracy: 0.9505\n",
      "Epoch 105/200\n",
      "3616/4118 [=========================>....] - ETA: 0s - loss: 0.1652 - accuracy: 0.9311\n",
      "Epoch 00105: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 67us/sample - loss: 0.1728 - accuracy: 0.9259 - val_loss: 0.2228 - val_accuracy: 0.9117\n",
      "Epoch 106/200\n",
      "3456/4118 [========================>.....] - ETA: 0s - loss: 0.2548 - accuracy: 0.8947\n",
      "Epoch 00106: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 70us/sample - loss: 0.2652 - accuracy: 0.8919 - val_loss: 0.1657 - val_accuracy: 0.9291\n",
      "Epoch 107/200\n",
      "3584/4118 [=========================>....] - ETA: 0s - loss: 0.1841 - accuracy: 0.9247\n",
      "Epoch 00107: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 67us/sample - loss: 0.1793 - accuracy: 0.9267 - val_loss: 0.1417 - val_accuracy: 0.9379\n",
      "Epoch 108/200\n",
      "3520/4118 [========================>.....] - ETA: 0s - loss: 0.2062 - accuracy: 0.9145\n",
      "Epoch 00108: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 68us/sample - loss: 0.2019 - accuracy: 0.9153 - val_loss: 0.0835 - val_accuracy: 0.9806\n",
      "Epoch 109/200\n",
      "3936/4118 [===========================>..] - ETA: 0s - loss: 0.1791 - accuracy: 0.9276\n",
      "Epoch 00109: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 76us/sample - loss: 0.1798 - accuracy: 0.9276 - val_loss: 0.2004 - val_accuracy: 0.9311\n",
      "Epoch 110/200\n",
      "3360/4118 [=======================>......] - ETA: 0s - loss: 0.2165 - accuracy: 0.9170\n",
      "Epoch 00110: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 70us/sample - loss: 0.2155 - accuracy: 0.9182 - val_loss: 0.2395 - val_accuracy: 0.9388\n",
      "Epoch 111/200\n",
      "3424/4118 [=======================>......] - ETA: 0s - loss: 0.1900 - accuracy: 0.9252\n",
      "Epoch 00111: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 69us/sample - loss: 0.2024 - accuracy: 0.9206 - val_loss: 0.2646 - val_accuracy: 0.9262\n",
      "Epoch 112/200\n",
      "3264/4118 [======================>.......] - ETA: 0s - loss: 0.1792 - accuracy: 0.9243\n",
      "Epoch 00112: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 68us/sample - loss: 0.1921 - accuracy: 0.9203 - val_loss: 0.3146 - val_accuracy: 0.8650\n",
      "Epoch 113/200\n",
      "3456/4118 [========================>.....] - ETA: 0s - loss: 0.1873 - accuracy: 0.9285\n",
      "Epoch 00113: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 73us/sample - loss: 0.1837 - accuracy: 0.9291 - val_loss: 0.4013 - val_accuracy: 0.8204\n",
      "Epoch 114/200\n",
      "3456/4118 [========================>.....] - ETA: 0s - loss: 0.1356 - accuracy: 0.9476\n",
      "Epoch 00114: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 68us/sample - loss: 0.1436 - accuracy: 0.9451 - val_loss: 0.4338 - val_accuracy: 0.7942\n",
      "Epoch 115/200\n",
      "3424/4118 [=======================>......] - ETA: 0s - loss: 0.2128 - accuracy: 0.9124\n",
      "Epoch 00115: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 69us/sample - loss: 0.2107 - accuracy: 0.9123 - val_loss: 0.3238 - val_accuracy: 0.8990\n",
      "Epoch 116/200\n",
      "3392/4118 [=======================>......] - ETA: 0s - loss: 0.1693 - accuracy: 0.9363\n",
      "Epoch 00116: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 69us/sample - loss: 0.1628 - accuracy: 0.9390 - val_loss: 0.1157 - val_accuracy: 0.9699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/200\n",
      "3296/4118 [=======================>......] - ETA: 0s - loss: 0.1279 - accuracy: 0.9493\n",
      "Epoch 00117: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 69us/sample - loss: 0.1352 - accuracy: 0.9461 - val_loss: 0.2599 - val_accuracy: 0.9078\n",
      "Epoch 118/200\n",
      "3584/4118 [=========================>....] - ETA: 0s - loss: 0.1493 - accuracy: 0.9361\n",
      "Epoch 00118: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 85us/sample - loss: 0.1476 - accuracy: 0.9371 - val_loss: 0.1838 - val_accuracy: 0.9272\n",
      "Epoch 119/200\n",
      "3552/4118 [========================>.....] - ETA: 0s - loss: 0.1232 - accuracy: 0.9524\n",
      "Epoch 00119: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 85us/sample - loss: 0.1282 - accuracy: 0.9495 - val_loss: 0.5061 - val_accuracy: 0.7573\n",
      "Epoch 120/200\n",
      "3520/4118 [========================>.....] - ETA: 0s - loss: 0.1774 - accuracy: 0.9310\n",
      "Epoch 00120: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 89us/sample - loss: 0.1783 - accuracy: 0.9308 - val_loss: 0.2576 - val_accuracy: 0.8942\n",
      "Epoch 121/200\n",
      "3520/4118 [========================>.....] - ETA: 0s - loss: 0.1698 - accuracy: 0.9347\n",
      "Epoch 00121: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 85us/sample - loss: 0.1665 - accuracy: 0.9352 - val_loss: 0.1178 - val_accuracy: 0.9621\n",
      "Epoch 122/200\n",
      "3904/4118 [===========================>..] - ETA: 0s - loss: 0.2009 - accuracy: 0.9232\n",
      "Epoch 00122: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 107us/sample - loss: 0.1968 - accuracy: 0.9240 - val_loss: 0.4285 - val_accuracy: 0.8359\n",
      "Epoch 123/200\n",
      "3936/4118 [===========================>..] - ETA: 0s - loss: 0.1492 - accuracy: 0.9405\n",
      "Epoch 00123: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 94us/sample - loss: 0.1517 - accuracy: 0.9388 - val_loss: 0.1382 - val_accuracy: 0.9515\n",
      "Epoch 124/200\n",
      "4000/4118 [============================>.] - ETA: 0s - loss: 0.1974 - accuracy: 0.9197\n",
      "Epoch 00124: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 76us/sample - loss: 0.1958 - accuracy: 0.9201 - val_loss: 0.1920 - val_accuracy: 0.9350\n",
      "Epoch 125/200\n",
      "3552/4118 [========================>.....] - ETA: 0s - loss: 0.1251 - accuracy: 0.9482\n",
      "Epoch 00125: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 69us/sample - loss: 0.1430 - accuracy: 0.9398 - val_loss: 0.4714 - val_accuracy: 0.7825\n",
      "Epoch 126/200\n",
      "3456/4118 [========================>.....] - ETA: 0s - loss: 0.1858 - accuracy: 0.9256\n",
      "Epoch 00126: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 73us/sample - loss: 0.1799 - accuracy: 0.9286 - val_loss: 0.1668 - val_accuracy: 0.9223\n",
      "Epoch 127/200\n",
      "4032/4118 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9430\n",
      "Epoch 00127: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 83us/sample - loss: 0.1279 - accuracy: 0.9429 - val_loss: 0.1056 - val_accuracy: 0.9718\n",
      "Epoch 128/200\n",
      "4064/4118 [============================>.] - ETA: 0s - loss: 0.1247 - accuracy: 0.9486\n",
      "Epoch 00128: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 79us/sample - loss: 0.1249 - accuracy: 0.9483 - val_loss: 0.4219 - val_accuracy: 0.8000\n",
      "Epoch 129/200\n",
      "3712/4118 [==========================>...] - ETA: 0s - loss: 0.2223 - accuracy: 0.9246\n",
      "Epoch 00129: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 67us/sample - loss: 0.2346 - accuracy: 0.9208 - val_loss: 0.5491 - val_accuracy: 0.7854\n",
      "Epoch 130/200\n",
      "3808/4118 [==========================>...] - ETA: 0s - loss: 0.2278 - accuracy: 0.9152\n",
      "Epoch 00130: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 66us/sample - loss: 0.2210 - accuracy: 0.9165 - val_loss: 0.1685 - val_accuracy: 0.9515\n",
      "Epoch 131/200\n",
      "3392/4118 [=======================>......] - ETA: 0s - loss: 0.1144 - accuracy: 0.9564\n",
      "Epoch 00131: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 70us/sample - loss: 0.1148 - accuracy: 0.9560 - val_loss: 0.2519 - val_accuracy: 0.9019\n",
      "Epoch 132/200\n",
      "4000/4118 [============================>.] - ETA: 0s - loss: 0.1012 - accuracy: 0.9607\n",
      "Epoch 00132: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 77us/sample - loss: 0.1026 - accuracy: 0.9604 - val_loss: 0.2396 - val_accuracy: 0.9146\n",
      "Epoch 133/200\n",
      "3712/4118 [==========================>...] - ETA: 0s - loss: 0.1199 - accuracy: 0.9539\n",
      "Epoch 00133: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 65us/sample - loss: 0.1265 - accuracy: 0.9517 - val_loss: 0.2749 - val_accuracy: 0.9107\n",
      "Epoch 134/200\n",
      "3808/4118 [==========================>...] - ETA: 0s - loss: 0.1721 - accuracy: 0.9296\n",
      "Epoch 00134: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 65us/sample - loss: 0.1685 - accuracy: 0.9310 - val_loss: 0.2369 - val_accuracy: 0.9039\n",
      "Epoch 135/200\n",
      "3808/4118 [==========================>...] - ETA: 0s - loss: 0.1121 - accuracy: 0.9590\n",
      "Epoch 00135: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 65us/sample - loss: 0.1125 - accuracy: 0.9592 - val_loss: 0.0950 - val_accuracy: 0.9757\n",
      "Epoch 136/200\n",
      "3744/4118 [==========================>...] - ETA: 0s - loss: 0.1307 - accuracy: 0.9447\n",
      "Epoch 00136: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 66us/sample - loss: 0.1554 - accuracy: 0.9381 - val_loss: 0.3808 - val_accuracy: 0.8437\n",
      "Epoch 137/200\n",
      "3488/4118 [========================>.....] - ETA: 0s - loss: 0.3016 - accuracy: 0.8962\n",
      "Epoch 00137: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 66us/sample - loss: 0.2765 - accuracy: 0.9031 - val_loss: 0.1705 - val_accuracy: 0.9417\n",
      "Epoch 138/200\n",
      "3424/4118 [=======================>......] - ETA: 0s - loss: 0.1135 - accuracy: 0.9553\n",
      "Epoch 00138: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 76us/sample - loss: 0.1150 - accuracy: 0.9541 - val_loss: 0.0964 - val_accuracy: 0.9738\n",
      "Epoch 139/200\n",
      "3392/4118 [=======================>......] - ETA: 0s - loss: 0.1671 - accuracy: 0.9407\n",
      "Epoch 00139: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 69us/sample - loss: 0.1558 - accuracy: 0.9437 - val_loss: 0.2066 - val_accuracy: 0.9359\n",
      "Epoch 140/200\n",
      "3392/4118 [=======================>......] - ETA: 0s - loss: 0.0991 - accuracy: 0.9620\n",
      "Epoch 00140: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 69us/sample - loss: 0.1002 - accuracy: 0.9614 - val_loss: 0.1318 - val_accuracy: 0.9592\n",
      "Epoch 141/200\n",
      "4064/4118 [============================>.] - ETA: 0s - loss: 0.0923 - accuracy: 0.9665\n",
      "Epoch 00141: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 79us/sample - loss: 0.0918 - accuracy: 0.9670 - val_loss: 0.0868 - val_accuracy: 0.9835\n",
      "Epoch 142/200\n",
      "3488/4118 [========================>.....] - ETA: 0s - loss: 0.1387 - accuracy: 0.9455\n",
      "Epoch 00142: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 68us/sample - loss: 0.1481 - accuracy: 0.9415 - val_loss: 0.1645 - val_accuracy: 0.9408\n",
      "Epoch 143/200\n",
      "3584/4118 [=========================>....] - ETA: 0s - loss: 0.1578 - accuracy: 0.9397\n",
      "Epoch 00143: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 68us/sample - loss: 0.1487 - accuracy: 0.9432 - val_loss: 0.1685 - val_accuracy: 0.9417\n",
      "Epoch 144/200\n",
      "3808/4118 [==========================>...] - ETA: 0s - loss: 0.0834 - accuracy: 0.9677\n",
      "Epoch 00144: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 65us/sample - loss: 0.0848 - accuracy: 0.9670 - val_loss: 0.3557 - val_accuracy: 0.8495\n",
      "Epoch 145/200\n",
      "3808/4118 [==========================>...] - ETA: 0s - loss: 0.1806 - accuracy: 0.9301\n",
      "Epoch 00145: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 65us/sample - loss: 0.1731 - accuracy: 0.9330 - val_loss: 0.2528 - val_accuracy: 0.8893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146/200\n",
      "3936/4118 [===========================>..] - ETA: 0s - loss: 0.0954 - accuracy: 0.9609\n",
      "Epoch 00146: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 64us/sample - loss: 0.0957 - accuracy: 0.9607 - val_loss: 0.2369 - val_accuracy: 0.8932\n",
      "Epoch 147/200\n",
      "3872/4118 [===========================>..] - ETA: 0s - loss: 0.1369 - accuracy: 0.9473\n",
      "Epoch 00147: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 64us/sample - loss: 0.1343 - accuracy: 0.9492 - val_loss: 0.1784 - val_accuracy: 0.9282\n",
      "Epoch 148/200\n",
      "3584/4118 [=========================>....] - ETA: 0s - loss: 0.1559 - accuracy: 0.9434\n",
      "Epoch 00148: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 67us/sample - loss: 0.1495 - accuracy: 0.9444 - val_loss: 0.1611 - val_accuracy: 0.9359\n",
      "Epoch 149/200\n",
      "3584/4118 [=========================>....] - ETA: 0s - loss: 0.1219 - accuracy: 0.9562\n",
      "Epoch 00149: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 68us/sample - loss: 0.1215 - accuracy: 0.9543 - val_loss: 0.0525 - val_accuracy: 0.9835\n",
      "Epoch 150/200\n",
      "3648/4118 [=========================>....] - ETA: 0s - loss: 0.1430 - accuracy: 0.9457\n",
      "Epoch 00150: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 65us/sample - loss: 0.1368 - accuracy: 0.9478 - val_loss: 0.2590 - val_accuracy: 0.9107\n",
      "Epoch 151/200\n",
      "3584/4118 [=========================>....] - ETA: 0s - loss: 0.1437 - accuracy: 0.9453\n",
      "Epoch 00151: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 69us/sample - loss: 0.1438 - accuracy: 0.9458 - val_loss: 0.1326 - val_accuracy: 0.9583\n",
      "Epoch 152/200\n",
      "3680/4118 [=========================>....] - ETA: 0s - loss: 0.0926 - accuracy: 0.9639\n",
      "Epoch 00152: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 71us/sample - loss: 0.0926 - accuracy: 0.9631 - val_loss: 0.2237 - val_accuracy: 0.9058\n",
      "Epoch 153/200\n",
      "3840/4118 [==========================>...] - ETA: 0s - loss: 0.0685 - accuracy: 0.9771\n",
      "Epoch 00153: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 64us/sample - loss: 0.0704 - accuracy: 0.9767 - val_loss: 0.1433 - val_accuracy: 0.9447\n",
      "Epoch 154/200\n",
      "3584/4118 [=========================>....] - ETA: 0s - loss: 0.0788 - accuracy: 0.9696\n",
      "Epoch 00154: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 67us/sample - loss: 0.0824 - accuracy: 0.9689 - val_loss: 0.2121 - val_accuracy: 0.9262\n",
      "Epoch 155/200\n",
      "3616/4118 [=========================>....] - ETA: 0s - loss: 0.1205 - accuracy: 0.9563\n",
      "Epoch 00155: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 68us/sample - loss: 0.1216 - accuracy: 0.9558 - val_loss: 0.2728 - val_accuracy: 0.8845\n",
      "Epoch 156/200\n",
      "3904/4118 [===========================>..] - ETA: 0s - loss: 0.1246 - accuracy: 0.9524\n",
      "Epoch 00156: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 64us/sample - loss: 0.1288 - accuracy: 0.9519 - val_loss: 0.3575 - val_accuracy: 0.8427\n",
      "Epoch 157/200\n",
      "3584/4118 [=========================>....] - ETA: 0s - loss: 0.1899 - accuracy: 0.9286\n",
      "Epoch 00157: val_loss did not improve from 0.04632\n",
      "4118/4118 [==============================] - 0s 68us/sample - loss: 0.1895 - accuracy: 0.9293 - val_loss: 0.1460 - val_accuracy: 0.9534\n",
      "Epoch 158/200\n",
      "3712/4118 [==========================>...] - ETA: 0s - loss: 0.1182 - accuracy: 0.9526\n",
      "Epoch 00158: val_loss improved from 0.04632 to 0.03695, saving model to best_model_2.h5\n",
      "4118/4118 [==============================] - 0s 71us/sample - loss: 0.1155 - accuracy: 0.9536 - val_loss: 0.0370 - val_accuracy: 0.9942\n",
      "Epoch 159/200\n",
      "3680/4118 [=========================>....] - ETA: 0s - loss: 0.0788 - accuracy: 0.9720\n",
      "Epoch 00159: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 68us/sample - loss: 0.0762 - accuracy: 0.9733 - val_loss: 0.1653 - val_accuracy: 0.9350\n",
      "Epoch 160/200\n",
      "3680/4118 [=========================>....] - ETA: 0s - loss: 0.0563 - accuracy: 0.9815\n",
      "Epoch 00160: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 67us/sample - loss: 0.0573 - accuracy: 0.9808 - val_loss: 0.0885 - val_accuracy: 0.9738\n",
      "Epoch 161/200\n",
      "3776/4118 [==========================>...] - ETA: 0s - loss: 0.0930 - accuracy: 0.9627\n",
      "Epoch 00161: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 66us/sample - loss: 0.0980 - accuracy: 0.9614 - val_loss: 0.4526 - val_accuracy: 0.8029\n",
      "Epoch 162/200\n",
      "3744/4118 [==========================>...] - ETA: 0s - loss: 0.1965 - accuracy: 0.9241\n",
      "Epoch 00162: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 66us/sample - loss: 0.1989 - accuracy: 0.9235 - val_loss: 0.1931 - val_accuracy: 0.9408\n",
      "Epoch 163/200\n",
      "3744/4118 [==========================>...] - ETA: 0s - loss: 0.1915 - accuracy: 0.9356\n",
      "Epoch 00163: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 69us/sample - loss: 0.1972 - accuracy: 0.9330 - val_loss: 0.8868 - val_accuracy: 0.6563\n",
      "Epoch 164/200\n",
      "3776/4118 [==========================>...] - ETA: 0s - loss: 0.1637 - accuracy: 0.9396\n",
      "Epoch 00164: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 65us/sample - loss: 0.1569 - accuracy: 0.9415 - val_loss: 0.0655 - val_accuracy: 0.9835\n",
      "Epoch 165/200\n",
      "3488/4118 [========================>.....] - ETA: 0s - loss: 0.0630 - accuracy: 0.9785\n",
      "Epoch 00165: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 68us/sample - loss: 0.0672 - accuracy: 0.9760 - val_loss: 0.2271 - val_accuracy: 0.9049\n",
      "Epoch 166/200\n",
      "3456/4118 [========================>.....] - ETA: 0s - loss: 0.0555 - accuracy: 0.9800\n",
      "Epoch 00166: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 69us/sample - loss: 0.0584 - accuracy: 0.9791 - val_loss: 0.1141 - val_accuracy: 0.9631\n",
      "Epoch 167/200\n",
      "3328/4118 [=======================>......] - ETA: 0s - loss: 0.1047 - accuracy: 0.9618\n",
      "Epoch 00167: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 77us/sample - loss: 0.1053 - accuracy: 0.9609 - val_loss: 0.3125 - val_accuracy: 0.8981\n",
      "Epoch 168/200\n",
      "3904/4118 [===========================>..] - ETA: 0s - loss: 0.0830 - accuracy: 0.9662\n",
      "Epoch 00168: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 76us/sample - loss: 0.0830 - accuracy: 0.9655 - val_loss: 0.0499 - val_accuracy: 0.9835\n",
      "Epoch 169/200\n",
      "3520/4118 [========================>.....] - ETA: 0s - loss: 0.0646 - accuracy: 0.9767\n",
      "Epoch 00169: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 69us/sample - loss: 0.0737 - accuracy: 0.9752 - val_loss: 0.1078 - val_accuracy: 0.9709\n",
      "Epoch 170/200\n",
      "3424/4118 [=======================>......] - ETA: 0s - loss: 0.0676 - accuracy: 0.9778\n",
      "Epoch 00170: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 69us/sample - loss: 0.0645 - accuracy: 0.9786 - val_loss: 0.1059 - val_accuracy: 0.9592\n",
      "Epoch 171/200\n",
      "3392/4118 [=======================>......] - ETA: 0s - loss: 0.0707 - accuracy: 0.9723\n",
      "Epoch 00171: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 73us/sample - loss: 0.0797 - accuracy: 0.9682 - val_loss: 0.1743 - val_accuracy: 0.9476\n",
      "Epoch 172/200\n",
      "3712/4118 [==========================>...] - ETA: 0s - loss: 0.1953 - accuracy: 0.9289\n",
      "Epoch 00172: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 85us/sample - loss: 0.1986 - accuracy: 0.9274 - val_loss: 0.2593 - val_accuracy: 0.9146\n",
      "Epoch 173/200\n",
      "3968/4118 [===========================>..] - ETA: 0s - loss: 0.3224 - accuracy: 0.9042\n",
      "Epoch 00173: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 103us/sample - loss: 0.3204 - accuracy: 0.9036 - val_loss: 0.2556 - val_accuracy: 0.9243\n",
      "Epoch 174/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3648/4118 [=========================>....] - ETA: 0s - loss: 0.1515 - accuracy: 0.9416\n",
      "Epoch 00174: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 117us/sample - loss: 0.1431 - accuracy: 0.9439 - val_loss: 0.0757 - val_accuracy: 0.9738\n",
      "Epoch 175/200\n",
      "4032/4118 [============================>.] - ETA: 0s - loss: 0.0562 - accuracy: 0.9834\n",
      "Epoch 00175: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 113us/sample - loss: 0.0557 - accuracy: 0.9837 - val_loss: 0.0655 - val_accuracy: 0.9757\n",
      "Epoch 176/200\n",
      "3616/4118 [=========================>....] - ETA: 0s - loss: 0.0515 - accuracy: 0.9815\n",
      "Epoch 00176: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 1s 140us/sample - loss: 0.0511 - accuracy: 0.9825 - val_loss: 0.1446 - val_accuracy: 0.9485\n",
      "Epoch 177/200\n",
      "3552/4118 [========================>.....] - ETA: 0s - loss: 0.0549 - accuracy: 0.9825\n",
      "Epoch 00177: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 94us/sample - loss: 0.0569 - accuracy: 0.9803 - val_loss: 0.0889 - val_accuracy: 0.9660\n",
      "Epoch 178/200\n",
      "3648/4118 [=========================>....] - ETA: 0s - loss: 0.1186 - accuracy: 0.9529\n",
      "Epoch 00178: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 99us/sample - loss: 0.1193 - accuracy: 0.9534 - val_loss: 0.2490 - val_accuracy: 0.8971\n",
      "Epoch 179/200\n",
      "3584/4118 [=========================>....] - ETA: 0s - loss: 0.1273 - accuracy: 0.9540\n",
      "Epoch 00179: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 79us/sample - loss: 0.1222 - accuracy: 0.9558 - val_loss: 0.0762 - val_accuracy: 0.9757\n",
      "Epoch 180/200\n",
      "3328/4118 [=======================>......] - ETA: 0s - loss: 0.0531 - accuracy: 0.9829\n",
      "Epoch 00180: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 69us/sample - loss: 0.0579 - accuracy: 0.9808 - val_loss: 0.0734 - val_accuracy: 0.9777\n",
      "Epoch 181/200\n",
      "4096/4118 [============================>.] - ETA: 0s - loss: 0.0955 - accuracy: 0.9656\n",
      "Epoch 00181: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 77us/sample - loss: 0.0969 - accuracy: 0.9648 - val_loss: 0.3006 - val_accuracy: 0.8796\n",
      "Epoch 182/200\n",
      "3392/4118 [=======================>......] - ETA: 0s - loss: 0.1152 - accuracy: 0.9575\n",
      "Epoch 00182: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 70us/sample - loss: 0.1180 - accuracy: 0.9548 - val_loss: 0.4389 - val_accuracy: 0.8019\n",
      "Epoch 183/200\n",
      "3456/4118 [========================>.....] - ETA: 0s - loss: 0.1353 - accuracy: 0.9479\n",
      "Epoch 00183: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 67us/sample - loss: 0.1258 - accuracy: 0.9526 - val_loss: 0.1068 - val_accuracy: 0.9660\n",
      "Epoch 184/200\n",
      "3392/4118 [=======================>......] - ETA: 0s - loss: 0.0635 - accuracy: 0.9761\n",
      "Epoch 00184: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 70us/sample - loss: 0.0635 - accuracy: 0.9747 - val_loss: 0.1059 - val_accuracy: 0.9738\n",
      "Epoch 185/200\n",
      "3520/4118 [========================>.....] - ETA: 0s - loss: 0.0651 - accuracy: 0.9759\n",
      "Epoch 00185: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 72us/sample - loss: 0.0657 - accuracy: 0.9755 - val_loss: 0.1830 - val_accuracy: 0.9330\n",
      "Epoch 186/200\n",
      "4096/4118 [============================>.] - ETA: 0s - loss: 0.0636 - accuracy: 0.9751\n",
      "Epoch 00186: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 77us/sample - loss: 0.0662 - accuracy: 0.9747 - val_loss: 0.2019 - val_accuracy: 0.9165\n",
      "Epoch 187/200\n",
      "3392/4118 [=======================>......] - ETA: 0s - loss: 0.0831 - accuracy: 0.9676\n",
      "Epoch 00187: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 73us/sample - loss: 0.0909 - accuracy: 0.9658 - val_loss: 0.0690 - val_accuracy: 0.9796\n",
      "Epoch 188/200\n",
      "3840/4118 [==========================>...] - ETA: 0s - loss: 0.1029 - accuracy: 0.9648\n",
      "Epoch 00188: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 97us/sample - loss: 0.1226 - accuracy: 0.9597 - val_loss: 0.3603 - val_accuracy: 0.8340\n",
      "Epoch 189/200\n",
      "4032/4118 [============================>.] - ETA: 0s - loss: 0.1622 - accuracy: 0.9373\n",
      "Epoch 00189: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 110us/sample - loss: 0.1616 - accuracy: 0.9376 - val_loss: 0.2846 - val_accuracy: 0.8806\n",
      "Epoch 190/200\n",
      "3808/4118 [==========================>...] - ETA: 0s - loss: 0.1382 - accuracy: 0.9519\n",
      "Epoch 00190: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 121us/sample - loss: 0.1323 - accuracy: 0.9534 - val_loss: 0.1074 - val_accuracy: 0.9641\n",
      "Epoch 191/200\n",
      "3904/4118 [===========================>..] - ETA: 0s - loss: 0.0554 - accuracy: 0.9793\n",
      "Epoch 00191: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 93us/sample - loss: 0.0548 - accuracy: 0.9798 - val_loss: 0.0625 - val_accuracy: 0.9825\n",
      "Epoch 192/200\n",
      "3552/4118 [========================>.....] - ETA: 0s - loss: 0.0428 - accuracy: 0.9842 ETA: 0s - loss: 0.0387 - accuracy\n",
      "Epoch 00192: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 84us/sample - loss: 0.0428 - accuracy: 0.9847 - val_loss: 0.1222 - val_accuracy: 0.9583\n",
      "Epoch 193/200\n",
      "4096/4118 [============================>.] - ETA: 0s - loss: 0.0341 - accuracy: 0.9880\n",
      "Epoch 00193: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 96us/sample - loss: 0.0342 - accuracy: 0.9881 - val_loss: 0.0561 - val_accuracy: 0.9893\n",
      "Epoch 194/200\n",
      "3584/4118 [=========================>....] - ETA: 0s - loss: 0.0350 - accuracy: 0.9891\n",
      "Epoch 00194: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 82us/sample - loss: 0.0376 - accuracy: 0.9876 - val_loss: 0.1347 - val_accuracy: 0.9544\n",
      "Epoch 195/200\n",
      "4096/4118 [============================>.] - ETA: 0s - loss: 0.1153 - accuracy: 0.9578\n",
      "Epoch 00195: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 89us/sample - loss: 0.1152 - accuracy: 0.9577 - val_loss: 0.0945 - val_accuracy: 0.9728\n",
      "Epoch 196/200\n",
      "4032/4118 [============================>.] - ETA: 0s - loss: 0.2589 - accuracy: 0.9144\n",
      "Epoch 00196: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 77us/sample - loss: 0.2605 - accuracy: 0.9140 - val_loss: 0.5287 - val_accuracy: 0.8097\n",
      "Epoch 197/200\n",
      "3360/4118 [=======================>......] - ETA: 0s - loss: 0.1025 - accuracy: 0.9622\n",
      "Epoch 00197: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 71us/sample - loss: 0.0933 - accuracy: 0.9655 - val_loss: 0.1944 - val_accuracy: 0.9262\n",
      "Epoch 198/200\n",
      "3360/4118 [=======================>......] - ETA: 0s - loss: 0.0789 - accuracy: 0.9735\n",
      "Epoch 00198: val_loss did not improve from 0.03695\n",
      "4118/4118 [==============================] - 0s 73us/sample - loss: 0.0764 - accuracy: 0.9745 - val_loss: 0.2955 - val_accuracy: 0.8883\n",
      "Epoch 199/200\n",
      "4032/4118 [============================>.] - ETA: 0s - loss: 0.0412 - accuracy: 0.9876\n",
      "Epoch 00199: val_loss improved from 0.03695 to 0.03632, saving model to best_model_2.h5\n",
      "4118/4118 [==============================] - 0s 81us/sample - loss: 0.0412 - accuracy: 0.9876 - val_loss: 0.0363 - val_accuracy: 0.9903\n",
      "Epoch 200/200\n",
      "3392/4118 [=======================>......] - ETA: 0s - loss: 0.0592 - accuracy: 0.9767\n",
      "Epoch 00200: val_loss did not improve from 0.03632\n",
      "4118/4118 [==============================] - 0s 73us/sample - loss: 0.0637 - accuracy: 0.9750 - val_loss: 0.0596 - val_accuracy: 0.9825\n"
     ]
    }
   ],
   "source": [
    "#Create a matrix to store the results where row represnets the model and column represents the result tested on fold i\n",
    "w, h = 4, 5\n",
    "matrix = [[0 for x in range(w)] for y in range(h)] \n",
    "\n",
    "# Running CV with Tensorflow\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
    "mc1 = ModelCheckpoint('best_model_1.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "for i in range(1,5):\n",
    "    print(\"Testing on Fold\", i)\n",
    "    x_train_df = pd.DataFrame()\n",
    "    y_train_linear_df = pd.DataFrame()\n",
    "    y_train_class_df = pd.DataFrame()\n",
    "    x_test_df = pd.DataFrame()\n",
    "    y_test_linear_df = pd.DataFrame()\n",
    "    y_test_class_df = pd.DataFrame()\n",
    "\n",
    "    # Getting train data set up\n",
    "    print(\"# Getting train data set up\")\n",
    "    for j in [x for x in all_list if x != i]:\n",
    "        x_train_df = x_train_df.append(folds_x[j])\n",
    "        y_train_linear_df = y_train_linear_df.append(folds_y_linear[j])\n",
    "        y_train_class_df = y_train_class_df.append(folds_y_class[j])\n",
    "\n",
    "    # Getting test data set up \n",
    "    print(\"# Getting test data set up\")\n",
    "    x_test_df = folds_x[i]\n",
    "    y_test_linear_df= folds_y_linear[i]\n",
    "    y_test_class_df = folds_y_class[i]\n",
    "    \n",
    "    # Regressor Model\n",
    "    model_linear.add(layers.Dense(121, activation='relu', input_shape=(100,)))\n",
    "    model_linear.add(layers.Dropout(rate=0.2))\n",
    "    model_linear.add(layers.Dense(146, activation='relu'))\n",
    "    model_linear.add(layers.Dropout(rate=0.3))\n",
    "    model_linear.add(layers.Dense(196, activation='relu'))\n",
    "    model_linear.add(layers.Dropout(rate=0.3))\n",
    "    model_linear.add(layers.Dense(256, activation='relu'))\n",
    "    model_linear.add(layers.Dropout(rate=0.3))\n",
    "    model_linear.add(layers.Dense(312, activation='relu'))\n",
    "    model_linear.add(layers.Dropout(rate=0.3))\n",
    "    model_linear.add(layers.Dense(256, activation='relu'))\n",
    "    model_linear.add(layers.Dropout(rate=0.3))\n",
    "    model_linear.add(layers.Dense(196, activation='relu'))\n",
    "    model_linear.add(layers.Dropout(rate=0.3))\n",
    "    model_linear.add(layers.Dense(146, activation='relu'))\n",
    "    model_linear.add(layers.Dropout(rate=0.3))\n",
    "    model_linear.add(layers.Dense(96, activation='relu'))\n",
    "    model_linear.add(layers.Dropout(rate=0.2))\n",
    "    model_linear.add(layers.Dense(46, activation='relu'))\n",
    "    model_linear.add(layers.Dropout(rate=0.2))\n",
    "    model_linear.add(layers.Dense(1))\n",
    "    model_linear.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "    model_linear.fit(x_train_df, y_train_linear_df, epochs=300, validation_split = 0.2, verbose=1, callbacks=[es,mc1])\n",
    "    model_linear = load_model('best_model_1.h5')\n",
    "    predictions = model_linear.predict(x_test_df)\n",
    "    matrix[i-1][0] = (mean_squared_error(y_test_linear_df, predictions))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Result</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLPRegressor</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>186.599299</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118.379617</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>121.663151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104.193848</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.519938</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MLPRegressor         \n",
       "0    186.599299  0  0  0\n",
       "1    118.379617  0  0  0\n",
       "2    121.663151  0  0  0\n",
       "3    104.193848  0  0  0\n",
       "4     11.519938  0  0  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "number_of_learners = 1\n",
    "score_df = pd.DataFrame(matrix, columns=[\"MLPRegressor\", \"\", \"\",\"\"])\n",
    "for i in range(number_of_learners):\n",
    "    score_df.iloc[4,i] = np.sqrt(score_df.iloc[:4,i].mean())\n",
    "display(score_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>4. Deployment Workflow</h1>"
   ]
  },
  {
   "attachments": {
    "Deployment%20Workflow.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Deployment%20Workflow.png](attachment:Deployment%20Workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, input the first 48 hours of the patient data into either of the models, M2R or M2C. As the patient data is passed into either of the models, it will then be preprocessed using Design Matrix 2 which both models used this to preprocess input data. Design Matrix 2 transformed the input patient data in such a way that every parameters is grouped into a 12-hour bin (total of 4 bins for the first 48 hours). If a parameter is measured multiple times within a 12-hour bin, a mean of those measurement will be recorded to represent the measurement for that 12-hour bin instead. The data will then be used with Tensorflow in the Model for both M2R and M2C. \n",
    "\n",
    "For mortality prediction, the true positives is that a patient is predicted to die and he/she actually dies and the true negatives is that a patient is predicted to live and he/she lives. In an actual scenario which concerns life and death, this boils down to the morality for the prediction. Some patients prefer to know the 'truth', in this case, they want the actual result to be very close to the predicted one or in fact exactly the same as what was predicted and so sensitivity is very important to them. As the higher the sensitivity, the higher the chance the predicted result becomes true. They would rather be told that they are going to die and they actually die than being told that they will live, giving them false hope, but in the end die. A common myth of admission into ICU is that patients hardly have the chance of exiting alive. Thus, when one admits into the ICU, he/she is more concerned about whether he/she will die. To achieve the highest possible tpr (sensitivity) on the AUC curve, the point to choose on the curve will be as far to the right as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
