{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>IS4242 Group Project</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Import necessary libraries</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, confusion_matrix, accuracy_score, mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.pipeline import Pipeline as imPipeline\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "Ensure that you are in the root folder of all the fold folders and target files\n",
    "read_text(fold_name):\n",
    "    fold_name: this is the name of the fold you want to read ALL patient files of. It will be read into a 2 dimensional\n",
    "    list. If you would like to retrieve just the first patient instead, you will need to change the line \n",
    "    \"txt_all.extend(txt[1:])\" to \"txt_all.append(txt[1:])\" and you will be to use \"read_text(fold1.txt)[0]\" to retrieve\n",
    "    the relevant patient's data\n",
    "read_ans(file_name):\n",
    "    file_name: this is the name of the file you want to read ALL targets of. It will be read into a 2 dimensional\n",
    "    list. To retrieve the first patient's target: read_ans(ans.csv)[0]\n",
    "put_single_into_dataframe(txt): This functions takes in 2 dimensional list ie the output of read_text(fold1.txt) \n",
    "put_multiple_into_dataframe(txt): Multiple is for using it with the output of read_text after you wanted to change it to append\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(fold_name):\n",
    "    txt_all = list()\n",
    "    for f in os.listdir(fold_name): # for each file in the directory\n",
    "        with open(os.path.join(fold_name, f), 'r') as fp: # open each file\n",
    "            txt = fp.readlines() # read inside the file\n",
    "        recordid = txt[1].rstrip('\\n').split(',')[-1] # get recordid\n",
    "        txt = [[int(recordid)] + t.rstrip('\\n').split(',') for t in txt] # preface each row with the recordid as all patients are 1 file\n",
    "        txt_all.extend(txt[1:]) # skip the parameter list\n",
    "    return txt_all\n",
    "\n",
    "def read_ans(file_name):\n",
    "    txt_all = list()\n",
    "    with open(file_name, 'r') as fp: # opens the csv file\n",
    "        txt = fp.readlines() \n",
    "    for i in range(1, len(txt)): # similar to above read_text\n",
    "        record_id, length_of_stay, hospital_death = txt[i].rstrip('\\n').split(',')\n",
    "        txt_all.append([record_id, length_of_stay, hospital_death])\n",
    "    return txt_all\n",
    "\n",
    "def put_multiple_into_dataframe(txt_all):\n",
    "    df = pd.DataFrame()\n",
    "    for i in txt_all:\n",
    "        df2 = pd.DataFrame(i, columns=['recordid', 'time', 'parameter', 'value'])\n",
    "        df = df.append(df2, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def put_single_into_dataframe(txt_all):\n",
    "    df = pd.DataFrame(txt_all, columns=['recordid', 'time', 'parameter', 'value'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recordid</th>\n",
       "      <th>time</th>\n",
       "      <th>parameter</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>132539</td>\n",
       "      <td>00:00</td>\n",
       "      <td>RecordID</td>\n",
       "      <td>132539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>132539</td>\n",
       "      <td>00:00</td>\n",
       "      <td>Age</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>132539</td>\n",
       "      <td>00:00</td>\n",
       "      <td>Gender</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>132539</td>\n",
       "      <td>00:00</td>\n",
       "      <td>Height</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>132539</td>\n",
       "      <td>00:00</td>\n",
       "      <td>ICUType</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   recordid   time parameter   value\n",
       "0    132539  00:00  RecordID  132539\n",
       "1    132539  00:00       Age      54\n",
       "2    132539  00:00    Gender       0\n",
       "3    132539  00:00    Height      -1\n",
       "4    132539  00:00   ICUType       4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading features\n",
    "fold1 = put_single_into_dataframe(read_text(\"./Project_Data/Fold1\"))\n",
    "fold2 = put_single_into_dataframe(read_text(\"./Project_Data/Fold2\"))\n",
    "fold3 = put_single_into_dataframe(read_text(\"./Project_Data/Fold3\"))\n",
    "fold4 = put_single_into_dataframe(read_text(\"./Project_Data/Fold4\"))\n",
    "df_feat = fold1.copy()\n",
    "df_feat = df_feat.append(fold2)\n",
    "df_feat = df_feat.append(fold3)\n",
    "df_feat = df_feat.append(fold4)\n",
    "df_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recordid</th>\n",
       "      <th>days_in_hospital</th>\n",
       "      <th>mortality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>132539</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>132540</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>132541</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>132543</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>132545</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  recordid days_in_hospital mortality\n",
       "0   132539                5         0\n",
       "1   132540                8         0\n",
       "2   132541               19         0\n",
       "3   132543                9         0\n",
       "4   132545                4         0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading Target\n",
    "df_target = pd.DataFrame(read_ans('./Project_Data/Fold1_Outcomes.csv'), columns=['recordid', 'days_in_hospital', 'mortality'])\n",
    "df_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of record ids: 4000\n",
      "HR             57.13450\n",
      "MAP            36.44000\n",
      "SysABP         36.41250\n",
      "DiasABP        36.39175\n",
      "Urine          34.22950\n",
      "Weight         32.29125\n",
      "NISysABP       24.58275\n",
      "NIDiasABP      24.55250\n",
      "NIMAP          24.21775\n",
      "Temp           21.60125\n",
      "GCS            15.39075\n",
      "RespRate       13.76275\n",
      "FiO2            8.09750\n",
      "MechVent        7.78600\n",
      "pH              6.08875\n",
      "PaCO2           5.82325\n",
      "PaO2            5.81700\n",
      "HCT             4.56775\n",
      "K               3.61000\n",
      "Platelets       3.52600\n",
      "Creatinine      3.49575\n",
      "BUN             3.47900\n",
      "HCO3            3.40325\n",
      "Mg              3.39750\n",
      "Na              3.39250\n",
      "Glucose         3.25525\n",
      "WBC             3.22750\n",
      "SaO2            2.04625\n",
      "Lactate         2.00600\n",
      "Height          1.00000\n",
      "ICUType         1.00000\n",
      "RecordID        1.00000\n",
      "Gender          1.00000\n",
      "Age             1.00000\n",
      "Bilirubin       0.79775\n",
      "AST             0.79550\n",
      "ALT             0.79425\n",
      "ALP             0.77300\n",
      "Albumin         0.58900\n",
      "TroponinT       0.53150\n",
      "TroponinI       0.10875\n",
      "Cholesterol     0.07875\n",
      "Name: parameter, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bin_feat = ['MechVent']\n",
    "num_feat = ['Albumin', 'ALP', 'ALT', 'AST', 'Bilirubin', 'BUN', 'Cholesterol',\n",
    "           'Creatinine', 'DiasABP', 'FiO2', 'GCS', 'Glucose', 'HCO3', 'HCT',\n",
    "           'HR', 'K', 'Lactate', 'Mg', 'MAP', 'NA', 'NIDiasABP', 'NIMAP',\n",
    "           'NISysABP', 'PaCO2', 'PaO2', 'pH', 'Platelets', 'RespRate', 'SaO2',\n",
    "           'SysABP', 'Temp', 'Tropl', 'TropT', 'Urine', 'WBC', 'Weight']\n",
    "\n",
    "print(\"Number of record ids:\", len(df_feat['recordid'].unique()))\n",
    "unique_count = df_feat['parameter'].value_counts()/4000\n",
    "print(unique_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Analysis of Features</h2>\n",
    "<p>The data above shows the average number of times a variable observed per patient. Based on the data above and the feature description we classify the features into these categories:\n",
    "<ul>\n",
    "    <li>General Descriptors (static data) that are collected when the patient is admitted to the ICU. Weight is not included as weight are measured multiple times as a time series data. Each of the descriptors will be included as a feature into the model.</li>\n",
    "    <li>Rare features: measured on average less than one time per patient (less than 1.0). We use the <u>existence</u> of these measurements for each patient as a feature.</li>\n",
    "    <li>Features that measured often or more that one time per patient (more than 1.0). Calculate the hourly average of each measurements and put them into 48 columns. <i>Example, average HR on the first hour to HR_1, average HR on the second hour to HR_2, and so on.</i></li>\n",
    "</ul>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rare features ['Bilirubin', 'AST', 'ALT', 'ALP', 'Albumin', 'TroponinT', 'TroponinI', 'Cholesterol']\n",
      "Normal features ['HR', 'MAP', 'SysABP', 'DiasABP', 'Urine', 'Weight', 'NISysABP', 'NIDiasABP', 'NIMAP', 'Temp', 'GCS', 'RespRate', 'FiO2', 'MechVent', 'pH', 'PaCO2', 'PaO2', 'HCT', 'K', 'Platelets', 'Creatinine', 'BUN', 'HCO3', 'Mg', 'Na', 'Glucose', 'WBC', 'SaO2', 'Lactate']\n"
     ]
    }
   ],
   "source": [
    "stat_feat = ['Age', 'Gender', 'Height', 'ICUType', 'RecordID'] #General Descriptors\n",
    "rare_feat = []\n",
    "nor_feat = []\n",
    "for index, value in unique_count.items():\n",
    "    if value < 1.0:\n",
    "        rare_feat.append(index)\n",
    "    elif index not in stat_feat:\n",
    "        nor_feat.append(index)\n",
    "print(\"Rare features\", rare_feat)\n",
    "print(\"Normal features\", nor_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'Gender', 'Height', 'ICUType', 'RecordID', 'Bilirubin', 'AST', 'ALT', 'ALP', 'Albumin', 'TroponinT', 'TroponinI', 'Cholesterol', 'HR_1', 'HR_2', 'HR_3', 'HR_4', 'HR_5', 'HR_6', 'HR_7', 'HR_8', 'HR_9', 'HR_10', 'HR_11', 'HR_12', 'HR_13', 'HR_14', 'HR_15', 'HR_16', 'HR_17', 'HR_18', 'HR_19', 'HR_20', 'HR_21', 'HR_22', 'HR_23', 'HR_24', 'HR_25', 'HR_26', 'HR_27', 'HR_28', 'HR_29', 'HR_30', 'HR_31', 'HR_32', 'HR_33', 'HR_34', 'HR_35', 'HR_36', 'HR_37', 'HR_38', 'HR_39', 'HR_40', 'HR_41', 'HR_42', 'HR_43', 'HR_44', 'HR_45', 'HR_46', 'HR_47', 'HR_48', 'MAP_1', 'MAP_2', 'MAP_3', 'MAP_4', 'MAP_5', 'MAP_6', 'MAP_7', 'MAP_8', 'MAP_9', 'MAP_10', 'MAP_11', 'MAP_12', 'MAP_13', 'MAP_14', 'MAP_15', 'MAP_16', 'MAP_17', 'MAP_18', 'MAP_19', 'MAP_20', 'MAP_21', 'MAP_22', 'MAP_23', 'MAP_24', 'MAP_25', 'MAP_26', 'MAP_27', 'MAP_28', 'MAP_29', 'MAP_30', 'MAP_31', 'MAP_32', 'MAP_33', 'MAP_34', 'MAP_35', 'MAP_36', 'MAP_37', 'MAP_38', 'MAP_39', 'MAP_40', 'MAP_41', 'MAP_42', 'MAP_43', 'MAP_44', 'MAP_45', 'MAP_46', 'MAP_47', 'MAP_48', 'SysABP_1', 'SysABP_2', 'SysABP_3', 'SysABP_4', 'SysABP_5', 'SysABP_6', 'SysABP_7', 'SysABP_8', 'SysABP_9', 'SysABP_10', 'SysABP_11', 'SysABP_12', 'SysABP_13', 'SysABP_14', 'SysABP_15', 'SysABP_16', 'SysABP_17', 'SysABP_18', 'SysABP_19', 'SysABP_20', 'SysABP_21', 'SysABP_22', 'SysABP_23', 'SysABP_24', 'SysABP_25', 'SysABP_26', 'SysABP_27', 'SysABP_28', 'SysABP_29', 'SysABP_30', 'SysABP_31', 'SysABP_32', 'SysABP_33', 'SysABP_34', 'SysABP_35', 'SysABP_36', 'SysABP_37', 'SysABP_38', 'SysABP_39', 'SysABP_40', 'SysABP_41', 'SysABP_42', 'SysABP_43', 'SysABP_44', 'SysABP_45', 'SysABP_46', 'SysABP_47', 'SysABP_48', 'DiasABP_1', 'DiasABP_2', 'DiasABP_3', 'DiasABP_4', 'DiasABP_5', 'DiasABP_6', 'DiasABP_7', 'DiasABP_8', 'DiasABP_9', 'DiasABP_10', 'DiasABP_11', 'DiasABP_12', 'DiasABP_13', 'DiasABP_14', 'DiasABP_15', 'DiasABP_16', 'DiasABP_17', 'DiasABP_18', 'DiasABP_19', 'DiasABP_20', 'DiasABP_21', 'DiasABP_22', 'DiasABP_23', 'DiasABP_24', 'DiasABP_25', 'DiasABP_26', 'DiasABP_27', 'DiasABP_28', 'DiasABP_29', 'DiasABP_30', 'DiasABP_31', 'DiasABP_32', 'DiasABP_33', 'DiasABP_34', 'DiasABP_35', 'DiasABP_36', 'DiasABP_37', 'DiasABP_38', 'DiasABP_39', 'DiasABP_40', 'DiasABP_41', 'DiasABP_42', 'DiasABP_43', 'DiasABP_44', 'DiasABP_45', 'DiasABP_46', 'DiasABP_47', 'DiasABP_48', 'Urine_1', 'Urine_2', 'Urine_3', 'Urine_4', 'Urine_5', 'Urine_6', 'Urine_7', 'Urine_8', 'Urine_9', 'Urine_10', 'Urine_11', 'Urine_12', 'Urine_13', 'Urine_14', 'Urine_15', 'Urine_16', 'Urine_17', 'Urine_18', 'Urine_19', 'Urine_20', 'Urine_21', 'Urine_22', 'Urine_23', 'Urine_24', 'Urine_25', 'Urine_26', 'Urine_27', 'Urine_28', 'Urine_29', 'Urine_30', 'Urine_31', 'Urine_32', 'Urine_33', 'Urine_34', 'Urine_35', 'Urine_36', 'Urine_37', 'Urine_38', 'Urine_39', 'Urine_40', 'Urine_41', 'Urine_42', 'Urine_43', 'Urine_44', 'Urine_45', 'Urine_46', 'Urine_47', 'Urine_48', 'Weight_1', 'Weight_2', 'Weight_3', 'Weight_4', 'Weight_5', 'Weight_6', 'Weight_7', 'Weight_8', 'Weight_9', 'Weight_10', 'Weight_11', 'Weight_12', 'Weight_13', 'Weight_14', 'Weight_15', 'Weight_16', 'Weight_17', 'Weight_18', 'Weight_19', 'Weight_20', 'Weight_21', 'Weight_22', 'Weight_23', 'Weight_24', 'Weight_25', 'Weight_26', 'Weight_27', 'Weight_28', 'Weight_29', 'Weight_30', 'Weight_31', 'Weight_32', 'Weight_33', 'Weight_34', 'Weight_35', 'Weight_36', 'Weight_37', 'Weight_38', 'Weight_39', 'Weight_40', 'Weight_41', 'Weight_42', 'Weight_43', 'Weight_44', 'Weight_45', 'Weight_46', 'Weight_47', 'Weight_48', 'NISysABP_1', 'NISysABP_2', 'NISysABP_3', 'NISysABP_4', 'NISysABP_5', 'NISysABP_6', 'NISysABP_7', 'NISysABP_8', 'NISysABP_9', 'NISysABP_10', 'NISysABP_11', 'NISysABP_12', 'NISysABP_13', 'NISysABP_14', 'NISysABP_15', 'NISysABP_16', 'NISysABP_17', 'NISysABP_18', 'NISysABP_19', 'NISysABP_20', 'NISysABP_21', 'NISysABP_22', 'NISysABP_23', 'NISysABP_24', 'NISysABP_25', 'NISysABP_26', 'NISysABP_27', 'NISysABP_28', 'NISysABP_29', 'NISysABP_30', 'NISysABP_31', 'NISysABP_32', 'NISysABP_33', 'NISysABP_34', 'NISysABP_35', 'NISysABP_36', 'NISysABP_37', 'NISysABP_38', 'NISysABP_39', 'NISysABP_40', 'NISysABP_41', 'NISysABP_42', 'NISysABP_43', 'NISysABP_44', 'NISysABP_45', 'NISysABP_46', 'NISysABP_47', 'NISysABP_48', 'NIDiasABP_1', 'NIDiasABP_2', 'NIDiasABP_3', 'NIDiasABP_4', 'NIDiasABP_5', 'NIDiasABP_6', 'NIDiasABP_7', 'NIDiasABP_8', 'NIDiasABP_9', 'NIDiasABP_10', 'NIDiasABP_11', 'NIDiasABP_12', 'NIDiasABP_13', 'NIDiasABP_14', 'NIDiasABP_15', 'NIDiasABP_16', 'NIDiasABP_17', 'NIDiasABP_18', 'NIDiasABP_19', 'NIDiasABP_20', 'NIDiasABP_21', 'NIDiasABP_22', 'NIDiasABP_23', 'NIDiasABP_24', 'NIDiasABP_25', 'NIDiasABP_26', 'NIDiasABP_27', 'NIDiasABP_28', 'NIDiasABP_29', 'NIDiasABP_30', 'NIDiasABP_31', 'NIDiasABP_32', 'NIDiasABP_33', 'NIDiasABP_34', 'NIDiasABP_35', 'NIDiasABP_36', 'NIDiasABP_37', 'NIDiasABP_38', 'NIDiasABP_39', 'NIDiasABP_40', 'NIDiasABP_41', 'NIDiasABP_42', 'NIDiasABP_43', 'NIDiasABP_44', 'NIDiasABP_45', 'NIDiasABP_46', 'NIDiasABP_47', 'NIDiasABP_48', 'NIMAP_1', 'NIMAP_2', 'NIMAP_3', 'NIMAP_4', 'NIMAP_5', 'NIMAP_6', 'NIMAP_7', 'NIMAP_8', 'NIMAP_9', 'NIMAP_10', 'NIMAP_11', 'NIMAP_12', 'NIMAP_13', 'NIMAP_14', 'NIMAP_15', 'NIMAP_16', 'NIMAP_17', 'NIMAP_18', 'NIMAP_19', 'NIMAP_20', 'NIMAP_21', 'NIMAP_22', 'NIMAP_23', 'NIMAP_24', 'NIMAP_25', 'NIMAP_26', 'NIMAP_27', 'NIMAP_28', 'NIMAP_29', 'NIMAP_30', 'NIMAP_31', 'NIMAP_32', 'NIMAP_33', 'NIMAP_34', 'NIMAP_35', 'NIMAP_36', 'NIMAP_37', 'NIMAP_38', 'NIMAP_39', 'NIMAP_40', 'NIMAP_41', 'NIMAP_42', 'NIMAP_43', 'NIMAP_44', 'NIMAP_45', 'NIMAP_46', 'NIMAP_47', 'NIMAP_48', 'Temp_1', 'Temp_2', 'Temp_3', 'Temp_4', 'Temp_5', 'Temp_6', 'Temp_7', 'Temp_8', 'Temp_9', 'Temp_10', 'Temp_11', 'Temp_12', 'Temp_13', 'Temp_14', 'Temp_15', 'Temp_16', 'Temp_17', 'Temp_18', 'Temp_19', 'Temp_20', 'Temp_21', 'Temp_22', 'Temp_23', 'Temp_24', 'Temp_25', 'Temp_26', 'Temp_27', 'Temp_28', 'Temp_29', 'Temp_30', 'Temp_31', 'Temp_32', 'Temp_33', 'Temp_34', 'Temp_35', 'Temp_36', 'Temp_37', 'Temp_38', 'Temp_39', 'Temp_40', 'Temp_41', 'Temp_42', 'Temp_43', 'Temp_44', 'Temp_45', 'Temp_46', 'Temp_47', 'Temp_48', 'GCS_1', 'GCS_2', 'GCS_3', 'GCS_4', 'GCS_5', 'GCS_6', 'GCS_7', 'GCS_8', 'GCS_9', 'GCS_10', 'GCS_11', 'GCS_12', 'GCS_13', 'GCS_14', 'GCS_15', 'GCS_16', 'GCS_17', 'GCS_18', 'GCS_19', 'GCS_20', 'GCS_21', 'GCS_22', 'GCS_23', 'GCS_24', 'GCS_25', 'GCS_26', 'GCS_27', 'GCS_28', 'GCS_29', 'GCS_30', 'GCS_31', 'GCS_32', 'GCS_33', 'GCS_34', 'GCS_35', 'GCS_36', 'GCS_37', 'GCS_38', 'GCS_39', 'GCS_40', 'GCS_41', 'GCS_42', 'GCS_43', 'GCS_44', 'GCS_45', 'GCS_46', 'GCS_47', 'GCS_48', 'RespRate_1', 'RespRate_2', 'RespRate_3', 'RespRate_4', 'RespRate_5', 'RespRate_6', 'RespRate_7', 'RespRate_8', 'RespRate_9', 'RespRate_10', 'RespRate_11', 'RespRate_12', 'RespRate_13', 'RespRate_14', 'RespRate_15', 'RespRate_16', 'RespRate_17', 'RespRate_18', 'RespRate_19', 'RespRate_20', 'RespRate_21', 'RespRate_22', 'RespRate_23', 'RespRate_24', 'RespRate_25', 'RespRate_26', 'RespRate_27', 'RespRate_28', 'RespRate_29', 'RespRate_30', 'RespRate_31', 'RespRate_32', 'RespRate_33', 'RespRate_34', 'RespRate_35', 'RespRate_36', 'RespRate_37', 'RespRate_38', 'RespRate_39', 'RespRate_40', 'RespRate_41', 'RespRate_42', 'RespRate_43', 'RespRate_44', 'RespRate_45', 'RespRate_46', 'RespRate_47', 'RespRate_48', 'FiO2_1', 'FiO2_2', 'FiO2_3', 'FiO2_4', 'FiO2_5', 'FiO2_6', 'FiO2_7', 'FiO2_8', 'FiO2_9', 'FiO2_10', 'FiO2_11', 'FiO2_12', 'FiO2_13', 'FiO2_14', 'FiO2_15', 'FiO2_16', 'FiO2_17', 'FiO2_18', 'FiO2_19', 'FiO2_20', 'FiO2_21', 'FiO2_22', 'FiO2_23', 'FiO2_24', 'FiO2_25', 'FiO2_26', 'FiO2_27', 'FiO2_28', 'FiO2_29', 'FiO2_30', 'FiO2_31', 'FiO2_32', 'FiO2_33', 'FiO2_34', 'FiO2_35', 'FiO2_36', 'FiO2_37', 'FiO2_38', 'FiO2_39', 'FiO2_40', 'FiO2_41', 'FiO2_42', 'FiO2_43', 'FiO2_44', 'FiO2_45', 'FiO2_46', 'FiO2_47', 'FiO2_48', 'MechVent_1', 'MechVent_2', 'MechVent_3', 'MechVent_4', 'MechVent_5', 'MechVent_6', 'MechVent_7', 'MechVent_8', 'MechVent_9', 'MechVent_10', 'MechVent_11', 'MechVent_12', 'MechVent_13', 'MechVent_14', 'MechVent_15', 'MechVent_16', 'MechVent_17', 'MechVent_18', 'MechVent_19', 'MechVent_20', 'MechVent_21', 'MechVent_22', 'MechVent_23', 'MechVent_24', 'MechVent_25', 'MechVent_26', 'MechVent_27', 'MechVent_28', 'MechVent_29', 'MechVent_30', 'MechVent_31', 'MechVent_32', 'MechVent_33', 'MechVent_34', 'MechVent_35', 'MechVent_36', 'MechVent_37', 'MechVent_38', 'MechVent_39', 'MechVent_40', 'MechVent_41', 'MechVent_42', 'MechVent_43', 'MechVent_44', 'MechVent_45', 'MechVent_46', 'MechVent_47', 'MechVent_48', 'pH_1', 'pH_2', 'pH_3', 'pH_4', 'pH_5', 'pH_6', 'pH_7', 'pH_8', 'pH_9', 'pH_10', 'pH_11', 'pH_12', 'pH_13', 'pH_14', 'pH_15', 'pH_16', 'pH_17', 'pH_18', 'pH_19', 'pH_20', 'pH_21', 'pH_22', 'pH_23', 'pH_24', 'pH_25', 'pH_26', 'pH_27', 'pH_28', 'pH_29', 'pH_30', 'pH_31', 'pH_32', 'pH_33', 'pH_34', 'pH_35', 'pH_36', 'pH_37', 'pH_38', 'pH_39', 'pH_40', 'pH_41', 'pH_42', 'pH_43', 'pH_44', 'pH_45', 'pH_46', 'pH_47', 'pH_48', 'PaCO2_1', 'PaCO2_2', 'PaCO2_3', 'PaCO2_4', 'PaCO2_5', 'PaCO2_6', 'PaCO2_7', 'PaCO2_8', 'PaCO2_9', 'PaCO2_10', 'PaCO2_11', 'PaCO2_12', 'PaCO2_13', 'PaCO2_14', 'PaCO2_15', 'PaCO2_16', 'PaCO2_17', 'PaCO2_18', 'PaCO2_19', 'PaCO2_20', 'PaCO2_21', 'PaCO2_22', 'PaCO2_23', 'PaCO2_24', 'PaCO2_25', 'PaCO2_26', 'PaCO2_27', 'PaCO2_28', 'PaCO2_29', 'PaCO2_30', 'PaCO2_31', 'PaCO2_32', 'PaCO2_33', 'PaCO2_34', 'PaCO2_35', 'PaCO2_36', 'PaCO2_37', 'PaCO2_38', 'PaCO2_39', 'PaCO2_40', 'PaCO2_41', 'PaCO2_42', 'PaCO2_43', 'PaCO2_44', 'PaCO2_45', 'PaCO2_46', 'PaCO2_47', 'PaCO2_48', 'PaO2_1', 'PaO2_2', 'PaO2_3', 'PaO2_4', 'PaO2_5', 'PaO2_6', 'PaO2_7', 'PaO2_8', 'PaO2_9', 'PaO2_10', 'PaO2_11', 'PaO2_12', 'PaO2_13', 'PaO2_14', 'PaO2_15', 'PaO2_16', 'PaO2_17', 'PaO2_18', 'PaO2_19', 'PaO2_20', 'PaO2_21', 'PaO2_22', 'PaO2_23', 'PaO2_24', 'PaO2_25', 'PaO2_26', 'PaO2_27', 'PaO2_28', 'PaO2_29', 'PaO2_30', 'PaO2_31', 'PaO2_32', 'PaO2_33', 'PaO2_34', 'PaO2_35', 'PaO2_36', 'PaO2_37', 'PaO2_38', 'PaO2_39', 'PaO2_40', 'PaO2_41', 'PaO2_42', 'PaO2_43', 'PaO2_44', 'PaO2_45', 'PaO2_46', 'PaO2_47', 'PaO2_48', 'HCT_1', 'HCT_2', 'HCT_3', 'HCT_4', 'HCT_5', 'HCT_6', 'HCT_7', 'HCT_8', 'HCT_9', 'HCT_10', 'HCT_11', 'HCT_12', 'HCT_13', 'HCT_14', 'HCT_15', 'HCT_16', 'HCT_17', 'HCT_18', 'HCT_19', 'HCT_20', 'HCT_21', 'HCT_22', 'HCT_23', 'HCT_24', 'HCT_25', 'HCT_26', 'HCT_27', 'HCT_28', 'HCT_29', 'HCT_30', 'HCT_31', 'HCT_32', 'HCT_33', 'HCT_34', 'HCT_35', 'HCT_36', 'HCT_37', 'HCT_38', 'HCT_39', 'HCT_40', 'HCT_41', 'HCT_42', 'HCT_43', 'HCT_44', 'HCT_45', 'HCT_46', 'HCT_47', 'HCT_48', 'K_1', 'K_2', 'K_3', 'K_4', 'K_5', 'K_6', 'K_7', 'K_8', 'K_9', 'K_10', 'K_11', 'K_12', 'K_13', 'K_14', 'K_15', 'K_16', 'K_17', 'K_18', 'K_19', 'K_20', 'K_21', 'K_22', 'K_23', 'K_24', 'K_25', 'K_26', 'K_27', 'K_28', 'K_29', 'K_30', 'K_31', 'K_32', 'K_33', 'K_34', 'K_35', 'K_36', 'K_37', 'K_38', 'K_39', 'K_40', 'K_41', 'K_42', 'K_43', 'K_44', 'K_45', 'K_46', 'K_47', 'K_48', 'Platelets_1', 'Platelets_2', 'Platelets_3', 'Platelets_4', 'Platelets_5', 'Platelets_6', 'Platelets_7', 'Platelets_8', 'Platelets_9', 'Platelets_10', 'Platelets_11', 'Platelets_12', 'Platelets_13', 'Platelets_14', 'Platelets_15', 'Platelets_16', 'Platelets_17', 'Platelets_18', 'Platelets_19', 'Platelets_20', 'Platelets_21', 'Platelets_22', 'Platelets_23', 'Platelets_24', 'Platelets_25', 'Platelets_26', 'Platelets_27', 'Platelets_28', 'Platelets_29', 'Platelets_30', 'Platelets_31', 'Platelets_32', 'Platelets_33', 'Platelets_34', 'Platelets_35', 'Platelets_36', 'Platelets_37', 'Platelets_38', 'Platelets_39', 'Platelets_40', 'Platelets_41', 'Platelets_42', 'Platelets_43', 'Platelets_44', 'Platelets_45', 'Platelets_46', 'Platelets_47', 'Platelets_48', 'Creatinine_1', 'Creatinine_2', 'Creatinine_3', 'Creatinine_4', 'Creatinine_5', 'Creatinine_6', 'Creatinine_7', 'Creatinine_8', 'Creatinine_9', 'Creatinine_10', 'Creatinine_11', 'Creatinine_12', 'Creatinine_13', 'Creatinine_14', 'Creatinine_15', 'Creatinine_16', 'Creatinine_17', 'Creatinine_18', 'Creatinine_19', 'Creatinine_20', 'Creatinine_21', 'Creatinine_22', 'Creatinine_23', 'Creatinine_24', 'Creatinine_25', 'Creatinine_26', 'Creatinine_27', 'Creatinine_28', 'Creatinine_29', 'Creatinine_30', 'Creatinine_31', 'Creatinine_32', 'Creatinine_33', 'Creatinine_34', 'Creatinine_35', 'Creatinine_36', 'Creatinine_37', 'Creatinine_38', 'Creatinine_39', 'Creatinine_40', 'Creatinine_41', 'Creatinine_42', 'Creatinine_43', 'Creatinine_44', 'Creatinine_45', 'Creatinine_46', 'Creatinine_47', 'Creatinine_48', 'BUN_1', 'BUN_2', 'BUN_3', 'BUN_4', 'BUN_5', 'BUN_6', 'BUN_7', 'BUN_8', 'BUN_9', 'BUN_10', 'BUN_11', 'BUN_12', 'BUN_13', 'BUN_14', 'BUN_15', 'BUN_16', 'BUN_17', 'BUN_18', 'BUN_19', 'BUN_20', 'BUN_21', 'BUN_22', 'BUN_23', 'BUN_24', 'BUN_25', 'BUN_26', 'BUN_27', 'BUN_28', 'BUN_29', 'BUN_30', 'BUN_31', 'BUN_32', 'BUN_33', 'BUN_34', 'BUN_35', 'BUN_36', 'BUN_37', 'BUN_38', 'BUN_39', 'BUN_40', 'BUN_41', 'BUN_42', 'BUN_43', 'BUN_44', 'BUN_45', 'BUN_46', 'BUN_47', 'BUN_48', 'HCO3_1', 'HCO3_2', 'HCO3_3', 'HCO3_4', 'HCO3_5', 'HCO3_6', 'HCO3_7', 'HCO3_8', 'HCO3_9', 'HCO3_10', 'HCO3_11', 'HCO3_12', 'HCO3_13', 'HCO3_14', 'HCO3_15', 'HCO3_16', 'HCO3_17', 'HCO3_18', 'HCO3_19', 'HCO3_20', 'HCO3_21', 'HCO3_22', 'HCO3_23', 'HCO3_24', 'HCO3_25', 'HCO3_26', 'HCO3_27', 'HCO3_28', 'HCO3_29', 'HCO3_30', 'HCO3_31', 'HCO3_32', 'HCO3_33', 'HCO3_34', 'HCO3_35', 'HCO3_36', 'HCO3_37', 'HCO3_38', 'HCO3_39', 'HCO3_40', 'HCO3_41', 'HCO3_42', 'HCO3_43', 'HCO3_44', 'HCO3_45', 'HCO3_46', 'HCO3_47', 'HCO3_48', 'Mg_1', 'Mg_2', 'Mg_3', 'Mg_4', 'Mg_5', 'Mg_6', 'Mg_7', 'Mg_8', 'Mg_9', 'Mg_10', 'Mg_11', 'Mg_12', 'Mg_13', 'Mg_14', 'Mg_15', 'Mg_16', 'Mg_17', 'Mg_18', 'Mg_19', 'Mg_20', 'Mg_21', 'Mg_22', 'Mg_23', 'Mg_24', 'Mg_25', 'Mg_26', 'Mg_27', 'Mg_28', 'Mg_29', 'Mg_30', 'Mg_31', 'Mg_32', 'Mg_33', 'Mg_34', 'Mg_35', 'Mg_36', 'Mg_37', 'Mg_38', 'Mg_39', 'Mg_40', 'Mg_41', 'Mg_42', 'Mg_43', 'Mg_44', 'Mg_45', 'Mg_46', 'Mg_47', 'Mg_48', 'Na_1', 'Na_2', 'Na_3', 'Na_4', 'Na_5', 'Na_6', 'Na_7', 'Na_8', 'Na_9', 'Na_10', 'Na_11', 'Na_12', 'Na_13', 'Na_14', 'Na_15', 'Na_16', 'Na_17', 'Na_18', 'Na_19', 'Na_20', 'Na_21', 'Na_22', 'Na_23', 'Na_24', 'Na_25', 'Na_26', 'Na_27', 'Na_28', 'Na_29', 'Na_30', 'Na_31', 'Na_32', 'Na_33', 'Na_34', 'Na_35', 'Na_36', 'Na_37', 'Na_38', 'Na_39', 'Na_40', 'Na_41', 'Na_42', 'Na_43', 'Na_44', 'Na_45', 'Na_46', 'Na_47', 'Na_48', 'Glucose_1', 'Glucose_2', 'Glucose_3', 'Glucose_4', 'Glucose_5', 'Glucose_6', 'Glucose_7', 'Glucose_8', 'Glucose_9', 'Glucose_10', 'Glucose_11', 'Glucose_12', 'Glucose_13', 'Glucose_14', 'Glucose_15', 'Glucose_16', 'Glucose_17', 'Glucose_18', 'Glucose_19', 'Glucose_20', 'Glucose_21', 'Glucose_22', 'Glucose_23', 'Glucose_24', 'Glucose_25', 'Glucose_26', 'Glucose_27', 'Glucose_28', 'Glucose_29', 'Glucose_30', 'Glucose_31', 'Glucose_32', 'Glucose_33', 'Glucose_34', 'Glucose_35', 'Glucose_36', 'Glucose_37', 'Glucose_38', 'Glucose_39', 'Glucose_40', 'Glucose_41', 'Glucose_42', 'Glucose_43', 'Glucose_44', 'Glucose_45', 'Glucose_46', 'Glucose_47', 'Glucose_48', 'WBC_1', 'WBC_2', 'WBC_3', 'WBC_4', 'WBC_5', 'WBC_6', 'WBC_7', 'WBC_8', 'WBC_9', 'WBC_10', 'WBC_11', 'WBC_12', 'WBC_13', 'WBC_14', 'WBC_15', 'WBC_16', 'WBC_17', 'WBC_18', 'WBC_19', 'WBC_20', 'WBC_21', 'WBC_22', 'WBC_23', 'WBC_24', 'WBC_25', 'WBC_26', 'WBC_27', 'WBC_28', 'WBC_29', 'WBC_30', 'WBC_31', 'WBC_32', 'WBC_33', 'WBC_34', 'WBC_35', 'WBC_36', 'WBC_37', 'WBC_38', 'WBC_39', 'WBC_40', 'WBC_41', 'WBC_42', 'WBC_43', 'WBC_44', 'WBC_45', 'WBC_46', 'WBC_47', 'WBC_48', 'SaO2_1', 'SaO2_2', 'SaO2_3', 'SaO2_4', 'SaO2_5', 'SaO2_6', 'SaO2_7', 'SaO2_8', 'SaO2_9', 'SaO2_10', 'SaO2_11', 'SaO2_12', 'SaO2_13', 'SaO2_14', 'SaO2_15', 'SaO2_16', 'SaO2_17', 'SaO2_18', 'SaO2_19', 'SaO2_20', 'SaO2_21', 'SaO2_22', 'SaO2_23', 'SaO2_24', 'SaO2_25', 'SaO2_26', 'SaO2_27', 'SaO2_28', 'SaO2_29', 'SaO2_30', 'SaO2_31', 'SaO2_32', 'SaO2_33', 'SaO2_34', 'SaO2_35', 'SaO2_36', 'SaO2_37', 'SaO2_38', 'SaO2_39', 'SaO2_40', 'SaO2_41', 'SaO2_42', 'SaO2_43', 'SaO2_44', 'SaO2_45', 'SaO2_46', 'SaO2_47', 'SaO2_48', 'Lactate_1', 'Lactate_2', 'Lactate_3', 'Lactate_4', 'Lactate_5', 'Lactate_6', 'Lactate_7', 'Lactate_8', 'Lactate_9', 'Lactate_10', 'Lactate_11', 'Lactate_12', 'Lactate_13', 'Lactate_14', 'Lactate_15', 'Lactate_16', 'Lactate_17', 'Lactate_18', 'Lactate_19', 'Lactate_20', 'Lactate_21', 'Lactate_22', 'Lactate_23', 'Lactate_24', 'Lactate_25', 'Lactate_26', 'Lactate_27', 'Lactate_28', 'Lactate_29', 'Lactate_30', 'Lactate_31', 'Lactate_32', 'Lactate_33', 'Lactate_34', 'Lactate_35', 'Lactate_36', 'Lactate_37', 'Lactate_38', 'Lactate_39', 'Lactate_40', 'Lactate_41', 'Lactate_42', 'Lactate_43', 'Lactate_44', 'Lactate_45', 'Lactate_46', 'Lactate_47', 'Lactate_48']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height</th>\n",
       "      <th>ICUType</th>\n",
       "      <th>Bilirubin</th>\n",
       "      <th>AST</th>\n",
       "      <th>ALT</th>\n",
       "      <th>ALP</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>TroponinT</th>\n",
       "      <th>...</th>\n",
       "      <th>Lactate_39</th>\n",
       "      <th>Lactate_40</th>\n",
       "      <th>Lactate_41</th>\n",
       "      <th>Lactate_42</th>\n",
       "      <th>Lactate_43</th>\n",
       "      <th>Lactate_44</th>\n",
       "      <th>Lactate_45</th>\n",
       "      <th>Lactate_46</th>\n",
       "      <th>Lactate_47</th>\n",
       "      <th>Lactate_48</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RecordID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 1404 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Age, Gender, Height, ICUType, Bilirubin, AST, ALT, ALP, Albumin, TroponinT, TroponinI, Cholesterol, HR_1, HR_2, HR_3, HR_4, HR_5, HR_6, HR_7, HR_8, HR_9, HR_10, HR_11, HR_12, HR_13, HR_14, HR_15, HR_16, HR_17, HR_18, HR_19, HR_20, HR_21, HR_22, HR_23, HR_24, HR_25, HR_26, HR_27, HR_28, HR_29, HR_30, HR_31, HR_32, HR_33, HR_34, HR_35, HR_36, HR_37, HR_38, HR_39, HR_40, HR_41, HR_42, HR_43, HR_44, HR_45, HR_46, HR_47, HR_48, MAP_1, MAP_2, MAP_3, MAP_4, MAP_5, MAP_6, MAP_7, MAP_8, MAP_9, MAP_10, MAP_11, MAP_12, MAP_13, MAP_14, MAP_15, MAP_16, MAP_17, MAP_18, MAP_19, MAP_20, MAP_21, MAP_22, MAP_23, MAP_24, MAP_25, MAP_26, MAP_27, MAP_28, MAP_29, MAP_30, MAP_31, MAP_32, MAP_33, MAP_34, MAP_35, MAP_36, MAP_37, MAP_38, MAP_39, MAP_40, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 1404 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating dataframe\n",
    "columns = stat_feat.copy()\n",
    "columns.extend(rare_feat)\n",
    "for feat in nor_feat:\n",
    "    for hour in range(1,49):\n",
    "        columns.append(feat + '_' +  str(hour))\n",
    "print(columns)\n",
    "\n",
    "df = pd.DataFrame(columns=columns)\n",
    "df = df.set_index('RecordID')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_values = {} # the sum of the values\n",
    "count = {} # num of occurences of a measurement\n",
    "for index,row in df_feat.iterrows():\n",
    "    if row['parameter'] == 'RecordID' and index != 0:\n",
    "        # count the average of the previous record\n",
    "        for key,value in tot_values.items():\n",
    "            tot_values[key] = float(tot_values[key])/count[key]\n",
    "            \n",
    "        df = df.append(tot_values, ignore_index=True)\n",
    "        tot_values.clear()\n",
    "        count.clear()\n",
    "        for feat in rare_feat:\n",
    "            tot_values[feat] = 0;\n",
    "            count[feat] = 1;\n",
    "        \n",
    "    if row['parameter'] in stat_feat:\n",
    "        tot_values[row['parameter']] = row['value']\n",
    "        count[row['parameter']] = 1\n",
    "    elif row['parameter'] in rare_feat:\n",
    "        tot_values[row['parameter']] = 1\n",
    "        count[row['parameter']] = 1\n",
    "    elif row['parameter'] in nor_feat:\n",
    "        hour = int(row['time'][0:2]) + 1\n",
    "        if hour == 49: hour-=1\n",
    "        col = row['parameter'] + '_' + str(hour)\n",
    "        tot_values[col] = row['value']\n",
    "        if col in count:\n",
    "            count[col] = count[col] + 1\n",
    "        else:\n",
    "            count[col] = 1\n",
    "    \n",
    "# count the average of the previous record\n",
    "for key,value in tot_values.items():\n",
    "    tot_values[key] = float(tot_values[key])/count[key]\n",
    "            \n",
    "df = df.append(tot_values, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height</th>\n",
       "      <th>ICUType</th>\n",
       "      <th>Bilirubin</th>\n",
       "      <th>AST</th>\n",
       "      <th>ALT</th>\n",
       "      <th>ALP</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>TroponinT</th>\n",
       "      <th>...</th>\n",
       "      <th>Lactate_40</th>\n",
       "      <th>Lactate_41</th>\n",
       "      <th>Lactate_42</th>\n",
       "      <th>Lactate_43</th>\n",
       "      <th>Lactate_44</th>\n",
       "      <th>Lactate_45</th>\n",
       "      <th>Lactate_46</th>\n",
       "      <th>Lactate_47</th>\n",
       "      <th>Lactate_48</th>\n",
       "      <th>RecordID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>132539.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>175.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>132540.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>132541.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>180.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>132543.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>132545.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3992</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>142665.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3993</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>142667.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3994</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>142670.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3995</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>142671.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3996</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>157.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>142673.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3997 rows × 1405 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age  Gender  Height  ICUType  Bilirubin  AST  ALT  ALP  Albumin  \\\n",
       "0     54.0     0.0    -1.0      4.0        NaN  NaN  NaN  NaN      NaN   \n",
       "1     76.0     1.0   175.3      2.0        0.0  0.0  0.0  0.0      0.0   \n",
       "2     44.0     0.0    -1.0      3.0        1.0  1.0  1.0  1.0      1.0   \n",
       "3     68.0     1.0   180.3      3.0        1.0  1.0  1.0  1.0      1.0   \n",
       "4     88.0     0.0    -1.0      3.0        0.0  0.0  0.0  0.0      1.0   \n",
       "...    ...     ...     ...      ...        ...  ...  ...  ...      ...   \n",
       "3992  70.0     0.0    -1.0      4.0        0.0  0.0  0.0  0.0      0.0   \n",
       "3993  25.0     1.0    -1.0      3.0        0.0  0.0  0.0  0.0      0.0   \n",
       "3994  44.0     1.0    -1.0      3.0        1.0  1.0  1.0  1.0      0.0   \n",
       "3995  37.0     1.0    -1.0      3.0        1.0  1.0  1.0  1.0      1.0   \n",
       "3996  78.0     0.0   157.5      4.0        1.0  1.0  1.0  1.0      1.0   \n",
       "\n",
       "      TroponinT  ...  Lactate_40  Lactate_41  Lactate_42  Lactate_43  \\\n",
       "0           NaN  ...         NaN         NaN         NaN         NaN   \n",
       "1           0.0  ...         NaN         NaN         NaN         NaN   \n",
       "2           0.0  ...         NaN         NaN         NaN         NaN   \n",
       "3           0.0  ...         NaN         NaN         NaN         NaN   \n",
       "4           0.0  ...         NaN         NaN         NaN         NaN   \n",
       "...         ...  ...         ...         ...         ...         ...   \n",
       "3992        0.0  ...         NaN         NaN         NaN         NaN   \n",
       "3993        0.0  ...         NaN         NaN         NaN         NaN   \n",
       "3994        0.0  ...         NaN         NaN         NaN         NaN   \n",
       "3995        0.0  ...         NaN         NaN         NaN         NaN   \n",
       "3996        0.0  ...         NaN         NaN         1.8         NaN   \n",
       "\n",
       "      Lactate_44  Lactate_45  Lactate_46  Lactate_47  Lactate_48  RecordID  \n",
       "0            NaN         NaN         NaN         NaN         NaN  132539.0  \n",
       "1            NaN         NaN         NaN         NaN         NaN  132540.0  \n",
       "2            NaN         NaN         NaN         NaN         NaN  132541.0  \n",
       "3            NaN         NaN         NaN         NaN         NaN  132543.0  \n",
       "4            NaN         NaN         NaN         NaN         NaN  132545.0  \n",
       "...          ...         ...         ...         ...         ...       ...  \n",
       "3992         NaN         NaN         NaN         NaN         NaN  142665.0  \n",
       "3993         NaN         NaN         NaN         NaN         NaN  142667.0  \n",
       "3994         NaN         NaN         NaN         NaN         NaN  142670.0  \n",
       "3995         NaN         NaN         NaN         NaN         NaN  142671.0  \n",
       "3996         NaN         NaN         NaN         NaN         NaN  142673.0  \n",
       "\n",
       "[3997 rows x 1405 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_x_for_design_matrix_3(df_feat):\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    tot_values = {} # the sum of the values\n",
    "    count = {} # num of occurences of a measurement\n",
    "    for index,row in df_feat.iterrows():\n",
    "        if row['parameter'] == 'RecordID' and index != 0:\n",
    "            # count the average of the previous record\n",
    "            for key,value in tot_values.items():\n",
    "                tot_values[key] = float(tot_values[key])/count[key]\n",
    "\n",
    "            df = df.append(tot_values, ignore_index=True)\n",
    "            tot_values.clear()\n",
    "            count.clear()\n",
    "            for feat in rare_feat:\n",
    "                tot_values[feat] = 0;\n",
    "                count[feat] = 1;\n",
    "\n",
    "        if row['parameter'] in stat_feat:\n",
    "            tot_values[row['parameter']] = row['value']\n",
    "            count[row['parameter']] = 1\n",
    "        elif row['parameter'] in rare_feat:\n",
    "            tot_values[row['parameter']] = 1\n",
    "            count[row['parameter']] = 1\n",
    "        elif row['parameter'] in nor_feat:\n",
    "            hour = int(row['time'][0:2]) + 1\n",
    "            if hour == 49: hour-=1\n",
    "            col = row['parameter'] + '_' + str(hour)\n",
    "            tot_values[col] = row['value']\n",
    "            if col in count:\n",
    "                count[col] = count[col] + 1\n",
    "            else:\n",
    "                count[col] = 1\n",
    "\n",
    "    # count the average of the previous record\n",
    "    for key,value in tot_values.items():\n",
    "        tot_values[key] = float(tot_values[key])/count[key]\n",
    "\n",
    "    df = df.append(tot_values, ignore_index=True)\n",
    "    \n",
    "    df = df.astype({'RecordID': 'int32', 'ICUType':'int32'})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all folds\n",
    "\n",
    "x_fold = {}\n",
    "y_fold = {}\n",
    "for i in range (1,5):\n",
    "    string = \"./Project_Data/Fold\"+str(i)\n",
    "    y_file = \"./Project_Data/Fold\"+str(i)+\"_Outcomes.csv\"\n",
    "    x_fold[i] = preprocess_x_for_design_matrix_3(put_single_into_dataframe(read_text(string)))\n",
    "    y_fold[i] = pd.read_csv(y_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ALP  ALT  AST   Age  Albumin  BUN_1  BUN_10  BUN_11  BUN_12  BUN_13  \\\n",
      "RecordID                                                                        \n",
      "132539   -1.0 -1.0 -1.0  54.0     -1.0   -1.0    -1.0    13.0    -1.0    -1.0   \n",
      "132540    0.0  0.0  0.0  76.0      0.0   -1.0    -1.0    -1.0    -1.0    -1.0   \n",
      "132541    1.0  1.0  1.0  44.0      1.0   -1.0    -1.0    -1.0    -1.0    -1.0   \n",
      "132543    1.0  1.0  1.0  68.0      1.0   23.0    -1.0    -1.0    -1.0    -1.0   \n",
      "132545    0.0  0.0  0.0  88.0      1.0   -1.0    -1.0    -1.0    -1.0    -1.0   \n",
      "\n",
      "          ...  pH_44  pH_45  pH_46  pH_47  pH_48  pH_5  pH_6  pH_7  pH_8  pH_9  \n",
      "RecordID  ...                                                                   \n",
      "132539    ...   -1.0   -1.0  -1.00   -1.0   -1.0 -1.00  -1.0 -1.00  -1.0  -1.0  \n",
      "132540    ...   -1.0   -1.0   7.37   -1.0   -1.0  7.34  -1.0  7.36  -1.0  -1.0  \n",
      "132541    ...   -1.0   -1.0  -1.00   -1.0   -1.0 -1.00  -1.0 -1.00  -1.0  -1.0  \n",
      "132543    ...   -1.0   -1.0  -1.00   -1.0   -1.0 -1.00  -1.0 -1.00  -1.0  -1.0  \n",
      "132545    ...   -1.0   -1.0  -1.00   -1.0   -1.0 -1.00  -1.0 -1.00  -1.0  -1.0  \n",
      "\n",
      "[5 rows x 1404 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LogisticRegression --> SelectKBest: 350 and PCA: 60 has a score of 0.6691885547519351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LogisticRegression --> SelectKBest: 350 and PCA: 70 has a score of 0.6562618956985153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LogisticRegression --> SelectKBest: 350 and PCA: 150 has a score of 0.6957556147696993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LogisticRegression --> SelectKBest: 350 and PCA: 200 has a score of 0.7139639639639641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LogisticRegression --> SelectKBest: 400 and PCA: 60 has a score of 0.6643351097576449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LogisticRegression --> SelectKBest: 400 and PCA: 70 has a score of 0.6815124984139069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LogisticRegression --> SelectKBest: 400 and PCA: 150 has a score of 0.6901725669331302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LogisticRegression --> SelectKBest: 400 and PCA: 200 has a score of 0.691346275853318\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 60, RF Estimators: 100, and RF depth: 3 has a score of 0.6702353762212918\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 60, RF Estimators: 100, and RF depth: 4 has a score of 0.6787368354269763\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 60, RF Estimators: 100, and RF depth: 5 has a score of 0.637149473417079\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 60, RF Estimators: 125, and RF depth: 3 has a score of 0.6655088186778327\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 60, RF Estimators: 125, and RF depth: 4 has a score of 0.6769762720466946\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 60, RF Estimators: 125, and RF depth: 5 has a score of 0.6245241720593834\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 60, RF Estimators: 150, and RF depth: 3 has a score of 0.6270460601446517\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 60, RF Estimators: 150, and RF depth: 4 has a score of 0.6593389163811699\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 60, RF Estimators: 150, and RF depth: 5 has a score of 0.6710918665144017\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 70, RF Estimators: 100, and RF depth: 3 has a score of 0.6712346148965868\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 70, RF Estimators: 100, and RF depth: 4 has a score of 0.6706477604364928\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 70, RF Estimators: 100, and RF depth: 5 has a score of 0.6610836188300977\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 70, RF Estimators: 125, and RF depth: 3 has a score of 0.6345324197436873\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 70, RF Estimators: 125, and RF depth: 4 has a score of 0.6825593198832636\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 70, RF Estimators: 125, and RF depth: 5 has a score of 0.6412574546377363\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 70, RF Estimators: 150, and RF depth: 3 has a score of 0.6649219642177389\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 70, RF Estimators: 150, and RF depth: 4 has a score of 0.6650647125999238\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 70, RF Estimators: 150, and RF depth: 5 has a score of 0.6657943154422028\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 150, RF Estimators: 100, and RF depth: 3 has a score of 0.5904390305798758\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 150, RF Estimators: 100, and RF depth: 4 has a score of 0.6056972465423169\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 150, RF Estimators: 100, and RF depth: 5 has a score of 0.572643065600812\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 150, RF Estimators: 125, and RF depth: 3 has a score of 0.626871589899759\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 150, RF Estimators: 125, and RF depth: 4 has a score of 0.588963963963964\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 150, RF Estimators: 125, and RF depth: 5 has a score of 0.572500317218627\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 150, RF Estimators: 150, and RF depth: 3 has a score of 0.5504694835680751\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 150, RF Estimators: 150, and RF depth: 4 has a score of 0.5744036289810938\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 150, RF Estimators: 150, and RF depth: 5 has a score of 0.5921837330288034\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 200, RF Estimators: 100, and RF depth: 3 has a score of 0.535322294125111\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 200, RF Estimators: 100, and RF depth: 4 has a score of 0.5190172566933131\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 200, RF Estimators: 100, and RF depth: 5 has a score of 0.5397157721101383\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 200, RF Estimators: 125, and RF depth: 3 has a score of 0.5511832254790001\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 200, RF Estimators: 125, and RF depth: 4 has a score of 0.5591136911559447\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 200, RF Estimators: 125, and RF depth: 5 has a score of 0.5422059383326989\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 200, RF Estimators: 150, and RF depth: 3 has a score of 0.5505963710189062\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 200, RF Estimators: 150, and RF depth: 4 has a score of 0.5322294125111026\n",
      "Using Random Forest --> SelectKBest: 350, PCA: 200, RF Estimators: 150, and RF depth: 5 has a score of 0.5538161400837457\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 60, RF Estimators: 100, and RF depth: 3 has a score of 0.6439220911051896\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 60, RF Estimators: 100, and RF depth: 4 has a score of 0.6762308082730617\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 60, RF Estimators: 100, and RF depth: 5 has a score of 0.6868100494861058\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 60, RF Estimators: 125, and RF depth: 3 has a score of 0.664049612993275\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 60, RF Estimators: 125, and RF depth: 4 has a score of 0.6666666666666666\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 60, RF Estimators: 125, and RF depth: 5 has a score of 0.6825434589519096\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 60, RF Estimators: 150, and RF depth: 3 has a score of 0.6727255424438523\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 60, RF Estimators: 150, and RF depth: 4 has a score of 0.6558177896206066\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 60, RF Estimators: 150, and RF depth: 5 has a score of 0.6393541428752696\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 70, RF Estimators: 100, and RF depth: 3 has a score of 0.6703622636721229\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 70, RF Estimators: 100, and RF depth: 4 has a score of 0.6672852429894682\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 70, RF Estimators: 100, and RF depth: 5 has a score of 0.6490610328638498\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 70, RF Estimators: 125, and RF depth: 3 has a score of 0.6681575942139323\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 70, RF Estimators: 125, and RF depth: 4 has a score of 0.6412733155690903\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 70, RF Estimators: 125, and RF depth: 5 has a score of 0.611137545996701\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 70, RF Estimators: 150, and RF depth: 3 has a score of 0.6429069914985408\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 70, RF Estimators: 150, and RF depth: 4 has a score of 0.658466565156706\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 70, RF Estimators: 150, and RF depth: 5 has a score of 0.6662384215201116\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 150, RF Estimators: 100, and RF depth: 3 has a score of 0.5983694962568201\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 150, RF Estimators: 100, and RF depth: 4 has a score of 0.5796853191219389\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 150, RF Estimators: 100, and RF depth: 5 has a score of 0.6131677452099986\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 150, RF Estimators: 125, and RF depth: 3 has a score of 0.6283307955843167\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 150, RF Estimators: 125, and RF depth: 4 has a score of 0.5910100241086157\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 150, RF Estimators: 125, and RF depth: 5 has a score of 0.611264433447532\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 150, RF Estimators: 150, and RF depth: 3 has a score of 0.6102651947722371\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 150, RF Estimators: 150, and RF depth: 4 has a score of 0.5824768430402234\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 150, RF Estimators: 150, and RF depth: 5 has a score of 0.5874730364166983\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 200, RF Estimators: 100, and RF depth: 3 has a score of 0.5672027661464282\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 200, RF Estimators: 100, and RF depth: 4 has a score of 0.5495654104809035\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 200, RF Estimators: 100, and RF depth: 5 has a score of 0.5269318614389037\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 200, RF Estimators: 125, and RF depth: 3 has a score of 0.5231569597766781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Random Forest --> SelectKBest: 400, PCA: 200, RF Estimators: 125, and RF depth: 4 has a score of 0.5310557035909148\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 200, RF Estimators: 125, and RF depth: 5 has a score of 0.5264877553609948\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 200, RF Estimators: 150, and RF depth: 3 has a score of 0.5564807765511991\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 200, RF Estimators: 150, and RF depth: 4 has a score of 0.552055576703464\n",
      "Using Random Forest --> SelectKBest: 400, PCA: 200, RF Estimators: 150, and RF depth: 5 has a score of 0.5558780611597514\n",
      "Using KNeighbors --> SelectKBest: 350, PCA: 60, and K_Neighbours: 3 has a score of 0.6263005963710189\n",
      "Using KNeighbors --> SelectKBest: 350, PCA: 60, and K_Neighbours: 5 has a score of 0.6208761578479889\n",
      "Using KNeighbors --> SelectKBest: 350, PCA: 60, and K_Neighbours: 7 has a score of 0.6145476462377871\n",
      "Using KNeighbors --> SelectKBest: 350, PCA: 70, and K_Neighbours: 3 has a score of 0.631582286511864\n",
      "Using KNeighbors --> SelectKBest: 350, PCA: 70, and K_Neighbours: 5 has a score of 0.6047138687983759\n",
      "Using KNeighbors --> SelectKBest: 350, PCA: 70, and K_Neighbours: 7 has a score of 0.6443820581144525\n",
      "Using KNeighbors --> SelectKBest: 350, PCA: 150, and K_Neighbours: 3 has a score of 0.6224781119147316\n",
      "Using KNeighbors --> SelectKBest: 350, PCA: 150, and K_Neighbours: 5 has a score of 0.6210189062301739\n",
      "Using KNeighbors --> SelectKBest: 350, PCA: 150, and K_Neighbours: 7 has a score of 0.6094087044791271\n",
      "Using KNeighbors --> SelectKBest: 350, PCA: 200, and K_Neighbours: 3 has a score of 0.6349606648902424\n",
      "Using KNeighbors --> SelectKBest: 350, PCA: 200, and K_Neighbours: 5 has a score of 0.6119147316330414\n",
      "Using KNeighbors --> SelectKBest: 350, PCA: 200, and K_Neighbours: 7 has a score of 0.631725034894049\n",
      "Using KNeighbors --> SelectKBest: 400, PCA: 60, and K_Neighbours: 3 has a score of 0.5788605506915367\n",
      "Using KNeighbors --> SelectKBest: 400, PCA: 60, and K_Neighbours: 5 has a score of 0.6405437127268113\n",
      "Using KNeighbors --> SelectKBest: 400, PCA: 60, and K_Neighbours: 7 has a score of 0.6451116609567313\n",
      "Using KNeighbors --> SelectKBest: 400, PCA: 70, and K_Neighbours: 3 has a score of 0.6229063570612867\n",
      "Using KNeighbors --> SelectKBest: 400, PCA: 70, and K_Neighbours: 5 has a score of 0.6546440807004188\n",
      "Using KNeighbors --> SelectKBest: 400, PCA: 70, and K_Neighbours: 7 has a score of 0.6411305671869053\n",
      "Using KNeighbors --> SelectKBest: 400, PCA: 150, and K_Neighbours: 3 has a score of 0.6190997335363533\n",
      "Using KNeighbors --> SelectKBest: 400, PCA: 150, and K_Neighbours: 5 has a score of 0.6248096688237533\n",
      "Using KNeighbors --> SelectKBest: 400, PCA: 150, and K_Neighbours: 7 has a score of 0.6180529120669966\n",
      "Using KNeighbors --> SelectKBest: 400, PCA: 200, and K_Neighbours: 3 has a score of 0.6124857251617815\n",
      "Using KNeighbors --> SelectKBest: 400, PCA: 200, and K_Neighbours: 5 has a score of 0.6076322801674916\n",
      "Using KNeighbors --> SelectKBest: 400, PCA: 200, and K_Neighbours: 7 has a score of 0.631439538129679\n",
      "Using Decision Tree --> SelectKBest: 350, PCA: 60, and DT Min_samples_split: 2 has a score of 0.620416190838726\n",
      "Using Decision Tree --> SelectKBest: 350, PCA: 60, and DT Min_samples_split: 4 has a score of 0.5973702575815252\n",
      "Using Decision Tree --> SelectKBest: 350, PCA: 60, and DT Min_samples_split: 8 has a score of 0.5599067377236392\n",
      "Using Decision Tree --> SelectKBest: 350, PCA: 70, and DT Min_samples_split: 2 has a score of 0.5621114071818297\n",
      "Using Decision Tree --> SelectKBest: 350, PCA: 70, and DT Min_samples_split: 4 has a score of 0.5972116482679863\n",
      "Using Decision Tree --> SelectKBest: 350, PCA: 70, and DT Min_samples_split: 8 has a score of 0.5706445882502221\n",
      "Using Decision Tree --> SelectKBest: 350, PCA: 150, and DT Min_samples_split: 2 has a score of 0.54918474812841\n",
      "Using Decision Tree --> SelectKBest: 350, PCA: 150, and DT Min_samples_split: 4 has a score of 0.5828257835300088\n",
      "Using Decision Tree --> SelectKBest: 350, PCA: 150, and DT Min_samples_split: 8 has a score of 0.5353698769191726\n",
      "Using Decision Tree --> SelectKBest: 350, PCA: 200, and DT Min_samples_split: 2 has a score of 0.5609376982616419\n",
      "Using Decision Tree --> SelectKBest: 350, PCA: 200, and DT Min_samples_split: 4 has a score of 0.5393192488262911\n",
      "Using Decision Tree --> SelectKBest: 350, PCA: 200, and DT Min_samples_split: 8 has a score of 0.5186365943408197\n",
      "Using Decision Tree --> SelectKBest: 400, PCA: 60, and DT Min_samples_split: 2 has a score of 0.5735629996193377\n",
      "Using Decision Tree --> SelectKBest: 400, PCA: 60, and DT Min_samples_split: 4 has a score of 0.6070612866387515\n",
      "Using Decision Tree --> SelectKBest: 400, PCA: 60, and DT Min_samples_split: 8 has a score of 0.5707714757010531\n",
      "Using Decision Tree --> SelectKBest: 400, PCA: 70, and DT Min_samples_split: 2 has a score of 0.5929609186651441\n",
      "Using Decision Tree --> SelectKBest: 400, PCA: 70, and DT Min_samples_split: 4 has a score of 0.5386213678467199\n",
      "Using Decision Tree --> SelectKBest: 400, PCA: 70, and DT Min_samples_split: 8 has a score of 0.5829685319121939\n",
      "Using Decision Tree --> SelectKBest: 400, PCA: 150, and DT Min_samples_split: 2 has a score of 0.6083618830097703\n",
      "Using Decision Tree --> SelectKBest: 400, PCA: 150, and DT Min_samples_split: 4 has a score of 0.5405088186778327\n",
      "Using Decision Tree --> SelectKBest: 400, PCA: 150, and DT Min_samples_split: 8 has a score of 0.5518335236645096\n",
      "Using Decision Tree --> SelectKBest: 400, PCA: 200, and DT Min_samples_split: 2 has a score of 0.5296440807004188\n",
      "Using Decision Tree --> SelectKBest: 400, PCA: 200, and DT Min_samples_split: 4 has a score of 0.593960157340439\n",
      "Using Decision Tree --> SelectKBest: 400, PCA: 200, and DT Min_samples_split: 8 has a score of 0.5625396523283848\n",
      "Using Gaussian Process --> SelectKBest: 350 and PCA: 60 has a score of 0.5980839994924502\n",
      "Using Gaussian Process --> SelectKBest: 350 and PCA: 70 has a score of 0.6055703590914858\n",
      "Using Gaussian Process --> SelectKBest: 350 and PCA: 150 has a score of 0.5839994924501967\n",
      "Using Gaussian Process --> SelectKBest: 350 and PCA: 200 has a score of 0.6158641035401597\n",
      "Using Gaussian Process --> SelectKBest: 400 and PCA: 60 has a score of 0.5986867148838979\n",
      "Using Gaussian Process --> SelectKBest: 400 and PCA: 70 has a score of 0.6189411242228143\n",
      "Using Gaussian Process --> SelectKBest: 400 and PCA: 150 has a score of 0.5955779723385356\n",
      "Using Gaussian Process --> SelectKBest: 400 and PCA: 200 has a score of 0.5795742926024616\n",
      "Using MLP classifier --> SelectKBest: 350, PCA: 60, and Hidden Layer: (45, 45) has a score of 0.6171329780484709\n",
      "Using MLP classifier --> SelectKBest: 350, PCA: 60, and Hidden Layer: (100,) has a score of 0.6125808907499048\n",
      "Using MLP classifier --> SelectKBest: 350, PCA: 60, and Hidden Layer: (30, 30, 30) has a score of 0.5876157847988834\n",
      "Using MLP classifier --> SelectKBest: 350, PCA: 70, and Hidden Layer: (45, 45) has a score of 0.6200672503489405\n",
      "Using MLP classifier --> SelectKBest: 350, PCA: 70, and Hidden Layer: (100,) has a score of 0.5961172440045679\n",
      "Using MLP classifier --> SelectKBest: 350, PCA: 70, and Hidden Layer: (30, 30, 30) has a score of 0.6111058241339932\n",
      "Using MLP classifier --> SelectKBest: 350, PCA: 150, and Hidden Layer: (45, 45) has a score of 0.6290128156325339\n",
      "Using MLP classifier --> SelectKBest: 350, PCA: 150, and Hidden Layer: (100,) has a score of 0.6046345641416064\n",
      "Using MLP classifier --> SelectKBest: 350, PCA: 150, and Hidden Layer: (30, 30, 30) has a score of 0.5389703083365055\n",
      "Using MLP classifier --> SelectKBest: 350, PCA: 200, and Hidden Layer: (45, 45) has a score of 0.6182908260373049\n",
      "Using MLP classifier --> SelectKBest: 350, PCA: 200, and Hidden Layer: (100,) has a score of 0.6090280421266336\n",
      "Using MLP classifier --> SelectKBest: 350, PCA: 200, and Hidden Layer: (30, 30, 30) has a score of 0.6094721482045425\n",
      "Using MLP classifier --> SelectKBest: 400, PCA: 60, and Hidden Layer: (45, 45) has a score of 0.6235883771095039\n",
      "Using MLP classifier --> SelectKBest: 400, PCA: 60, and Hidden Layer: (100,) has a score of 0.6086156579114326\n",
      "Using MLP classifier --> SelectKBest: 400, PCA: 60, and Hidden Layer: (30, 30, 30) has a score of 0.605395888846593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP classifier --> SelectKBest: 400, PCA: 70, and Hidden Layer: (45, 45) has a score of 0.6046504250729603\n",
      "Using MLP classifier --> SelectKBest: 400, PCA: 70, and Hidden Layer: (100,) has a score of 0.6134532419743688\n",
      "Using MLP classifier --> SelectKBest: 400, PCA: 70, and Hidden Layer: (30, 30, 30) has a score of 0.5942297931734551\n",
      "Using MLP classifier --> SelectKBest: 400, PCA: 150, and Hidden Layer: (45, 45) has a score of 0.6050945311508692\n",
      "Using MLP classifier --> SelectKBest: 400, PCA: 150, and Hidden Layer: (100,) has a score of 0.6099162542824514\n",
      "Using MLP classifier --> SelectKBest: 400, PCA: 150, and Hidden Layer: (30, 30, 30) has a score of 0.5877585331810684\n",
      "Using MLP classifier --> SelectKBest: 400, PCA: 200, and Hidden Layer: (45, 45) has a score of 0.5941980713107474\n",
      "Using MLP classifier --> SelectKBest: 400, PCA: 200, and Hidden Layer: (100,) has a score of 0.5990515163050375\n",
      "Using MLP classifier --> SelectKBest: 400, PCA: 200, and Hidden Layer: (30, 30, 30) has a score of 0.5924375079304657\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'scaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-5da406faf1b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mnN_components\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"PCA\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             est = Pipeline(steps=[\n\u001b[1;32m--> 170\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[1;34m'scaler'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[1;34m'f_selecter'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVarianceThreshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthreshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnThreshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[1;34m'dim_reducer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnN_components\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scaler' is not defined"
     ]
    }
   ],
   "source": [
    "#For MODEL 3\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "num_feat = []\n",
    "for feat in nor_feat:\n",
    "    for hour in range(1,49):\n",
    "        num_feat.append(feat + '_' +  str(hour))\n",
    "cat_feat = [\"ICUType\"]\n",
    "\n",
    "cat_transformer = Pipeline(steps=[('imputer', SimpleImputer(-1, strategy='most_frequent')),\n",
    "                                 ('OneHotEncoder', OneHotEncoder(handle_unknown='ignore', sparse=False))])\n",
    "\n",
    "stat_feat_cp = stat_feat.copy()\n",
    "stat_feat_cp.remove(\"RecordID\")\n",
    "stat_feat_cp.remove(\"ICUType\")\n",
    "prepro = ColumnTransformer(\n",
    "    remainder = 'passthrough',\n",
    "    transformers=[\n",
    "        ('cat', cat_transformer, cat_feat),\n",
    "    ('num', SimpleImputer(-1, strategy='mean'), num_feat),\n",
    "    ('stat', SimpleImputer(-1, strategy='mean'), stat_feat_cp)])\n",
    "\n",
    "parameters = {'VarianceThreshold':[0,0.5], 'PCA':[60,70, 150, 200],\n",
    "              'SelectKBest':[350,400],\n",
    "              'RFDepth':[3,4,5],\n",
    "              'RFEst': [100,125,150],\n",
    "              'KNeighbours':[3,5,7],\n",
    "              'DT_min_samples_split':[2,4,8],\n",
    "              'MLPClassifier_hiddenLayer':[(45,45,), (100,), (30,30,30)],\n",
    "              'DecisionTreeRegressor_min_samples_leaf':[1,3],\n",
    "              'MLPRegressor_hiddenLayer':[(45,45,), (100,), (30,30,30)]\n",
    "             }\n",
    "    \n",
    "all_list = [1,2,3,4]\n",
    "for i in range(2,5):\n",
    "    print(\"Testing on Fold\", i)\n",
    "    x_train_df = pd.DataFrame()\n",
    "    y_train_df = pd.DataFrame()\n",
    "    \n",
    "    # Getting train data set up\n",
    "    for j in [x for x in all_list if x != i]: \n",
    "#         string = \"./Project_Data/Fold\"+str(j)\n",
    "#         y_file = \"./Project_Data/Fold\"+str(j)+\"_Outcomes.csv\"\n",
    "#         x_train_df = x_train_df.append(put_single_into_dataframe(read_text(string)))\n",
    "#         y_train_df = y_train_df.append(pd.read_csv(y_file))\n",
    "        x_train_df = x_train_df.append(x_fold[j])\n",
    "        y_train_df = y_train_df.append(y_fold[j])\n",
    "#     x_train_df = preprocess_x_for_design_matrix_3(x_train_df)\n",
    "    y_train_df = y_train_df.drop(['Length_of_stay'], axis=1)\n",
    "    \n",
    "    # Replace -1 with NaN\n",
    "#     x_train_df = x_train_df.replace(-1, np.nan)\n",
    "    # Replace not known length of stay to 2\n",
    "    y_train_df = y_train_df.replace(-1, 2)\n",
    "    \n",
    "    train_df = x_train_df.merge(y_train_df, on=\"RecordID\", how='outer')\n",
    "    train_df = train_df.set_index(\"RecordID\")\n",
    "    train_df = train_df.replace(np.nan, -1)\n",
    "    X_train = train_df.loc[:, train_df.columns != 'In-hospital_death']\n",
    "    Y_train = train_df['In-hospital_death']\n",
    "    \n",
    "        \n",
    "    x_test_df = x_train_df.iloc[0:0]\n",
    "    y_test_df = y_train_df.iloc[0:0]\n",
    "    # Getting test data set up\n",
    "    x_test_df = x_test_df.append(x_fold[i])\n",
    "    y_test_df = y_test_df.append(y_fold[i])\n",
    "    y_test_df = y_test_df.drop(['Length_of_stay'], axis=1)\n",
    "     # Replace -1 with NaN\n",
    "#     x_test_df = x_test_df.replace(-1, np.nan)\n",
    "    # Replace not known length of stay to 2\n",
    "    y_test_df = y_test_df.replace(-1, 2)\n",
    "    \n",
    "    test_df = x_test_df.merge(y_test_df, on=\"RecordID\", how='outer')\n",
    "    test_df = test_df.set_index(\"RecordID\")\n",
    "    test_df = test_df.replace(np.nan, -1)\n",
    "    X_test = test_df.loc[:, test_df.columns != 'In-hospital_death']\n",
    "    Y_test = test_df['In-hospital_death']\n",
    "    \n",
    "    print(X_train.head())\n",
    "    \n",
    "    # Logistic Regression\n",
    "    for k_val in parameters['SelectKBest']:\n",
    "        for nN_components in parameters[\"PCA\"]:\n",
    "            model = imPipeline(steps=[('imputer', prepro),\n",
    "                                      ('smote', SMOTE()),\n",
    "                                      ('scaler', StandardScaler()),\n",
    "                                    ('f_selecter', SelectKBest(k = k_val)),\n",
    "                                    ('dim_reducer', PCA(n_components = nN_components)),\n",
    "                                   ('lr', LogisticRegression())])\n",
    "            model = model.fit(X_train, Y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            print ( \"Using LogisticRegression --> SelectKBest: {} and PCA: {} has a score of {}\".format(k_val, nN_components, roc_auc_score(Y_test, y_pred)) )\n",
    "            \n",
    "    # Random Forest\n",
    "    for k_val in parameters['SelectKBest']:\n",
    "        for nN_components in parameters[\"PCA\"]:\n",
    "            for est in parameters['RFEst']:\n",
    "                for depth in parameters['RFDepth']:\n",
    "                    model = imPipeline(steps=[('imputer', prepro),\n",
    "                                      ('smote', SMOTE()),\n",
    "                                      ('scaler', StandardScaler()),\n",
    "                                            ('f_selecter', SelectKBest(k = k_val)),\n",
    "                                            ('dim_reducer', PCA(n_components = nN_components)),\n",
    "                                           ('classifier', RandomForestClassifier(n_estimators=est, max_depth=depth))])\n",
    "                    model = model.fit(X_train, Y_train)\n",
    "                    y_pred = model.predict(X_test)\n",
    "                    print ( \"Using Random Forest --> SelectKBest: {}, PCA: {}, RF Estimators: {}, and RF depth: {} has a score of {}\".format(k_val, nN_components, est, depth, roc_auc_score(Y_test, y_pred)) )\n",
    "                    \n",
    "    # K-Neighbors\n",
    "    for k_val in parameters['SelectKBest']:\n",
    "        for nN_components in parameters[\"PCA\"]:\n",
    "            for k_neigh in parameters['KNeighbours']:\n",
    "                model = imPipeline(steps=[('imputer', prepro),\n",
    "                                      ('smote', SMOTE()),\n",
    "                                      ('scaler', StandardScaler()),\n",
    "                                        ('f_selecter', SelectKBest(k = k_val)),\n",
    "                                        ('dim_reducer', PCA(n_components = nN_components)),\n",
    "                                       ('classifier', KNeighborsClassifier(n_neighbors=k_neigh))])\n",
    "                model = model.fit(X_train, Y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                print ( \"Using KNeighbors --> SelectKBest: {}, PCA: {}, and K_Neighbours: {} has a score of {}\".format(k_val, nN_components, k_neigh, roc_auc_score(Y_test, y_pred)) )\n",
    "            \n",
    "    # Decision Tree\n",
    "    for k_val in parameters['SelectKBest']:\n",
    "        for nN_components in parameters[\"PCA\"]:\n",
    "            for min_sample in parameters['DT_min_samples_split']:\n",
    "                model = imPipeline(steps=[('imputer', prepro),\n",
    "                                      ('smote', SMOTE()),\n",
    "                                      ('scaler', StandardScaler()),\n",
    "                                        ('f_selecter', SelectKBest(k = k_val)),\n",
    "                                        ('dim_reducer', PCA(n_components = nN_components)),\n",
    "                                       ('classifier', DecisionTreeClassifier(min_samples_split = min_sample))])\n",
    "                model = model.fit(X_train, Y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                print ( \"Using Decision Tree --> SelectKBest: {}, PCA: {}, and DT Min_samples_split: {} has a score of {}\".format(k_val, nN_components, min_sample, roc_auc_score(Y_test, y_pred)) )\n",
    "                \n",
    "    # Gaussian Process\n",
    "    for k_val in parameters['SelectKBest']:\n",
    "        for nN_components in parameters[\"PCA\"]:\n",
    "            model = imPipeline(steps=[('imputer', prepro),\n",
    "                                      ('smote', SMOTE()),\n",
    "                                      ('scaler', StandardScaler()),\n",
    "                                        ('f_selecter', SelectKBest(k = k_val)),\n",
    "                                        ('dim_reducer', PCA(n_components = nN_components)),\n",
    "                                       ('classifier', GaussianProcessClassifier())])\n",
    "            model = model.fit(X_train, Y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            print ( \"Using Gaussian Process --> SelectKBest: {} and PCA: {} has a score of {}\".format(k_val, nN_components, roc_auc_score(Y_test, y_pred)) )\n",
    "            \n",
    "    # MLP Classifier\n",
    "    for k_val in parameters['SelectKBest']:\n",
    "        for nN_components in parameters[\"PCA\"]:\n",
    "            for hiddenLayer in parameters['MLPClassifier_hiddenLayer']:\n",
    "                model = imPipeline(steps=[('imputer', prepro),\n",
    "                                      ('smote', SMOTE()),\n",
    "                                      ('scaler', StandardScaler()),\n",
    "                                        ('f_selecter', SelectKBest(k = k_val)),\n",
    "                                        ('dim_reducer', PCA(n_components = nN_components)),\n",
    "                                       ('classifier', MLPClassifier(hidden_layer_sizes = hiddenLayer, learning_rate_init = 0.01))])\n",
    "                model = model.fit(X_train, Y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                print ( \"Using MLP classifier --> SelectKBest: {}, PCA: {}, and Hidden Layer: {} has a score of {}\".format(k_val, nN_components, hiddenLayer, roc_auc_score(Y_test, y_pred)) )\n",
    "                \n",
    "    # Creating regression model and parameters to try out\n",
    "    for nThreshold in parameters[\"VarianceThreshold\"]:\n",
    "        for nN_components in parameters[\"PCA\"]:\n",
    "            est = Pipeline(steps=[\n",
    "            ('scaler', scaler),\n",
    "            ('f_selecter', VarianceThreshold(threshold = nThreshold)),\n",
    "            ('dim_reducer', PCA(n_components = nN_components)),\n",
    "            ('classifier', LinearRegression())])\n",
    "            est.fit(x_train_df, y_linear_train_df)\n",
    "            prediction = est.predict(x_test_df)\n",
    "            print ( \"Using LinearRegression --> VarianceThreshold: {} and PCA: {} has a score of {}\".format(nThreshold, nN_components, mean_squared_error(y_linear_test_df, prediction)) )\n",
    "    for nThreshold in parameters[\"SelectKBest\"]:\n",
    "        for nN_components in parameters[\"PCA\"]:\n",
    "            for min_samples_leaf in parameters[\"DecisionTreeRegressor_min_samples_leaf\"]:\n",
    "                est = Pipeline(steps=[\n",
    "                ('scaler', scaler),\n",
    "                ('f_selecter', SelectKBest(k = 90)),\n",
    "                ('dim_reducer', PCA(n_components = nN_components)),\n",
    "                ('classifier', DecisionTreeRegressor(min_samples_leaf = min_samples_leaf))])\n",
    "                est.fit(x_train_df, y_linear_train_df)\n",
    "                prediction = est.predict(x_test_df)\n",
    "                print ( \"Using DecisionTreeRegressor({}) --> SelectKBest: 90 and PCA: {} has a score of {}\".format(min_samples_leaf, nN_components, mean_squared_error(y_linear_test_df, prediction)) )\n",
    "    for nThreshold in parameters[\"VarianceThreshold\"]:\n",
    "        for nN_components in parameters[\"PCA\"]:\n",
    "            for hiddenLayer in parameters[\"MLPRegressor_hiddenLayer\"]:\n",
    "                est = Pipeline(steps=[\n",
    "                ('scaler', scaler),\n",
    "                ('f_selecter', VarianceThreshold(threshold = nThreshold)),\n",
    "                ('dim_reducer', PCA(n_components = nN_components)),\n",
    "                ('classifier', MLPRegressor(hidden_layer_sizes = hiddenLayer, learning_rate_init = 0.01))])\n",
    "                est.fit(x_train_df, y_linear_train_df)\n",
    "                prediction = est.predict(x_test_df)\n",
    "                print ( \"Using MLPRegressor({}) --> SelectKBest: 90 and PCA: {} has a score of {}\".format(hiddenLayer, nN_components, mean_squared_error(y_linear_test_df, prediction)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALP</th>\n",
       "      <th>ALT</th>\n",
       "      <th>AST</th>\n",
       "      <th>Age</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>BUN_1</th>\n",
       "      <th>BUN_10</th>\n",
       "      <th>BUN_11</th>\n",
       "      <th>BUN_12</th>\n",
       "      <th>BUN_13</th>\n",
       "      <th>...</th>\n",
       "      <th>pH_44</th>\n",
       "      <th>pH_45</th>\n",
       "      <th>pH_46</th>\n",
       "      <th>pH_47</th>\n",
       "      <th>pH_48</th>\n",
       "      <th>pH_5</th>\n",
       "      <th>pH_6</th>\n",
       "      <th>pH_7</th>\n",
       "      <th>pH_8</th>\n",
       "      <th>pH_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>997</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>998</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7.34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.30</td>\n",
       "      <td>7.26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1405 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ALP  ALT  AST   Age  Albumin  BUN_1  BUN_10  BUN_11  BUN_12  BUN_13  ...  \\\n",
       "995  0.0  0.0  0.0  70.0      0.0    NaN     NaN     NaN     NaN     NaN  ...   \n",
       "996  0.0  0.0  0.0  25.0      0.0    NaN     NaN     NaN     NaN     NaN  ...   \n",
       "997  1.0  1.0  1.0  44.0      0.0    NaN    10.0     NaN     NaN     NaN  ...   \n",
       "998  1.0  1.0  1.0  37.0      1.0   65.0     NaN     NaN     NaN     NaN  ...   \n",
       "999  1.0  1.0  1.0  78.0      1.0    NaN     NaN    17.0     NaN     NaN  ...   \n",
       "\n",
       "     pH_44  pH_45  pH_46  pH_47  pH_48  pH_5  pH_6  pH_7  pH_8  pH_9  \n",
       "995    NaN    NaN    NaN    NaN    NaN  7.37   NaN   NaN   NaN   NaN  \n",
       "996    NaN    NaN    NaN    NaN    NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "997    NaN    NaN   7.41    NaN    NaN  7.39   NaN   NaN   NaN   NaN  \n",
       "998    NaN    NaN    NaN    NaN    NaN  7.45   NaN   NaN   NaN   NaN  \n",
       "999   7.34    NaN   7.31    NaN    NaN  7.30  7.26   NaN  3.67   3.6  \n",
       "\n",
       "[5 rows x 1405 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
